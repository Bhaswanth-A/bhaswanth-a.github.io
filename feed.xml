<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://bhaswanth-a.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://bhaswanth-a.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2026-02-20T20:41:37-05:00</updated><id>https://bhaswanth-a.github.io//feed.xml</id><title type="html">Bhaswanth Ayapilla</title><subtitle>Robotics engineer and researcher focused on learning-based autonomy, perception, and 3D vision, with work spanning mobile robots, space robotics, and foundation models for robotics.</subtitle><entry><title type="html">Advanced Deep Reinforcement Learning</title><link href="https://bhaswanth-a.github.io//posts/advanced-deep-rl/" rel="alternate" type="text/html" title="Advanced Deep Reinforcement Learning" /><published>2026-02-14T11:00:00-05:00</published><updated>2026-02-14T11:00:00-05:00</updated><id>https://bhaswanth-a.github.io//posts/advanced-deep-rl</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/advanced-deep-rl/"><![CDATA[<p><em>In Progress</em></p>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="learning" /><category term="rl" /><summary type="html"><![CDATA[In Progress]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning</title><link href="https://bhaswanth-a.github.io//posts/deep-rl/" rel="alternate" type="text/html" title="Deep Reinforcement Learning" /><published>2026-02-09T11:00:00-05:00</published><updated>2026-02-20T20:40:50-05:00</updated><id>https://bhaswanth-a.github.io//posts/deep-rl</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/deep-rl/"><![CDATA[<h1 id="finite-markov-decision-process">Finite Markov Decision Process</h1>

<p>A Finite Markov Decision Process (MDP) is a mathematical framework used to model sequential decision-making problems where an agent interacts with an environment over time. It is defined as a tuple $(S, A, p, r, \gamma)$, where each component captures one aspect of this interaction.</p>

<ul>
  <li>The state space $S$ is a finite set of all possible situations the agent can be in.</li>
  <li>The action space $A$ is a finite set of choices available to the agent.</li>
  <li>The transition function $p$ describes how the environment evolves: given the current state and action, it specifies the probability of transitioning to the next state.</li>
  <li>The reward function $r$ assigns a scalar feedback signal that tells the agent how desirable a state‚Äìaction pair is.</li>
  <li>The discount factor $\gamma \in [0,1]$ controls how much the agent values future rewards relative to immediate ones.</li>
</ul>

<h2 id="agent-policy-model-and-planning">Agent, Policy, Model, and Planning</h2>

<p>An agent is the decision-making entity. It perceives the environment through sensors, acts through actuators, and has goals it wants to achieve. A policy is the agent‚Äôs behavior: it specifies what action to take in each state. Formally, a policy is a probability distribution over actions given states.</p>

\[\pi(a \mid s) = \Pr(A_t = a \mid S_t = s), \forall t\]

<p>A model describes how the environment works ‚Äî it predicts the next state and reward given the current state and action. Planning means using this model to look ahead into the future and choose actions that achieve a goal. A plan is simply a sequence of actions. In reinforcement learning, the agent often does not know the model in advance and must learn through interaction.</p>

<p>The goal of reinforcement learning is to learn a policy. Policies are typically assumed to be stationary, meaning the same policy is used at every time step. During learning, the agent updates its policy based on experience.</p>

<h2 id="markovian-states-why-history-can-be-thrown-away">Markovian States (Why History Can Be Thrown Away)</h2>

<p>A state is called Markovian if it contains all the information needed to predict the future. This is known as the Markov property: given the current state and action, the future is independent of the past. Intuitively, this means the state is a sufficient summary of everything that has happened so far.</p>

\[\Pr(R_{t+1}, S_{t+1} \mid S_0, A_0, \dots, S_t, A_t)=\Pr(R_{t+1}, S_{t+1} \mid S_t, A_t)\]

<h2 id="rewards-and-returns">Rewards and Returns</h2>

<p>Rewards are scalar signals provided by the environment that indicate progress toward a goal. Importantly, rewards define what the agent should accomplish, not how to do it. Almost all goal-directed behavior can be framed as maximizing the expected cumulative reward.</p>

<p>The reward function is often written as an expectation:</p>

\[r(s, a) = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]\]

<p>In episodic tasks, interaction naturally breaks into episodes with a clear end, such as a game or navigating a maze. The return is the total reward accumulated until the terminal state:</p>

\[G_t = R_{t+1} + R_{t+2} + \dots + R_T\]

<p>Here, $T$ is the final time step of the episode. The agent‚Äôs objective is to maximize this return.</p>

<p>In continuing tasks, there is no natural episode boundary (e.g., robot control or stock trading). In this case, we use discounted returns to ensure the total reward remains finite and to prioritize near-term outcomes:</p>

\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}\]

<p>The discount factor $\gamma$ determines how farsighted the agent is.</p>

<h1 id="value-based-vs-policy-based-reinforcement-learning">Value-Based vs Policy-Based Reinforcement Learning</h1>

<p>Value-based methods focus on learning a value function: a function that tells us how good a state or state-action pair is in terms of expected future reward. Once this value function is learned, the policy is not learned explicitly; instead, it is implicit, typically derived by choosing the action with the highest value (for example, using $\epsilon-$greedy exploration). Algorithms like Q-learning fall into this category.</p>

<p>Policy-based methods flip this idea around. Instead of learning values and deriving actions from them, the agent directly learns the policy itself, which is a mapping from states to actions or action distributions. There is no requirement to maintain or estimate a value function. The policy parameters are adjusted so that actions sampled from the policy lead to higher long-term reward. This makes policy-based methods especially natural for continuous action spaces, where choosing an action by ‚Äúargmax over values‚Äù is awkward or ill-defined.</p>

<p>Actor-Critic methods sit exactly at the intersection of these two ideas. They learn both a policy (the actor) and a value function (the critic). The critic evaluates how good the actor‚Äôs actions are, and this feedback is used to update the policy more efficiently and with lower variance.</p>

<h1 id="policy-based-rl">Policy-Based RL</h1>

<p>Policy-based reinforcement learning can be cleanly framed as a direct optimization problem. We define an objective function $U(\theta)$, which measures how good a policy parameterized by $\theta$ is. This is typically the expected cumulative reward when following that policy.</p>

<p>Learning then proceeds by iteratively improving the policy parameters. At each iteration, we first roll out the current policy in the environment to collect trajectories (sequences of states and actions generated by following the policy). Using these trajectories, we estimate how changing the policy parameters would affect the objective. Finally, we update the parameters in the direction that increases the objective. Intuitively, this is just like standard gradient ascent.</p>

\[\theta_{\text{new}} = \theta_{\text{old}} + \alpha \nabla_\theta U(\theta)\]

<p><strong>Why are they useful?</strong></p>

<p>One major advantage of policy-based methods is that they handle high-dimensional and continuous action spaces naturally. Instead of evaluating many possible actions and selecting the best one, the policy directly outputs an action (or a distribution over actions). This is especially important in robotics, where actions are often continuous (e.g., torques, velocities, steering angles).</p>

<p>Another key benefit is the ability to learn stochastic policies. Rather than committing to a single action in each state, the policy can represent uncertainty or intentional randomness, which is often crucial for exploration or for modeling multimodal behaviors.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image.png" alt="image.png" /></p>

<h2 id="reinforce-or-monte-carlo-policy-gradient">REINFORCE (or Monte Carlo Policy Gradient)</h2>

<p>In policy-based reinforcement learning, the agent does not try to estimate values first and then derive a policy. Instead, it directly optimizes the policy itself. To do this, we need to define a clear objective that tells us how good a policy is.</p>

<p>A trajectory $\tau$ is one full rollout of the agent interacting with the environment. It is simply a sequence of states and actions:</p>

\[\tau = (s_0, a_0, s_1, a_1, \dots, s_H, a_H)\]

<p>where $H$ is the horizon (episode length). Intuitively, a trajectory is ‚Äúone attempt‚Äù by the agent to solve the task from start to finish.</p>

<p>Since the environment is stochastic (and the policy may be stochastic too), the agent does not experience the same trajectory every time. Instead, trajectories are sampled from a probability distribution induced by the policy. Therefore, a reasonable objective is to maximize the expected trajectory reward:</p>

<aside>
üí°

$$
U(\theta) = \mathbb{E}_{\tau \sim P(\tau; \theta)}[R(\tau)]
$$

</aside>

<p><em><strong>Intuition:</strong> ‚ÄúOn average, how much reward does my policy get when I deploy it in the environment?‚Äù</em></p>

<p>The expectation can also be written explicitly as a sum over all possible trajectories:</p>

\[U(\theta) = \sum_{\tau} P(\tau; \theta)\, R(\tau)\]

<p><strong>Probability of a Trajectory:</strong></p>

<p>A trajectory is generated by two components:</p>

<ol>
  <li>Environment dynamics: how states evolve</li>
  <li>Policy: how actions are chosen</li>
</ol>

<p>The probability of a trajectory factorizes as:</p>

<aside>
üí°

$$
P(\tau; \theta) = \prod_{t=0}^{H} P(s_{t+1} \mid s_t, a_t)\, \pi_\theta(a_t \mid s_t)
$$

</aside>

<ul>
  <li>The dynamics term $P(s_{t+1} \mid s_t,a_t)$ is fixed and unknown</li>
  <li>The policy term $\pi_{\theta}(a_t \mid s_t)$ is what we control and parameterize</li>
</ul>

<h3 id="derivation">Derivation</h3>

<p>Our goal is not to compute the objective itself, but to compute its gradient with respect to the policy parameters:</p>

\[\nabla_\theta U(\theta)= \nabla_\theta \mathbb{E}_{\tau \sim P(\tau; \theta)}[R(\tau)]\]

\[\nabla_\theta U(\theta)= \sum_{\tau} \nabla_\theta P(\tau; \theta)\, R(\tau)\]

<p>Now apply the log-derivative trick:</p>

\[\nabla_\theta P(\tau; \theta)= P(\tau; \theta)\, \nabla_\theta \log P(\tau; \theta)\]

<p>Substitute:</p>

\[\nabla_\theta U(\theta)= \sum_{\tau} P(\tau; \theta)\, \nabla_\theta \log P(\tau; \theta)\, R(\tau)\]

<p>This can be written as an expectation:</p>

\[\nabla_\theta U(\theta)= \mathbb{E}_{\tau \sim P(\tau; \theta)}\left[ \nabla_\theta \log P(\tau; \theta)\, R(\tau) \right]\]

<p>Recall the trajectory probability:</p>

\[\log P(\tau; \theta)= \sum_{t=0}^{H} \log P(s_{t+1} \mid s_t, a_t)+ \sum_{t=0}^{H} \log \pi_\theta(a_t \mid s_t)\]

<p>The environment dynamics do not depend on $\theta$, so their gradient is zero. This leaves:</p>

\[\nabla_\theta \log P(\tau; \theta)= \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t \mid s_t)\]

<p>Substitute back into the gradient:</p>

<aside>
üí°

$$
\nabla_\theta U(\theta)= \mathbb{E}_{\tau}\left[\sum_{t=0}^{H}\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, R(\tau)\right]
$$

</aside>

<p>Using Monte-Carlo sampling with $N$ trajectories:</p>

<aside>
üí°

$$
\nabla_\theta U(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{H}\nabla_\theta \log \pi_\theta(a_t^{(i)}\mid s_t^{(i)})\,R(\tau^{(i)})
$$

</aside>

<p><strong>Intuition:</strong></p>

<p>Each term $\nabla_\theta \log \pi_\theta(a_t^{(i)}\mid s_t^{(i)})$ asks: <em>‚ÄúHow should I change the policy parameters to make this action more likely in this state?‚Äù</em></p>

<p>That change is scaled by the total reward $R(\tau)$:</p>

<ul>
  <li>If the trajectory was good $\rightarrow$ reinforce the actions</li>
  <li>If the trajectory was bad $\rightarrow$ suppress the actions</li>
</ul>

<p>So learning boils down to: <em>Increase the probability of actions that led to good outcomes, decrease the probability of actions that led to bad ones.</em></p>

<p>Assigning the full trajectory return to every action is wasteful. An action at time $t$ cannot affect rewards that happened before it. So we define the return from time $t$ onward:</p>

\[G_t = \sum_{k=t}^{H} R(s_k, a_k)\]

<p>Replace $R(\tau)$  with $G_t$:</p>

<aside>
üí°

$$
\nabla_\theta U(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{H}\nabla_\theta \log \pi_\theta(a_t^{(i)}\mid s_t^{(i)})\,G_t^{(i)}
$$

</aside>

<h3 id="gaussian-policy">Gaussian Policy</h3>

<p>Assume a continuous action space. The policy outputs the parameters of a Gaussian:</p>

\[\pi_\theta(a \mid s) = \mathcal{N}\big(a \mid \mu_\theta(s), \sigma^2\big)\]

<p>To keep things simple:</p>

<ul>
  <li>Mean $\mu_\theta(s)$ depends on $\theta$</li>
  <li>Variance $\sigma^2$ is fixed</li>
</ul>

<p>So at state $s_t$:</p>

<ol>
  <li>The network outputs $\mu_\theta(s_t)$</li>
  <li>
    <p>The action is sampled:</p>

\[a_t \sim \mathcal{N}(\mu_\theta(s_t), \sigma^2)\]
  </li>
</ol>

<p><strong>For a 1D Gaussian:</strong></p>

\[\log \pi_\theta(a_t \mid s_t)= -\frac{1}{2\sigma^2}(a_t - \mu_\theta(s_t))^2 + C\]

<p>We ignore constants because they vanish under gradients.</p>

\[\nabla_\theta \log \pi_\theta(a_t \mid s_t)= \frac{(a_t - \mu_\theta(s_t))}{\sigma^2}\nabla_\theta \mu_\theta(s_t)\]

<p><strong>For a 2D Gaussian:</strong></p>

<p>The policy outputs a mean vector:</p>

\[\mu_\theta(s)=\begin{bmatrix}\mu_1(s) \\\mu_2(s)\end{bmatrix}\]

<p>We assume a Gaussian policy with fixed covariance:</p>

\[\pi_\theta(a \mid s)=\mathcal{N}\big(a \mid \mu_\theta(s), \Sigma\big)\]

\[a \sim \mathcal{N}(\mu_\theta(s), \Sigma)\]

\[\log \pi_\theta(a \mid s)=-\frac{1}{2}(a - \mu_\theta(s))^\top\Sigma^{-1}(a - \mu_\theta(s))+ C\]

\[\nabla_\theta \log \pi_\theta(a \mid s)=\Sigma^{-1}(a - \mu_\theta(s))\;\nabla_\theta \mu_\theta(s)\]

<p>This is the multi-dimensional generalization of the 1D case.</p>

<p><strong>Geometric Intuition:</strong></p>

<p>The vector $(a - \mu_\theta(s))$ points from the mean to the sampled action.</p>

<p>Then:</p>

<ul>
  <li>$\Sigma^{-1}$ scales that vector</li>
  <li>Directions with low variance get amplified</li>
  <li>Directions with high variance get damped</li>
</ul>

<p>So the gradient says: <em>‚ÄúMove the mean toward the sampled action, but trust directions where the policy is more confident.‚Äù</em></p>

<p><strong>REINFORCE Update in 2D:</strong></p>

\[\Delta \theta\propto\Sigma^{-1}(a_t - \mu_\theta(s_t))\nabla_\theta \mu_\theta(s_t)\; G_t\]

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%201.png" alt="image.png" /></p>

<h3 id="softmax-policy">Softmax Policy</h3>

<p>Now assume the action space is discrete. Instead of outputting a mean and variance, the policy outputs a score (logit) for each action. Let this score function be $h_\theta(s,a)$.</p>

<p>The policy converts scores into probabilities using softmax:</p>

\[\pi_\theta(a \mid s)=\frac{\exp(h_\theta(s, a))}{\sum_{b} \exp(h_\theta(s, b))}\]

<p>Key idea:</p>

<ul>
  <li>Higher score $\rightarrow$ higher probability</li>
  <li>Scores are unconstrained real numbers</li>
  <li>Softmax turns them into a valid probability distribution</li>
</ul>

\[\log \pi_\theta(a \mid s)=h_\theta(s, a)-\log \sum_{b} \exp(h_\theta(s, b))\]

\[\nabla_\theta \log \pi_\theta(a \mid s)
=
\nabla_\theta h_\theta(s,a)
-
\nabla_\theta \log \sum_{b} \exp(h_\theta(s,b))\]

\[\nabla_\theta \log \sum_{b} \exp(h_\theta(s,b))=\frac{1}{\sum_{b} \exp(h_\theta(s,b))}\nabla_\theta \sum_{b} \exp(h_\theta(s,b))\]

\[\nabla_\theta \sum_{b} \exp(h_\theta(s,b))=\sum_{b} \nabla_\theta \exp(h_\theta(s,b)) =\sum_{b} \exp(h_\theta(s,b)) \nabla_\theta h_\theta(s,b)\]

\[\nabla_\theta \log \sum_{b} \exp(h_\theta(s,b))=\frac{\sum_{b} \exp(h_\theta(s,b)) \nabla_\theta h_\theta(s,b)}{\sum_{b} \exp(h_\theta(s,b))} \\ =
\sum_{b}
\frac{\exp(h_\theta(s,b))}{\sum_{b'} \exp(h_\theta(s,b'))}
\nabla_\theta h_\theta(s,b)\]

<p>But this fraction is exactly the softmax probability:</p>

\[\pi_\theta(b \mid s)=\frac{\exp(h_\theta(s,b))}{\sum_{b'} \exp(h_\theta(s,b'))}\]

<p>So the entire term becomes:</p>

\[\nabla_\theta \log \sum_{b} \exp(h_\theta(s,b))=\sum_{b} \pi_\theta(b \mid s)\, \nabla_\theta h_\theta(s,b)\]

\[\nabla_\theta \log \pi_\theta(a \mid s)=\nabla_\theta h_\theta(s,a)-\sum_{b} \pi_\theta(b \mid s)\, \nabla_\theta h_\theta(s,b)\]

<p><strong>Intuition:</strong></p>

<p>The softmax policy gradient has two terms because learning in discrete action spaces is about redistributing probability mass, not independently increasing action scores. Increasing the score of a chosen action must be balanced by decreasing the scores of other actions to preserve normalization. The gradient therefore increases the score of the selected action while subtracting the probability-weighted average score gradient of all actions, ensuring that probability mass shifts toward better actions in a competitive manner.</p>

<p><strong>Algorithm:</strong></p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%202.png" alt="image.png" /></p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%203.png" alt="image.png" /></p>

<h3 id="limitations">Limitations</h3>

<p>Mathematically, the REINFORCE update is:</p>

\[\nabla_\theta U(\theta)\approx\sum_{t}\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, G_t\]

<p>The return $G_t$ is a very blunt learning signal. It mixes together two fundamentally different effects:</p>

<ol>
  <li>State quality: some states are inherently good or bad</li>
  <li>Action quality: some actions are better than others within the same state</li>
</ol>

<p>REINFORCE cannot distinguish between these. If the agent is in a bad state, all actions receive low returns and are suppressed, even if one of them was the best possible choice. Conversely, in a good state, all actions are reinforced, even if some were poor.</p>

<p>This is why REINFORCE updates are extremely noisy: it is trying to solve a fine-grained credit assignment problem using a coarse, trajectory-level signal.</p>

<h2 id="actor-critic">Actor-Critic</h2>

<p>Building upon the limitations of the REINFORCE algorithm, we infer a key conceptual insight: <em>we should not judge an action by how good the episode was, but by how good the action was relative to the state in which it was taken.</em></p>

<h3 id="baseline">Baseline</h3>

<p>REINFORCE updates the policy using returns:</p>

\[\nabla_\theta U(\theta)\approx\sum_t\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, G_t\]

<p>This estimator is unbiased, but extremely high variance. This is because the return $G_t$ contains a lot of irrelevant information. It depends on:</p>

<ul>
  <li>the quality of the state</li>
  <li>randomness in the environment</li>
  <li>future actions that had nothing to do with the current action</li>
</ul>

<p>So the policy gradient ends up reacting strongly to things the action did not cause. Intuitively, the algorithm is asking <em>‚ÄúWas this episode good?‚Äù</em>, when it should be asking <em>‚ÄúWas this action good for this state?‚Äù</em></p>

<p>If the agent is in a bad state, all actions receive low returns and are suppressed, even if one of them was the best possible choice. Conversely, in a good state, all actions are reinforced, even if some were poor. This is why REINFORCE updates are extremely noisy: it is trying to solve a fine-grained credit assignment problem using a coarse, trajectory-level signal.</p>

<p>The baseline exists to fix exactly this mismatch.</p>

<p>A baseline is simply a reference value that we subtract from the return before using it as a learning signal. So instead of using $G_t$, we use $(G_t-b)$.</p>

<p><strong>Baseline choices:</strong></p>

<ul>
  <li>
    <p>Constant baseline (single scalar): $b = \mathbb{E}[R(\tau)]$</p>

\[\hat{g}=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\left(G_t^{(i)} - b\right)\]

    <p>This baseline removes global reward bias. If all trajectories tend to have large positive (or negative) returns, the policy gradient would otherwise push parameters strongly even when actions are not particularly informative. The constant baseline recenters the learning signal so that updates reflect relative success across trajectories.</p>
  </li>
  <li>
    <p>Time-dependent Baseline (a vector length $T$): $b_t = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$</p>

\[\hat{g}=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\left(G_t^{(i)} - b_t\right)\]

    <p>Many tasks have systematic reward structure over time. Early actions tend to have larger remaining returns than late actions simply because more rewards are still ahead. A time-dependent baseline removes this predictable temporal effect, allowing the gradient to focus on deviations from the average behavior at that timestep.</p>
  </li>
  <li>
    <p>State-dependent Baseline (a function):</p>

\[b(s)=\mathbb{E}[r_t + r_{t+1} + \dots + r_{T-1} \mid s_t = s]=V^\pi(s)\]

\[\hat{g}=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{T}\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\left(G_t^{(i)} - V^\pi(s_t^{(i)})\right)\]

    <p>Subtracting this baseline removes the effect of how easy or difficult the state is, leaving only information about the quality of the chosen action. The learning signal now answers a precise question: <em>was this action better or worse than what I normally do in this state?</em> This produces stable, low-variance updates and leads directly to the advantage function.</p>
  </li>
</ul>

<p>We want to encourage an action not when it has high return, but when it has higher return than the other actions from that state, that is, when it has an advantage over the other actions. It may well be that a state is bad and all actions have low returns in that state, but we do not care. The actions that have higher returns than the rest are the ones we want to reinforce. Therefore, we need to calibrate the goodness of actions using state-dependent baselines.</p>

<p><strong>Why subtracting a baseline helps?</strong></p>

<p>Consider two states:</p>

<ul>
  <li>A good state where all actions give high reward</li>
  <li>A bad state where all actions give low reward</li>
</ul>

<p>Without a baseline, good states reinforce everything, and bad states suppress everything. This is useless for learning which action is better.</p>

<p>With a baseline, only actions that outperform the state average are reinforced, and only actions that underperform are suppressed.</p>

<p>So the baseline removes the effect of state quality from the learning signal.</p>

<p><strong>Does subtracting a baseline change the gradient?</strong></p>

<p>The true policy gradient is:</p>

\[\nabla_\theta U(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, G_t\right]\]

<p>Now suppose we subtract a baseline $b(s_t)$:</p>

\[\mathbb{E}\left[\sum_t\nabla_\theta \log \pi_\theta(a_t \mid s_t)\,(G_t - b(s_t))\right]\]

\[=\mathbb{E}\left[\sum_t\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, G_t\right]-\mathbb{E}\left[\sum_t\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, b(s_t)\right]\]

<p>Focus on the second term:</p>

\[\mathbb{E}\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, b(s_t)\right]\]

<p>Since $b(s_t)$ does not depend on the action, we can pull it out of the inner expectation:</p>

\[=\mathbb{E}_{s_t}\left[b(s_t)\;\mathbb{E}_{a_t \sim \pi_\theta(\cdot \mid s_t)}\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t)\right]\right]\]

\[\mathbb{E}_{a \sim \pi}[\nabla_\theta \log \pi(a)]=\sum_a \pi(a) \nabla_\theta \log \pi(a)=\sum_a \nabla_\theta \pi(a)=\nabla_\theta \sum_a \pi(a)=\nabla_\theta 1=0\]

<p>So the entire baseline term vanishes in expectation. This shows that subtracting any baseline that depends only on the state:</p>

<ul>
  <li>does not change the expected gradient</li>
  <li>does reduce variance</li>
</ul>

<p>So the estimator remains unbiased, but becomes far more stable.</p>

<h3 id="how-do-we-learn-the-state-dependent-baseline-vpis">How do we learn the state-dependent baseline $V^\pi(s)$?</h3>

<p>The value function is defined as the expected future return starting from a state and following the current policy:</p>

\[V^\pi(s)=\mathbb{E}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k}\;\middle \mid \;s_t = s\right]\]

<p>In real environments, the true value function is unknown. We only observe sampled trajectories, not expectations. Therefore, the value function must be learned from experience, using the same data generated by the policy.</p>

<p>There are two fundamental strategies for doing this:</p>

<ol>
  <li>Monte Carlo (MC) estimation</li>
  <li>Temporal-Difference (TD) estimation</li>
</ol>

<p>Both aim to approximate the same quantity $V^\pi(s)$, but they differ in <em>how much of the future they wait to observe</em> before making an update.</p>

<p><strong>Monte Carlo Value Estimation:</strong></p>

<p>Monte Carlo methods estimate the value of a state by waiting until the episode finishes, then using the actual observed return as the training target.</p>

<p><em><strong>Intuition:</strong> ‚ÄúDon‚Äôt guess the future. Wait and see what really happens.‚Äù</em></p>

<p>For a state visited at time $t$, the Monte Carlo return is:</p>

\[G_t=r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t} r_T\]

<p>This return is treated as a sample of the true value:</p>

\[V^\pi(s_t) \approx G_t\]

<p>If the value function is parameterized by $\phi$, it can be trained via regression:</p>

\[\mathcal{L}_{MC}(\phi)=\left( V_\phi(s_t) - G_t \right)^2\]

<p>Monte Carlo estimation is:</p>

<ul>
  <li>Unbiased, because it uses the true outcome</li>
  <li>High variance, because returns depend on everything that happens later</li>
  <li>Delayed, because updates occur only after the episode ends</li>
</ul>

<p>This makes MC estimation conceptually clean but often impractical for long-horizon or sparse-reward problems.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%204.png" alt="image.png" /></p>

<p><strong>Temporal-Difference (TD) Value Estimation:</strong></p>

<p>Temporal-Difference methods update the value function before the episode ends by bootstrapping from the current value estimate of the next state.</p>

<p><em><strong>Intuition:</strong> ‚ÄúUse what I already believe about the future, and correct myself if I‚Äôm wrong.‚Äù</em></p>

<p>Instead of waiting for the full return, TD uses a one-step lookahead:</p>

\[r_t + \gamma V_\phi(s_{t+1})\]

<p>The TD error is defined as:</p>

\[\delta_t=r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)\]

<p>This error measures how surprised the value function is by new experience. The value function is trained to minimize this prediction error:</p>

\[\mathcal{L}_{TD}(\phi)=\delta_t^2\]

<p>Temporal-Difference estimation is:</p>

<ul>
  <li>Lower variance, because it uses shorter-horizon targets</li>
  <li>Biased, because it relies on imperfect value estimates</li>
  <li>Online, allowing updates at every timestep</li>
</ul>

<p>In practice, the reduced variance and faster learning usually outweigh the bias.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%205.png" alt="image.png" /></p>

<p>Once a value function is available, it can be used as a baseline to compute advantages.</p>

<p>Monte Carlo Advantage:</p>

\[A_t^{MC}=G_t - V_\phi(s_t)\]

<p>TD Advantage:</p>

\[A_t^{TD}=r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)\]

<p>A positive advantage $A^{\pi}(s,a) &gt; 0$ means that action $a$ in state $s$ leads to returns higher than the expected value of that state ‚Äî it‚Äôs better than what the policy typically does from that state. A negative advantage means it‚Äôs worse. So computing advantages is essentially identifying which actions should be made more likely (positive advantage) and which should be made less likely (negative advantage).</p>

<h3 id="the-core-actor-critic-framework">The Core Actor-Critic Framework</h3>

<p>Actor-Critic methods maintain two separate components working in tandem: an actor (the policy network) that decides which actions to take, and a critic (the value function network) that evaluates how good those actions are. Think of it like a performer and a coach: the actor performs actions in the environment, while the critic watches and provides feedback on performance. The actor uses this feedback to improve its policy parameters through gradient ascent, while the critic learns to better evaluate states by observing the actual rewards received.
In the basic Actor-Critic algorithm, after collecting trajectories by running the current policy, you update the critic to fit a value function $V_{\phi}^{\pi}(s)$ using either Monte Carlo or Temporal Difference estimation. Then, for the policy gradient update, you compute:</p>

\[\nabla_\theta U(\theta) \approx \hat{g} = \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t^{(i)}  \mid  s_t^{(i)}) A^\pi(s_t^{i}, a_t^{i})\]

<p>where the advantage $A^\pi(s_t^i, a_t^i) = G_t^{(i)} - V_{\phi}^{\pi}(s_t^i)$ and $G_t^{(i)}$ is the sample return (the sum of all rewards from time $t$ onward in that trajectory).</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%206.png" alt="image.png" /></p>

<h3 id="advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</h3>

<p>The key innovation in A2C is in how it computes the advantage. Instead of using the full Monte Carlo return $G_t$ and subtracting the value function baseline, it uses a bootstrapped estimate. The advantage becomes:</p>

\[A^\pi(s_t^i, a_t^i) = R(s_t^i, a_t^i) + \gamma V_{\phi}^{\pi}(s_{t+1}^i) - V_{\phi}^{\pi}(s_t^i)\]

<p>This is essentially a one-step TD error.</p>

<p>Instead of waiting to collect the entire trajectory‚Äôs return, you‚Äôre using the immediate reward $R(s_t, a_t)$ plus the estimated value of the next state  $\gamma V(s_{t+1})$  and comparing that to your current state‚Äôs value $V(s_t)$.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%207.png" alt="image.png" /></p>

<p>In general, it is useful to remember the following about advantage function:</p>

\[A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)\]

\[\mathbb{E}_{a \sim \pi}[A^\pi(s,a)] = 0\]

<h3 id="why-this-difference-matters-a2c-vs-ac">Why This Difference Matters: A2C vs AC</h3>

<p>The distinction might seem subtle, but it has profound implications for learning efficiency and variance. To understand why, imagine you‚Äôre learning to play a game where each episode lasts 1000 steps. In basic Actor-Critic using full returns, the advantage at time step t=5 depends on everything that happens for the next 995 steps. This creates enormous variance. Maybe you made a great move at step 5, but 800 steps later something random happened that ruined the episode. Your advantage estimate would unfairly penalize that early good decision.
Advantage Actor-Critic solves this by using the TD error, which essentially asks: <em>‚ÄúGiven what I expected the value of my current state to be, did taking this action and moving to the next state exceed or fall short of expectations?‚Äù</em> This is a much more localized signal. You‚Äôre only looking one step ahead (or k steps if using k-step returns), and you‚Äôre bootstrapping the rest using your learned value function. This dramatically reduces variance at the cost of introducing some bias (since your value function estimate might be wrong).
Think of it like getting feedback on your work. Monte Carlo returns (used in basic Actor-Critic) are like waiting until a project is completely finished before getting any evaluation ‚Äî you get an unbiased assessment, but it‚Äôs very noisy because many things could have gone wrong along the way. The TD approach (used in Advantage Actor-Critic) is like getting incremental feedback after each task: ‚ÄúYou did better/worse than I expected given where you started.‚Äù It‚Äôs slightly biased by your coach‚Äôs potentially imperfect expectations, but it‚Äôs much more consistent and immediate.</p>

<p>From an implementation standpoint, both methods follow similar algorithmic structure ‚Äî they both initialize policy and critic parameters, sample trajectories, fit the value function, compute advantages, and update the policy. The crucial line that differs is step 3, where advantages are computed.</p>

<h3 id="actor-critic-as-policy-iteration">Actor-Critic as Policy Iteration</h3>

<p>Policy iteration alternates between two phases in a loop: policy evaluation and policy improvement. During policy evaluation, you fix your current policy $\pi$ and compute its value function, essentially answering ‚Äúhow good is each state under my current policy?‚Äù Then, during policy improvement, you create a new policy that‚Äôs greedy with respect to these values, meaning you switch to actions that have higher Q-values than the average. This two-step dance is guaranteed to converge to the optimal policy under certain conditions.</p>

<p>Actor-critic methods implement the same fundamental structure as classical policy iteration, but in a continuous, gradient-based way rather than through discrete policy updates.</p>

<p>The gradient update $\theta \leftarrow \theta + \alpha \nabla_\theta U(\theta)$ increases the probability of actions with positive advantages and decreases the probability of actions with negative advantages. This single update step is the policy improvement. You don‚Äôt need to do anything separately. It‚Äôs a soft, gradual version of the greedy policy switch in classical policy iteration.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%208.png" alt="image.png" /></p>

<p>Actor-critic is an on-policy method: you‚Äôre evaluating your current policy $\pi_\theta$, computing how good actions are under that specific policy, and then improving that same policy. The data you collected is intrinsically tied to the policy that generated it, just like in policy iteration where $V^{\pi}$ and $Q^{\pi}$ are defined relative to a specific policy $\pi$.</p>

<h2 id="asynchronous-deep-rl-for-on-policy-learning">Asynchronous Deep RL for On-Policy Learning</h2>

<h3 id="the-core-problem">The Core Problem</h3>

<p>When training neural networks for reinforcement learning, gradient updates need to be decorrelated for stable learning. However, this creates a fundamental problem for on-policy methods like REINFORCE and actor-critic. When you collect experience sequentially by running a single agent through an environment, consecutive states and actions are highly correlated ‚Äî each state naturally follows from the previous one. If you compute gradient updates from these sequential, correlated experiences, your value function approximator tends to oscillate and become unstable.</p>

<h3 id="the-asynchronous-solution-parallel-experience-collection">The Asynchronous Solution: Parallel Experience Collection</h3>

<p>The key insight of asynchronous deep RL is to parallelize the collection of experience to break these correlations and stabilize training. Instead of running a single agent, you run multiple independent threads of experience simultaneously ‚Äî one agent per thread. Each agent explores a different part of the environment at the same time, contributing experience tuples (state, action, reward, next state) from diverse regions of the state space. Because these parallel agents are in different states and taking different actions, their experiences are much less correlated than sequential data from a single agent would be.</p>

<p>This approach is particularly crucial for on-policy algorithms like actor-critic because they can‚Äôt simply use a replay buffer of old experiences (that would violate the on-policy constraint where data must come from the current policy). Instead, asynchronous parallel collection provides the decorrelation benefits you‚Äôd get from experience replay, while still maintaining the on-policy property ‚Äî all workers are running approximately the same current policy. The diversity comes from spatial exploration across parallel environments rather than temporal diversity from replaying old experiences.
The result is much more stable and efficient training. Neural networks converge faster, value function estimates are more accurate, and policies learn more robustly without the wild oscillations that plague sequential single-agent on-policy learning.</p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%209.png" alt="image.png" /></p>

<p><strong>How It Works in Practice</strong></p>

<p>The algorithm runs multiple worker agents in parallel, each with their own copy of the environment. These workers collect trajectories independently and asynchronously. Each worker periodically computes gradients based on its local experience and sends these gradients to a central parameter server (or directly updates shared parameters with appropriate synchronization). The key architectural choice is whether to do this fully asynchronously (A3C - Asynchronous Advantage Actor-Critic) where workers update independently whenever they‚Äôre ready, or synchronously (A2C) where all workers wait for each other and their gradients are averaged before updating the central network.</p>

<h3 id="asynchronous-advantage-actor-critic-a3c">Asynchronous Advantage Actor-Critic (A3C)</h3>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%2010.png" alt="image.png" /></p>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%2011.png" alt="image.png" /></p>

<h1 id="evolutionary-methods-for-policy-search">Evolutionary Methods for Policy Search</h1>

<h2 id="black-box-policy-optimization">Black-Box Policy Optimization</h2>

<p>Black-box policy optimization represents a fundamentally different approach to reinforcement learning compared to the gradient-based methods you‚Äôve studied. Instead of exploiting the structure of the problem (how states connect to each other, how rewards decompose over timesteps), black-box methods treat the entire RL problem as an opaque function to be optimized.</p>

<p>You‚Äôre solving:</p>

\[\max_{\theta} U(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)  \mid  \pi_\theta, \mu_0(s_0)]\]

<p>You only interact with this objective function by querying it. You try some parameters $\theta$, run the policy, get back a scalar reward $R(\tau)$, and that‚Äôs all you know. You don‚Äôt compute gradients, you don‚Äôt learn value functions, you don‚Äôt exploit the Markov structure of the MDP.</p>

<p><strong>Algorithm:</strong></p>

<ol>
  <li>Initialize policy parameters randomly</li>
  <li>Perturb the parameters (create variations, mutations, offspring)</li>
  <li>Evaluate each perturbed policy by running it in the environment</li>
  <li>Select which perturbations led to better performance</li>
  <li>Update your parameters toward the better-performing variations</li>
  <li>Repeat</li>
</ol>

<h2 id="cross-entropy-method-cem">Cross-Entropy Method (CEM)</h2>

<p>Principle: keep trying random variations of your policy, remember which ones worked best, and adjust your search distribution to generate more samples like the good ones.</p>

<p>CEM models your policy parameters as random variables drawn from a probability distribution. Specifically, it uses a multivariate Gaussian with a diagonal covariance matrix:</p>

\[\theta \sim \mathcal{N}(\mu, \sigma^2 I)\]

<p><strong>Algorithm:</strong></p>

<ol>
  <li>Initialization: Start with $Œº = 0$ and $œÉ¬≤ = 100I$. This means you‚Äôre initially searching with a lot of noise, exploring broadly.</li>
  <li>
    <p>Sampling: Generate n random parameter vectors by sampling from your current Gaussian. Think of this as creating $n$ ‚Äúoffspring‚Äù or ‚Äúmutants‚Äù of your current best guess:</p>

\[{\theta_i}_{i=1}^n \sim \mathcal{N}(\mu, \sigma^2 I)\]
  </li>
  <li>Evaluation: For each $\theta_i$, run the policy in the environment for $L$ episodes and compute the average score. This is the fitness: how well did that particular parameter vector perform?</li>
  <li>Selection: You don‚Äôt use all $n$ samples. Instead, you select only the top $[\rho n]$  performers (where $œÅ ‚â§ 1$, typically around 0.1 or 0.2). These are your winners, the parameter vectors that led to the best performance.</li>
  <li>
    <p>Update: Now you shift your search distribution toward these elites. The new mean becomes the average of the elite samples:</p>

\[\mu(j) = \frac{1}{[\rho n]} \sum_{i=1}^{[\rho n]} \theta'_i(j)\]

    <p>And the new variance is computed from how spread out the elites are:</p>

\[¬†\sigma^2(j) = \frac{1}{[\rho n]} \sum_{i=1}^{[\rho n]} [\theta'_i(j) - \mu(j)]^2 + \eta\]

    <p>$\eta$ is the added noise to prevent the variance from collapsing to zero, which would kill exploration.</p>
  </li>
</ol>

<p><img src="/assets/images/Deep%20Reinforcement%20Learning/image%2012.png" alt="image.png" /></p>

<p>CEM struggles as dimensionality grows. With neural networks having thousands or millions of parameters, the diagonal Gaussian assumption becomes too restrictive, and the selection mechanism (only keeping top $\rho$ fraction) throws away most of your sampled information.</p>

<h2 id="covariance-matrix-adaptation-evolution-strategy-cma-es">Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</h2>

<p>While CEM used a diagonal covariance matrix (assuming parameter independence), CMA-ES uses a full covariance matrix. This means it learns and exploits correlations between parameters, making it dramatically more powerful, though also more complex.</p>

<p>CEM sampled from:</p>

\[¬†\theta \sim \mathcal{N}(\mu, \sigma^2 I)\]

<p>CMA-ES generalizes this to:</p>

\[\theta \sim \mathcal{N}(\mu, \Sigma)\]

<p><strong>Visual Intuition:</strong></p>

<p>With diagonal covariance (CEM), your search distribution is always axis-aligned ‚Äî you can only scale search along the coordinate axes. But the optimal search direction might be diagonal.</p>

<p>CMA-ES adapts the ellipse to match the problem geometry. If the fitness landscape is like a valley running northeast-southwest, CMA-ES will stretch its sampling ellipse along that valley. This dramatically improves efficiency because you‚Äôre searching along the natural contours of the problem rather than fighting against them.</p>

<p><strong>Algorithm:</strong></p>

<ol>
  <li>Sample: Draw $n$ parameter vectors from your current Gaussian $\mathcal{N}(\mu_i, C_i)$, where $C$ is the covariance matrix at iteration $i$.</li>
  <li>Select Elites: Evaluate each sample‚Äôs fitness and keep the top performers, just like CEM.</li>
  <li>
    <p>Update Mean: Move your center $\mu$ toward the weighted average of the elite samples:</p>

\[\mu_{t+1} = \mu_t + \alpha \sum_{i=1}^{n_{elit}} w_i(\theta_i^{elit,t} - \mu_t)\]
  </li>
  <li>
    <p>Update Covariance:</p>

\[¬†\Sigma_{t+1} = \text{Cov}(\theta_1^{elit,t}, \theta_2^{elit,t}, \ldots) + \epsilon I\]

    <p>This tells you: ‚ÄúWhat directions in parameter space led to good performance?‚Äù The covariance matrix encodes these productive search directions. The $\epsilon I$ term (small noise on the diagonal) prevents the matrix from becoming singular.</p>
  </li>
</ol>

<h2 id="natural-evolutionary-strategies-nes">Natural Evolutionary Strategies (NES)</h2>

<p>CEM and CMA-ES had a clear selection mechanism: evaluate samples, keep the elites, throw away the rest. NES takes a different approach: every offspring contributes to the update, weighted by its fitness.</p>

<p>NES maintains a Gaussian distribution over policy parameters, but for computational tractability with neural networks, it uses a diagonal covariance with fixed variance:</p>

\[\theta \sim P_\mu(\theta) = \mathcal{N}(\mu, \sigma^2 I_d)\]

<p>Where:</p>

<ul>
  <li>$\mu \in ‚Ñù^d$  is the mean (what we‚Äôre optimizing)</li>
  <li>$œÉ¬≤I_d$ is a fixed diagonal covariance ($\sigma$ is a hyperparameter)</li>
  <li>$d$ is the parameter dimension</li>
</ul>

<p>The goal is to maximize the expected fitness:</p>

\[\max_\mu U(\mu) = \mathbb{E}_{\theta \sim P\mu(\theta)} [F(\theta)]\]

<p>where $F(\theta)$ is the fitness (total return) when running policy $\pi \theta$.</p>

<h3 id="derivation-1">Derivation</h3>

\[\nabla_\mu U(\mu) = \nabla_\mu \mathbb{E}_{\theta \sim P\mu(\theta)} [F(\theta)]\]

<p>We can write the expectation as an integral:</p>

\[\nabla_\mu U(\mu) = \nabla_\mu \int P_\mu(\theta) F(\theta) d\theta\]

\[= \int \nabla_\mu P_\mu(\theta) F(\theta) d\theta\]

<p>Log-likelihood trick:</p>

\[\nabla_\mu U(\mu) = \int P_\mu(\theta) \frac{\nabla_\mu P_\mu(\theta)}{P_\mu(\theta)} F(\theta) d\theta\]

\[= \int P_\mu(\theta) \nabla_\mu \log P_\mu(\theta) F(\theta) d\theta\]

\[= \mathbb{E}_{\theta \sim P\mu(\theta)} [\nabla_\mu \log P_\mu(\theta) F(\theta)]\]

<p>This is exactly the same trick used in policy gradients! Just like REINFORCE transforms policy gradients into an expectation, NES transforms parameter distribution gradients into an expectation.</p>

<p>Since we have an expectation, we can approximate it by sampling:</p>

\[¬†\nabla_\mu U(\mu) \approx \frac{1}{N} \sum_{i=1}^N \nabla_\mu \log P_\mu(\theta^{(i)}) F(\theta^{(i)})\]

<p>where each $\theta^i \sim P_\mu(\theta)$.</p>

<p>For our Gaussian with mean $\mu$ and fixed variance $\sigma^2I$:</p>

\[¬†\log P_\mu(\theta) = -\frac{|\theta - \mu|^2}{2\sigma^2} + \text{const}\]

<p>Taking the gradient with respect to $\mu$:</p>

\[\nabla_\mu \log P_\mu(\theta) = \frac{\theta - \mu}{\sigma^2}\]

<p>So the NES update becomes:</p>

\[\mu_{t+1} = \mu_t + \frac{\alpha}{N\sigma} \sum_{i=1}^N F(\theta_i) \epsilon_i\]

<p>where $\theta_i = \mu_t + \sigma \epsilon_i$ and $\epsilon_i \sim \mathcal{N}(0,I)$.</p>

<h1 id="value-based-rl">Value-Based RL</h1>

<h2 id="review-actor-critic-and-the-advantage-function">Review: Actor-Critic and the Advantage Function</h2>

<p>We saw previously that in actor-critic methods, we‚Äôre optimizing a policy gradient objective that looks like this:</p>

\[\nabla_\theta U(\theta) = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot A^{\pi}(s_t, a_t) \right]\]

<p>The advantage function plays a crucial role here, serving as our signal for which actions are better than average. The advantage is formally defined as:</p>

\[A^{\pi}(s_t, a_t) = r(s_t, a_t) + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_t)\]

<p><strong>Intuition:</strong> The term $r(s_t, a_t) + \gamma V^\pi(s_{t+1})$ represents the Q-value of the state-action pair, essentially asking <em>‚Äúwhat‚Äôs the total expected return if I take action $a_t$ in state $s_t$ and then follow $\pi$ afterward?‚Äù</em> By subtracting the value function $V^\pi(s_t)$, which represents the average value of being in that state, we get a relative measure. The advantage tells us: <em>‚Äúhow much better is this specific action compared to what we‚Äôd typically expect in this state?‚Äù</em></p>

<p>We can also express the advantage function in terms of Q-values and values:</p>

\[A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)\]

<p>General policy-based RL workflow:</p>

<ol>
  <li>We collect trajectories by rolling out our current policy in the environment</li>
  <li>We estimate how good each action was by computing returns or advantages through policy evaluation</li>
  <li>We improve the policy by adjusting it to favor actions with higher estimated advantages.</li>
</ol>

<h2 id="the-value-based-approach">The Value-Based Approach</h2>

<p>Now we arrive at a fundamental question that motivates value-based RL: if we had a perfect estimate of the advantage function, would we even need to maintain an explicit policy? The answer is no, and this insight drives the entire value-based methodology.</p>

<p>Think about it this way: in policy-based methods, we‚Äôre using the advantage estimates to nudge our policy parameters gradually in the direction of better actions. We collect data, estimate advantages using rollouts from our policy, compute gradients, and take small steps. But if we somehow had perfect knowledge of $A^\pi(s,a)$ for all state-action pairs, we could simply construct the optimal policy by taking the argmax:</p>

\[\pi(s) \leftarrow \argmax_a A^\pi(s,a)\]

<p>This leads to the value-based RL skeleton, which differs from policy-based methods primarily in steps 2 and 3:</p>

<ol>
  <li>Collect data in the environment using the current policy $\pi$ (with some exploration noise to ensure we visit diverse states).</li>
  <li>Estimate the advantage function $A^\pi(s,a)$ as perfectly as possible rather than just well enough to compute a policy gradient. This is where we invest most of our effort by building the most accurate model we can of which actions are truly better.</li>
  <li>Update the policy via argmax by setting $\pi‚Äô(s) = \argmax_a A^\pi(s, a)$ for all states. Notice this is ‚Äúoff-policy‚Äù in the sense that we‚Äôre not computing gradients of the policy itself; we‚Äôre simply choosing the best action according to our learned advantage estimates.</li>
</ol>

<p>We don‚Äôt need to worry about policy parameterization, gradient estimation, or learning rates for the policy. If our advantage estimates are good, the policy update is trivial ‚Äî just pick the best action.</p>

<h2 id="value-iteration-and-q-learning">Value Iteration and Q-Learning</h2>

<p>The theoretical algorithm that formalizes this idea is called dynamic programming or value iteration. The core of value iteration is the Bellman equation for the Q-function:</p>

\[Q(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim p(\cdot \mid s, a)} [V(s')]\]

<p><em><strong>Intuition:</strong> ‚ÄúThe value of taking action $a$ in state $s$ equals the immediate reward plus the discounted expected value of wherever we end up next.‚Äù</em></p>

<p>The value iteration algorithm alternates between two updates:</p>

<ol>
  <li>
    <p>Update 1: Update Q-values using the Bellman equation:</p>

\[Q(s, a) \leftarrow r(s, a) + \gamma \mathbb{E}_{s'} [V(s')]\]
  </li>
  <li>
    <p>Update 2: Update the value function by taking the max over actions:</p>

\[V(s) \leftarrow \max_a Q(s, a)\]
  </li>
</ol>

<p>These updates are performed iteratively until convergence. In tabular settings (where we have finite state and action spaces), we can literally maintain a table with entries $Q(s, a)$ for every possible state-action pair, and this algorithm is guaranteed to converge to the optimal Q-function.</p>

<p>But here‚Äôs the practical challenge: how do we implement this when we have large or continuous state spaces? We need function approximation.</p>

<h3 id="fitted-q-iteration-neural-networks-for-value-functions">Fitted Q-Iteration: Neural Networks for Value Functions</h3>

<p>To scale value-based methods to complex domains, we use neural networks to approximate the Q-function. This leads to fitted Q-iteration, where instead of maintaining a table, we train a neural network $Q_\theta(s,a)$ parameterized by weights $\theta$.</p>

<p>The training procedure draws inspiration from supervised learning, specifically, from temporal difference (TD) learning. Here‚Äôs how it works:</p>

<p>We want to train $Q_\theta$ to satisfy the Bellman equation, so we construct a target value:</p>

\[y(s, a) = r(s, a) + \gamma \max_{a'} Q_\theta(s', a')\]

<p>The target itself involves our Q-function evaluated at the next state $s‚Äô$. This creates a ‚Äúbootstrap‚Äù situation where we‚Äôre using our current estimates to create training targets for future estimates.</p>

<p>We then define a TD-error loss function which we want to minimize:</p>

\[\text{TD-Error}(\theta) = \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[ \left( Q_\theta(s, a) - y(s, a) \right)^2 \right]\]

<p>The gradient update becomes:</p>

\[\theta_{t+1} \leftarrow \theta_t - \alpha \nabla_\theta \text{TD-Error}(\theta) \big|_{\theta = \theta_t}\]

<p>This is not gradient descent in the traditional sense. Why? Because our target $y(s,a)$ itself depends on $\theta$ through the $\max_{a‚Äô} Q_\theta(s‚Äô, a‚Äô)$ term. When we compute the gradient, we‚Äôre treating the target as fixed (not differentiating through it), but the target is actually moving as $\theta$ changes. This creates a moving target problem that can lead to instability.</p>

<p>Think of it like trying to hit a bullseye that keeps shifting position as you aim. Traditional supervised learning has a fixed target. But in Q-learning, as our Q-estimates get better, the targets we‚Äôre aiming for also change, creating a feedback loop that can sometimes diverge.</p>

<p>Despite this theoretical concern, fitted Q-iteration can work in practice with the right tricks. The key insight is that even though we‚Äôre not doing true gradient descent on a fixed objective, we‚Äôre still iteratively improving our Q-estimates to better satisfy the Bellman equation.</p>

<p><strong>Why the Q-Learning Target Makes Sense:</strong></p>

<p>The target $y(s, a) = r(s, a) + \gamma \max_{a‚Äô} Q(s‚Äô, a‚Äô)$ comes directly from the Bellman optimality equation. When you take action $a$ in state $s$, your total expected return naturally splits into two components: the immediate reward $r(s,a)$ you receive right now, and the best possible future value you can obtain from wherever you land next, which is $\gamma \max_{a‚Äô} Q(s‚Äô, a‚Äô)$. The max operator appears because we‚Äôre learning the optimal Q-function, which assumes we‚Äôll always choose the best action going forward, even if our current exploratory policy doesn‚Äôt actually do that yet.</p>

<p>This creates a bootstrapping process where we use our current value estimates to improve themselves. Even though our Q-estimates start out imperfect, we can make progress by using the estimate at the next state $s‚Äô$ to improve the estimate at the current state $s$. The one-step reward $r(s,a)$ provides accurate immediate information, and combining it with our (gradually improving) estimate of future value pushes our Q-function toward satisfying the Bellman equation. Over many iterations, these improvements propagate backward through the state space, eventually converging to the optimal Q-function.</p>

<p>The reason we take the max rather than averaging over our current policy‚Äôs action distribution is what makes Q-learning ‚Äúoff-policy‚Äù ‚Äî it learns about the optimal policy directly, regardless of how we‚Äôre actually behaving during data collection. This allows us to learn from exploratory or even random actions, since we‚Äôre always targeting what the best action would be, not what we actually did.</p>

<h2 id="deep-q-networks-dqn">Deep Q-Networks (DQN)</h2>

<p>The breakthrough that made deep Q-learning practical came with the Deep Q-Network (DQN) algorithm, which introduced two critical innovations to address the instability issues.</p>

<h3 id="innovation-1-replay-buffer">Innovation 1: Replay Buffer</h3>

<p>Instead of training on data as it comes from the agent‚Äôs current policy, DQN stores transitions <code class="language-plaintext highlighter-rouge">(s, a, r, s')</code> in a large replay buffer (essentially a dataset of experiences). During training, we sample random mini-batches from this buffer rather than using consecutive experiences.</p>

<p><strong>Why does this help?</strong></p>

<p>First, it breaks the correlation between consecutive samples. If we trained on experiences in the order they occurred, the network would see highly correlated data (e.g., many frames from the same episode of a game), which can lead to overfitting to recent experiences and catastrophic forgetting of earlier lessons. Random sampling from the replay buffer gives us more independent and identically distributed-like data, which neural networks handle much better.</p>

<p>Second, the replay buffer provides data efficiency. Each experience can be used for multiple gradient updates rather than being discarded after a single use. This is particularly valuable in domains where collecting data is expensive.</p>

<h3 id="innovation-2-target-networks">Innovation 2: Target Networks</h3>

<p>To address the moving target problem, DQN maintains two sets of Q-network parameters: the current network $\theta_t$ that we‚Äôre actively training, and a target network $\theta_t^{target}$ that we use to compute the targets in the Bellman equation.</p>

<p>The TD-error loss now becomes:</p>

\[\min_\theta \mathbb{E}_{(s, a, r, s') \in \mathcal{D}} \left[ \left( Q_\theta(s, a) - \left( r(s, a) + \gamma \max_{a'} Q_{\theta_t^{\text{target}}}(s', a') \right) \right)^2 \right]\]

<p>The key idea is that we update $\theta_t$ on every gradient step, but we only update the target network $\theta_t^{target}$ occasionally (e.g., every few thousand steps) by copying the current network weights. This creates a period of stability where the targets remain fixed while we train, reducing the moving target problem. It‚Äôs like the bullseye staying still for a while so we can actually learn to aim at it before it moves again.</p>

<h3 id="algorithm">Algorithm</h3>

<p>The complete DQN algorithm works as follows:</p>

<ol>
  <li>
    <p>Collect data using policy $\pi_{\theta_t}$ (typically $\epsilon-$greedy: take argmax action with probability $1-\epsilon$, and random action with probability $\epsilon$ for exploration by sampling from a uniform distribution of actions $\pi \sim \text{Unif}(\mathcal{A})$ ).</p>

\[\pi(s) = \arg\max_{a'} Q_{\theta_t}(s, a')\]
  </li>
  <li>Store transitions <code class="language-plaintext highlighter-rouge">(s, a, r, s')</code> in replay buffer $\mathcal{D}$</li>
  <li>
    <p>Sample mini-batch from $\mathcal{D}$ and perform gradient descent on:</p>

\[\min_\theta \mathbb{E}_{(s, a, r, s') \in \mathcal{D}} \left[ \left( Q_\theta(s, a) - \left( r(s, a) + \gamma \max_{a'} Q_{\theta_t^{\text{target}}}(s', a') \right) \right)^2 \right]\]
  </li>
  <li>Periodically update target network: every $K$ steps, set $\theta_t^{target} \leftarrow \theta_t$</li>
</ol>

<p>The use of soft targets (keeping the target network fixed for periods) combined with the replay buffer‚Äôs decorrelated sampling creates a much more stable training process than naive fitted Q-iteration. This allowed DQN to achieve superhuman performance on many Atari games, marking a major milestone in deep reinforcement learning.</p>

<p>The fundamental insight of DQN is that even though we‚Äôre not doing true gradient descent (since we‚Äôre using target networks and treating parts of our objective as fixed), we can still make meaningful progress by carefully managing the optimization process to reduce instability.</p>

<h3 id="key-hyperparameters-and-design-choices">Key Hyperparameters and Design Choices</h3>

<p><strong>Update-to-Data (UTD) Ratio:</strong></p>

<p>The UTD ratio measures how many gradient updates we perform per environment step. If UTD = 1, we take one gradient step for each new transition we collect. If UTD = 4, we perform four gradient updates (sampling four different mini-batches from the replay buffer) for each new transition.</p>

<p>A higher UTD ratio means we‚Äôre extracting more learning from each piece of data, which can be sample-efficient. However, there‚Äôs a trade-off: if the UTD ratio is too high, we can overfit to the data in our replay buffer, and the rapid updates can destabilize training. The agent might start exploiting patterns in old data that are no longer relevant to the current policy. Selecting its value is problem-dependent, as there are cases where UTD is as high as 100, but sometimes as low as 0.001.</p>

<p>Example scenario for UTD=4. So the process looks like this:</p>

<ol>
  <li>Agent takes one action in environment $\rarr$ gets one new transition $\rarr$ add it to replay buffer</li>
  <li>Sample mini-batch #1 from replay buffer $\rarr$ compute loss $\rarr$ gradient step</li>
  <li>Sample mini-batch #2 from replay buffer $\rarr$ compute loss $\rarr$ gradient step</li>
  <li>Sample mini-batch #3 from replay buffer $\rarr$ compute loss $\rarr$ gradient step</li>
  <li>Sample mini-batch #4 from replay buffer $\rarr$ compute loss $\rarr$ gradient step</li>
  <li>Return to step 1, take another action in environment</li>
</ol>

<p>Those 4 gradient steps in the middle are all training on old data from the replay buffer, not on the single new transition we just collected. The new transition goes into the buffer and might get sampled in future mini-batches, but we‚Äôre primarily learning from historical experiences.</p>

<p><strong>Hard vs Soft Target Updates</strong></p>

<p>The target network update described above is called a ‚Äúhard update‚Äù, where we completely replace $\theta^{target}$ with $\theta$ all at once. An alternative approach is ‚Äúsoft updates,‚Äù where we gradually blend the target network toward the main network at every step:</p>

\[\theta^{\text{target}}_{i+1} \leftarrow (1 - \alpha) \theta^{\text{target}}_i + \alpha \theta_i\]

<p>Here $\alpha$ is a small constant like 0.005. This means the target network slowly tracks the main network, providing continuously updating targets rather than sudden jumps. Hard updates every $K$ steps with $K$ large create discrete jumps, while soft updates with small $\alpha$ create smooth tracking. Soft updates often provide more stable training because the targets change gradually rather than suddenly shifting every few thousand steps.</p>

<p>Note: You pick one strategy and stick with it throughout training. They‚Äôre two different design choices for solving the same problem (keeping targets stable), not complementary techniques you‚Äôd combine.</p>

<h2 id="challenges-with-dqn">Challenges with DQN</h2>

<h3 id="challenge-1-overestimation-bias-in-q-values">Challenge 1: Overestimation Bias in Q-Values</h3>

<p>One of the major practical challenges with DQN is that it tends to systematically overestimate Q-values. This is a fundamental issue with how the algorithm compounds errors through bootstrapping.</p>

<p><strong>The Root Cause: Error Accumulation Through Max Operations</strong></p>

<p>Our target is:</p>

\[y(s,a) = r(s,a) + \gamma \max_{a'} Q_{\theta^{\text{target}}}(s', a')\]

<p>The term $\max_{a‚Äô} Q_{\theta^{target}}(s‚Äô, a‚Äô)$ is supposed to represent the optimal state value $V^*(s‚Äô)$, which is the true value we‚Äôd get by following the optimal policy from state $s‚Äô$ onward. However, our Q-function $Q_{\theta^{target}}$ is not perfect and has some errors. Some actions might have Q-values that are too high (overestimates), while others might be too low (underestimates).</p>

<p>This problem becomes serious because we‚Äôre doing bootstrapping ‚Äî the Q-value we‚Äôre training becomes the target for earlier states. So if $Q_\theta(s‚Äô, a‚Äô)$ is overestimated, then when we compute the target for state $s$, we use this overestimate. This means $Q_\theta(s, a)$ will also become overestimated. Then when we compute targets for states that lead to $s$, they inherit this error, and so on. The overestimation compounds backward through the state space like a chain reaction.</p>

<p><strong>Double DQN: Decoupling Action Selection and Evaluation</strong></p>

<p>The problem is that we‚Äôre using the same Q-function both to select which action looks best AND to evaluate how good that action is. This creates the bias‚Äîwe pick the action with the largest estimation error, then trust that error as our target.</p>

<p>Double DQN fixes this by maintaining two Q-functions (initialized differently): $Q_{\theta^A}$ and $Q_{\theta^B}$. The key idea is to use one network to select the action and the other network to evaluate it. Here‚Äôs how the target computation changes:</p>

<ol>
  <li>
    <p>Use network A to determine which action looks best in state $s‚Äô$:</p>

\[a^* = \arg\max_{a} Q_{\theta^A}(s', a)\]
  </li>
  <li>
    <p>Use network B to evaluate that specific action:</p>

\[y(s,a) = r(s,a) + \gamma Q_{\theta^B}(s', a^*)\]
  </li>
</ol>

<p>Why does this help? If network A has a positive error on some action (thinks it‚Äôs better than it really is), it will select that action. But then network B evaluates it, and B‚Äôs errors are independent of A‚Äôs errors (since they were initialized differently and trained on different mini-batches). So B is unlikely to have the same overestimation for that same action. On average, this decorrelates the selection bias from the evaluation bias, dramatically reducing the systematic overestimation.</p>

<p>In practice with DQN, we already have two networks‚Äîthe main network $\theta$ and the target network $\theta^{target}$ ‚Äî so we can implement Double DQN without adding any extra parameters. We use the main network to select actions and the target network to evaluate them:</p>

\[a^* = \arg\max_{a} Q_{\theta}(s', a)\]

\[y(s,a) = r(s,a) + \gamma Q_{\theta^{\text{target}}}(s', a^*)\]

<h3 id="challenge-2-reducing-error-compounding-with-n-step-returns">Challenge 2: Reducing Error Compounding with N-Step Returns</h3>

<p>Another approach to reducing error accumulation is to use N-step returns instead of one-step TD targets. The standard DQN target uses only one step of real reward before bootstrapping:</p>

\[y(s,a) = r(s,a) + \gamma \max_{a'} Q_{\theta^{\text{target}}}(s', a')\]

<p>This means we trust the immediate reward $r(s,a)$ (which comes from the environment and is accurate), but then immediately rely on our Q-function estimate for everything after. If our Q-estimates are poor, this propagates error quickly.</p>

<p>With N-step returns, we instead accumulate N steps of actual rewards before bootstrapping. If we have a trajectory <code class="language-plaintext highlighter-rouge">(s, a, r, s', a', r', s'', ...)</code> in our replay buffer, we can construct an N-step target:</p>

\[y_N(s,a) = r(s,a) + \gamma r(s', a') + \gamma^2 r(s'', a'') + \cdots + \gamma^N \max_{a} Q_{\theta^{\text{target}}}(s_{N+1}, a)\]

<p>The intuition is that we‚Äôre trusting real data for longer before falling back on our potentially erroneous estimates.</p>

<p>The trade-off is that N-step returns introduce higher variance ‚Äî they depend on N different random rewards and state transitions ‚Äî and they can only be computed for transitions where we have N subsequent steps stored in the replay buffer. But when tuned properly, they often significantly improve learning by reducing the impact of Q-value errors on the targets. The most common value of N used in practice is 3.</p>

<h3 id="challenge-3-cold-start-problem">Challenge 3: Cold Start Problem</h3>

<p>At the very beginning of training, the replay buffer is empty, so DQN cannot perform any updates. The solution is a burn-in phase: before any Q-network training begins, the agent collects an initial dataset by acting in the environment for thousands of steps using either a random policy or highly exploratory $\epsilon-$greedy policy. This fills the replay buffer with diverse experiences. Only after this warm-up period do gradient updates begin. The Q-network starts with random weights and poor predictions, but it now has real environmental data to learn from.</p>

<h3 id="challenge-4-the-limited-exploration-problem">Challenge 4: The Limited Exploration Problem</h3>

<p>A more persistent challenge arises when the agent repeatedly visits similar states and fails to explore broadly. This can happen if the policy is too exploitative ($\epsilon$ too small), if certain states act as ‚Äúattractors‚Äù where the agent gets stuck, or if the state space is so vast that the explored region remains limited. When the replay buffer lacks diversity, the Q-function becomes specialized to only the visited states and performs poorly elsewhere, trapping the agent in a local optimum where its policy keeps leading it back to the same familiar territory.</p>

<p>This is why $\epsilon-$greedy exploration is essential throughout training. With probability $\epsilon$, the agent takes random actions instead of greedy ones, occasionally forcing it out of comfortable patterns and into new situations. In practice, $\epsilon$ typically starts at 1.0 during burn-in, then gradually decays to a small constant like 0.1 or 0.05. This schedule prioritizes early exploration to build buffer diversity, then shifts toward exploitation as the Q-function improves, while maintaining baseline exploration indefinitely to prevent getting stuck. Even a small constant $\epsilon$, over millions of steps, generates enough random detours to gradually expand the explored region and diversify the replay buffer.</p>

<h2 id="practical-training-details-for-dqn"><strong>Practical Training Details for DQN</strong></h2>

<p>A few additional implementation details that significantly impact DQN‚Äôs performance:</p>

<p><strong>Coverage Really Helps:</strong> Ensuring broad state-space coverage through effective exploration is crucial. If the replay buffer only contains data from a limited region of the state space, the Q-function will have poor estimates elsewhere, and the agent may fail to discover better behaviors. Techniques like decaying $\epsilon$ over time (starting with high exploration, gradually reducing it as we learn) help balance initial exploration with later exploitation.</p>

<p><strong>High UTD Can Destabilize Training:</strong> While performing many gradient updates per environment step seems attractive for sample efficiency, setting the UTD ratio too high can cause instability. The agent essentially overfits to its replay buffer, and rapid Q-value changes can violate the assumptions that make target networks helpful. Conservative UTD ratios tend to be more robust.</p>

<p><strong>Improved Loss Functions:</strong> Instead of using standard squared error, practitioners often use the Huber loss, which is quadratic for small errors but linear for large errors:</p>

\[L(x) = \begin{cases}\frac{x^2}{2}, &amp; |x| \leq \delta \\\delta |x| - \frac{\delta^2}{2}, &amp; |x| &gt; \delta\end{cases}\]

<p>In the context of DQN, $x$ represents the TD-error:</p>

\[x = Q_{\theta}(s,a) - \left( r + \gamma \max_{a'} Q_{\theta^{\text{target}}}(s', a') \right)\]

<p>$\delta(x)$ is a hyperparameter that we choose.</p>

<p>This makes training more robust to outliers and large TD-errors, which can occur especially early in training when Q-estimates are poor. The Huber loss prevents single large errors from causing dramatic parameter updates that destabilize learning.</p>

<h2 id="understanding-vs-in-value-based-rl">Understanding $V(s)$ in Value-Based RL</h2>

<h3 id="vs-is-implicitly-computed-not-separately-stored">$V(s)$ is Implicitly Computed, Not Separately Stored</h3>

<p>In DQN, we only learn the Q-function $Q_\theta(s,a)$ using a neural network. We don‚Äôt maintain a separate value function. However, $V(s)$ isn‚Äôt missing. It is simply computed implicitly whenever we need it through the relationship:</p>

\[V(s) = \max_a Q_\theta(s, a)\]

<p>This means $V(s)$ automatically stays synchronized with our Q-values. Every time the parameters $\theta$change and our Q-function improves, the value function $V(s)$ implicitly changes along with it because it‚Äôs derived from the Q-values. We‚Äôre not storing two separate functions that could fall out of sync; we‚Äôre storing one function (Q) and computing the other (V) from it on-demand (if at all needed).</p>

<h3 id="why-advantages-and-q-values-lead-to-the-same-policy">Why Advantages and Q-Values Lead to the Same Policy</h3>

<p>The advantage function is defined as $A(s,a) = Q(s,a) - V(s)$. You might wonder whether we should be taking the argmax over advantages or over Q-values. The answer is that it doesn‚Äôt matter, since they give the same result.</p>

<p>When we‚Äôre selecting an action, we compute $\argmax_a A(s,a) = \argmax_a [Q(s,a) - V(s)]$. The critical observation is that $V(s)$ is the same value for all actions in state $s$ at any given moment in time. Since $V(s)$ doesn‚Äôt depend on which action we‚Äôre considering, it‚Äôs just a constant that gets subtracted from all Q-values equally. Adding or subtracting the same constant from every option doesn‚Äôt change which option is largest, so:</p>

\[\argmax_a [Q(s,a) - V(s)] = \argmax_a Q(s,a)\]

<p>This is why DQN can work directly with Q-values for action selection without ever explicitly computing advantages. The policy is simply:</p>

\[\pi(s) = \argmax_a Q_\theta(s,a)\]

<h1 id="advanced-policy-gradients-bridging-policy-based-and-value-based-methods">Advanced Policy Gradients: Bridging Policy-Based and Value-Based Methods</h1>

<h2 id="motivation">Motivation</h2>

<p>Up to this point, we‚Äôve studied two distinct families of reinforcement learning algorithms. Policy gradient methods learn an explicit policy (perhaps with an on-policy value function for variance reduction), and they‚Äôre fundamentally on-policy ‚Äî they must collect data using the current policy being optimized. Value-based methods like Q-learning don‚Äôt maintain an explicit policy at all; instead, they learn a value function and derive the policy implicitly through argmax operations, which allows them to be off-policy and leverage replay buffers for sample efficiency.</p>

<p>A natural question arises: can we build a hybrid that combines the best of both worlds? Such an algorithm should be off-policy to benefit from replay buffers and reuse old data, yet it should maintain an explicit policy network that we can optimize using actor-critic style updates. This would give us the expressiveness and continuous action handling of policy gradients with the sample efficiency of value-based methods. The answer is yes, and this leads us to a family of algorithms called off-policy policy gradient methods. The challenge is: how do we derive a principled way to make this work?</p>

<h2 id="the-performance-difference-lemma">The Performance Difference Lemma</h2>

<p>The key mathematical tool that enables off-policy policy gradients is the Performance Difference Lemma. This lemma provides an exact relationship between the performance of any two policies $\pi$ and $\mu$.</p>

<p>Recall that the advantage function for a policy $\mu$ is defined as:</p>

<aside>
üí°

$$
A^{\mu}(s, a) = Q^{\mu}(s, a) - V^{\mu}(s)
$$

</aside>

<p>This measures how much better action $a$ is compared to the average value of being in state $s$ under policy $\mu$. Importantly, we have the property that the expected advantage under the policy‚Äôs own action distribution is zero:</p>

<aside>
üí°

$$
\mathbb{E}_{a \sim \mu(\cdot|s)} [A^{\mu}(s, a)] = 0
$$

</aside>

<p>This is because when we average the advantage over all actions weighted by how often the policy takes them, the positive and negative deviations from the mean cancel out.</p>

<p>Now, the Performance Difference Lemma states that for any two policies $\pi$ (the policy we‚Äôre evaluating) and $\mu$ (some other policy), the difference in their expected returns can be expressed as:</p>

<aside>
üí°

$$
J(\pi) - J(\mu) = \mathbb{E}_{\tau \sim P_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right]
$$

</aside>

<p><strong>What the above equation means:</strong> The trajectories $\tau$ are generated by following policy $\pi$ (so states are distributed according to $\pi$‚Äôs state visitation), but the advantages being summed are computed with respect to policy $\mu$. It connects the performance of $\pi$ to advantages measured under a different policy $\mu$.</p>

<p><strong>Why is this useful?</strong> This lemma tells us that to improve $\pi$ relative to $\mu$, we want to maximize the expected sum of $\mu$‚Äôs advantages along trajectories generated by $\pi$. If we can accurately estimate $A^\mu$ (using a critic), we can optimize $\pi$ to take actions that have high advantage according to $\mu$.</p>

<h3 id="derivation-2">Derivation</h3>

<p>We start with the definition of the return:</p>

\[J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right]\]

<p>Now, notice that the entire sum of rewards is just $V^\pi(s_0)$, the value of the initial state under policy $\pi$. So we can write:</p>

\[J(\pi) = \mathbb{E}_{s_0 \sim p(s_0)} [V^{\pi}(s_0)]\]

<p>The key insight here is that the trajectory distribution $p_\pi(\tau)$ can be decomposed: first we sample $s_0$ from some initial state distribution $p(s_0)$, and then the rest of the trajectory unfolds according to $\pi$. Since $s_0$ is sampled independently of the policy, the expectation over trajectories becomes an expectation over just the initial state.</p>

<p>Similarly, for policy $\mu$:</p>

\[J(\mu) = \mathbb{E}_{s_0 \sim p(s_0)} [V^{\mu}(s_0)]\]

<p>Now let‚Äôs compute the difference:</p>

\[J(\pi) - J(\mu) = \mathbb{E}_{s_0 \sim p(s_0)} [V^{\pi}(s_0)] - \mathbb{E}_{s_0 \sim p(s_0)} [V^{\mu}(s_0)]\]

<p>Since both expectations are over the same distribution, we can combine them:</p>

\[J(\pi) - J(\mu) = \mathbb{E}_{s_0 \sim p(s_0)} [V^{\pi}(s_0) - V^{\mu}(s_0)]\]

<p>We want to express this difference in terms of advantages. To do this, we need to ‚Äúunroll‚Äù the value function recursively using the Bellman equation.</p>

<p>Recall the Bellman equation for $V^\pi$:</p>

\[V^{\pi}(s) = \mathbb{E}_{a \sim \pi(\cdot|s), s' \sim p(\cdot|s,a)} [r(s,a) + \gamma V^{\pi}(s')]\]

<p>Let‚Äôs apply this to $s_0$:</p>

\[V^{\pi}(s_0) = \mathbb{E}_{a_0 \sim \pi(\cdot|s_0), s_1 \sim p(¬∑|s_0, a_0)} [r(s_0, a_0) + \gamma V^{\pi}(s_1)]\]

<p>Now we want to introduce $V^\mu$ into this expression. Notice that we can write:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [r(s_0, a_0) + \gamma V^{\pi}(s_1)] - V^{\mu}(s_0)\]

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [r(s_0, a_0) + \gamma V^{\pi}(s_1) - \gamma V^{\mu}(s_1) + \gamma V^{\mu}(s_1)] - V^{\mu}(s_0)\]

<p>Rearranging:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [r(s_0, a_0) + \gamma V^{\mu}(s_1) - V^{\mu}(s_0) + \gamma(V^{\pi}(s_1) - V^{\mu}(s_1))]\]

<p>Recall that:</p>

\[A^{\mu}(s_0, a_0) = Q^{\mu}(s_0, a_0) - V^{\mu}(s_0) = r(s_0, a_0) + \gamma V^{\mu}(s_1) - V^{\mu}(s_0)\]

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [A^{\mu}(s_0, a_0) + \gamma(V^{\pi}(s_1) - V^{\mu}(s_1))]\]

<p>Now we have a recursive structure. The last term $\gamma (V^\pi(s_1) - V^\mu(s_1))$ has the same form as what we started with, just at time step 1 instead of time step 0. We can apply the same trick again:</p>

\[V^{\pi}(s_1) - V^{\mu}(s_1) = \mathbb{E}_{a_1 \sim \pi, s_2} [A^{\mu}(s_1, a_1) + \gamma(V^{\pi}(s_2) - V^{\mu}(s_2))]\]

<p>Substituting this back:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [A^{\mu}(s_0, a_0) + \gamma \mathbb{E}_{a_1 \sim \pi, s_2} [A^{\mu}(s_1, a_1) + \gamma(V^{\pi}(s_2) - V^{\mu}(s_2))]]\]

<p>Distributing the outer expectation and the $\gamma$:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{a_0 \sim \pi, s_1} [A^{\mu}(s_0, a_0)] + \gamma \mathbb{E}_{a_0 \sim \pi, s_1, a_1 \sim \pi, s_2} [A^{\mu}(s_1, a_1)] + \gamma^2 \mathbb{E}_{...} [V^{\pi}(s_2) - V^{\mu}(s_2)]\]

<p>If we keep expanding this pattern infinitely (which is valid as long as $\gamma &lt;1$ ensures convergence), we get:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \sum_{t=0}^{\infty} \gamma^t \mathbb{E}_{s_0, a_0, s_1, a_1, ..., s_t, a_t} [A^{\mu}(s_t, a_t)]\]

<table>
  <tbody>
    <tr>
      <td>where the expectation is taken over trajectories generated by policy $\pi$ (so $s_0 \sim p(s_0)$, then $a_0 \sim \pi(\cdot</td>
      <td>s_0)$, then $s_1 \sim p(\cdot</td>
      <td>s_0,a_0)$, and so on).</td>
    </tr>
  </tbody>
</table>

<p>We can write this more compactly as:</p>

\[V^{\pi}(s_0) - V^{\mu}(s_0) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right]\]

<p>Finally, we take the expectation over the initial state distribution:</p>

\[J(\pi) - J(\mu) = \mathbb{E}_{s_0 \sim p(s_0)} [V^{\pi}(s_0) - V^{\mu}(s_0)]\]

\[= \mathbb{E}_{s_0 \sim p(s_0)} \left[ \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right] \right]\]

\[J(\pi) - J(\mu) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right]\]

<h2 id="the-optimization-problem">The Optimization Problem</h2>

<p>The Performance Difference Lemma gives us an exact expression for $J(\pi) - J(\mu)$, but how do we use this to actually optimize $\pi$? We can frame this as a constrained optimization problem:</p>

<aside>
üí°

$$
\max_{\pi} \left( J(\pi) - J(\mu) \right) = \max_{\pi} \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right]
$$

</aside>

<p>But there‚Äôs a catch: the expectation is over trajectories sampled from $\pi$. This means to evaluate this expression, we need to actually run policy $\pi$ in the environment, collect full trajectories, and compute advantages along those trajectories. Every time we update $\pi$, we‚Äôd need fresh data from the new $\pi$. This is on-policy learning, and it is sample-inefficient because we can‚Äôt reuse old data.</p>

<p>When we write $\tau \sim p_\pi(\tau)$, we‚Äôre sampling an entire trajectory according to $\pi$‚Äôs distribution. But what does this really mean? A trajectory is a sequence $(s_0, a_0, s_1, a_1, s_2, a_2, ‚Ä¶)$, and the distribution over trajectories can be factored:</p>

<ul>
  <li>First, we sample $s_0$ from the initial state distribution $p(s_0)$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Then we sample $a_0$ from $\pi(\cdot</td>
          <td>s_0)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Then the environment gives us $s_1$ from $p(\cdot</td>
          <td>s_0, a_0)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Then we sample $a_1$ from $\pi(\cdot</td>
          <td>s_1)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>And so on‚Ä¶</li>
</ul>

<p>The key is that which states we visit is determined by the policy. Policy $\pi$ creates a specific pattern of state visitation over time. We call this the ‚Äústate visitation distribution‚Äù or ‚Äústate marginal distribution‚Äù and denote it $p_\pi(s_t)$ for time step $t$, or more generally $d^\pi(s)$ for the discounted state visitation.</p>

<p>Here‚Äôs where we make a crucial approximation that enables off-policy learning. Instead of collecting data under $\pi$ (the policy we‚Äôre optimizing), we can collect data under $\mu$ (some behavioral policy, perhaps an older version of $\pi$) and correct for the distribution mismatch. We rewrite the expectation over $\pi$‚Äôs trajectories as:</p>

<aside>
üí°

$$
\mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right] \approx \sum_{t=0}^{\infty} \mathbb{E}_{s_t \sim p_{\mu}(s_t)} \left[ \mathbb{E}_{a_t \sim \pi(\cdot|s_t)} [\gamma^t A^{\mu}(s_t, a_t)] \right]
$$

</aside>

<p>This approximation says: instead of following $\pi$‚Äôs entire trajectory distribution (which determines which states we visit), we use states visited by $\mu$, but we still choose actions according to $\pi$ at each state. The key insight is that when $\pi$ and $\mu$ are close (which we‚Äôll enforce with a constraint), the states visited by the two policies are similar, so this approximation is reasonable.</p>

<p>Now the inner expectation only involves sampling actions from $\pi$ given states from $\mu$‚Äôs distribution:</p>

\[\mathbb{E}_{s \sim d^{\mu}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [A^{\mu}(s, a)] \right]\]

<p>where $d^\mu(s)$ represents the discounted state visitation distribution under policy $\mu$. This is something we can estimate from a replay buffer filled with $\mu$‚Äôs experiences.</p>

<p>We add a constraint to keep $\pi$ close to $\mu$, typically measured by a divergence $D(\pi,\mu)$. This gives us the constrained optimization problem:</p>

<aside>
üí°

$$
\max_{\pi} \mathbb{E}_{s \sim d^{\mu}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [A^{\mu}(s, a)]  - \alpha D(\pi, \mu)\right]
$$

</aside>

<p>where $\alpha$ controls how much we penalize deviation from $\mu$. The divergence $D$ can be chosen in different ways, such as KL divergence, total variation distance, etc., and each choice leads to a different algorithm.</p>

<table>
  <tbody>
    <tr>
      <td>If $D(\pi(\cdot</td>
      <td>s), \mu(\cdot</td>
      <td>s))$ is small, then it implies that $p_\pi(s) \sim p_\mu(s)$.</td>
    </tr>
  </tbody>
</table>

<h3 id="some-additional-math">Some Additional Math</h3>

<aside>
üí°

$$
\mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t f(s_t, a_t) \right] = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [f(s,a)] \right]
$$

</aside>

<p>The left side sums over time steps in a trajectory. The right side removes the explicit time dependence by using the discounted state visitation distribution $d^\pi(s)$.</p>

<p>The discounted state visitation distribution $d^\pi(s)$ is defined as:</p>

<aside>
üí°

$$
d^{\pi}(s) = (1-\gamma) \sum_{t=0}^{\infty} \gamma^t p_{\pi}(s_t = s)
$$

</aside>

<p>This is a distribution (not just a sum) because of the normalization factor $(1-\gamma)$. It represents ‚Äúhow often do we visit state $s$, weighted by how far in the future it is.‚Äù The factor $(1-\gamma)$ ensures that when we sum over all states $s$, we get $1$ (a valid probability distribution).</p>

<p>Let‚Äôs start with the left side and manipulate it:</p>

\[\mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t f(s_t, a_t) \right]\]

\[= \sum_{t=0}^{\infty} \gamma^t \mathbb{E}_{\tau \sim p_{\pi}(\tau)} [f(s_t, a_t)]\]

\[= \sum_{t=0}^{\infty} \gamma^t \mathbb{E}_{s_t \sim p_{\pi}(s_t)} \left[ \mathbb{E}_{a_t \sim \pi(\cdot|s_t)} [f(s_t, a_t)] \right]\]

<p>Here $p_\pi(s_t)$ is the marginal distribution over states at time $t$ when following policy $\pi$.</p>

<p>Now multiply and divide by $(1-\gamma)$:</p>

\[= \frac{1}{1-\gamma} \sum_{t=0}^{\infty} (1-\gamma) \gamma^t \mathbb{E}_{s_t \sim p_{\pi}(s_t)} \left[ \mathbb{E}_{a_t \sim \pi(\cdot|s_t)} [f(s_t, a_t)] \right]\]

<p>By the definition of $d^\pi(s)$, we can write:</p>

\[\sum_{t=0}^{\infty} (1-\gamma) \gamma^t \mathbb{E}_{s_t \sim p_{\pi}(s_t)} [g(s_t)] = \mathbb{E}_{s \sim d^{\pi}(s)} [g(s)]\]

<p>This is because:</p>

\[\mathbb{E}_{s \sim d^{\pi}(s)} [g(s)] = \sum_s d^{\pi}(s) g(s) = \sum_s \left[ (1-\gamma) \sum_{t=0}^{\infty} \gamma^t p_{\pi}(s_t=s) \right] g(s)\]

<p>Rearranging the sums:</p>

\[= \sum_{t=0}^{\infty} (1-\gamma) \gamma^t \sum_s p_{\pi}(s_t=s) g(s) = \sum_{t=0}^{\infty} (1-\gamma) \gamma^t \mathbb{E}_{s_t \sim p_{\pi}(s_t)} [g(s_t)]\]

<table>
  <tbody>
    <tr>
      <td>Applying this to our case where $g(s) = E_{a \sim \pi(¬∑</td>
      <td>s)}[f(s,a)]$:</td>
    </tr>
  </tbody>
</table>

\[= \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [f(s,a)] \right]\]

<p>This identity is useful because it converts a time-dependent sum (iterating through a trajectory) into a time-independent expectation over states. Instead of thinking about ‚Äúwhat happens at time 0, time 1, time 2, etc.‚Äù, we think about ‚Äúwhat happens in state $s$, weighted by how often we visit that state.‚Äù This is often more convenient for analysis and for designing algorithms.</p>

<p>So we have:</p>

<aside>
üí°

$$
J(\pi) - J(\mu) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\mu}(s_t, a_t) \right] = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [A^\mu(s,a)] \right]
$$

</aside>

<h2 id="different-divergence-choices-lead-to-different-algorithms">Different Divergence Choices Lead to Different Algorithms</h2>

<p>The natural choice of divergence here is the Total Variation (TV) distance between the two policy distributions.</p>

\[D(\pi,\mu) = D_{TV}(\pi(\cdot|s), \mu(\cdot|s))\]

<p>The total variation distance between two probability distributions $p$ and $q$ over a space $\mathcal{X}$ is defined as:</p>

\[D_{TV}(p(\cdot), q(\cdot)) = \sum_{x \in \mathcal{X}} |p(x) - q(x)|\]

\[D_{TV}(p(\cdot), q(\cdot)) = \int_x |p(x) - q(x)| \, dx\]

<table>
  <tbody>
    <tr>
      <td><strong>Intuition:</strong> At every single point x in your space, you measure how much $p$ and $q$ disagree at that point, and that‚Äôs just $</td>
      <td>p(x) - q(x)</td>
      <td>$. Then you sum up all these local disagreements across the entire space. The result is the total amount by which the two distributions differ from each other.</td>
    </tr>
  </tbody>
</table>

<p>But since it is very hard to compute the total variation distance in a very large action space using sampling, we look at other methods for estimating this.</p>

<p>The choice of divergence measure $D(\pi,\mu)$ significantly impacts the resulting algorithm and its properties:</p>

<table>
  <tbody>
    <tr>
      <td>**Choice 1: $D_{KL}(\pi</td>
      <td>¬†</td>
      <td>\mu)$ :** This is the KL divergence from $\pi$ to $\mu$, measuring how much information is lost when using $\mu$ to approximate $\pi$. Using this divergence leads to the <strong>AWR (Advantage Weighted Regression)</strong> algorithm.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>**Choice 2: $D_{KL}(\mu</td>
      <td>¬†</td>
      <td>\pi)$ :** This is the reverse KL divergence, from $\mu$ to $\pi$. Using this divergence leads to <strong>PPO (Proximal Policy Optimization)</strong> and <strong>TRPO (Trust Region Policy Optimization)</strong> algorithms.</td>
    </tr>
  </tbody>
</table>

\[D_{KL}(p||q) = \sum_\mathcal{X} p(x) \log{\frac{p(x)}{q(x)}}\]

\[D_{KL}(p||q) = \int_x p(x) \log{\frac{p(x)}{q(x)}}\]

<p>The mathematical properties of these divergences are different. The KL divergence satisfies several useful inequalities, particularly Pinsker‚Äôs inequality, which relates KL divergence to total variation distance:</p>

\[D_{TV}(p, q) \leq \sqrt{\frac{1}{2} D_{KL}(p || q)}\]

\[D_{TV}(p, q) \leq \sqrt{\frac{1}{2} D_{KL}(q || p)}\]

<p>These relationships help bound how different the state distributions under $\pi$ and $\mu$ can be, which justifies the approximation we made earlier.</p>

<h2 id="advantage-weighted-regression-awr">Advantage Weighted Regression (AWR)</h2>

<table>
  <tbody>
    <tr>
      <td>AWR is the algorithm that arises when we choose $D_KL(\pi</td>
      <td>¬†</td>
      <td>\mu)$ as our divergence measure ‚Äî that is, we penalize $\pi$ for being too different from $\mu$ in the ‚Äúforward‚Äù KL direction.</td>
    </tr>
  </tbody>
</table>

<p>The key insight of AWR is that this constrained optimization problem ‚Äî maximizing the expected advantage while staying close to $\mu$ in KL divergence ‚Äî has an analytical solution. We don‚Äôt need to run gradient descent on this objective; we can directly write down what the optimal policy looks like.</p>

<p>To see why, let‚Äôs think about a simpler version of the problem. Suppose we want to find a distribution $p(x)$ that maximizes the expected value of some function $f(x)$, while staying close to some reference distribution $q(x)$ in KL divergence:</p>

\[\max_{p(x)} \mathbb{E}_{x \sim p(x)} [f(x)] - \alpha D_{KL}(p \| q)\]

<p>This is a standard problem in variational inference, and its solution is:</p>

\[p^*(x) \propto q(x) \exp\left(\frac{f(x)}{\alpha}\right)\]

<p>The optimal distribution is the reference distribution $q$, reweighted by the exponential of the function $f$, scaled by $1/\alpha$. Intuitively, this makes sense: we start from $q$ and upweight regions where $f$ is large (good actions) and downweight regions where $f$ is small (bad actions), with $\alpha$ controlling how aggressively we do this reweighting.</p>

<p>Applying this general solution to our policy optimization problem, where $p$ is $\pi$, $q$ is $\mu$, and $f(x)$ is $A^\mu(s,a)$, we get the optimal policy:</p>

\[\pi^*(a|s) \propto \mu(a|s) \exp\left(\frac{A^{\mu}(s, a)}{\alpha}\right)\]

<p>This is the AWR oracle solution. The optimal policy takes the behavior policy $\mu$ and reweights each action by the exponential of its advantage under $\mu$, scaled by the temperature parameter $\alpha$. Actions with high positive advantage (much better than average) get upweighted exponentially, while actions with negative advantage (worse than average) get downweighted exponentially.</p>

<p>The temperature $\alpha$ controls the sharpness of this reweighting. A very small $\alpha$ makes the exponential very peaked, concentrating all the probability mass on the single best action (approaching a deterministic greedy policy). A large $\alpha$ makes the exponential flat, keeping the policy close to $\mu$ regardless of the advantages (safe but slow improvement).</p>

<h3 id="how-do-we-actually-use-this-oracle-solution">How Do We Actually Use This Oracle Solution?</h3>

<table>
  <tbody>
    <tr>
      <td>The optimal policy $\pi^*(a</td>
      <td>s) \propto \mu(a</td>
      <td>s) \exp(A^\mu(s,a)/\alpha)$ is not directly a neural network we can deploy, it‚Äôs defined implicitly through this proportionality relationship. We need to find a way to fit a practical parameterized policy $\pi_\theta$ to this oracle.</td>
    </tr>
  </tbody>
</table>

<p>We use regression: we find the parameters $\theta$ that make $\pi_\theta$ as close as possible to $\pi^*$ by minimizing the KL divergence between them. This is equivalent to maximum likelihood estimation, where the ‚Äúlabels‚Äù are the exponential weights:</p>

\[\max_{\theta} \mathbb{E}_{s \sim d^{\mu}(s)} \left[ \mathbb{E}_{a \sim \mu(\cdot|s)} \left[ \exp\left(\frac{A^{\mu}(s,a)}{\alpha}\right) \log \pi_{\theta}(a|s) \right] \right]\]

<table>
  <tbody>
    <tr>
      <td>This is where the name ‚ÄúAdvantage Weighted Regression‚Äù comes from. We‚Äôre doing a weighted regression of $\log \pi_\theta(a</td>
      <td>s)$, where the weights are the exponentiated advantages $\exp(A^\mu(s,a)/\alpha)$. Actions that were particularly good (high advantage) contribute more to the regression loss, effectively pushing the policy to assign higher probability to those actions.</td>
    </tr>
  </tbody>
</table>

<h3 id="the-practical-awr-algorithm">The Practical AWR Algorithm</h3>

<p>Putting this all together, the AWR algorithm works as follows. First we collect data using the current behavior policy $\mu$ and store transitions in a replay buffer. We then train a critic to estimate the advantage function $A^\mu(s,a)$ from this data. Finally we update the policy by solving the weighted regression problem:</p>

\[\max_{\theta} \mathbb{E}_{(s,a) \sim \mathcal{D}} \left[ \exp\left(\frac{A^{\mu}(s,a)}{\alpha}\right) \log \pi_{\theta}(a|s) \right]\]

<p>where the expectation is taken over state-action pairs sampled from the replay buffer $\mathcal{D}$. This is just a weighted maximum likelihood problem ‚Äî we‚Äôre fitting the policy to the data, but upweighting good actions and downweighting bad ones according to their exponentiated advantages.</p>

<h3 id="why-awr-is-off-policy">Why AWR is Off-Policy</h3>

<p>Notice that nowhere in the final objective do we need to sample new actions from $\pi$. We use actions that were already stored in the replay buffer (sampled from $\mu$). This makes AWR genuinely off-policy: we can collect a large dataset using any behavior policy, train a critic on that dataset, and then improve the policy using weighted regression on the same dataset. We can even reuse old data from earlier versions of the policy, just like DQN‚Äôs replay buffer.</p>

<p>This is the major practical advantage of AWR over standard policy gradient methods. Standard REINFORCE or actor-critic methods must discard old data after each policy update (since the gradient estimates are only valid for the current policy). AWR can reuse all historical data, making it much more sample efficient.</p>

<h3 id="the-role-of-temperature-alpha">The Role of Temperature $\alpha$</h3>

<p>The temperature parameter $\alpha$ plays a crucial role that‚Äôs worth understanding deeply. When $\alpha$ is very small, the exponential weights become very large for positive advantage actions and very small for negative advantage ones. The policy update becomes aggressive ‚Äî it strongly concentrates probability on the best observed actions. This can lead to fast improvement but risks overfitting to the finite sample of observed actions in the replay buffer.</p>

<p>When $\alpha$ is large, the exponential weights are all close to 1, and the regression becomes nearly unifor, which means that the policy doesn‚Äôt change much. This is safe but slow.</p>

<p>In practice, $\alpha$ needs to be tuned carefully. Too small and the policy collapses to imitating a few high-advantage actions (losing diversity and exploration). Too large and the policy barely improves. The right value of $\alpha$ depends on the scale of the advantage function, which itself depends on the reward scale of the specific problem.</p>

<h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h2>

<p>PPO is one of the most widely used RL algorithms today, powering many real-world systems including early versions of ChatGPT‚Äôs RLHF training. It arises from a specific design choice in how we handle the off-policy optimization problem.</p>

<p>Recall that our general off-policy optimization problem (after approximating $d^\pi$ with $d^\mu$) is:</p>

\[\max_{\pi} \mathbb{E}_{s \sim d^{\mu}(s)} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [A^{\mu}(s, a)] \right] - \alpha D(\pi, \mu)\]

<table>
  <tbody>
    <tr>
      <td>AWR used $D_{KL}(\pi</td>
      <td>¬†</td>
      <td>\mu)$ as the divergence, which gave a nice closed-form solution. PPO instead uses $D_{KL}(\mu</td>
      <td>¬†</td>
      <td>\pi)$, the reverse KL, leading to a very different (but equally principled) algorithm. The full PPO objective is:</td>
    </tr>
  </tbody>
</table>

<aside>
üí°

$$
\max_{\theta} \mathbb{E}_{s \sim d^{\mu}(s)} \left[ \mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} [A^{\mu}(s, a)] \right] - \alpha D_{KL}(\mu(\cdot|s) \| \pi_{\theta}(\cdot|s))
$$

</aside>

<p>The inner expectation is over actions sampled from $\pi_\theta$, not from $\mu$. This means we need to sample new actions from our current policy at each state. But what if we want to learn from the actions that $\mu$ already took? This is where importance sampling comes in.</p>

<h3 id="importance-sampling">Importance Sampling</h3>

<p>Importance sampling is a technique for computing an expectation under one distribution using samples from a different distribution:</p>

\[\mathbb{E}_{x \sim p(x)}[f(x)] = \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)} f(x)\right]\]

<p>The ratio $p(x)/q(x)$ is called the importance weight. It corrects for the fact that we‚Äôre sampling from the wrong distribution. Applied to our policy gradient, if we want to compute an expectation under $\pi_\theta$ but we only have actions sampled from $\mu$, we can write:</p>

<aside>
üí°

$$
\mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} [A^{\mu}(s,a)] = \mathbb{E}_{a \sim \mu(\cdot|s)} \left[ \frac{\pi_{\theta}(a|s)}{\mu(a|s)} A^{\mu}(s,a) \right]
$$

</aside>

<p>This is the ‚Äúimportance-weighted policy gradient.‚Äù Now we can use actions already stored in our replay buffer (sampled from $\mu$) and just reweight them by the probability ratio. Let‚Äôs define this ratio for convenience:</p>

\[r(\theta) = \frac{\pi_{\theta}(a|s)}{\mu(a|s)}\]

<p>The full PPO objective (before clipping) becomes:</p>

\[\nabla_{\theta} \left[ \mathbb{E}_{s \sim d^{\mu}(s), a \sim \mu(\cdot|s)} \left[ \frac{\pi_{\theta}(a|s)}{\mu(a|s)} A^{\mu}(s,a) \right] - \alpha D_{KL}(\mu(\cdot|s) \| \pi_{\theta}(\cdot|s)) \right]\]

\[D_{KL}(\mu(\cdot|s) \| \pi_{\theta}(\cdot|s)) = \mathbb{E}_{a \sim \mu(\cdot|s)} \left[ \log \mu(a|s) - \log \pi_{\theta}(a|s) \right]\]

\[\nabla_{\theta} D_{KL}(\mu \| \pi_{\theta}) = -\mathbb{E}_{a \sim \mu(\cdot|s)} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) \right]\]

<h3 id="do-we-actually-need-the-kl-term">Do We Actually Need the KL Term?</h3>

<p>At this point, we might think: the KL divergence term is there for theoretical reasons (to keep the approximation valid), but is it strictly necessary in practice? The answer is nuanced.</p>

<p>Strictly speaking, no the KL term is not a fundamental part of the optimization; it‚Äôs a regularizer to prevent $\pi_\theta$ from drifting too far from $\mu$. But if we simply remove it and maximize the importance-weighted objective without any constraint, our policy updates can become very large, completely invalidating the $d^\pi \approx d^\mu$ approximation and causing catastrophic policy degradation.</p>

<p><strong>How does $D_{KL}$ prevent this?</strong></p>

<p>The KL term penalizes $\theta$ whenever $\pi_\theta$ deviates from $\mu$. When you take the gradient of the full objective and do gradient ascent, the gradient of the first term (importance-weighted advantage) pushes $\theta$ toward higher-advantage actions, potentially making large changes. The gradient of the KL term simultaneously pushes $\theta$ back toward $\mu$, resisting those large changes.</p>

<p>The parameter $\alpha$ controls the strength of this resistance. A large $\alpha$ means the KL penalty dominates, keeping $\pi_\theta$ very close to $\mu$. A small $\alpha$ lets the advantage term dominate, allowing larger updates. At any given gradient step, these two forces balance each other, and the equilibrium point is a policy that improves on the advantage objective but doesn‚Äôt stray too far from $\mu$.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Key Insight:</strong> notice that $D_{KL}(\mu</td>
      <td>¬†</td>
      <td>\pi_\theta)$ being small implies that $\pi_\theta$ and $\mu$ assign similar probabilities to all actions, which means the importance ratio $r(\theta) = \pi_\theta(a</td>
      <td>s)/\mu(a</td>
      <td>s)$ stays close to 1. So instead of explicitly computing and penalizing the KL divergence (which requires knowing $\mu$‚Äôs probabilities for all actions), we can equivalently constrain the importance ratio $r(\theta)$ to stay close to 1. This leads to the PPO clipping trick.</td>
    </tr>
  </tbody>
</table>

<h3 id="the-ppo-clip-objective">The PPO Clip Objective</h3>

<p>The practical PPO implementation replaces the KL penalty with a hard clip on the importance ratio:</p>

\[\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_{s \sim d^{\mu}(s), a \sim \mu(\cdot|s)} \left[ \text{clip}\left(\frac{\pi_{\theta}(a|s)}{\mu(a|s)}, 1-\varepsilon, 1+\varepsilon\right) \cdot A^{\mu} \right]\]

<p>where $\epsilon$ is a hyperparameter (typically 0.1 or 0.2) that controls how much the policy is allowed to change. The clip function simply caps the importance ratio: if $r(\theta) &lt; 1-\epsilon$, we use $1-\epsilon$; if $r(\theta) &lt; 1+\epsilon$, we use $1+\epsilon$; otherwise we use $r(\theta)$ as-is.</p>

<p><strong>Intuition:</strong> We‚Äôre saying ‚Äúwe‚Äôll only trust our importance-weighted estimates when $\pi_\theta$ and $\mu$ are reasonably close (within $\epsilon$ of each other). Beyond that, we don‚Äôt let the gradient push us further.‚Äù This prevents the runaway updates that would occur without the KL penalty, but in a computationally simpler and more stable way.</p>

<p>In practice, the actual PPO objective uses a min to ensure we take a conservative update:</p>

\[\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min\left( r(\theta) \cdot A^{\mu}, \; \text{clip}(r(\theta), 1-\varepsilon, 1+\varepsilon) \cdot A^{\mu} \right) \right]\]

<p>The min ensures that we never benefit from going outside the trust region, even if the clipped value happens to be larger than the unclipped value in certain cases.</p>

<h3 id="problems-with-the-clip-objective">Problems with the Clip Objective</h3>

<p>The clipping approach works well in practice but has some subtle issues worth understanding. Let‚Äôs think through what happens in different cases by considering the sign of the advantage and the size of the ratio.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Case 1:</strong> $r(\theta) &lt; 1-\epsilon$ and $A^\mu(s,a) &gt;0$. The action has low probability under $\pi_\theta$ relative to $\mu$, but it‚Äôs a good action (positive advantage). The clip fires and we use $r(\theta)\cdot A^\mu$ instead of $(1-\epsilon)¬∑A^\mu$. We still receive a gradient signal pushing us to increase $\pi_\theta(a</td>
      <td>s)$, and this is correct behavior.</td>
    </tr>
  </tbody>
</table>

<p><strong>Case 2:</strong> $r(\theta) &lt; 1-\epsilon$ and $A^\mu(s,a) &lt; 0$. The action has low probability under $\pi_\theta$, and it‚Äôs a bad action (negative advantage). We get no gradient signal here because of the min objective, since $\pi_\theta$ is already avoiding this and we use $(1-\epsilon)¬∑A^\mu$ instead of $r(\theta)\cdot A^\mu$. This is a good behavior.</p>

<p><strong>Case 3:</strong> $r(\theta) &gt; 1+\epsilon$ and $A^\mu(s,a) &lt; 0$. The action has high probability under $\pi_\theta$ but it‚Äôs a bad action. The clip fires and we use $(1+\epsilon)¬∑A^\mu$. We receive a gradient signal ‚Äúunlearning‚Äù this bad action, and hence this is correct behavior.</p>

<p><strong>Case 4:</strong> $r(\theta) &gt; 1+\epsilon$ and $A^\mu(s,a) &gt;0$. The action has high probability under $\pi_\theta$ and it‚Äôs a good action. The clip fires and we receive no gradient signal to further increase this action‚Äôs probability. This is a conservative choice ‚Äî we‚Äôre saying ‚Äúwe‚Äôve already increased this action‚Äôs probability enough; let‚Äôs not push further without fresh data.‚Äù This is what makes PPO safe, but it also hurts exploration: the policy won‚Äôt keep pushing toward good actions that it‚Äôs already committing to, which can slow learning.</p>

<p>The last case is particularly interesting because it represents a deliberate conservative choice: we‚Äôre not giving gradient signal to reinforce good actions that are already highly likely. This is justified theoretically (we don‚Äôt want to stray too far from the trust region), but can slow exploration in practice.</p>

<h3 id="the-asymmetric-clip-dapo">The Asymmetric Clip: DAPO</h3>

<p>The clipping issues above motivate a variant called the asymmetric clip, used in the DAPO algorithm (particularly relevant for LLM training). The idea is to use different $\epsilon$ values for the upper and lower clipping thresholds:</p>

\[\text{clip}\left(r(\theta), 1-\varepsilon_{\text{low}}, 1+\varepsilon_{\text{high}}\right) \quad \text{where} \quad \varepsilon_{\text{high}} &gt; \varepsilon_{\text{low}}\]

<p>By using a larger $\varepsilon_{\text{high}}$ than $\varepsilon_{\text{low}}$, we allow the policy to more aggressively increase probability on good actions (larger trust region in the ‚Äúincrease‚Äù direction) while being more conservative about decreasing probability on bad actions (smaller trust region in the ‚Äúdecrease‚Äù direction). This asymmetry encourages more exploration by reducing the cases where we get no gradient signal for good actions.</p>

<h2 id="option-2-off-policy-actor-critic-with-a-learned-q-function">Option 2: Off-Policy Actor-Critic with a Learned Q-Function</h2>

<p>Instead of using $A^\mu$ in our objective, what if we used $A^\pi$, the advantage of the current policy $\pi$? This requires learning $Q^\pi$ off-policy. The objective becomes:</p>

\[\max_{\theta} \mathbb{E}_{s, a \sim \mu} [Q^{\pi}_{\phi}(s, a)]\]

<p>The crucial insight here is that $Q^\pi$ can be learned from any data in the replay buffer because the Bellman equation for $Q^\pi$ doesn‚Äôt require on-policy data. We train $Q^\pi_\phi$ using the standard TD loss:</p>

\[\mathcal{L}(\phi) = \mathbb{E}_{s, a, r, s' \sim \text{RB}} \left[ \left( Q_{\phi}(s,a) - y(s,a) \right)^2 \right]\]

<p>where the target $y(s,a)$ uses the current policy $\pi_\theta$ to select the next action:</p>

\[y(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot|s,a), a' \sim \pi_{\theta}(\cdot|s')} [Q^{\text{target}}_{\bar{\phi}}(s', a')]\]

<table>
  <tbody>
    <tr>
      <td>In practice, we compute this by sampling $a‚Äô \sim \pi_\theta(\cdot</td>
      <td>s‚Äô)$ from the current policy and querying the target network $Q^{\text{target}}_{\bar{\phi}}(s‚Äô, a‚Äô)$. All the machinery we learned from value-based methods applies directly: target networks, soft/hard target updates, replay buffers, and so on. This is the foundation of DDPG and TD3, which we‚Äôll cover next.</td>
    </tr>
  </tbody>
</table>

<p><strong>What are $\pi$ and $\mu$?</strong></p>

<ul>
  <li>$\mu$ is the behavior policy ‚Äî the policy that actually interacts with the environment and collects data. It‚Äôs the policy whose transitions <code class="language-plaintext highlighter-rouge">(s, a, r, s')</code> get stored in the replay buffer.</li>
  <li>$\pi_\theta$ is the target policy ‚Äî the policy we‚Äôre trying to optimize. It‚Äôs a neural network parameterized by $\theta$ that we‚Äôre improving through gradient ascent on the Q-function objective. Crucially, $\pi_\theta$ does NOT directly collect data. Instead, it participates in computing the Bellman target by selecting actions at the next state $s‚Äô$.</li>
</ul>

<h3 id="why-is-this-off-policy">Why Is This Off-Policy?</h3>

<p>This looks like standard Q-learning because the critic training is essentially the same TD loss. But the crucial difference is that the data in the replay buffer was collected by $\mu$, not by $\pi_\theta$. In on-policy methods, every gradient update must use fresh data from the current policy ‚Äî you cannot reuse old data because the gradient estimates would be biased. Here, we‚Äôre explicitly reusing old data collected by $\mu$, which makes it off-policy.</p>

<p>The reason this works is that the Bellman equation for $Q^\pi$ holds for any state-action pair, regardless of which policy generated the data. When we train the critic using transitions from the replay buffer, we‚Äôre just supervising $Q_\phi(s,a)$ toward the TD target, and this supervision is valid regardless of how $(s,a)$ was generated. The states and actions just need to be real experiences from the environment, and don‚Äôt need to come from $\pi_\theta$ itself.</p>

<p>The policy update is also off-policy for the same reason. When we maximize $\mathbb{E}\left[Q_\phi(s, \pi_\theta(s))\right]$, the states $s$ come from $\mu$‚Äôs replay buffer, not from $\pi_\theta$‚Äôs own trajectories. We‚Äôre asking <em>‚Äúgiven states that $\mu$ visited, what action would $\pi_\theta$ take, and how good does the critic think that action is?‚Äù</em> This is an off-policy evaluation of $\pi_\theta$ using $\mu$‚Äôs state distribution.</p>

<h3 id="the-subtle-difference-from-pure-q-learning">The Subtle Difference from Pure Q-Learning</h3>

<p>In pure Q-learning (like DQN), there‚Äôs no separate actor network ‚Äî the policy is just the implicit argmax of the Q-function. Here, we have an explicit policy network $\pi_\theta$ that we‚Äôre optimizing separately. This matters because in continuous action spaces, you can‚Äôt take an argmax over infinitely many actions. Instead, you maintain $\pi_\theta$ as a differentiable function and backpropagate through $Q_\phi$ into $\pi_\theta$ to push the policy toward higher-value actions. This is the actor-critic structure: the critic ($Q_\phi$) evaluates actions and the actor ($\pi_\theta$) is optimized to maximize those evaluations, all using off-policy data from $\mu$ stored in the replay buffer.</p>

<h3 id="why-do-we-maximize-qpi-and-not-api">Why do we maximize $Q^\pi$ and not $A^\pi$?</h3>

<p>Recall the definition of advantage:</p>

\[A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)\]

<p>Now when we write the policy optimization objective, we‚Äôre taking an expectation over states $s$ and actions $a \sim \pi_\theta$:</p>

\[\max_{\theta} \mathbb{E}_{s, a \sim \pi_{\theta}} \left[ A^{\pi}(s, a) \right] = \max_{\theta} \mathbb{E}_{s, a \sim \pi_{\theta}} \left[ Q^{\pi}(s, a) - V^{\pi}(s) \right]\]

<p>Now look at the $V^\pi(s)$ term. It depends only on $s$, not on the action $a$. Since we‚Äôre optimizing over $\theta$, which only affects how actions are chosen, not which states we visit (we‚Äôre using states from $\mu$‚Äôs replay buffer), $V^\pi(s)$ is just a constant with respect to the optimization over $\theta$. Subtracting a constant from an objective doesn‚Äôt change where the maximum is. So:</p>

\[\max_{\theta} \mathbb{E}_{s, a \sim \pi_{\theta}} \left[ Q^{\pi}(s, a) - V^{\pi}(s) \right] = \max_{\theta} \mathbb{E}_{s, a \sim \pi_{\theta}} \left[ Q^{\pi}(s, a) \right]\]

<p>They have the same maximizer. The policy that maximizes expected Q-value is identical to the policy that maximizes expected advantage.</p>

<h2 id="ddpg-and-td3-off-policy-actor-critic-for-continuous-actions"><strong>DDPG and TD3: Off-Policy Actor-Critic for Continuous Actions</strong></h2>

<p>These algorithms are specifically designed for continuous action spaces, where taking the argmax over actions (as in DQN) is intractable.</p>

<p>The key idea is to maintain both an explicit policy network $\pi_\theta$ (the actor) and a Q-function $Q_\phi$ (the critic), and alternate between updating them. The critic is trained using the TD loss above, and the actor is updated to maximize the Q-values:</p>

\[\max_{\theta} \mathbb{E}_{s \sim \text{RB}} [Q_{\phi}(s, \pi_{\theta}(s))]\]

<p>Since $\pi_\theta(s)$ is differentiable with respect to $\theta$, we can backpropagate through the Q-function all the way into the policy. This is called the deterministic policy gradient. The policy just shifts its output to wherever the Q-function says is best, guided by gradient ascent through the critic.</p>

<p>Both DDPG and TD3 inherit all the stability mechanisms we‚Äôve discussed: replay buffers to decorrelate data, target networks to stabilize the Q-function training, and soft target updates to prevent sudden shifts. TD3 additionally incorporates ideas like Double DQN (using two Q-networks to reduce overestimation bias) and delayed policy updates (updating the actor less frequently than the critic, giving the critic time to settle before the policy chases it).</p>

<h3 id="the-deterministic-policy-gradient-how-backprop-works-through-the-critic">The Deterministic Policy Gradient: How Backprop Works Through the Critic</h3>

<p>Now, the key question is: how exactly do we compute $\nabla_\theta \mathbb{E}[Q^\pi_\phi(s, a)]$ when $a$ comes from $\pi_\theta$? If the policy is deterministic, meaning it outputs a single action $a_\theta(s)$ rather than a distribution, then we can apply the chain rule directly:</p>

\[\nabla_{\theta} Q^{\pi}_{\phi}(s, a_{\theta}(s)) = \nabla_{a} Q^{\pi}_{\phi}(s, a) \big|_{a = a_{\theta}(s)} \cdot \nabla_{\theta} a_{\theta}(s)\]

<p>This is the deterministic policy gradient.</p>

<p><strong>Interpretation:</strong> First, ask the critic ‚Äúwhich direction should I push the action to increase $Q$?‚Äù (that‚Äôs $\nabla_a Q$), and then ask the policy ‚Äúhow does changing $\theta$ change the action?‚Äù (that‚Äôs $\nabla_\theta a_\theta(s)$). Multiply these together via the chain rule and you have a gradient that flows all the way from the Q-function back into the policy parameters. This is exactly what DDPG uses ‚Äî the gradient signal travels from the Q-network, through the action, into the policy network.</p>

<p><strong>What is the policy is stochastic?</strong></p>

<table>
  <tbody>
    <tr>
      <td>If the policy is stochastic instead, outputting a Gaussian distribution $\pi_\theta(\cdot</td>
      <td>s) = \mathcal{N}(\mu_\theta(s), \sum_\theta(s))$, then we can‚Äôt directly differentiate through sampling. Instead, we use the reparameterization trick: sampling $a \sim \pi_\theta(\cdot</td>
      <td>s)$ is equivalent to sampling $\epsilon \sim \mathcal{N}(0, I)$ and computing $a = \pi_\theta(s) + \epsilon \cdot \sigma_\theta(s)$. Now $a$ is a deterministic function of $\epsilon$ and $\theta$, so we can backpropagate through it. This is what SAC uses.</td>
    </tr>
  </tbody>
</table>

<h3 id="ddpg-and-td3-the-full-algorithm">DDPG and TD3: The Full Algorithm</h3>

<p>With the deterministic policy gradient in hand, we can now describe DDPG and its improved variant TD3. The core structure has two interacting components: a deterministic policy network $\pi_\theta(s)$ (the actor) and a Q-function network $Q_\phi(s,a)$ (the critic).</p>

<p><strong>Training the Critic:</strong> We sample transitions <code class="language-plaintext highlighter-rouge">(s, a, r, s')</code> from the replay buffer and minimize the TD loss:</p>

\[\mathcal{L}(\phi) = \mathbb{E}_{s,a,r,s' \sim \text{RB}} \left[ \left( Q_{\phi}(s,a) - y(s,a) \right)^2 \right]\]

<p>where the target is computed using the target networks and the current policy:</p>

\[y(s,a) = r(s,a) + \gamma \min\left( Q^{\text{target}}_{\phi_1}(s', a'), Q^{\text{target}}_{\phi_2}(s', a') \right), \quad a' \leftarrow \pi_{\theta}(s') + \varepsilon\]

<p>Notice two things here. First, we use two Q-networks $Q_{\phi1}$ and $Q_{\phi2}$ and take the minimum of their predictions when computing the target. This is the Double Q-learning trick borrowed from DQN ‚Äî it directly addresses overestimation bias, because if either network overestimates, the $\min$ operator selects the more conservative (lower) estimate. Second, we add a small noise $\epsilon$ to the next action $a‚Äô$ when computing the target. This is called target policy smoothing and prevents the policy from exploiting sharp Q-function peaks that might be artifacts of function approximation error.</p>

<p><strong>Data Collection:</strong> Data is collected by running the deterministic policy with added exploration noise:</p>

\[a \sim \pi_{\theta}(s) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)\]

<p>Since a deterministic policy always outputs the same action for a given state, it would never explore without this noise. The noise provides the necessary stochasticity during data collection.</p>

<p><strong>Training the Actor:</strong> The policy is updated to maximize the Q-function using the deterministic policy gradient:</p>

\[\theta' \leftarrow \theta + \eta \nabla_{\theta} \mathbb{E}_{s \sim \text{RB}} \left[ Q_{\phi_1}(s, \pi_{\theta}(s)) \right]\]

<p>One important detail in TD3: the actor is updated less frequently than the critic, typically once every $d$ critic updates ($d=2$ is common). The motivation is to let the critic stabilize before the policy chases after it. If the policy updates as fast as the critic, it might end up chasing noisy or inaccurate Q-value estimates, destabilizing training.</p>

<h3 id="is-there-a-separate-policy-mu">Is there a separate policy $\mu$?</h3>

<p>Yes and no. There is no separate policy $\mu$ that‚Äôs distinct from $\pi_\theta$. Instead, $\mu$ is just an older or noisier version of $\pi_\theta$ itself.</p>

<p>In DDPG/TD3, the ‚Äúbehavior policy‚Äù $\mu$ that collects data is simply:</p>

\[\mu(a|s) = \pi_{\theta_{\text{old}}}(s) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)\]

<p>where $\pi_{\theta_{\text{old}}}$ is the policy from some previous iteration (or even the current iteration), and $\epsilon$ is exploration noise. So $\mu$ is not a separately maintained network ‚Äî it‚Äôs the actor network $\pi_\theta$, possibly from a few steps ago, with Gaussian noise added for exploration.</p>

<p><strong>The Replay Buffer Perspective</strong></p>

<p>The replay buffer in DDPG/TD3 contains transitions <code class="language-plaintext highlighter-rouge">(s, a, r, s')</code> that were collected by running $\pi_\theta + \epsilon$ at various points during training. So the $\mu$ we keep referring to is really a mixture of past versions of $\pi_\theta$, all with exploration noise added.</p>

<p>When we say ‚Äústates come from $d^\mu$,‚Äù we mean: the states in the replay buffer were visited by various historical versions of $\pi_\theta$ (with noise), and we‚Äôre reusing those states to train the current $\pi_\theta$.</p>

<p><strong>Why This Works (Off-Policy Nature)</strong></p>

<p>The key insight is that DDPG/TD3 don‚Äôt care which policy generated the data in the replay buffer, as long as:</p>

<ol>
  <li>The Q-function can be learned from that data (which it can, because Bellman equations hold for any state-action pairs)</li>
  <li>The states cover regions where the current policy $\pi_\theta$ actually operates (which they do if $\pi_\theta$ hasn‚Äôt changed too drastically)</li>
</ol>

<p>So unlike PPO where $\mu$ is explicitly the ‚Äúold policy‚Äù from the previous iteration and we carefully track the distribution mismatch with importance ratios, in DDPG/TD3 we just say ‚Äú$\mu$ is whatever policy (or mixture of policies) generated the data in the replay buffer‚Äù and we don‚Äôt worry about correcting for distribution shift ‚Äî we just trust that the off-policy learning will work as long as the data is reasonably diverse and recent.</p>

<p><strong>Comparison to AWR/PPO</strong></p>

<p>To clarify the distinction:</p>

<ul>
  <li>PPO/AWR: $\mu$ is explicitly defined as $\pi_{\theta_{\text{old}}}$ from the previous iteration. We track it carefully and either use importance sampling (PPO) or solve a constrained optimization (AWR) to account for the distribution shift.</li>
  <li>DDPG/TD3: $\mu$ is implicitly ‚Äúwhatever generated the replay buffer data.‚Äù We don‚Äôt explicitly track it or correct for it ‚Äî we just assume the off-policy Q-learning will handle the distribution shift automatically.</li>
</ul>

<p>This is one reason DDPG/TD3 can be less stable than SAC. There‚Äôs no explicit mechanism to keep $\pi_\theta$ close to the data distribution, so if the policy changes too quickly, it can end up in regions of the state-action space where the Q-function is poorly trained, leading to bad updates. SAC‚Äôs entropy regularization helps with this by keeping the policy ‚Äúspread out‚Äù and thus naturally staying closer to the data distribution.</p>

<h2 id="soft-actor-critic-sac">Soft Actor-Critic (SAC)</h2>

<p>SAC represents a conceptually deeper departure from DDPG/TD3 than just an engineering improvement. It is built on a fundamentally different objective called Maximum Entropy RL.</p>

<h3 id="what-is-maximum-entropy-rl">What is Maximum Entropy RL?</h3>

<p>Standard RL seeks a policy that maximizes expected cumulative reward:</p>

\[\pi^*_{\text{RL}} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_t \gamma^t r(s_t, a_t) \right]\]

<p>Maximum Entropy RL adds a bonus for the policy being uncertain (i.e., having high entropy):</p>

\[\pi^*_{\text{maxent}} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t \left( r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \right) \right]\]

<table>
  <tbody>
    <tr>
      <td>where $\mathcal{H}(\pi(\cdot</td>
      <td>s_t))$ is the entropy of the policy‚Äôs action distribution at state $s_t$:</td>
    </tr>
  </tbody>
</table>

\[\mathcal{H}(\pi(\cdot|s_t)) = \mathbb{E}_{a \sim \pi(\cdot|s_t)} \left[ -\log \pi(\cdot|s_t) \right]\]

<p>So at every time step, the policy is rewarded not just for taking high-reward actions, but also for being as random as possible ‚Äî for spreading probability across many actions rather than collapsing onto a single one. The temperature $\alpha$ controls this trade-off: large $\alpha$ means exploration is heavily rewarded, small $\alpha$ recovers standard RL.</p>

<p><strong>Intuition:</strong> Think of standard RL as a student who memorizes only the answer to a specific exam problem. MaxEnt RL is like a student who tries to understand all plausible solutions to a problem ‚Äî even if one solution is clearly best, it maintains some understanding of other approaches. This breadth of knowledge turns out to be crucial for generalization and robustness.</p>

<h3 id="simplifying-the-maxent-objective">Simplifying the MaxEnt Objective</h3>

<table>
  <tbody>
    <tr>
      <td>The entropy term can be pulled inside the sum and absorbed into a modified reward function. Since $\mathcal{H}(\pi(\cdot</td>
      <td>s_t)) = \mathbb{E}_{a \sim \pi(\cdot</td>
      <td>s_t)} \left[ -\log \pi(\cdot</td>
      <td>s_t) \right]$, we can write the whole objective as:</td>
    </tr>
  </tbody>
</table>

\[\pi^*_{\text{maxent}} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t \left( r(s_t, a_t) - \alpha \log \pi(a_t|s_t) \right) \right]\]

<p>This gives us a modified reward:</p>

\[\tilde{r}_{\alpha}(s, a) = r(s, a) - \alpha \log \pi(a|s)\]

<table>
  <tbody>
    <tr>
      <td>This is an elegant simplification: we‚Äôve turned MaxEnt RL into standard RL, just with a reward function that includes a penalty for being too deterministic. The $-\alpha \log \pi(a</td>
      <td>s)$ term acts as an intrinsic reward for exploration ‚Äî actions that the policy assigns low probability to (meaning they‚Äôre surprising/exploratory) have a large $-\log \pi$ value and thus contribute more intrinsic reward.</td>
    </tr>
  </tbody>
</table>

<p>Backup the entropy: the entropy bonus at each time step gets backed up through the Bellman equation, just like rewards do, so the Q-function automatically accounts for future entropy bonuses throughout the trajectory.</p>

<h3 id="the-sac-q-function">The SAC Q-Function</h3>

<p>Substituting the modified reward into the Bellman equation gives SAC‚Äôs Q-function update:</p>

\[Q(s, a) \leftarrow r(s,a) + \gamma \mathbb{E}_{a' \sim \pi} \left[ Q(s', a') - \alpha \log \pi(a'|s') \right]\]

<table>
  <tbody>
    <tr>
      <td>Compare this to TD3‚Äôs Bellman update, which just has $r(s,a) + \gamma \mathbb{E}[Q(s‚Äô,a‚Äô)]$. SAC‚Äôs update additionally subtracts $\alpha \log \pi(a‚Äô</td>
      <td>s‚Äô)$ from the target, which means Q-values are lower for actions that the policy takes with high probability. This elegantly discourages the policy from collapsing onto a single action even when it‚Äôs the best-reward action.</td>
    </tr>
  </tbody>
</table>

<h3 id="sacs-policy-update">SAC‚Äôs Policy Update</h3>

<p>For the policy update, SAC also benefits from the MaxEnt objective. The gradient update for the policy becomes:</p>

\[\theta' \leftarrow \theta + \eta \nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} \left[ Q_{\phi}(s, a) - \alpha \log \pi_{\theta}(a|s) \right]\]

<p>The policy is now simultaneously trying to maximize Q-values (exploit good actions) and maximize entropy (stay diverse and exploratory). These are competing objectives kept in balance by $\alpha$. When $\alpha$ is large, the policy spreads out; when $\alpha$ is small, it concentrates on high-Q actions. This balance is what gives SAC its remarkable stability. Unlike DDPG which can suddenly collapse its policy onto a single action, SAC always maintains a floor of exploration.</p>

<h3 id="practical-design-choices-in-sac">Practical Design Choices in SAC</h3>

<p>SAC makes three key practical design decisions that distinguish it from DDPG/TD3.</p>

<ol>
  <li>
    <p>Like TD3, SAC uses two critic networks and takes the minimum of their Q-value estimates. This is critical in SAC because the policy is stochastic and is being trained to maximize Q-values. If either Q-network overestimates, the policy will exploit that overestimation and degrade. The $\min$ operator provides a conservative lower bound:</p>

\[Q_{\text{target}} = r + \gamma \mathbb{E}_{a'} \left[ \min(Q_{\phi_1}(s', a'), Q_{\phi_2}(s', a')) - \alpha \log \pi(a'|s') \right]\]
  </li>
  <li>SAC requires no explicit exploration noise during data collection, unlike DDPG/TD3 which must add Gaussian noise to a deterministic policy. Because $\pi_\theta$ is inherently stochastic (a Gaussian whose variance is also learned), every action sampled from it naturally has uncertainty. The entropy term in the objective actively encourages this uncertainty to remain non-negligible throughout training, so exploration emerges automatically from the policy itself.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The temperature $\alpha$ is dynamically tuned during training. Rather than treating $\alpha$ as a fixed hyperparameter, SAC frames it as a constrained optimization: we want the policy to maintain at least a target level of entropy $\mathcal{H}_{\text{target}}$ (which is typically set to $-</td>
          <td>\mathcal{A}</td>
          <td>$, the negative dimensionality of the action space). The value of $\alpha$ is then automatically adjusted to ensure this constraint is met. If the policy‚Äôs entropy falls below the target, $\alpha$ increases to encourage more exploration; if it‚Äôs above the target, $\alpha$ decreases to let the policy specialize.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<h3 id="why-does-adding-entropy-help">Why Does Adding Entropy Help?</h3>

<p>The entropy bonus has two deeply important practical benefits. First, a stochastic policy is simply easier to optimize than a deterministic one. The optimization landscape is smoother because the policy‚Äôs output is a distribution, not a point ‚Äî gradients flow more reliably and the policy doesn‚Äôt get stuck in sharp local optima as easily. Second, and more fundamentally, the entropy bonus creates ‚Äúmore ways to succeed.‚Äù Rather than converging to a single behavior that achieves high reward, MaxEnt RL finds a distribution over behaviors that are all approximately equally good. If the environment has multiple valid strategies, SAC discovers all of them. This makes SAC much more robust: if one strategy stops working (due to environment changes or unseen situations), the policy still knows about other strategies and can switch.</p>

<p>Think of it like this: a deterministic policy is like a chess player who has memorized one perfect opening. A MaxEnt policy is like a chess player who has mastered many different openings. Against a novel opponent, the second player is far more adaptable.</p>

<h2 id="summary-ddpg-vs-td3-vs-sac">Summary: DDPG vs TD3 vs SAC</h2>

<p>All three algorithms share the same fundamental structure, an off-policy actor-critic with a replay buffer and target networks, but differ in key design choices.</p>

<ul>
  <li>DDPG uses a deterministic policy with a single Q-network, making it simple but prone to overestimation and instability.</li>
  <li>TD3 fixes DDPG‚Äôs instability with two Q-networks (clipped double Q), delayed policy updates, and target policy smoothing.</li>
  <li>SAC goes further by switching to a stochastic policy with a MaxEnt objective, which provides natural exploration, better optimization landscapes, and automatic entropy tuning, but at the cost of slightly more complexity.</li>
</ul>

<p>In practice, SAC tends to be the most reliable of the three, which is why it remains a dominant algorithm for continuous control tasks today.</p>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="learning" /><category term="rl" /><summary type="html"><![CDATA[Finite Markov Decision Process]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introduction to Reinforcement Learning</title><link href="https://bhaswanth-a.github.io//posts/intro-to-rl/" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning" /><published>2026-01-19T11:00:00-05:00</published><updated>2026-02-20T20:40:50-05:00</updated><id>https://bhaswanth-a.github.io//posts/intro-to-rl</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/intro-to-rl/"><![CDATA[<p><em>In Progress</em></p>

<p>This blog is a collection of my notes based on the book ‚ÄúReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto‚Äù.</p>

<h1 id="finite-markov-decision-processes">Finite Markov Decision Processes</h1>

<h2 id="context">Context</h2>

<p><img src="Introduction%20to%20Reinforcement%20Learning%202530b449d41980ac9b8cc4af04f345ba/image.png" alt="The agent-environment interaction" /></p>

<p>The agent-environment interaction</p>

<p>Reinforcement learning is built around the interaction between an agent and an environment over time. At each step:</p>

<ul>
  <li>The agent observes the current situation (state).</li>
  <li>It chooses an action based on a policy.</li>
  <li>The environment responds by providing a reward and transitioning to a new state.</li>
</ul>

<p>This back-and-forth loop defines the learning problem.</p>

<p>Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized.</p>

<aside>
üí°

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots¬†= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

</aside>

<p>where $0 \le \gamma \le 1,$  is called the discount rate.</p>

<p>The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. If $\gamma = 0,$  the agent is ‚Äúmyopic‚Äù in being concerned only with maximizing immediate rewards.</p>

<p>In case of episodic tasks that have a ‚Äútermination state‚Äù, the discounted rewards can be written as</p>

<aside>
üí°

$$
G_t =  \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
$$

</aside>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>In the most general case, the environment‚Äôs response at time $t+1$ can depend on everything that has happened earlier. Thus, the dynamics can be defined by the complete probability distribution:</p>

\[\Pr\{ R_{t+1} = r , S_{t+1} = s' \mid S_0, A_0, R_1, \ldots, S_{t-1}, A_{t-1}, R_t, S_t, A_t \}\]

<p>If the state signal has the Markov property, then the environment‚Äôs response at time $t+1$ depends only on the current state and action. In this case, the environment‚Äôs dynamics are simplified to:</p>

\[p(s', r \mid s, a) = \Pr\{ R_{t+1} = r,  S_{t+1} = s' \mid S_t = s, A_t = a \}\]

<p>Given these dynamics, we can compute everything else we might want to know about the environment.</p>

<p><strong>Expected reward for a state‚Äìaction pair:</strong></p>

<aside>
üí°

$$
r(s,a)= \mathbb{E}[R_{t+1}\mid S_t=s, A_t=a]  = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} p(s',r\mid s,a).
$$

</aside>

<p>From definition of conditional expectation</p>

\[\mathbb{E}[R_{t+1}\mid S_t=s, A_t=a] = \sum_{r\in\mathcal{R}} r \Pr\{R_{t+1}=r \mid S_t=s, A_t=a\}\]

<p>Using law of total probability</p>

\[\Pr\{R_{t+1}=r \mid S_t=s, A_t=a\}

= \sum_{s'\in\mathcal{S}} \Pr\{S_{t+1}=s', R_{t+1}=r \mid S_t=s, A_t=a\} \\

= \sum_{s'\in\mathcal{S}} p(s',r\mid s,a).\]

<p>Plugging back in, we get</p>

\[r(s,a)=   \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} p(s',r\mid s,a)\]

<p><strong>State transition probability:</strong></p>

<aside>
üí°

$$
p(s'\mid s,a)
= \sum_{r\in\mathcal{R}} \Pr\{S_{t+1}=s', R_{t+1}=r \mid S_t=s, A_t=a\}

= \sum_{r\in\mathcal{R}} p(s',r\mid s,a).
$$

</aside>

<p><strong>Expected rewards for state‚Äìaction‚Äìnext-state triples:</strong></p>

<aside>
üí°

$$
r(s,a,s')  = \mathbb{E}[R_{t+1}\mid S_t=s, A_t=a, S_{t+1}=s']

= \frac{\sum_{r\in\mathcal{R}} r p(s',r\mid s,a)}{p(s'\mid s,a)}
$$

</aside>

<p>Start from conditional expectation</p>

\[r(s,a,s') = \mathbb{E}[R_{t+1}\mid S_t=s, A_t=a, S_{t+1}=s'] \\  =\sum_{r\in\mathcal{R}} r \Pr\{R_{t+1}=r \mid S_t=s, A_t=a, S_{t+1}=s'\}\]

<p>Then using Bayes‚Äô rule</p>

\[\Pr\{R_{t+1}=r \mid S_t=s, A_t=a, S_{t+1}=s'\}

=\frac{\Pr\{S_{t+1}=s', R_{t+1}=r \mid S_t=s, A_t=a\}}{\Pr\{S_{t+1}=s' \mid S_t=s, A_t=a\}}

=\frac{p(s',r\mid s,a)}{p(s'\mid s,a)}.\]

<p>Plugging this in above, we get</p>

\[r(s,a,s') 
= \frac{\sum_{r\in\mathcal{R}} r p(s',r\mid s,a)}{p(s'\mid s,a)}\]

<h2 id="value-functions">Value Functions</h2>

<h3 id="state-value-function">State-Value Function</h3>

<aside>
üí°

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid S_t = s]

= \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s \right]
$$

</aside>

<p>This is the expected long-term return (sum of discounted rewards) if you start in state $s$ and follow policy $\pi$.</p>

<p><em><strong>Intuition:</strong> ‚ÄúIf I‚Äôm standing in this state, and I keep behaving according to my current policy, how good is this situation in the long run?‚Äù</em></p>

<h3 id="action-value-function">Action-Value Function</h3>

<aside>
üí°

$$
q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t \mid S_t = s, A_t = a]

= \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s, A_t = a \right]
$$

</aside>

<p>This is the expected long-term return if you start in state $s,$ take action $a$ immediately, and then afterwards follow policy $\pi$.</p>

<p><em><strong>Intuition:</strong> ‚ÄúIf I‚Äôm in this state and I try this particular move right now, and then keep following my usual policy, how good will things turn out?‚Äù</em></p>

<h3 id="relationship-between-state-value-and-action-value-functions"><strong>Relationship between state-value and action-value functions</strong></h3>

<aside>
üí°

$$
v_\pi(s) = \sum_a\pi(a|s)\space q_\pi(s,a)
$$

</aside>

<p>The value functions $v_\pi$ and $q_\pi$ can be estimated from experience. For example, if an agent follows policy $\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state‚Äôs value, $v_\pi(s),$ as the number of times that state is encountered approaches infinity. If separate averages are kept for each action taken in a state, then these averages will similarly converge to the action values, $q_\pi(s,a).$  These estimation methods are called Monte Carlo methods, and are discussed in the sections below.</p>

<h3 id="bellman-equation">Bellman Equation</h3>

\[v_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid S_t = s] =  \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t = s]\]

<p>Consider random variables $X,Y,Z$</p>

\[\mathbb{E}[X \mid Z] = \sum_y \mathbb{E}[X \mid Y=y, Z] \space \Pr(Y=y \mid Z)\]

<p>Let</p>

\[X = R_{t+1} + \gamma G_{t+1}, \quad Y = A_t, \quad Z = \{ S_t=s \}\]

<p>Then</p>

\[\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}  \mid S_t=s] = \sum_a \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}  \mid S_t=s, A_t=a]  \Pr(A_t=a \mid S_t=s) \\ v_\pi(s)
 = \sum_a \pi(a \mid s) \space  \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}  \mid S_t=s, A_t=a]\]

<p>Applying the same law again</p>

\[\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t=s, A_t=a]
\\
= \sum_{s'} \sum_r \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] \space  p(s',r \mid s,a)\]

<p>Given $R_{t+1} = r$ we know the first term is just $r.$</p>

<p>For the second term, by Markov property, the future return $G_{t+1}$ depends on the future only through $S_{t+1},$ so</p>

\[\mathbb{E}_{\pi}[G_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] = \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1}=s']\]

<p>Therefore the inner expectation becomes</p>

\[r + \gamma \space \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1}=s']\]

<p>Plugging this back in, we get</p>

\[v_{\pi}(s) = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1}=s']  \Big]\]

<p>Finally, recognise that $\mathbb{E}<em>{\pi}[ G</em>{t+1} \mid S_{t+1}=s‚Äô]  = v_\pi(s‚Äô)$. So we have</p>

<aside>
üí°

$$
v_{\pi}(s) = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_{\pi}(s') \Big]
$$

</aside>

<p><strong>Intuition:</strong></p>

<p>In a nutshell, the Bellman equation says that:  The value of a state = immediate reward + future value. But instead of computing everything into the infinite future directly, it breaks the problem down recursively.</p>

<p>The value of a state $v_\pi(s)$ is obtained by:</p>

<ol>
  <li>Looking at all possible actions $a$ that policy $\pi$ might choose</li>
  <li>For each action, look at all possible next states $s‚Äô$ and rewards $r$ that the environment could produce</li>
  <li>Weight each outcome by how likely it is</li>
  <li>Add up the immediate reward $r$ plus the discounted value of the next state $\gamma v_\pi(s‚Äô)$</li>
</ol>

<h2 id="optimal-value-functions">Optimal Value Functions</h2>

<p>There exists at least one policy that is better than or equal to all other policies. This is called the optimal policy.</p>

<p><strong>Optimal state-value function:</strong></p>

<aside>
üí°

$$
v_*(s) = \max_{\pi} v_\pi(s)
$$

</aside>

<p>for all $s \in S$.</p>

<p><strong>Optimal action-value function:</strong></p>

<aside>
üí°

$$
q_*(s,a) = \max_\pi q_\pi(s,a)
$$

</aside>

<p>for all $s \in S, \space a \in \mathcal{A}(s).$</p>

<p>For the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_<em>$ in terms of $v_</em>$ as:</p>

<aside>
üí°

$$
q_*(s,a) = \mathbb{E}_{\pi}[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_{t}=s, A_t=a] 
$$

</aside>

<h3 id="bellman-optimality-equation">Bellman Optimality Equation</h3>

<p><strong>Bellman Optimality for state-value function:</strong></p>

<p>It expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.</p>

\[{v_{*}(s)} = \max_{a \in \mathcal{A}(s)} q_{\pi_{}}(s,a) \newline = \max_{a} \mathbb{E}_{\pi_{*}}\left[G_t \middle| S_t = s, A_t = a \right] \newline 
= \max_{a} \mathbb{E}_{\pi_{*}}\left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s, A_t = a \right] \newline
= \max_{a} \mathbb{E}_{\pi_{*}}\left[ R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2} \middle| S_t = s, A_t = a \right] \newline
= \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_t = s, A_t = a] \newline
= \max_{a \in \mathcal{A}(s)} \sum_{s',r} p(s',r \mid s,a) \big[ r + \gamma v_{*}(s') \big]\]

<p>So,</p>

<aside>
üí°

$$
v_*(s)= \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_t = s, A_t = a]\\ = \max_{a \in \mathcal{A}(s)} \sum_{s',r} p(s',r \mid s,a) \big[ r + \gamma v_{*}(s') \big]
$$

</aside>

<p><strong>Bellman Optimality for action-value function:</strong></p>

<aside>
üí°

$$
q_*(s,a) = \mathbb{E}\big[R_{t+1} + \gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s, A_t=a\big] \\
= \sum_{s',r}p(s',r \mid s,a) \big[r + \gamma \max_{a'}q_*(s',a') \big]
$$

</aside>

<p><img src="Introduction%20to%20Reinforcement%20Learning%202530b449d41980ac9b8cc4af04f345ba/image%201.png" alt="Backup diagrams for (a) v‚àó and (b) q‚àó" /></p>

<p>Backup diagrams for (a) v‚àó and (b) q‚àó</p>

<h3 id="optimal-value-functions-to-optimal-policies">Optimal Value Functions to Optimal Policies</h3>

<p><strong>Using the Optimal State-Value Function $v_*$:</strong></p>

<p>Once we know $v_*,$ we can derive an optimal policy by:</p>

<ul>
  <li>Looking at the actions that achieve the maximum in the Bellman optimality equation</li>
  <li>Any policy that assigns probability only to these maximizing actions is optimal</li>
</ul>

<p>This process is like a one-step lookahead search ‚Äî $v_*$ already accounts for all long-term returns, so just looking at the immediate action + expected next-state values is enough.</p>

<p><em>Simply put, a policy that is greedy w.r.t $v_{*}$ (chooses the best action based only on short-term consequences evaluated via $v_{*}$) is optimal in the long run, because $v_{*}$ already encodes all future returns.</em></p>

<p><strong>Using the Optimal Action-Value Function $q_*$:</strong></p>

<p>With $q_*$ the process is even simpler. For any state $s$, just pick the action $a$ that maximizes $q_*(s,a)$.</p>

<p>This is easier because:</p>

<ul>
  <li>$q_*$ already represents the cached results of the one-step lookahead</li>
  <li>No need to know transition probabilities $p(s‚Äô,r \mid s,a)$ or successor states</li>
  <li>Directly gives the optimal expected return for each state‚Äìaction pair</li>
</ul>

<p><strong>Key insights:</strong></p>

<ul>
  <li>$v_* \implies$need one-step lookahead to find best actions</li>
  <li>$q_* \implies$<strong>no lookahead needed</strong>, just take the $\max_a q_*(s,a)$</li>
</ul>

<h2 id="practical-limits">Practical Limits</h2>

<ol>
  <li><strong>True optimality is rarely achievable</strong>
    <ul>
      <li>Computing the exact optimal policy is usually too expensive (requires solving the Bellman optimality equation exactly).</li>
      <li>Even with a complete model of the environment, tasks like chess are too complex to solve optimally.</li>
    </ul>
  </li>
  <li><strong>Constraints in practice</strong>
    <ul>
      <li>Computation per step is limited (agent can‚Äôt spend forever planning)</li>
      <li>Memory is limited (can‚Äôt store values for every possible state)</li>
    </ul>
  </li>
  <li><strong>Tabular vs Function Approximation</strong>
    <ul>
      <li>Tabular methods: possible when state/action space is small (store values in arrays/tables)</li>
      <li>Function approximation: required when state space is huge or continuous (use compact parameterized functions, e.g. neural networks)</li>
    </ul>
  </li>
  <li><strong>Approximating optimal behavior</strong>
    <ul>
      <li>Not all states matter equally</li>
      <li>Agents can focus on frequent states and ignore rare states with little effect on overall performance</li>
    </ul>
  </li>
</ol>

<h1 id="dynamic-programming">Dynamic Programming</h1>

<h2 id="policy-evaluation">Policy Evaluation</h2>

<h2 id="policy-improvement">Policy Improvement</h2>

<h2 id="policy-iteration">Policy Iteration</h2>

<h2 id="value-iteration">Value Iteration</h2>

<h2 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h2>

<h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2>

<h2 id="efficiency">Efficiency</h2>

<h1 id="monte-carlo-methods">Monte Carlo Methods</h1>

<h1 id="temporal-difference-learning">Temporal Difference Learning</h1>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="learning" /><category term="rl" /><summary type="html"><![CDATA[In Progress]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi-Modal Machine Learning</title><link href="https://bhaswanth-a.github.io//posts/mmml/" rel="alternate" type="text/html" title="Multi-Modal Machine Learning" /><published>2026-01-19T11:00:00-05:00</published><updated>2026-01-19T11:00:00-05:00</updated><id>https://bhaswanth-a.github.io//posts/mmml</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/mmml/"><![CDATA[]]></content><author><name>&lt;author_id&gt;</name></author><category term="CMU MRSD" /><category term="Robotics" /><category term="ml" /><category term="learning" /><category term="nnets" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reinforcement Learning</title><link href="https://bhaswanth-a.github.io//posts/reinforcement-learning/" rel="alternate" type="text/html" title="Reinforcement Learning" /><published>2026-01-19T11:00:00-05:00</published><updated>2026-02-20T20:40:50-05:00</updated><id>https://bhaswanth-a.github.io//posts/reinforcement-learning</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/reinforcement-learning/"><![CDATA[<h1 id="deep-learning">Deep Learning</h1>

<p>This series of blogs are my notes from the class <a href="https://deeplearning.cs.cmu.edu/S25/index.html">10-703 Deep Reinforcement Learning and Control</a>, taught by <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a> and <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a> at CMU, as well as learnings from the book ‚ÄúReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto‚Äù. For my own sake of understanding and simplicity, the blog has been divided into 2 categories:</p>
<ol>
  <li><a href="https://bhaswanth-a.github.io/posts/intro-to-rl/">Introduction to Reinforcement Learning</a></li>
  <li><a href="https://bhaswanth-a.github.io/posts/deep-rl/">Deep Reinforcement Learning</a></li>
  <li><a href="">Advanced Deep Reinforcement Learning</a></li>
</ol>]]></content><author><name>&lt;author_id&gt;</name></author><category term="CMU MRSD" /><category term="Robotics" /><category term="rl" /><category term="learning" /><category term="nnets" /><summary type="html"><![CDATA[Deep Learning]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Robot Learning</title><link href="https://bhaswanth-a.github.io//posts/robot-learning/" rel="alternate" type="text/html" title="Robot Learning" /><published>2026-01-19T11:00:00-05:00</published><updated>2026-01-19T11:00:00-05:00</updated><id>https://bhaswanth-a.github.io//posts/robot-learning</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/robot-learning/"><![CDATA[]]></content><author><name>&lt;author_id&gt;</name></author><category term="CMU MRSD" /><category term="Robotics" /><category term="rl" /><category term="learning" /><category term="nnets" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Memory Management in C++</title><link href="https://bhaswanth-a.github.io//posts/memory-management-cpp/" rel="alternate" type="text/html" title="Memory Management in C++" /><published>2026-01-01T11:00:00-05:00</published><updated>2026-01-01T11:00:00-05:00</updated><id>https://bhaswanth-a.github.io//posts/memory-management-cpp</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/memory-management-cpp/"><![CDATA[<h1 id="a-complete-guide-to-memory-management-in-c">A Complete Guide to Memory Management in C++</h1>

<h3 id="introduction-the-power-and-responsibility-of-c">Introduction: The Power and Responsibility of C++</h3>

<p>As C++ creator Bjarne Stroustrup once said, ‚ÄúC makes it easy to shoot yourself in the foot; C++ makes it harder, but when you do, you blow your whole leg off.‚Äù This quote perfectly captures the essence of memory management in C++.</p>

<p>Unlike languages with automatic garbage collection like Java or C#, which have an internal process to release memory, C++ grants the programmer direct control over memory. This control is a primary source of its power and performance, allowing for the creation of extremely fast and efficient code.</p>

<p>However, this control comes with the responsibility of managing resources manually. A failure to release an unused resource is called a <strong>memory leak</strong>. A leaked resource becomes unavailable for reuse <strong>by the program itself</strong>, gradually consuming memory until the process exits. Memory leaks are a common cause of bugs and instability.</p>

<p>This guide will equip you with the principles and modern techniques to manage memory safely and effectively in C++. The goal is to transform what can be a daunting task into a manageable and systematic one, enabling you to write C++ code that is not only powerful but also robust and safe.</p>

<h2 id="1-the-two-worlds-of-memory-the-stack-and-the-heap">1. The Two Worlds of Memory: The Stack and The Heap</h2>

<p>A C++ program primarily uses two distinct areas of memory for its data: the <strong>Stack</strong> and the <strong>Heap</strong> (also known as the free store).</p>

<ol>
  <li><strong>The Stack</strong> The stack is a highly organized and fast region of memory where data is allocated and deallocated automatically. It operates on a ‚ÄúLast-In, First-Out‚Äù (LIFO) principle. The stack is where local variables and function call information are stored. When a function is called, its variables are ‚Äúpushed‚Äù onto the stack; when the function exits, they are automatically ‚Äúpopped‚Äù off. Its primary limitation is its fixed and relatively small size.</li>
  <li><strong>The Heap</strong> The heap is a large, less organized pool of memory available for data that needs to have a long lifetime or is too large to fit on the stack. Unlike the stack, memory on the heap must be allocated and deallocated <em>manually</em> by the programmer using the <code class="language-plaintext highlighter-rouge">new</code> and <code class="language-plaintext highlighter-rouge">delete</code> operators. While flexible, this manual control is where most memory management errors occur.</li>
</ol>

<p>The following table synthesizes the key differences between these two memory regions.</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>The Stack</th>
      <th>The Heap (Free Store)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Allocation/Deallocation</strong></td>
      <td>Automatic (managed by the compiler)</td>
      <td>Manual (managed by the programmer via <code class="language-plaintext highlighter-rouge">new</code>/<code class="language-plaintext highlighter-rouge">delete</code>)</td>
    </tr>
    <tr>
      <td><strong>Speed</strong></td>
      <td>Fast allocation and deallocation</td>
      <td>Slower due to more complex management</td>
    </tr>
    <tr>
      <td><strong>Size</strong></td>
      <td>Fixed and limited in size (~2MB)</td>
      <td>Large and flexible</td>
    </tr>
    <tr>
      <td><strong>Management</strong></td>
      <td>Managed by the compiler/runtime (via function calls)</td>
      <td>Handled by the programmer and memory manager</td>
    </tr>
    <tr>
      <td><strong>Risk</strong></td>
      <td>Low risk of errors</td>
      <td>Prone to errors like fragmentation and leaks</td>
    </tr>
  </tbody>
</table>

<p>It is important to note that the actual physical location of these two areas of memory is ultimately the same, the RAM.</p>

<p><em>Understanding the difference between the stack and the heap is fundamental, as the most common and dangerous errors arise from mismanaging the heap.</em></p>

<h2 id="2-the-perils-of-manual-memory-management">2. The Perils of Manual Memory Management</h2>

<p>C-style or ‚Äúnaive‚Äù C++ approaches to memory management are a frequent source of bugs. A common but unreliable approach is the ‚ÄúSandwich Pattern,‚Äù where a call to <code class="language-plaintext highlighter-rouge">new</code> is followed by some working code and then a corresponding call to <code class="language-plaintext highlighter-rouge">delete</code>. This pattern is insidious because it offers no guarantee that the <code class="language-plaintext highlighter-rouge">delete</code> statement will ever be reached ‚Äî an exception or a premature loop exit can easily cause it to be skipped, leading to a memory leak.</p>

<h3 id="21-memory-leaks-the-silent-resource-drain">2.1. Memory Leaks: The Silent Resource Drain</h3>

<p>A memory leak occurs when heap-allocated memory is no longer needed by the program but is not released back to the operating system with <code class="language-plaintext highlighter-rouge">delete</code>. This memory becomes unusable for the remainder of the program‚Äôs execution, slowly draining available resources.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">cause_a_leak</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Memory is allocated for an integer on the heap.</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">leaky_ptr</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">(</span><span class="mi">42</span><span class="p">);</span>

    <span class="c1">// The function returns, but `delete leaky_ptr;` was never called.</span>
    <span class="c1">// The pointer `leaky_ptr` is gone, but the memory it pointed to is now lost.</span>
<span class="p">}</span> <span class="c1">// Memory leak occurs here.</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="22-dangling-pointers-touching-freed-memory">2.2. Dangling Pointers: Touching Freed Memory</h3>

<p>A dangling pointer is a pointer that continues to point to a memory location that has already been deallocated (freed via <code class="language-plaintext highlighter-rouge">delete</code>). Attempting to access or use a dangling pointer leads to <strong>undefined behavior</strong>, which can manifest as corrupted data, unexpected crashes, or security vulnerabilities.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">create_dangling_pointer</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">ptr</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">dangling_ptr</span> <span class="o">=</span> <span class="n">ptr</span><span class="p">;</span>

    <span class="c1">// The memory is freed.</span>
    <span class="k">delete</span> <span class="n">ptr</span><span class="p">;</span>

    <span class="c1">// `dangling_ptr` now points to deallocated memory.</span>
    <span class="c1">// Using it here is dangerous and results in undefined behavior.</span>
    <span class="c1">// *dangling_ptr = 100; // CRASH! Or worse...</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="23-double-free-deleting-twice">2.3. Double Free: Deleting Twice</h3>

<p>A double free error occurs when the program attempts to <code class="language-plaintext highlighter-rouge">delete</code> the same memory location more than once. This action also leads to undefined behavior and can corrupt the internal data structures that the heap manager uses to track memory, often leading to a crash.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">cause_double_free</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">ptr</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">(</span><span class="mi">20</span><span class="p">);</span>
    <span class="k">delete</span> <span class="n">ptr</span><span class="p">;</span>

    <span class="c1">// Attempting to delete the same memory again is a serious error.</span>
    <span class="c1">// delete ptr; // Undefined behavior!</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="24-buffer-overruns--underruns-out-of-bounds">2.4. Buffer Overruns &amp; Underruns: Out of Bounds</h3>

<p>C++ does not have built-in range checking for raw arrays. This means it is possible to write or read past the allocated boundaries of an array, an error known as an ‚Äúinvalid write‚Äù or ‚Äúinvalid read.‚Äù This is a major source of security vulnerabilities, as it can be exploited to overwrite critical program data or execute arbitrary code.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">buffer_overrun</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Allocate a heap array for 10 integers (indices 0 through 9).</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">numbers</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>

    <span class="c1">// This loop writes one element past the end of the array.</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Off-by-one error: should be i &lt; 10</span>
        <span class="c1">// On the last iteration (i=10), we write out of bounds.</span>
        <span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> <span class="c1">// Invalid write!</span>
    <span class="p">}</span>

    <span class="k">delete</span><span class="p">[]</span> <span class="n">numbers</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="25-heap-fragmentation-the-checkerboard-of-memory">2.5. Heap Fragmentation: The Checkerboard of Memory</h3>

<p>When a program performs many frequent allocations and deallocations of small objects on the heap, the memory can become fragmented. Imagine a solid block of memory that, over time, turns into a <strong>checkerboard of used and free chunks</strong>.</p>

<p>The primary consequence is that even if the total amount of free memory is sufficient, the program may be unable to find a single <em>contiguous</em> block large enough to satisfy a large allocation request. This can slow down an application or, in extreme cases, cause it to fail.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="rouge-code"><pre><span class="c1">// Heap Fragmentation Simulation</span>
<span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;new&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// We will simulate frequent allocations of small amounts of memory</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">NUM_SMALL_CHUNKS</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">SMALL_SIZE</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span> <span class="c1">// Small objects like station codes</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span> <span class="n">pointers</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Step 1: Filling the heap with small chunks...</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_SMALL_CHUNKS</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">pointers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="k">new</span> <span class="kt">char</span><span class="p">[</span><span class="n">SMALL_SIZE</span><span class="p">]);</span>
    <span class="p">}</span>

    <span class="c1">// Step 2: Create the "Checkerboard"</span>
    <span class="c1">// We deallocate every second chunk. This leaves holes of free memory </span>
    <span class="c1">// separated by small "islands" of still-allocated memory.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Step 2: Creating a checkerboard pattern by deallocating every other block...</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_SMALL_CHUNKS</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">delete</span><span class="p">[]</span> <span class="n">pointers</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="n">pointers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span> 
    <span class="p">}</span>

    <span class="c1">// Step 3: Attempt a large contiguous allocation</span>
    <span class="c1">// Total free memory is (NUM_SMALL_CHUNKS / 2) * SMALL_SIZE.</span>
    <span class="c1">// However, the largest *contiguous* block is only SMALL_SIZE.</span>
    <span class="kt">size_t</span> <span class="n">large_request</span> <span class="o">=</span> <span class="n">SMALL_SIZE</span> <span class="o">*</span> <span class="mi">5</span><span class="p">;</span> 
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Step 3: Attempting to allocate a large block: "</span> <span class="o">&lt;&lt;</span> <span class="n">large_request</span> <span class="o">&lt;&lt;</span> <span class="s">" bytes...</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>

    <span class="k">try</span> <span class="p">{</span>
        <span class="kt">char</span><span class="o">*</span> <span class="n">large_block</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span><span class="p">[</span><span class="n">large_request</span><span class="p">];</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Success: Contiguous block found!</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
        <span class="k">delete</span><span class="p">[]</span> <span class="n">large_block</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">bad_alloc</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// The heap manager cannot find a single block large enough</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failure: "</span> <span class="o">&lt;&lt;</span> <span class="n">e</span><span class="p">.</span><span class="n">what</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" - The heap is too fragmented! [2]</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Cleanup remaining pointers</span>
    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">p</span> <span class="o">:</span> <span class="n">pointers</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">delete</span><span class="p">[]</span> <span class="n">p</span><span class="p">;</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><em>These classic pitfalls demonstrate that manual memory management is fragile because it relies on the programmer to perform cleanup. Modern C++ provides a robust philosophy that solves these problems by making cleanup  and .</em></p>

<h2 id="3-the-guiding-principle-raii-resource-acquisition-is-initialization">3. The Guiding Principle: RAII (Resource Acquisition Is Initialization)</h2>

<p><strong>RAII</strong> is the central pillar of modern C++ resource management. The principle is simple yet powerful: resource ownership should be tied to an object‚Äôs lifetime. This means that an object should acquire a resource in its constructor and release it in its destructor.</p>

<p>The RAII lifecycle works in three deterministic steps:</p>

<ol>
  <li>A resource (such as heap memory, a file handle, or a network socket) is acquired in an object‚Äôs constructor.</li>
  <li>The owning object is declared on the <strong>stack</strong>.</li>
  <li>When the object goes out of scope (for example, at the end of a function), its destructor is <strong>automatically and guaranteed</strong> to be called. The destructor then releases the resource.</li>
</ol>

<p>This pattern makes resource release deterministic and automatic, ensuring that resources are properly cleaned up even if errors occur or exceptions are thrown.</p>

<p>Let‚Äôs look at a practical example.</p>

<p><strong>Before RAII (Manual Management)</strong> Here, the <code class="language-plaintext highlighter-rouge">widget</code> class manually allocates and deallocates memory, requiring an explicit destructor.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">widget</span> <span class="p">{</span>
<span class="nl">private:</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">data</span><span class="p">;</span>
<span class="nl">public:</span>
    <span class="n">widget</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">data</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">[</span><span class="n">size</span><span class="p">];</span> <span class="c1">// 1. Acquire resource in constructor</span>
    <span class="p">}</span>
    <span class="o">~</span><span class="n">widget</span><span class="p">()</span> <span class="p">{</span>
        <span class="k">delete</span><span class="p">[]</span> <span class="n">data</span><span class="p">;</span> <span class="c1">// 3. Release resource in destructor</span>
    <span class="p">}</span>
    <span class="kt">void</span> <span class="n">do_something</span><span class="p">()</span> <span class="p">{}</span>
<span class="p">};</span>

<span class="kt">void</span> <span class="n">functionUsingWidget</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">widget</span> <span class="n">w</span><span class="p">(</span><span class="mi">1000000</span><span class="p">);</span> <span class="c1">// 2. Object is created on the stack</span>
    <span class="n">w</span><span class="p">.</span><span class="n">do_something</span><span class="p">();</span>
<span class="p">}</span> <span class="c1">// w goes out of scope, its destructor is automatically called.</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Modern C++ with RAII (Using a Smart Pointer)</strong> By using a smart pointer (<code class="language-plaintext highlighter-rouge">std::unique_ptr</code>), we delegate memory ownership to a dedicated RAII object. This eliminates the need for an explicit destructor in our class, making the code simpler and safer.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;memory&gt;</span><span class="cp">
</span>
<span class="k">class</span> <span class="nc">widget</span> <span class="p">{</span>
<span class="nl">private:</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">[]</span><span class="o">&gt;</span> <span class="n">data</span><span class="p">;</span> <span class="c1">// The smart pointer now owns the memory</span>
<span class="nl">public:</span>
    <span class="n">widget</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">[]</span><span class="o">&gt;</span><span class="p">(</span><span class="n">size</span><span class="p">);</span> <span class="c1">// 1. Resource is acquired</span>
    <span class="p">}</span>
    <span class="c1">// 3. No explicit destructor needed! The unique_ptr handles it.</span>
    <span class="kt">void</span> <span class="n">do_something</span><span class="p">()</span> <span class="p">{}</span>
<span class="p">};</span>

<span class="kt">void</span> <span class="n">functionUsingWidget</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">widget</span> <span class="n">w</span><span class="p">(</span><span class="mi">1000000</span><span class="p">);</span> <span class="c1">// 2. Object created on the stack</span>
    <span class="n">w</span><span class="p">.</span><span class="n">do_something</span><span class="p">();</span>
<span class="p">}</span> <span class="c1">// w goes out of scope, its member `data` is automatically destroyed, releasing the memory.</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p><em>RAII is the philosophy, and smart pointers are the primary tool you will use to implement it for dynamically allocated memory.</em></p>

<h2 id="4-the-modern-toolkit-smart-pointers">4. The Modern Toolkit: Smart Pointers</h2>

<p>Smart pointers are class templates provided by the C++ Standard Library. They act as wrappers around a raw pointer, automatically managing its lifetime and ensuring that the memory it points to is correctly deallocated. They are the C++ way of enforcing RAII for heap memory.</p>

<h3 id="41-stdunique_ptr-the-default-choice-for-exclusive-ownership">4.1. <code class="language-plaintext highlighter-rouge">std::unique_ptr</code>: The Default Choice for Exclusive Ownership</h3>

<p>A <code class="language-plaintext highlighter-rouge">std::unique_ptr</code> maintains <strong>exclusive ownership</strong> of a heap-allocated object. This means:</p>

<ul>
  <li>It cannot be copied. You can only <strong>move</strong> ownership from one <code class="language-plaintext highlighter-rouge">unique_ptr</code> to another.</li>
  <li>This rule is enforced at compile time, guaranteeing that only one <code class="language-plaintext highlighter-rouge">unique_ptr</code> can own the resource at any given time.</li>
  <li>It is a <strong>‚Äúzero-cost abstraction‚Äù</strong>, meaning it has no performance or memory overhead compared to a raw pointer.</li>
</ul>

<p>For these reasons, <code class="language-plaintext highlighter-rouge">std::unique_ptr</code> should be your default choice for managing dynamic memory.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;memory&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">use_unique_pointer</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Create a unique_ptr using std::make_unique (preferred way since C++14).</span>
    <span class="k">auto</span> <span class="n">my_ptr</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">42</span><span class="p">);</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Value: "</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">my_ptr</span> <span class="o">&lt;&lt;</span> <span class="sc">'\n'</span><span class="p">;</span>

    <span class="c1">// No need to call delete. The memory is automatically freed when my_ptr goes out of scope.</span>
<span class="p">}</span> <span class="c1">// my_ptr is destroyed here.</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="42-stdshared_ptr-for-shared-ownership-scenarios">4.2. <code class="language-plaintext highlighter-rouge">std::shared_ptr</code>: For Shared Ownership Scenarios</h3>

<p>A <code class="language-plaintext highlighter-rouge">std::shared_ptr</code> is used when a resource needs to be owned by <strong>multiple pointers simultaneously</strong>.</p>

<ul>
  <li>It uses a technique called <strong>reference counting</strong>. It keeps a count of how many <code class="language-plaintext highlighter-rouge">shared_ptr</code> instances are pointing to the resource.</li>
  <li>The resource is only deleted when the last <code class="language-plaintext highlighter-rouge">shared_ptr</code> pointing to it is destroyed, causing the reference count to drop to zero.</li>
  <li><strong>Trade-off:</strong> This flexibility comes at a cost. <code class="language-plaintext highlighter-rouge">shared_ptr</code> incurs a small performance overhead for storing and atomically updating the reference count.</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;memory&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">use_shared_pointer</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">ptr1</span><span class="p">;</span>

    <span class="p">{</span>
        <span class="c1">// Create a shared_ptr. Reference count is 1.</span>
        <span class="k">auto</span> <span class="n">ptr2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span>

        <span class="c1">// Copy the shared_ptr. Both now point to the same memory.</span>
        <span class="c1">// The reference count becomes 2.</span>
        <span class="n">ptr1</span> <span class="o">=</span> <span class="n">ptr2</span><span class="p">;</span>

        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Value: "</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">ptr1</span> <span class="o">&lt;&lt;</span> <span class="sc">'\n'</span><span class="p">;</span>
    <span class="p">}</span> <span class="c1">// ptr2 goes out of scope. Reference count drops to 1. The memory is NOT deleted.</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"ptr1 is still valid."</span> <span class="o">&lt;&lt;</span> <span class="sc">'\n'</span><span class="p">;</span>

<span class="p">}</span> <span class="c1">// ptr1 goes out of scope. Reference count drops to 0. The memory is now deleted.</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="43-choosing-the-right-smart-pointer">4.3. Choosing the Right Smart Pointer</h3>

<p>This table provides a clear guide for when to use each type of pointer.</p>

<table>
  <thead>
    <tr>
      <th>Pointer Type</th>
      <th>Ownership Semantics</th>
      <th>Performance Overhead</th>
      <th>Safety</th>
      <th>Primary Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Raw Pointer (</strong><code class="language-plaintext highlighter-rouge">new</code><strong>/</strong><code class="language-plaintext highlighter-rouge">delete</code><strong>)</strong></td>
      <td>Manual / Unclear</td>
      <td>None</td>
      <td>Prone to leaks and errors</td>
      <td>Legacy code or low-level interaction with C APIs.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">std::unique_ptr</code></td>
      <td><strong>Exclusive / Unique</strong></td>
      <td><strong>None (Zero-cost)</strong></td>
      <td><strong>High (Compile-time checks)</strong></td>
      <td><strong>The default choice for all owning pointers.</strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">std::shared_ptr</code></td>
      <td>Shared / Reference-counted</td>
      <td>Yes (Reference count)</td>
      <td>High (Runtime checks)</td>
      <td>For shared ownership, such as in graph data structures or implementing an Observer pattern where the lifetime of the subject and observers are not strictly nested.</td>
    </tr>
  </tbody>
</table>

<p><em>Managing memory for a single thread is one challenge; ensuring safety when multiple threads are involved adds another layer of complexity.</em></p>

<h2 id="5-memory-in-a-multi-threaded-world">5. Memory in a Multi-Threaded World</h2>

<p>Memory management becomes significantly more complex in concurrent applications because all threads in a process typically share the same heap. Without proper safeguards, this shared access can lead to chaos.</p>

<h3 id="51-the-ultimate-danger-data-races">5.1. The Ultimate Danger: Data Races</h3>

<p>A <strong>data race</strong> is the most dangerous type of concurrency bug. It occurs when:</p>

<ol>
  <li>Two or more threads access the <strong>same memory location</strong> concurrently.</li>
  <li>At least one of the accesses is a <strong>write</strong>.</li>
  <li>There is <strong>no synchronization mechanism</strong> to protect the access.</li>
</ol>

<p>A data race results in <strong>undefined behavior</strong>. This is not just a theoretical problem; it can lead to silent data corruption, unpredictable crashes, and other hard-to-diagnose issues.</p>

<h3 id="52-preventing-data-races-with-synchronization">5.2. Preventing Data Races with Synchronization</h3>

<p>To prevent data races, C++ provides two primary tools that ensure operations on shared memory are orderly and safe.</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">std::mutex</code> : A mutex (short for ‚Äúmutual exclusion‚Äù) acts as a lock. It ensures that only one thread can access a ‚Äúcritical section‚Äù of code at a time. A thread must <code class="language-plaintext highlighter-rouge">lock()</code> the mutex to enter the critical section and <code class="language-plaintext highlighter-rouge">unlock()</code> it upon exit, allowing other threads to proceed. While you can manually call <code class="language-plaintext highlighter-rouge">.lock()</code> and <code class="language-plaintext highlighter-rouge">.unlock()</code>, using a <strong>Resource Acquisition Is Initialization (RAII)</strong> helper like <code class="language-plaintext highlighter-rouge">std::lock_guard</code> ensures the mutex is always released, even if an exception occurs.</p>

    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="rouge-code"><pre> <span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span> <span class="cp">#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
</span> <span class="cp">#include</span> <span class="cpf">&lt;mutex&gt;</span><span class="cp">
</span> <span class="cp">#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span>    
 <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">mtx</span><span class="p">;</span>          <span class="c1">// The "lock"</span>
 <span class="kt">int</span> <span class="n">shared_counter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// Shared resource</span>
    
 <span class="kt">void</span> <span class="nf">increment</span><span class="p">(</span><span class="kt">int</span> <span class="n">iterations</span><span class="p">)</span> <span class="p">{</span>
     <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">iterations</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
         <span class="c1">// The lock_guard constructor locks the mutex.</span>
         <span class="c1">// Its destructor (at the end of the loop scope) automatically unlocks it. [6]</span>
         <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mtx</span><span class="p">);</span> 
            
         <span class="c1">// --- CRITICAL SECTION START ---</span>
         <span class="n">shared_counter</span><span class="o">++</span><span class="p">;</span> 
         <span class="c1">// --- CRITICAL SECTION END ---</span>
     <span class="p">}</span>
 <span class="p">}</span>
    
 <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
     <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">t1</span><span class="p">(</span><span class="n">increment</span><span class="p">,</span> <span class="mi">1000</span><span class="p">);</span>
     <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">t2</span><span class="p">(</span><span class="n">increment</span><span class="p">,</span> <span class="mi">1000</span><span class="p">);</span>
     <span class="n">t1</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
     <span class="n">t2</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
     <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Final Counter: "</span> <span class="o">&lt;&lt;</span> <span class="n">shared_counter</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
 <span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">std::atomic</code> : The <code class="language-plaintext highlighter-rouge">std::atomic</code> template provides types that guarantee that operations (like reads, writes, and increments) are ‚Äúatomic.‚Äù Unlike a mutex, which stops other threads, atomic operations are <strong>indivisible</strong> at the hardware level; they cannot be interrupted mid-operation [source text in prompt]. By default, atomic operations provide <strong>inter-thread synchronization</strong>, meaning a <code class="language-plaintext highlighter-rouge">store</code> in one thread synchronizes with a <code class="language-plaintext highlighter-rouge">load</code> in another.</p>

    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre> <span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span> <span class="cp">#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
</span> <span class="cp">#include</span> <span class="cpf">&lt;atomic&gt;</span><span class="cp">
</span>    
 <span class="c1">// std::atomic provides data-race-free access without a full lock.</span>
 <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">atomic_counter</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    
 <span class="kt">void</span> <span class="nf">atomic_increment</span><span class="p">(</span><span class="kt">int</span> <span class="n">iterations</span><span class="p">)</span> <span class="p">{</span>
     <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">iterations</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
         <span class="c1">// This operation is indivisible and safe from data races.</span>
         <span class="c1">// It is shorthand for atomic_counter.fetch_add(1);</span>
         <span class="n">atomic_counter</span><span class="o">++</span><span class="p">;</span> 
     <span class="p">}</span>
 <span class="p">}</span>
    
 <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
     <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">t1</span><span class="p">(</span><span class="n">atomic_increment</span><span class="p">,</span> <span class="mi">1000</span><span class="p">);</span>
     <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">t2</span><span class="p">(</span><span class="n">atomic_increment</span><span class="p">,</span> <span class="mi">1000</span><span class="p">);</span>
     <span class="n">t1</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
     <span class="n">t2</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
        
     <span class="c1">// Default load() is sequentially consistent (memory_order_seq_cst).</span>
     <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Final Atomic Counter: "</span> <span class="o">&lt;&lt;</span> <span class="n">atomic_counter</span><span class="p">.</span><span class="n">load</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
 <span class="p">}</span>
    
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ol>

<p>By eliminating data races from your program, the C++ memory model guarantees <strong>sequentially consistent</strong> execution, meaning the result of your multi-threaded program will be predictable and reliable, as if the operations of all threads were executed in some single sequential order.</p>

<p><em>While smart pointers handle most dynamic memory needs, sometimes you need finer control over how containers like</em> <code class="language-plaintext highlighter-rouge">std::vector</code> <em>get their memory. This is the job of allocators.</em></p>

<h2 id="6-under-the-hood-allocators">6. Under the Hood: Allocators</h2>

<p>An <strong>allocator</strong> is a component of the C++ Standard Library responsible for handling all memory allocation and deallocation requests for containers like <code class="language-plaintext highlighter-rouge">std::vector</code>, <code class="language-plaintext highlighter-rouge">std::map</code>, and <code class="language-plaintext highlighter-rouge">std::list</code>.</p>

<p>By default, all standard containers use <code class="language-plaintext highlighter-rouge">std::allocator</code>, which is a general-purpose allocator that simply calls the global <code class="language-plaintext highlighter-rouge">operator new</code> and <code class="language-plaintext highlighter-rouge">operator delete</code> functions. However, there are scenarios where you might want to provide a <strong>custom allocator</strong>.</p>

<p>There are two primary reasons for writing a custom allocator:</p>

<ol>
  <li><strong>Performance:</strong> For applications that perform many frequent allocations of small amounts of memory (like in a <code class="language-plaintext highlighter-rouge">std::list</code> or <code class="language-plaintext highlighter-rouge">std::map</code>), the default allocator can be slow and lead to heap fragmentation. A custom allocator that uses a <strong>memory pool</strong> ‚Äî a pre-allocated large block of memory‚Äîcan serve these small requests much faster by simply handing out chunks from the pool.</li>
  <li><strong>Specialized Memory:</strong> Custom allocators can encapsulate access to different types of memory, such as shared memory that needs to be accessible by multiple processes, or memory managed by a third-party garbage collector.</li>
</ol>

<p><em>Knowing the theory is essential, but a skilled C++ programmer also needs practical tools and habits to diagnose problems and write robust code.</em></p>

<h2 id="7-in-the-trenches-debugging-and-best-practices">7. In the Trenches: Debugging and Best Practices</h2>

<h3 id="71-finding-leaks-and-errors-with-valgrind">7.1. Finding Leaks and Errors with Valgrind</h3>

<p><strong>Valgrind</strong> is a powerful instrumentation framework for dynamically analyzing programs. Its Memcheck tool is invaluable for finding memory leaks and memory errors (like invalid reads and writes) in C++ programs.</p>

<p>Here is a simple step-by-step guide to using it:</p>

<ol>
  <li><strong>Step 1: Compile with Debug Symbols</strong> To get meaningful output with file names and line numbers, compile your program with debug symbols using the <code class="language-plaintext highlighter-rouge">ggdb3</code> or <code class="language-plaintext highlighter-rouge">Og</code> flag.</li>
  <li><strong>Step 2: Run Your Program via Valgrind</strong> Execute your program through Valgrind with flags that provide detailed leak information.</li>
  <li><strong>Step 3: Interpret the Output</strong>
    <ul>
      <li><strong>A Clean Run:</strong> A successful run with no leaks will show a ‚ÄúHEAP SUMMARY‚Äù.</li>
      <li><strong>A Leak Report:</strong> If a leak is detected, Valgrind will show you where the leaked memory was allocated. The backtrace points directly to the <code class="language-plaintext highlighter-rouge">new</code> or <code class="language-plaintext highlighter-rouge">malloc</code> call that is the source of the leak.</li>
    </ul>
  </li>
</ol>

<p><img src="/assets/images/Cpp/image.png" alt="image.png" /></p>

<blockquote>
  <p>Resource: <a href="https://stackoverflow.com/questions/5134891/how-do-i-use-valgrind-to-find-memory-leaks">https://stackoverflow.com/questions/5134891/how-do-i-use-valgrind-to-find-memory-leaks</a></p>

</blockquote>

<h3 id="72-a-defensive-coders-checklist">7.2. A Defensive Coder‚Äôs Checklist</h3>

<p>Adopting good habits is the best way to prevent memory errors before they happen.</p>

<ul>
  <li><strong>Prefer Stack Allocation:</strong> If data doesn‚Äôt need to outlive the function it‚Äôs created in and isn‚Äôt excessively large, always prefer creating it on the stack. It‚Äôs faster and automatically managed.</li>
  <li><strong>Embrace RAII and Smart Pointers:</strong> For all heap allocations, use <code class="language-plaintext highlighter-rouge">std::unique_ptr</code> by default. Only use <code class="language-plaintext highlighter-rouge">std::shared_ptr</code> when you are certain that shared ownership is a necessary part of your design.</li>
  <li><strong>Trust Nothing:</strong> Never assume function arguments are valid, especially raw pointers. Always check for <code class="language-plaintext highlighter-rouge">nullptr</code> and validate inputs to prevent crashes and security exploits.</li>
  <li><strong>Initialize Everything:</strong> Always initialize variables when you declare them to avoid using indeterminate, garbage values. <strong>Crucially, initialize pointers to</strong> <code class="language-plaintext highlighter-rouge">nullptr</code> <strong>so that</strong> <code class="language-plaintext highlighter-rouge">delete</code> <strong>can be safely called on them even if they never end up owning a resource.</strong> This prevents a common class of errors when cleaning up objects in complex states.</li>
  <li><strong>Write ‚ÄúBoring‚Äù Code:</strong> Avoid being overly ‚Äúclever‚Äù or tricky. Simple, readable, and direct code is easier to maintain, reason about, and is far less prone to subtle bugs. Make your solution fit the problem.</li>
  <li><strong>Use Safe Wrappers:</strong> When interfacing with legacy C-style APIs that return raw pointers (e.g., from <code class="language-plaintext highlighter-rouge">malloc</code>), immediately wrap the returned pointer in an appropriate smart pointer or a custom RAII class to ensure its lifetime is managed safely.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Modern C++ has transformed memory management from a manual, error-prone chore into a safe and systematic process. By understanding and applying a few core principles, you can write code that is both highly performant and exceptionally robust.</p>

<p>Here are the three most important takeaways:</p>

<ol>
  <li><strong>Scope is Your Garbage Collector:</strong> The RAII principle is the foundation of C++ memory safety. By tying the lifetime of a resource to a stack-based object, cleanup becomes automatic, predictable, and exception-safe.</li>
  <li><strong>Smart Pointers are Your Primary Tool:</strong> <code class="language-plaintext highlighter-rouge">std::unique_ptr</code> should be your default choice for all dynamically allocated memory. It is safe, has no overhead, and clearly communicates the exclusive ownership of a resource.</li>
  <li><strong>Ownership is the Central Concept:</strong> Always be clear about which part of your code <em>owns</em> a resource and is therefore responsible for its cleanup. Modern C++ features like smart pointers are designed to make this ownership explicit and verifiable by the compiler.</li>
</ol>

<p>By internalizing these principles, you can confidently wield the power of C++ to build applications that are not only fast but also safe, maintainable, and correct.</p>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="C++" /><category term="cpp" /><category term="c++" /><category term="memory" /><category term="notebooklm" /><summary type="html"><![CDATA[A Complete Guide to Memory Management in C++]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Diffusion and Flow Matching</title><link href="https://bhaswanth-a.github.io//posts/diffusion-flow-matching/" rel="alternate" type="text/html" title="Diffusion and Flow Matching" /><published>2026-01-01T11:00:00-05:00</published><updated>2026-01-01T11:00:00-05:00</updated><id>https://bhaswanth-a.github.io//posts/diffusion-flow-matching</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/diffusion-flow-matching/"><![CDATA[<h1 id="diffusion-and-flow-matching">Diffusion and Flow Matching</h1>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="diffusion" /><category term="flow" /><category term="nnnets" /><category term="genai" /><summary type="html"><![CDATA[Diffusion and Flow Matching]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/ldr.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/ldr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">All About Search Algorithms</title><link href="https://bhaswanth-a.github.io//posts/all-about-search-algorithms/" rel="alternate" type="text/html" title="All About Search Algorithms" /><published>2025-11-30T11:00:00-05:00</published><updated>2025-12-08T16:17:11-05:00</updated><id>https://bhaswanth-a.github.io//posts/all-about-search-algorithms</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/all-about-search-algorithms/"><![CDATA[<p><em>In progress</em></p>

<h1 id="to-do">TO-DO:</h1>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Refine RRT, RRT-Connect, RRT*</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />POMDP</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Multi-robot planning</li>
</ul>

<h1 id="graph-search-problem">Graph Search Problem</h1>

<p>Once a robot converts the environment into a discrete representation, whether by grid decomposition, Voronoi skeletons, adaptive cell decompositions, or lattices, the planning problem becomes a classical least-cost path search on a graph. Each state represents a robot configuration, and each edge carries a transition cost, typically the motion cost between configurations.</p>

<p>The goal of search is to compute a path from a designated start state $s_{start}$ to a goal state $s_{goal}$ that minimizes cumulative edge cost. Many search algorithms operate by estimating or computing the optimal cost-to-come, expressed as:</p>

\[g(s) = \min_{s'' \in \text{pred}(s)} \left[ g(s'') + c(s'', s) \right]\]

<p>Here, $g(s)$ is the minimum cost from start to $s$, $c(s‚Äô‚Äò,s)$ is the cost of transitioning from predecessor $s‚Äô‚Äô$ to $s$, and the recursion states that the optimal cost-to-come is obtained by choosing the predecessor that yields the cheapest total.</p>

<p>Because this recurrence defines a dynamic programming relation, the overall optimal path can be reconstructed greedily by backtracking from the goal using the minimizing predecessor:</p>

\[s' = \arg\min_{s'' \in \text{pred}(s)} \left[ g(s'') + c(s'', s) \right]\]

<p>This backtracking forms the optimal path even though search expands states in the forward direction.</p>

<h1 id="a">A*</h1>

<p>A* expands states in increasing order of:</p>

\[f(s) = g(s) + h(s)\]

<p>where</p>

<ul>
  <li>$g(s) =$  best-known cost from start to $s$</li>
  <li>$h(s) =$  admissible heuristic estimating $s$ to goal</li>
</ul>

<p>A consistent heuristic ensures that every state g-value is optimal exactly at the moment the state is expanded. That is the heart of A<em>‚Äôs correctness proof: A</em> never has to revisit or ‚Äúfix‚Äù a g-value later, because consistency ensures no cheaper path will ever emerge after expansion.</p>

<p>The algorithm expands only states whose f-values are competitive with the optimal path to the goal. This is why A* is so efficient: it prunes everything that cannot possibly lead to the optimal solution.</p>

<p><strong>Pseudocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>main()
g(s_start) = 0; all other g-values = INF;
OPEN = {s_start};
computePath();

computePath()
while(s_goal is not expanded and OPEN != 0)
	remove s with the smallest [f(s) = g(s)+h(s)] from OPEN;
	insert s into CLOSED;
	for every successor s‚Äô of s such that s‚Äô not in CLOSED:
		if g(s‚Äô) &gt; g(s) + c(s,s‚Äô):
			g(s‚Äô) = g(s) + c(s,s‚Äô);
			insert s‚Äô into OPEN;
</pre></td></tr></tbody></table></code></pre></div></div>

<p>For every expanded state $g(s)$ is optimal. For every other state, $g(s)$ is an upper-bound.</p>

<p>A* performs provably minimal number of state expansions required to guarantee optimality ‚Äì optimal in terms of the computations.</p>

<h2 id="multi-goal-a">Multi-Goal A*</h2>

<p>Many robotic tasks require a path to <em>any one</em> of multiple possible goals. Examples include:</p>

<ul>
  <li>picking the best parking spot out of many,</li>
  <li>intercepting a moving target at any predicted point along its trajectory,</li>
  <li>exploration where several frontier cells represent different potential ‚Äúcompletion‚Äù points.</li>
</ul>

<p>The key insight is that A* does not need special logic for multiple goals. Instead, introduce an imaginary super-goal connected to each real goal with zero-cost edges.</p>

<p>Once we add the super-goal node, the problem becomes identical to standard A*: now we just search from the start to this single super-goal. The f-values handle all the rest.</p>

<p>This equivalence is mathematically correct because:</p>

\[c^*(s_{\text{start}}, g_{\text{imag}})

= \min_i c^*(s_{\text{start}}, g_i)

\quad \text{(zero-cost jump at the end)}\]

<p>This works even with unequal goal quality ‚Äî e.g., one parking spot may be worse than another. In that case each real goal is connected to the imaginary super-goal with an edge equal to the additional cost (e.g., ‚Äúgoal cost‚Äù), so A* automatically prefers the better goal.</p>

<p>The transformed graph remains admissible for the original problem, and A* still returns the optimal plan among all candidate goals.</p>

<p>To incorporate this, assign each goal a terminal cost $w_i$, and connect to the imaginary goal with edge cost $w_i:$</p>

\[c(g_i, g_{\text{imag}}) = w_i\]

\[c^*(s_{\text{start}}, g_{\text{imag}})

= \min_i \left[ c^*(s_{\text{start}}, g_i) + w_i \right]\]

<h2 id="weighted-a">Weighted A*</h2>

<p>Weighted A* modifies the f-value:</p>

\[f(s) = g(s) + \epsilon  h(s)\]

<p>with $\epsilon&gt;1$.</p>

<p>By inflating the heuristic weight, the search biases more strongly toward the goal, reducing exploration of irrelevant detours. This yields an $\epsilon-$suboptimal algorithm, meaning:</p>

\[\text{cost(solution)} \le \epsilon \cdot \text{cost(optimal solution)}\]

<h3 id="why-it-works">Why it works?</h3>

<p>The quality of A*‚Äôs heuristic is captured by the difference:</p>

\[e(s) = h(s) - h^*(s)\]

<ul>
  <li>$h(s):$ the heuristic estimate</li>
  <li>$h^*(s):$ the true cost-to-go</li>
  <li>$e(s):$ indicates how much the heuristic underestimates the real cost</li>
</ul>

<p>Since heuristics must be admissible, $e(s)$ is non-positive (never &gt; 0).</p>

<p>Regions where the heuristic underestimates a lot (very negative $e(s)$) form local minima in the error landscape. A* expands many states inside these ‚Äúdeep valleys‚Äù because the f-values look artificially good.</p>

<p>A deep valley $\rightarrow$ many states look promising $\rightarrow$ A* wastes expansions.</p>

<p>Weighted A* modifies f-values:</p>

\[f(s) = g(s) + \epsilon h(s), \quad \epsilon &gt; 1\]

<p>Multiplying the heuristic:</p>

<ul>
  <li>raises the $h(s)$</li>
  <li>makes the heuristic less conservative</li>
  <li>makes error $e(s) = \epsilon h(s) - h^*(s)$</li>
  <li>turns deep valleys into shallow ones</li>
</ul>

<p>This reduces the ‚Äúfunnel effect‚Äù and helps the search avoid unnecessary exploration.</p>

<p>Because local minima become shallow:</p>

<ul>
  <li>A* commits sooner toward the goal</li>
  <li>explores fewer misleading states</li>
  <li>avoids wasting time in heuristic dips</li>
</ul>

<p>Thus the search becomes directed, not exploratory.</p>

<p>No one currently knows precisely how the set of expanded states depends on the heuristic landscape. This remains an active research topic in heuristic search.</p>

<h2 id="backward-a">Backward A*</h2>

<p>Backward A* simply reverses the direction of search. Instead of searching forward from the start, it starts from the goal and expands backwards:</p>

<p><strong>Pseudocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>main()
g(s_goal) = 0; all other g-values = INF;
OPEN = {s_goal};
computePath();

computePath()
while(s_start is not expanded and OPEN != 0)
	remove s with the smallest [f(s) = g(s)+h(s)] from OPEN;
	insert s into CLOSED;
	for every predecessor s‚Äô of s such that s‚Äô not in CLOSED:
		if g(s‚Äô) &gt; c(s',s) + g(s):
			g(s‚Äô) = c(s',s) + g(s);
			insert s‚Äô into OPEN;
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="heuristics">Heuristics</h2>

<p>The efficiency of A* is driven entirely by how good your heuristic is. A heuristic represents an ‚Äúoptimistic guess‚Äù of how far we still are from the goal. When the heuristic is informative, A* expands only a small portion of the graph; when the heuristic is uninformative, A* degenerates toward Dijkstra, expanding almost everything.</p>

<p>The formal requirement for heuristics in A* is admissibility: the heuristic must never overestimate the true cost-to-go. In symbolic terms:</p>

\[0 \le h(s) \le c^*(s,s_{goal})\]

<p>where $c^*(s,s_{goal})$ is the minimal cost from $s$ to $s_{goal}$.</p>

<p>This ensures that A* never prunes or skips any potentially optimal path. But admissibility alone does not guarantee efficiency. For A* to run fast, the heuristic must not only be admissible, but also consistent (monotone). Consistency means:</p>

\[h(s_{goal}, s_{goal})=0\]

\[h(s) \le c(s,s') + h(s')\]

<p>for every $s\ne s_{goal}$ and its successors $s‚Äô.$</p>

<p><img src="/assets/images/Search_algorithms/image.png" alt="image.png" /></p>

<p>Admissibility provably follows from consistency and often (not always) consistency follows from admissibility.</p>

<h3 id="functions">Functions</h3>

<p>For grid-based navigation:</p>

<ul>
  <li>Euclidean distance
    <ul>
      <li>Admissable for 4-connected and 8-connected grid</li>
    </ul>
  </li>
  <li>Manhattan distance: $h(x,y) = abs(x-x_{goal}) + abs(y-y_{goal})$
    <ul>
      <li>Admissable for 4-connected grid (perfect heuristic)</li>
      <li>Inadmissable for 8-connected grid (Octile function is the perfect heuristic)</li>
    </ul>
  </li>
  <li>Diagonal distance: $h(x,y) = max( abs(x-x_{goal}), abs(y-y_{goal}))$</li>
</ul>

<p><strong>Octile Heuristic:</strong></p>

<p>In more complex spaces like a 3D lattice $(x,y,\theta)$ or high-DoF manipulators, a na√Øve heuristic becomes almost useless because it collapses the structure of the real problem into something too simple to guide search well.</p>

<p>A common and powerful strategy is to compute a lower-dimensional search to define a heuristic for a higher-dimensional state. For example, running a 2D Dijkstra search on the grid (starting from the goal) produces distances that account for real obstacles; these distances can then be used as heuristic values for every $(x,y,\theta)$ configuration. This improves guidance but can still fail in problems where orientation or arm constraints dominate the difficulty.</p>

<p>In many robotic tasks, even a sophisticated lower-dimensional heuristic is not enough because the true difficulty comes from complex geometric or kinematic constraints. In these cases, we often turn to inadmissible heuristics that violate the admissibility rule but encode meaningful structure‚Äîsuch as end-effector distance through obstacles or orientation differences of manipulated objects. These heuristics are informative but unsafe to use alone because they can lead A* into local minima or cause it to miss valid solutions.</p>

<h3 id="key-properties-of-combining-heuristics">Key Properties of Combining Heuristics</h3>

<p>If two heuristics $h_1(s)$ and $h_2(s)$ are consistent, then using their maximum,</p>

\[h(s) = \max(h_1(s), h_2(s))\]

<p>remains consistent and therefore admissible. This is useful but extremely limited: the max operation destroys information, produces new local minima, and requires every heuristic to be admissible, which is unrealistic in high-dimensional robotics.</p>

<p>More generally, if heuristics are combined additively,</p>

\[h(s) = h_1(s) + h_2(s)\]

<p>the result is typically $\epsilon-$consistent, meaning the heuristic may overestimate by at most a bounded factor. Weighted A* is equivalent to using such $\epsilon-$consistent heuristics and guarantees</p>

\[\text{cost(solution)} \le \epsilon \cdot \text{cost(optimal)}\]

<h3 id="need-for-multiple-heuristics">Need for Multiple Heuristics</h3>

<p>Real-world manipulation or navigation tasks often have many distinct modes of difficulty, where each heuristic is strong in some region but weak (or utterly misleading) in others. For instance:</p>

<ul>
  <li>One heuristic might capture distance in the workspace.</li>
  <li>Another might encode object orientation costs.</li>
  <li>A third might describe arm retraction or clearance constraints.</li>
</ul>

<p>No single heuristic, admissible or inadmissible, gives uniformly good guidance across the entire state space.</p>

<p>This motivates algorithms that can leverage many heuristics simultaneously without losing theoretical guarantees.</p>

<h2 id="multi-heuristic-a">Multi-Heuristic A*</h2>

<h3 id="version-1">Version 1</h3>

<ul>
  <li>Given $N$ inadmissible heuristics</li>
  <li>Run $N$ independent searches</li>
  <li>Hope one of them reaches goal</li>
</ul>

<p>Problems:</p>

<ul>
  <li>Each search has its own local minima</li>
  <li>$N$ times more work</li>
  <li>No completeness guarantees or bounds on solution quality</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>Within the while loop of the ComputePath function:
	for i=1‚Ä¶N:
		remove s with the smallest [f(s) = g(s)+w1*h(s)] from OPENi ;
		expand s;
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/images/Search_algorithms/image%201.png" alt="image.png" /></p>

<h3 id="version-2">Version 2</h3>

<ul>
  <li>Given $N$ inadmissible heuristics</li>
  <li>Run $N$ independent searches</li>
  <li>Hope one of them reaches goal</li>
  <li>Share information (g-values) between searches</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>Within the while loop of the ComputePath function (CLOSED is shared):
	for i=1‚Ä¶N:
		remove s with the smallest [f(s) = g(s)+w1*h(s)] from OPENi ;
		expand s and also insert/update its successors into all other OPEN lists;
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/images/Search_algorithms/image%202.png" alt="image.png" /></p>

<p>Benefits:</p>

<ul>
  <li>Searches help each other to circumvent local minima</li>
  <li>States are expanded at most once across ALL searches</li>
</ul>

<p>Remaining Problem: No completeness guarantees or bounds on solution quality</p>

<h3 id="version-3">Version 3</h3>

<ul>
  <li>Given $N$ inadmissible heuristics</li>
  <li>Run $N$ independent searches</li>
  <li>Hope one of them reaches goal</li>
  <li>Share information (g-values) between searches</li>
  <li>Search with admissible heuristics controls expansions</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>Within the while loop of the ComputePath function 
(CLOSED is shared among searches 1‚Ä¶N. Search 0 has its own CLOSED):
	for i=1‚Ä¶N:
		if(min. f-value in OPENi ‚â§ w2* min. f-value in OPEN0 )
			remove s with the smallest [f(s) = g(s)+w1*hi(s)] from OPENi ;
			expand s and also insert/update its successors into all other OPEN lists;
		else
			remove s with the smallest [f(s) = g(s)+w1*h0(s)] from OPEN0 ;
			expand s and also insert/update its successors into all other OPEN lists;
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/images/Search_algorithms/image%203.png" alt="image.png" /></p>

<p>The algorithm runs $N+1$ parallel A*-like searches:</p>

<ul>
  <li>Search 0: the anchor search using admissible $h_0$</li>
  <li>Search 1‚Ä¶N: searches using inadmissible heuristics $h_1, h_2,‚Ä¶,h_N$</li>
</ul>

<p>Each search has its own OPEN list (priority queue).</p>

<p>But:</p>

<ul>
  <li>searches 1‚Ä¶N share the same CLOSED list</li>
  <li>search 0 has its own CLOSED list</li>
  <li>all searches share g-values</li>
</ul>

<p>If the inadmissible search is ‚Äúgood enough‚Äù: Search i‚Äôs best node looks promising enough compared to the anchor search.</p>

\[\text{min}f_i \le w_2.\text{min}f_0\]

<p>If the inadmissible search is not good enough: Search i‚Äôs best node is not competitive. Do not trust its heuristic right now.</p>

\[\text{min}f_i &gt; w_2.\text{min}f_0\]

<p><strong>Need for two weights:</strong></p>

<ul>
  <li>$w_1$ inflates heuristics inside each individual search (making $f=g+w_1h$)</li>
  <li>$w_2$ compares searches against the anchor and decides if inadmissible searches are allowed to expand</li>
</ul>

<p>Together, they ensure:</p>

\[\text{cost(solution)} \le w_1 w_2 \cdot \text{optimal cost}\]

<ul>
  <li>Searches 1‚Ä¶N (inadmissible): share a single CLOSED list
    <ul>
      <li>so no state is expanded more than once among all inadmissible searches</li>
    </ul>
  </li>
  <li>Anchor search has a separate CLOSED list
    <ul>
      <li>so anchor may expand a state once, even if inadmissible searches already expanded it</li>
    </ul>
  </li>
</ul>

<p>So each state is expanded at most twice.</p>

<p><img src="/assets/images/Search_algorithms/image%204.png" alt="image.png" /></p>

<h1 id="interleaving-planning-and-execution">Interleaving Planning and Execution</h1>

<p>In real-world robotics, planning is not a one-time activity. The robot must continuously update its plan while moving, because the world is not fully known beforehand, and conditions may change during execution.</p>

<p>Typical reasons why planning must be repeated are:</p>

<ul>
  <li>The environment is only partially known (unknown obstacles revealed during motion).</li>
  <li>The environment might be dynamic (people, vehicles, or other robots moving).</li>
  <li>The robot may not follow its plan perfectly due to actuation or drift errors.</li>
  <li>Its state estimation may be imprecise, causing localization errors.</li>
</ul>

<p>In such cases, the robot must be able to re-plan quickly instead of starting A* from scratch.</p>

<p>When a robot must plan repeatedly, there are three main approaches:</p>

<ol>
  <li>Anytime heuristic search ‚Äî returns the best available solution within a time limit and keeps improving it.</li>
  <li>Incremental heuristic search ‚Äî speeds up repeated searches by reusing past search efforts, like g-values, OPEN lists, or search trees.</li>
  <li>Real-time heuristic search ‚Äî only plans a few steps ahead, executes them, then re-plans later.</li>
</ol>

<h2 id="anytime-heuristic-search">Anytime Heuristic Search</h2>

<p>A basic idea of Anytime search is to start with a fast, suboptimal solution and improve it as more time becomes available.</p>

<p>Weighted A* uses:</p>

\[f(s) = g(s) + \epsilon \cdot h(s), \quad \epsilon &gt; 1\]

<ul>
  <li>With large $\epsilon$, the solution is found quickly but may not be optimal.</li>
  <li>As $\epsilon$ approaches 1, the solution becomes optimal but search becomes slower.</li>
</ul>

<p>A simple but inefficient approach: run Weighted A* multiple times with decreasing $\epsilon$ ( $2.5 \rightarrow 1.5 \rightarrow 1.0$ ), restarting every time. But this is wasteful, because many values do not change between searches.</p>

<h3 id="ara-anytime-repairing-a">ARA<em>: Anytime Repairing A</em></h3>

<p>ARA* improves on this by reusing the results of previous searches, instead of restarting from scratch.</p>

<p>ARA* solves this by:</p>

<ul>
  <li>Quickly returning a first solution using a high weight</li>
  <li>Reducing the weight step-by-step to improve solution quality</li>
  <li>Reusing previous g-values, OPEN lists, and search tree structure</li>
  <li>Avoiding needless re-expansions</li>
</ul>

<p><strong>Pseudocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre>main()
g(s_start) = 0; all other g-values = INF;
OPEN = {s_start};
computePathWithReuse();

all v-values are initially INF;
initialize OPEN with all overconsistent states;
computePathWithReuse()
while(f(s_goal) &gt; minimum f-value in OPEN)
	remove s with the smallest [f(s) = g(s)+h(s)] from OPEN;
	insert s into CLOSED;
	v(s) = g(s);
	for every successor s‚Äô of s such that s‚Äô not in CLOSED:
		if g(s‚Äô) &gt; g(s) + c(s,s‚Äô):
			g(s‚Äô) = g(s) + c(s,s‚Äô);
			insert s‚Äô into OPEN;
</pre></td></tr></tbody></table></code></pre></div></div>

\[g(s') = \text{min}_{s'' \in pred(s')}v(s'')+c(s'',s')\]

<p>$OPEN:$ set of all states with $v(s) &gt; g(s)$. All other states have $v(s)=g(s).$</p>

<p>ARA* keeps two cost values for each state:</p>

<ul>
  <li>The current best-known cost $g(s)$</li>
  <li>The cost when the state was last expanded $v(s)$. If the state was never expanded, then its value is $INF$.</li>
</ul>

<p>The difference between these two tells ARA* whether a state needs to be re-expanded under the new (lower) weight.</p>

<ul>
  <li>
    <p>Consistent: $v(s) = g(s)$</p>

    <p>The state‚Äôs value matches the last expansion. So no rework is needed.</p>
  </li>
  <li>
    <p>Overconsistent: $v(s) &gt; g(s)$</p>

    <p>The state‚Äôs cost has improved since it was expanded. So it might need re-expansion.</p>
  </li>
  <li>
    <p>Underconsistent: $v(s) &lt; g(s)$</p>

    <p>State‚Äôs cost has worsened (rare unless environment changes). So also needs to be fixed.</p>
  </li>
</ul>

<p>ARA* re-expands only the inconsistent states rather than the whole search space.</p>

<p><strong>How ARA* Improves the Solution Without Restarting?</strong></p>

<p>When the weight $\epsilon$ decreases:</p>

<ol>
  <li>ARA* inserts all inconsistent states into OPEN</li>
  <li>It runs a Weighted-A*-like search again, but:
    <ul>
      <li>g-values remain</li>
      <li>the old search tree remains</li>
      <li>states that were consistent stay untouched</li>
    </ul>
  </li>
  <li>Expansion continues until:
    <ul>
      <li>A new solution with the smaller $\epsilon$ is found</li>
      <li>No g-value can be further improved</li>
    </ul>
  </li>
</ol>

<p>This allows ARA* to give multiple solutions, each guaranteed to be within $\epsilon.$optimal.</p>

<p>At every iteration, ARA* guarantees:</p>

\[\text{cost(solution)} \le \epsilon \cdot \text{cost(optimal)}\]

<p><strong>Pros:</strong></p>

<p>ARA* saves time due to:</p>

<ul>
  <li>Reusing previous g(s) values</li>
  <li>Reusing the previous OPEN list</li>
  <li>Only expanding inconsistent states</li>
  <li>Avoiding recomputation of the whole search tree</li>
  <li>Avoiding redundant expansions</li>
</ul>

<p><strong>Pseudocode using weighted A*:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre>main()
g(s_start) = 0; all other g-values = INF;
OPEN = {s_start};
while e ‚â• 1
	CLOSED = {}; INCONS = {};
	computePathwithReuse();
	publish current e suboptimal solution;
	decrease e;
	initialize OPEN = OPEN U INCONS;

all v-values are initially INF;
initialize OPEN with all overconsistent states;
computePathWithReuse()
while(f(s_goal) &gt; minimum f-value in OPEN)
	remove s with the smallest [f(s) = g(s)+eh(s)] from OPEN;
	insert s into CLOSED;
	v(s) = g(s);
	for every successor s‚Äô of s:
		if g(s‚Äô) &gt; g(s) + c(s,s‚Äô):
			g(s‚Äô) = g(s) + c(s,s‚Äô);
			if s' not in CLOSED then insert s‚Äô into OPEN;
			otherwise insert s' into INCONS
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">if s' not in CLOSED then insert s‚Äô into OPEN</code> exists because in weighted A*, the g-values are not guaranteed to be optimal. So even if a successor has been expanded before, there is a chance that its g-value can be made better. Hence it is updated but it is not put in the OPEN list again because it has been expanded before.</p>

<p>In regular A*, the g-values are already optimal if the state is CLOSED, so they don‚Äôt need to be updated.</p>

<p><img src="/assets/images/Search_algorithms/image%205.png" alt="image.png" /></p>

<p>Example:</p>

<p><img src="/assets/images/Search_algorithms/image%206.png" alt="image.png" /></p>

<h2 id="real-time-heuristic-search">Real-Time Heuristic Search</h2>

<p>Traditional A* and its variants assume that planning is completed before execution begins. This assumption breaks down in realistic robotics settings:</p>

<ul>
  <li>The environment is often partially known, and important obstacles become visible only during execution.</li>
  <li>The world may be dynamic, forcing the robot to react immediately to avoid collisions.</li>
  <li>Robots experience imprecise motion and noisy localization, which invalidates long pre-computed plans.</li>
  <li>Some tasks require millisecond-level reactivity, making full-path planning infeasible.</li>
</ul>

<p>Real-time search treats planning and acting as a continuous loop: gather sensor data ‚Üí plan a few steps ‚Üí execute one step ‚Üí repeat.</p>

<p>Real-time search limits the amount of planning:</p>

<ul>
  <li>At each iteration, the agent may expand only N nodes (where N is small and fixed).</li>
  <li>After that limited search, the agent chooses the next action that looks best.</li>
  <li>The agent then moves exactly one step, updates the map, and begins the next lookahead.</li>
</ul>

<p>This approach ensures that the robot always responds in bounded time, even in unknown or rapidly changing environments. It never aims to produce a global path immediately; instead, it incrementally constructs a path through repeated short-range planning.</p>

<p>When the map is unknown, real-time search relies on the Freespace Assumption: All unexplored cells are assumed to be free until the robot‚Äôs sensors reveal otherwise. This lets the planner behave optimistically and aggressively move toward the goal. When the robot detects new obstacles, they are incorporated into the map and the next real-time search cycle adapts accordingly. The Freespace Assumption enables motion in unknown worlds but also creates the need for heuristic correction because the robot can fall into local minima where the heuristic dramatically underestimates real cost-to-go.</p>

<p><img src="/assets/images/Search_algorithms/image%207.png" alt="image.png" /></p>

<h3 id="lrta-learning-real-time-a">LRTA*: Learning Real-Time A*</h3>

<p>At each iteration:</p>

<ol>
  <li>
    <p>The agent evaluates all immediate successors $s‚Äô$ of the current state $s$ and moves to the minimum, as follows:</p>

\[s = \text{argmin}_{s' \in succ(s)}c(s,s')+h(s')\]
  </li>
  <li>
    <p>Before leaving state $s$, LRTA* updates the heuristic:</p>

\[h(s) \leftarrow \min_{s' \in succ(s)} \left( c(s, s') + h(s') \right)\]
  </li>
</ol>

<p>This update increases the heuristic at states where the robot got stuck, reducing the attractiveness of that region. Over many steps, LRTA* guarantees that the heuristic monotonically increases toward the true cost-to-go.</p>

<p>LRTA* thus learns a better heuristic online and ensures eventual escape from every local minimum.</p>

<p><img src="/assets/images/Search_algorithms/image%208.png" alt="image.png" /></p>

<p><img src="/assets/images/Search_algorithms/image%209.png" alt="image.png" /></p>

<h3 id="convergence-properties-of-lrta">Convergence Properties of LRTA*</h3>

<p>LRTA* guarantees that the robot will reach the goal in a finite number of steps:</p>

<ul>
  <li>If the graph is finite, action costs are positive, and a solution exists, LRTA* will reach the goal in finite time.</li>
  <li>It will never oscillate infinitely because heuristic updates eliminate the cause of repeated cycles.</li>
  <li>The learned heuristic converges to a consistent, admissible function that reflects encountered terrain.</li>
  <li>All actions are reversible.</li>
  <li>h-values remain admissible and consistent.</li>
</ul>

<p>This makes LRTA* suitable for unknown environments where the robot must ‚Äúfeel its way‚Äù toward the goal.</p>

<h3 id="multi-step-lookahead-in-lrta">Multi-Step Lookahead in LRTA*</h3>

<p>Basic LRTA* examines only immediate neighbors, which can be shortsighted.</p>

<p>A more powerful variant performs an N-step lookahead:</p>

<ul>
  <li>Expand up to N nodes using a limited search (A*, BFS, or Dijkstra-like expansion).</li>
  <li>Compute locally optimal g-values for these nodes.</li>
  <li>
    <p>Use local dynamic programming updates:</p>

\[h(s) = \min_{s' \in succ(s)} \left( c(s, s') + h(s') \right)\]
  </li>
  <li>
    <p>Move on path to the state $s$</p>

\[s = \text{argmin}_{s' \in OPEN}g(s')+h(s')\]
  </li>
</ul>

<p>This deeper lookahead reduces the number of heuristic updates needed and helps the robot anticipate dead ends earlier.</p>

<h3 id="rtaa-real-time-adaptive-a">RTAA*: Real-Time Adaptive A*</h3>

<p>RTAA* is a more efficient form of heuristic learning suitable for large-scale real-time robotics. Instead of performing full DP updates like LRTA<em>, RTAA</em> computes a single batch update per cycle.
After expanding a bounded search frontier during lookahead, RTAA* identifies the most promising frontier node $s^*$ (the one minimizing $g+h$), and updates the heuristics of all expanded states using:</p>

\[h(u) \leftarrow f(s^*) - g(u)\]

<h1 id="prm">PRM</h1>

<p>For manipulation tasks, the robot‚Äôs configuration is a vector of joint angles $Q={q_1,\ldots,q_n}$, which defines a point in a continuous, high-dimensional space called configuration space (C-space). Planning requires computing a continuous path from $Q_{start}$ to $Q_{goal}$ that satisfies all constraints:</p>

<ul>
  <li>Joint limits</li>
  <li>No collisions with obstacles</li>
  <li>No self-collisions</li>
</ul>

<p>A direct grid discretization of this space becomes infeasible because the number of grid cells grows exponentially with dimension. Although resolution-complete planners guarantee completeness and quality bounds, they become computationally intractable in 6- to 12-dimensional spaces typically encountered in manipulation.</p>

<p>Sampling-based methods exploit the insight that although the configuration space is high-dimensional, the free space is continuous and ‚Äúbenign‚Äù, meaning a sparse set of samples can capture enough structure of the environment to allow feasible paths to be found.</p>

<h3 id="overview">Overview</h3>

<p>PRMs operate in two phases:</p>

<p><strong>Preprocessing Phase (Learning Phase)</strong></p>

<p>A roadmap graph $G$ is built using random samples in $C_{free}$. Each sample is connected to nearby samples through short, locally planned paths, typically straight-line interpolation in joint space.</p>

<p><strong>Query Phase</strong></p>

<p>Given arbitrary start and goal configurations:</p>

<ol>
  <li>Connect both to the roadmap using a local planner.</li>
  <li>Run a graph search (A<em>, Dijkstra, Weighted A</em>) on the augmented roadmap.</li>
</ol>

<p>If the roadmap is sufficiently dense, start and goal will connect to the same component of the graph, making a valid continuous path available.</p>

<p>A single roadmap can answer many queries efficiently, which is why PRMs are well suited for multi-query environments such as manipulation in structured spaces.</p>

<h3 id="preprocessing-phase">Preprocessing Phase</h3>

<p>Sampling: We repeatedly generate random configurations $\alpha(i)$ in $C_{free}$. Each sample must satisfy all constraints (no collisions, valid joint limits). Samples can be uniform or follow more sophisticated biasing strategies.</p>

<p>Adding Vertices: Whenever a sample lies in free space, it is added as a vertex in the roadmap graph.</p>

<p>Neighborhood Selection: For each new sample, we identify a set of nearby vertices in the existing graph. Some commonly used neighborhood definitions:</p>

<ol>
  <li>K nearest neighbors (distance in configuration space)</li>
  <li>Nearest neighbor in each connected component of the roadmap</li>
  <li>All vertices within a radius $r$</li>
</ol>

<p>The goal is to avoid creating too many edges while guaranteeing connectivity.</p>

<p>Connecting Neighbors: For each neighbor vertex $q$, we call a local planner to determine whether a smooth, collision-free path exists between $\alpha(i)$ and $q$. Even a simple straight-line interpolation in joint space is often sufficient. If this local plan is collision-free, an edge is added to the graph.</p>

<p>Connected Components: The roadmap grows organically, gradually connecting isolated components until the free space is covered well enough to support queries.</p>

<p><strong>Pseudocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>BUILD_ROADMAP
   G.init();  i ‚Üê 0;
   while i &lt; N
       if Œ±(i) ‚àà C_free then
           G.add_vertex(Œ±(i));  i ‚Üê i + 1;
           for each q ‚àà NEIGHBORHOOD(Œ±(i), G)
               if ( (not G.same_component(Œ±(i), q)) and CONNECT(Œ±(i), q) ) then
                   G.add_edge(Œ±(i), q);

</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="query-phase-using-the-roadmap">Query Phase: Using the Roadmap</h3>

<p>Once the roadmap is built, answering queries becomes fast:</p>

<ol>
  <li>Given $q_I$ and $q_G$, connect each to nearby vertices in the roadmap using the same local planner</li>
  <li>Insert both into the graph as temporary nodes</li>
  <li>Use a shortest-path algorithm such as Dijkstra‚Äôs or A*</li>
  <li>If both nodes lie in the same connected component, the resulting path is a valid collision-free motion</li>
</ol>

<h3 id="sampling-strategies">Sampling Strategies</h3>

<p>Uniform sampling is the simplest approach but can be highly inefficient. Certain regions of configuration space ‚Äî such as narrow passages between obstacles ‚Äî have extremely small measure and are rarely sampled. PRMs address this with multiple sampling strategies:</p>

<ol>
  <li>Uniform sampling: Samples drawn uniformly from free space. Fast and easy, but may fail to represent narrow corridors adequately.</li>
  <li>Connectivity-Based Sampling: Sample existing vertices with probability inversely proportional to how well-connected they are. Poorly connected vertices (likely in narrow regions) get sampled more often, increasing coverage of difficult areas.</li>
  <li>Obstacle-Biased Sampling: Sampling is biased toward the boundary of obstacles. This helps generate samples near constraints where the roadmap usually needs more detail.</li>
  <li>Gaussian Sampling: Gaussian sampling generates small clusters of samples around a chosen point. If one sample lies in collision and another lies in free space, the free one is kept. This produces samples near obstacle boundaries without explicitly detecting the boundary.</li>
  <li>Bridge Sampling: Sampling $q_1, q_2, q_3$ from a Gaussian and keeping the middle configuration if it is in free space but neighbors are in collision. This helps find narrow passages effectively.</li>
  <li>Sampling Away from Obstacles: Useful in cluttered environments where the robot needs large free areas well represented.</li>
</ol>

<p>Each sampling heuristic targets a failure mode of uniform sampling, making the resulting roadmap more robust.</p>

<h3 id="pros-and-cons">Pros and Cons</h3>

<p>PRMs excel in multi-query, high-dimensional spaces:</p>

<p><strong>Strengths</strong></p>

<ul>
  <li>Extremely efficient once the roadmap is built</li>
  <li>Scales well to high-dimensional C-spaces</li>
  <li>Easy to integrate with collision checkers and kinematic constraint solvers</li>
  <li>Probabilistically complete</li>
</ul>

<p><strong>Weaknesses</strong></p>

<ul>
  <li>Roadmap building can be expensive initially</li>
  <li>Difficult to handle dynamic environments (PRM is primarily offline)</li>
  <li>Narrow passages require careful sampling strategies</li>
  <li>Connectivity depends heavily on sample quality</li>
</ul>

<h1 id="rrt">RRT</h1>

<p>RRTs are particularly effective for single-query planning problems where a robot must find a feasible path between a specific start and goal configuration but does not expect to reuse planning work later. They scale well to high-dimensional configuration spaces and do not require a preprocessing phase. Compared to PRMs, RRTs sacrifice optimality but gain significant speed and robustness, especially when collision checking is expensive or the environment is cluttered.</p>

<h3 id="basic-structure">Basic Structure</h3>

<p>An RRT begins with a tree whose root is the start configuration. At each iteration, the planner draws a random configuration from the configuration space and finds the closest node in the tree. Instead of jumping directly to the sample, the algorithm takes only a small step toward it. This step is usually a straight-line interpolation in configuration space. If this short motion is collision-free, the new configuration is added to the tree.</p>

<p>This fundamental operation is the EXTEND operator. Conceptually, EXTEND tries to walk from a known configuration toward a random sample by a limited distance. This simple mechanism produces a tree that organically spreads into every region of the free space, with more growth where space is wide open and less growth in constrained areas.</p>

<p>The algorithm terminates when the tree grows sufficiently close to the goal, or when a direct local connection to the goal becomes feasible.</p>

<p><strong>Psuedocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre>BUILD_RRT
  T.init(q_init);
  for k = 1 to K
      q_rand ‚Üê RANDOM_STATE();
      EXTEND(T, q_rand);
      
EXTEND(T, q):
      q_near ‚Üê NEAREST_NEIGHBOR(q, T);
      q_new  ‚Üê EXTEND(q_near, q);
      if q_new ‚â† NULL then
          T.add_vertex(q_new);
          T.add_edge(q_near, q_new);
          if q_new = q_goal then
              return PATH(T, q_goal);
  return PATH_NOT_FOUND;

</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="limitations">Limitations</h3>

<p>Although RRTs are powerful explorers, they do not compute optimal or even high-quality paths. Because the algorithm grows outward from the start without explicitly optimizing anything, the resulting path is usually long, jagged, and geometrically irregular. Once a branch is created, the tree structure restricts future updates; the planner cannot easily reorganize previous decisions.</p>

<p>As a result, RRTs tend to lock themselves into a single homotopy class early on. They cannot ‚Äúundo‚Äù their earlier tree structure, so even with more computation, the path does not improve. This makes standard RRT a great planner for fast feasibility but a poor choice for optimal path quality.</p>

<h2 id="rrt-connect-a-faster-bi-directional-variant">RRT-Connect: A Faster Bi-Directional Variant</h2>

<p>RRT-Connect significantly accelerates planning by growing two trees simultaneously: one from the start configuration and one from the goal. At each iteration, a random sample is drawn, and one tree grows toward it using the standard EXTEND step. Once that extension succeeds, the algorithm immediately attempts to ‚Äúpull‚Äù the other tree toward the newly created node.</p>

<p>This ‚Äúaggressive connection‚Äù uses a CONNECT operation, which repeatedly performs EXTEND toward the same sample until either a collision is detected or the sample is reached. The result is a strong contraction motion: the two trees attempt to meet in the middle as fast as possible.</p>

<p>RRT-Connect tends to find feasible paths much faster than basic RRT. It inherits the same exploration bias and probabilistic completeness, but because of bi-directional growth and aggressive connection, it usually solves hard planning problems in far fewer expansions. However, like RRT, it does not produce optimal solutions.</p>

<p>The mechanics of EXTEND can be written as:</p>

\[q_{\text{new}} = q_{\text{near}} + \varepsilon \cdot \frac{q_{\text{rand}} - q_{\text{near}}}{|q_{\text{rand}} - q_{\text{near}}|}\]

<p>The algorithm adds $q_{new}$ to the tree only if the local connection segment:</p>

\[\tau(s) = q_{\text{near}} + s(q_{\text{new}} - q_{\text{near}}), \quad s \in [0,1]\]

<p>lies entirely inside $C_{free}.$</p>

<p>This geometric structure ensures the tree grows smoothly rather than in erratic jumps.</p>

<p><strong>Pseudocode:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre></td><td class="rouge-code"><pre>RRT_CONNECT(q_start, q_goal, N, Œµ):
    T_start.init(q_start)
    T_goal.init(q_goal)

    for k = 1 to N:
        q_rand ‚Üê SAMPLE_CONFIG()

        # 1) Grow T_start toward the random sample
        if EXTEND(T_start, q_rand, Œµ) ‚â† TRAPPED:
            q_new ‚Üê LAST_ADDED_VERTEX(T_start)

            # 2) Try to CONNECT T_goal toward q_new aggressively
            if CONNECT(T_goal, q_new, Œµ) = REACHED:
                return EXTRACT_PATH_BIDIRECTIONAL(T_start, T_goal, q_new)

        SWAP(T_start, T_goal)    # alternate which tree is "start" vs "goal"

    return FAILURE

EXTEND(T, q_target, Œµ):
    q_near ‚Üê NEAREST_NEIGHBOR(T, q_target)
    q_new  ‚Üê STEER(q_near, q_target, Œµ)

    if COLLISION_FREE(q_near, q_new):
        T.ADD_VERTEX(q_new)
        T.ADD_EDGE(q_near, q_new)
        if q_new = q_target:
            return REACHED
        else:
            return ADVANCED
    else:
        return TRAPPED

CONNECT(T, q_target, Œµ):
    status ‚Üê ADVANCED
    while status = ADVANCED:
        status ‚Üê EXTEND(T, q_target, Œµ)
    return status

</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="why-rrt-paths-need-smoothing">Why RRT Paths Need Smoothing</h3>

<p>Because RRTs grow by random exploration, the final paths are rarely smooth. They often zig-zag through space inefficiently. A common remedy is the short-cutting technique, which repeatedly attempts to bypass intermediate vertices on the path by checking whether the straight-line segment between two non-adjacent configurations is collision-free.</p>

<p>If the shortcut succeeds, the interior of the path is replaced with the direct link:</p>

\[\text{if } \texttt{CONNECT}(q_i, q_j) = \text{free} \quad\Rightarrow\quad \text{replace segment }( q_i \to q_{i+1} \to \dots \to q_j ) \text{ with } (q_i \to q_j)\]

<p>This tends to produce significantly shorter and smoother paths while keeping computation lightweight.</p>

<h2 id="rrt-1">RRT*</h2>

<p>RRT* was introduced to address the largest limitation of standard RRT: its inability to improve path quality over time. RRT* modifies the tree structure so that every new sample can potentially improve not just its own branch but also the structure of the nearby tree.</p>

<p>When a new node $x_{new}$ is added:</p>

<ol>
  <li>
    <p>Parent Selection:
RRT* considers all tree nodes within a radius $r(n)$ around $x_{new}$. Among these candidates, it chooses as parent the neighbor that minimizes the total path cost:</p>

\[x_{\text{parent}} = \arg\min_{x \in X_{\text{near}}} \big( \text{Cost}(x) + c(x, x_{\text{new}}) \big)\]
  </li>
  <li>
    <p>Rewiring:</p>

    <p>After adding $x_{new}$, the planner checks whether using $x_{new}$  as a parent would reduce the cost of any neighboring nodes. If so, those nodes are ‚Äúrewired‚Äù to point to $x_{new}$, improving the overall tree.</p>
  </li>
</ol>

<p>The rewiring step is the critical innovation: it continuously reorganizes the tree as new samples arrive, allowing the planner to refine earlier decisions instead of being trapped by them.</p>

<h3 id="asymptotic-optimality-of-rrt">Asymptotic Optimality of RRT*</h3>

<p>RRT* is asymptotically optimal, unlike RRT or RRT-Connect.</p>

<p>As the number of samples $n$ approaches infinity:</p>

\[\lim_{n \to \infty} \text{Cost}( \text{best path from RRT}^* )

= \text{Cost}( \text{optimal path} )\]

<p>This result is provable and depends on choosing the neighbor radius:</p>

\[r(n) = \gamma \left( \frac{\log n}{n} \right)^{1/d}\]

<p>where $d$ is the dimension of the configuration space.</p>

<p>Under this radius schedule, the tree becomes dense enough to guarantee optimal connections while still maintaining computational efficiency.</p>

<h3 id="trade-offs-between-rrt-rrt-connect-and-rrt">Trade-Offs Between RRT, RRT-Connect, and RRT*</h3>

<p>There is a clear computational-optimality trade-off between these methods:</p>

<ul>
  <li>RRT is extremely fast and good at finding any feasible path, but provides very low path quality.</li>
  <li>RRT-Connect is usually the fastest at finding a feasible path, especially in cluttered spaces.</li>
  <li>RRT* is slower per iteration but continuously improves the solution, eventually reaching the optimum.</li>
</ul>

<p>In practice, the choice depends on the task:</p>

<ul>
  <li>For real-time feasibility (e.g., robot moving through unknown space), RRT or RRT-Connect is preferred.</li>
  <li>For motion tasks requiring precise, smooth, or cost-minimal paths (e.g., manipulation, surgical robots), RRT* provides the needed optimality.</li>
</ul>

<h1 id="markov-property">Markov Property</h1>

<p>In search-based planning, the Markov property means: When you stand in a state $s$, everything you need to decide ‚Äì what successors you can go to and what they will cost ‚Äì is fully determined by the current state only, not by how you got there.</p>

<p>So:</p>

\[\text{succ}(s) = \text{function of } s\]

\[c(s, s') = \text{function of } (s, s'), \quad s' \in \text{succ}(s)\]

<p>No ‚Äúmemory‚Äù of the previous path is allowed in $succ$ or $c$ (i.e., no dependence on the history of the path leading up to it).</p>

<p>Algorithms like Dijkstra, A*, dynamic programming, etc., assume that if you reach the same state again with higher cost, that path is always worse for the future. This is only true if the future behavior (successors and costs) depends only on the state, not on the path you took.</p>

<p>If the Markov property is broken:</p>

<ul>
  <li>You might incorrectly prune states</li>
  <li>The algorithm can become incomplete (miss a solution) or sub-optimal</li>
</ul>

<h2 id="independent-vs-dependent-variables">Independent vs Dependent Variables</h2>

<p>Each search state $s$ contains a set of variables:</p>

\[X(s) = \{ X_{\text{ind}}(s), X_{\text{dep}}(s) \}\]

<p><strong>Independent variables $X_{ind}(s)$:</strong></p>

<ul>
  <li>These define the state identity</li>
  <li>Two states are the same if and only if their independent variables are equal</li>
</ul>

\[s = s' \quad \text{iff} \quad X_{\text{ind}}(s) = X_{\text{ind}}(s')\]

<p><strong>Dependent variables $X_{dep}(s)$:</strong></p>

<ul>
  <li>Variables derived from the independent variables</li>
  <li>Used for successor generation, cost computation, heuristics, collision checks</li>
</ul>

<p>The Markov property holds if dependent variables depend only on the independent ones:</p>

\[X_{\text{dep}}(s) = f\big(X_{\text{ind}}(s)\big)\]

<p>This means the planner can fully determine successors, transition costs, valid or invalid states, just from the current state itself.</p>

<p>Markov property is violated when you compute dependent variables using history:</p>

\[X_{\text{dep}}(s) = f\big(X_{\text{ind}}(s), \text{history}\big)\]

<h2 id="incompleteness">Incompleteness</h2>

<p>Incompleteness arises when your state representation or successor function violates the assumptions needed for graph search. It can happen for three major reasons:</p>

<ul>
  <li>Incorrect state definition (missing independent variables)</li>
  <li>Dependent variables rely on history</li>
  <li>Successor generation fails to enumerate all possible transitions</li>
</ul>

<p>To guarantee completeness:</p>

<ul>
  <li>Include every future-relevant variable in $X_{ind}$</li>
  <li>Ensure dependent variables are pure functions of $X_{ind}$</li>
  <li>Ensure successor generation is fully determined</li>
  <li>Ensure edge costs depend only on $(s,s‚Äô)$</li>
</ul>

<h2 id="dominance">Dominance</h2>

<p>Dominance is a pruning rule used in optimal search to reduce the number of states explored. The basic idea is that if two states represent the exact same situation, but one reached that situation with a lower g-value, and both share the same future possibilities, then the more expensive state can be discarded. This makes the search faster without losing optimality.</p>

<p>Two states can only be compared if they are identical in all independent variables:</p>

\[X_{\text{ind}}(s_1) = X_{\text{ind}}(s_2)\]

<p>If they differ in any independent variable, then they cannot dominate each other.</p>

<p>Dominance only works if independent variables fully determine all future successors and costs. That is the Markov property.</p>

\[\text{succ}(s) = f(X_{\text{ind}}(s)), \quad

c(s,s') = g(X_{\text{ind}}(s), X_{\text{ind}}(s'))\]

<h1 id="task-planning">Task Planning</h1>

<p>Task planning is a branch of robotics that deals with reasoning about high-level actions ‚Äî actions that manipulate objects, change symbolic states of the world, and achieve complex goals. Unlike motion planning, which concerns itself with geometric paths and kinematic feasibility, task planning is fundamentally symbolic. It focuses on how the world can be transformed step-by-step through discrete, logically defined actions.</p>

<p>Task planning involves deciding what sequence of actions a robot must perform to accomplish a goal. These actions may involve picking objects up, stacking them, assembling parts, or performing multi-step procedures such as constructing a birdcage. The key challenge is not continuous robot motion but rather logical ordering: which actions must precede others, what dependencies exist, and how actions change the state of the world.</p>

<h2 id="blocksworld-example">Blocksworld Example</h2>

<p><img src="/assets/images/Search_algorithms/image%2010.png" alt="image.png" /></p>

<p>One of the most famous examples in symbolic AI is Blocksworld. In this environment, various blocks can be stacked on one another or placed on a table. The robot‚Äôs task is to transform an initial arrangement of blocks into some desired configuration.</p>

<p>For instance, consider a start state where block A sits on block B, and block C sits on the table. The goal state may require placing C on A, and A on the table. The core challenge is deciding in what order the robot must move blocks to reach this arrangement. Movement is constrained ‚Äî for example, the robot can only move blocks that have no other block on top of them.</p>

<p>The key insight is that Blocksworld is purely symbolic. The robot does not care about precise coordinates or continuous motion but instead about discrete relationships like:</p>

<ul>
  <li>‚ÄúOn(A, B)‚Äù</li>
  <li>‚ÄúClear(A)‚Äù (nothing is stacked on top of A)</li>
  <li>‚ÄúBlock(C)‚Äù (C is a movable block)</li>
</ul>

<p>These predicates define the symbolic state of the world.</p>

<h3 id="representing-blocksworld-as-a-state-space-graph">Representing Blocksworld as a State-Space Graph</h3>

<p>To use search algorithms, we must represent Blocksworld as a graph where each node is a symbolic state and edges correspond to actions. A state is simply the collection of all true statements (predicates) about the world at that moment. For example:</p>

\[\text{State} =

{ \text{On(A, B)},  \text{On(B, Table)},\ \text{On(C, Table)},\ \text{Clear(A)},\ \text{Clear(C)},\ \text{Block(A)},\ \text{Block(B)},\ \text{Block(C)} }\]

<p>Actions such as <code class="language-plaintext highlighter-rouge">Move(b, x, y)</code> transform one state into another by modifying these predicates. Each action has preconditions that must be true before it can be executed (e.g., the block must be clear, and the target must be clear) and effects that describe how the state changes. If action preconditions are satisfied, the action is applicable, and it creates a successor state in the search graph.</p>

<p>This transforms Blocksworld into a state-space search problem, where A* or breadth-first search can discover the shortest action sequence from the start state to any state matching the goal description.</p>

<h2 id="strips">STRIPS</h2>

<p>STRIPS (Stanford Research Institute Problem Solver) provides a structured way to define symbolic planning problems. It describes a planning problem using three components: state representation, goal representation, and action representation.</p>

<h3 id="state-representation-in-strips">State Representation in STRIPS</h3>

<p>A state is represented as a conjunction of positive literals, meaning it is a set of statements about the world that are true. For example:</p>

\[\text{On(A,B)} \wedge \text{On(B,Table)} \wedge \text{Clear(A)} \wedge \text{Block(A)} \wedge \text{Block(B)}\]

<p>Crucially, STRIPS uses the closed-world assumption: any fact not listed in the state is assumed to be false. This means we only list what is true; everything else is false by default.</p>

<h3 id="goal-representation-in-strips">Goal Representation in STRIPS</h3>

<p>Goals in STRIPS are also defined as a conjunction of positive literals. The goal may be fully specified (e.g., <code class="language-plaintext highlighter-rouge">On(B,C) ‚àß On(C,A) ‚àß On(A,Table)</code>) or partially specified. A partial goal describes only what must be true, allowing any additional conditions.</p>

<p>For example, ‚ÄúBlock A must be on the table‚Äù would be represented as:</p>

\[\text{On(A,Table)}\]

<p>Any state that satisfies this literal is a valid goal.</p>

<h3 id="action-representation-in-strips">Action Representation in STRIPS</h3>

<p>Actions have two components:</p>

<ol>
  <li><strong>Preconditions:</strong> These are conditions that must be true for the action to be applicable.
They are expressed as a conjunction of positive literals.</li>
  <li><strong>Effects:</strong> Effects describe how the state changes after the action is executed. They consist of:
    <ul>
      <li>Add list (facts that become true)</li>
      <li>Delete list (facts that become false)</li>
    </ul>
  </li>
</ol>

<p>For example, the action <code class="language-plaintext highlighter-rouge">MoveToTable(b,x)</code> has preconditions:</p>

\[\text{On(b,x)} \wedge \text{Clear(b)} \wedge \text{Block(b)} \wedge \text{Block(x)}\]

<p>Its effects include adding:</p>

\[\text{On(b,Table)},  \text{Clear(x)}\]

<p>and deleting:</p>

\[\text{On(b,x)}\]

<p>STRIPS therefore provides a fully formal way to describe how actions transform the world.</p>

<h2 id="from-strips-descriptions-to-graph-search">From STRIPS Descriptions to Graph Search</h2>

<p>Once a domain is represented in STRIPS, we can automatically construct the successor function used in search algorithms. A program reads the preconditions of each action and checks whether they are satisfied in the current state. If so, it applies the effects, generating a successor state.</p>

<p>This converts symbolic descriptions into a search graph implicitly, without manually enumerating states. A*, breadth-first search, or any other search algorithm can operate on this graph to find a valid plan.</p>

<p>This process is known as domain-independent planning because the planner itself is universal ‚Äî only the domain description changes.</p>

<h3 id="example-blocksworld-in-the-symbolic-graph">Example: Blocksworld in the Symbolic Graph</h3>

<p>Start state:</p>

\[\text{On(A,B)} \wedge \text{On(B,Table)} \wedge \text{On(C,Table)} \wedge \

\text{Block(A)} \wedge \text{Block(B)} \wedge \text{Block(C)} \wedge \

\text{Clear(A)} \wedge \text{Clear(C)}\]

<p>Goal state:</p>

\[\text{On(B,C)} \wedge \text{On(C,A)} \wedge \text{On(A,Table)}\]

<p>Given actions like <code class="language-plaintext highlighter-rouge">Move(b,x,y)</code> and <code class="language-plaintext highlighter-rouge">MoveToTable(b,x)</code>, the planner examines whether their preconditions hold in the current symbolic state. If yes, applying the action produces a successor symbolic state.</p>

<p>This creates a branching graph of symbolic states, each representing a different configuration of blocks. We can apply A* or Weighted A* to search this symbolic state space, treating each action as an edge of cost 1 (or any defined cost).</p>

<h3 id="need-for-heuristics">Need for Heuristics</h3>

<p>Symbolic planning expands into enormous state spaces very quickly. With just a few objects and predicates, the number of possible states becomes huge. To make A* practical, we need a heuristic function that estimates how close a symbolic state is to the goal. A heuristic in symbolic planning tries to answer:</p>

\[h(s) = \text{How many steps remain before } s \text{ satisfies all goal predicates?}\]

<p>But symbolic states are not geometric; they are sets of logical predicates. So devising heuristics requires reasoning about the structure of the symbolic problem.</p>

<p><strong>Domain-Independent Heuristic 1: Counting Unsatisfied Goal Literals</strong></p>

<p>The simplest heuristic counts how many goal predicates are missing from the current state. For example, if the goal has three predicates and the current state satisfies only one, then:</p>

\[h(s) = \#{ \text{goal literals not yet true in } s }\]

<p>This heuristic is not admissible, because satisfying one goal literal might require a long chain of actions ‚Äî more than one step. However, even though it breaks admissibility, it is still often useful, especially with Weighted A*, where we intentionally accept suboptimality for speed.</p>

<p>This heuristic is incredibly easy to compute and already restricts the search greatly.</p>

<p><strong>Domain-Independent Heuristic 2: Relaxing Delete Effects (Empty-Delete-List Heuristic)</strong></p>

<p>The more powerful approach is to compute a heuristic by solving a relaxed version of the planning problem. The common relaxation is to assume that: No action deletes any predicate. In other words, actions only add facts; they never remove them.</p>

<p>This is the delete-relaxation or empty-delete-list heuristic, and it turns the planning problem into a monotone logical problem where the state only grows over time. This version is dramatically easier to solve because we never have to undo actions.</p>

<p>The heuristic value is the cost of the optimal solution to this relaxed problem, making it admissible (but expensive to compute). Despite its computational overhead, this relaxation forms the basis of many state-of-the-art planners (e.g., FF, HSP), because it provides extremely informative heuristics that dramatically cut down the search.</p>

<h2 id="total-vs-partial-ordering-of-plans">Total vs Partial Ordering of Plans</h2>

<p>Symbolic planning domains have extremely high branching factors. For example, in Blocksworld with four blocks, you may have dozens of possible actions in each state ‚Äî every movable block can be moved onto every other clear block or the table.</p>

<p>This leads to large branching factors, which greatly slow down state-space search. Therefore, even with STRIPS, storing the full state graph explicitly is impossible. The graph is constructed implicitly, via successor generation rules. But despite this, raw branching can still overwhelm A* unless heuristics are strong.</p>

<p>Traditional STRIPS planning (and A* search on symbolic states) implicitly produces total-order plans, meaning: All actions are placed in a fixed sequence, even if their ordering does not matter.</p>

<p>For example, if the goal requires stacking block C on A and stacking D on B, these two actions are independent: doing C‚ÜíA first or D‚ÜíB first are both permissible.</p>

<p>However, classical planners produce one fixed total ordering. This is unnecessarily rigid. Many tasks permit partial orders ‚Äî some actions can be done in any order or even in parallel. This motivates the introduction of Partial-Order Planning (POP).</p>

<h2 id="partial-order-planning-pop">Partial-Order Planning (POP)</h2>

<p>Partial-Order Planning represents plans not as strict sequences but as collections of actions plus constraints describing which actions must precede which.</p>

<p>A POP ‚Äúplan state‚Äù contains three things:</p>

<ol>
  <li>The currently selected set of actions</li>
  <li>A set of ordering constraints  <code class="language-plaintext highlighter-rouge">A &lt; B</code> meaning ‚ÄúA must happen before B‚Äù. No cycles allowed (i.e., <code class="language-plaintext highlighter-rouge">A &lt; B</code> and <code class="language-plaintext highlighter-rouge">B &lt; A</code> is a cycle and makes such state invalid).</li>
  <li>A set of causal links <code class="language-plaintext highlighter-rouge">A =p&gt; B</code> meaning ‚ÄúA achieves precondition p required by B‚Äù.
required by action B)</li>
</ol>

<p>The key idea is to add constraints only when necessary. The planner builds the plan ‚Äúfrom the top down‚Äù:</p>

<ul>
  <li>Start with a fake Start action and a fake Finish action.</li>
  <li>The Start action asserts all predicates true in the initial state.</li>
  <li>The Finish action requires all predicates in the goal.</li>
</ul>

<p>The planner then gradually adds actions needed to satisfy Finish‚Äôs preconditions, then actions needed to satisfy those actions‚Äô preconditions, and so on.</p>

<p>At each step, it only imposes ordering constraints needed to avoid contradictions or causal violations. This produces a partial-order plan, where any linearization of the graph is a valid execution sequence.</p>

<p>When adding a new action A to satisfy a precondition p for some action B, the planner must ensure that no other action deletes p between A and B. If such an action C exists, the planner resolves the threat by adding either:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">C &lt; A</code> (C happens earlier), or</li>
  <li><code class="language-plaintext highlighter-rouge">B &lt; C</code> (C happens later)</li>
</ul>

<p>If both constraints would violate existing ordering constraints (by creating a cycle), the partial plan becomes invalid and is discarded. This careful handling of ‚Äúthreats‚Äù ensures the consistency of causal links and keeps the plan logically sound.</p>

<p>POP does not search over symbolic world states. Instead, it searches directly in the space of possible partial plans. Each state in the POP search is a partially constructed plan with:</p>

<ul>
  <li>actions included</li>
  <li>ordering constraints</li>
  <li>causal links</li>
</ul>

<p>The search terminates when all preconditions of all actions are satisfied ‚Äî meaning the partial plan encodes a complete, feasible ordering. The search is typically done using depth-first search, because POP states are often deep but narrow, and we are usually satisfied with any working plan, not necessarily the shortest.</p>

<h1 id="planning-under-uncertainty">Planning Under Uncertainty</h1>

<p>Robots rarely operate in perfectly predictable environments. They slip, their actuators have unmodeled dynamics, their sensors may be noisy, and sometimes the world itself behaves unpredictably. Up to this point, much of planning assumed a deterministic world: if the planner tells the robot to move from state $s$ to state $s‚Äô$, it will reach $s‚Äô$ exactly. In these settings, planning reduces neatly to graph search ‚Äî find a sequence of actions that leads from the start to the goal. But the real world violates this assumption constantly. The robot may attempt to go ‚Äúeast‚Äù but drift slightly north; it may attempt to grasp but fail; it may plan along a cliff edge but slip into a dangerous region. Planning while ignoring all these uncertainties leads to solutions that are often suboptimal and sometimes catastrophic.</p>

<p>Deterministic graph search assumes the robot transitions perfectly from one state to the next. But when actions have unpredictable outcomes, executing a fixed plan no longer guarantees arriving at the intended states. The robot might replan on-the-fly when deviations occur, but this reactive strategy is inefficient and risky.</p>

<p>Thus arises the need to model execution uncertainty at planning time, not after failures occur. This is achieved by converting the search graph into an MDP, where at least one action has multiple possible successor states, each with an associated probability and cost.</p>

<h2 id="markov-decision-processes-mdps">Markov Decision Processes (MDPs)</h2>

<p>An MDP extends the deterministic planning framework by allowing each action to have multiple possible outcomes. Formally, an MDP is defined by:</p>

<ul>
  <li>A set of states $S$</li>
  <li>A set of actions $A(s)$ available in each state</li>
  <li>For each action $a$ in state $s$:
    <ul>
      <li>a set of possible successor states</li>
      <li>a probability distribution over these successors</li>
      <li>an associated cost for each transition</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/Search_algorithms/image%2011.png" alt="image.png" /></p>

<h3 id="policy">Policy</h3>

<p>The crucial shift when moving from deterministic planning to MDPs is that you no longer compute a single sequence of actions, since the robot may not follow the intended path. Instead, you compute a policy:</p>

\[\pi : S \rightarrow A\]

<p>A policy maps every state to an action ‚Äî telling the robot what to do regardless of which branch of the uncertainty tree reality follows. This is why planning in MDPs is fundamentally more complex. Instead of a single path, the solution is often a directed acyclic graph (DAG) of possible transitions, representing the branching structure of all possible outcomes and the prescribed actions at each.</p>

<h2 id="objectives-for-planning-under-uncertainty">Objectives for Planning Under Uncertainty</h2>

<p>In MDPs, there are multiple ways to define optimality. The lecture focuses on two:</p>

<ol>
  <li><strong>Expected-cost planning:</strong> Minimize the expected total cost of reaching the goal.</li>
  <li><strong>Minimax planning:</strong> Minimize the worst-case cost, assuming the world behaves in the most adversarial possible way.</li>
</ol>

<h2 id="minimax-planning">Minimax Planning</h2>

<p>Minimax planning asks: <em>‚ÄúGiven all possible uncertain outcomes, what policy minimizes the worst-case cost-to-goal?‚Äù</em></p>

<p>Formally:</p>

\[\pi^* = \arg\min_{\pi} \max_{\text{outcomes of } \pi}

{\text{cost-to-goal}}\]

<p>In other words:</p>

<ul>
  <li>The environment is treated as adversarial.</li>
  <li>The planner assumes that every action may yield the worst possible successor.</li>
  <li>The robot must be robust against the worst instance of randomness.</li>
</ul>

<p>This is extremely conservative but extremely safe.</p>

<p>A policy maps each possible state to an action, forming a directed acyclic graph (DAG) of possible future evolutions. The graph branches wherever uncertainty exists: one action in the current state leads to multiple possible next states, each requiring its own follow-up action.</p>

<p>The graph contains no cycles, because in minimax reasoning a cycle would imply the robot might get stuck forever, producing infinite worst-case cost. Thus, the optimal minimax strategy is always a branching, acyclic decision structure that covers every possible outcome of uncertainty.</p>

<h3 id="computing-minimax-plans-backward-a">Computing Minimax Plans: Backward A*</h3>

<p><strong>Algorithm:</strong></p>

<ul>
  <li>Initialize $g(s_{goal}) = 0$   and $g(s) = \infty,\ \forall s \neq s_{goal}$; $OPEN = {s_{goal}}$</li>
  <li>while $s_{start}$ not expanded:
    <ul>
      <li>remove $s$ with smallest $f(s) = g(s) + h(s)$ from $OPEN$</li>
      <li>insert $s$ into $CLOSED$</li>
      <li>for every $s‚Äô$ such that $s \in succ(s‚Äô,a)$ for some $a$ and $s‚Äô$ not in $CLOSED$
        <ul>
          <li>if $g(s‚Äô) &gt; \max_{u \in succ(s‚Äô,a)} \big(c(s‚Äô,u) + g(u) \big)$
            <ul>
              <li>$g(s‚Äô) = \max_{u \in succ(s‚Äô,a)} \big(c(s‚Äô,u) + g(u) \big)$</li>
              <li>insert $s‚Äô$ into $OPEN$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>After computing the minimax g-values of the states, the optimal policy is obtained by selecting an action that minimizes (think of this to be in the forward pass):</p>

\[\text{max}_{s' \in succ(s,a)}\{c(s,a,s')+g(s')\}\]

<p>This recursion directly encodes minimax reasoning: a state‚Äôs cost is determined by its worst possible successor, not its best or expected one.</p>

<p><img src="/assets/images/Search_algorithms/image%2012.png" alt="image.png" /></p>

<p>Backward minimax A* reduces to classical backward A* when uncertainty disappears. It maintains all the properties of A* (optimality, no re-expansion) if heuristics are consistent.</p>

<p><strong>Resulting Policy Structure:</strong></p>

<p>The resulting minimax policy is often:</p>

<ul>
  <li>A deterministic DAG, not a single path.</li>
  <li>Branching according to possible uncertain outcomes.</li>
  <li>Acyclic, because cycles would imply infinite worst-case cost.</li>
</ul>

<p>The branching reflects the policy telling the robot: ‚ÄúIf you end up in this state, take action $a$. If the uncertainty pushes you into another state, take action $b$.‚Äù</p>

<h3 id="why-not-forward-a">Why not Forward A*?</h3>

<p>In deterministic planning, forward A* works beautifully because the cost of a state is determined solely by the cost accumulated so far. When you expand a state, you know exactly its future cost-to-goal because the path is deterministic. However, in minimax planning under uncertainty, every action can lead to multiple possible successor states, and the cost of a state is the maximum over the costs of all these possible outcomes.</p>

<p>This means that the true cost of a state cannot be known until all of its successors have been evaluated. In other words, the value of a state depends on the value of its children, not the other way around. Forward A* requires knowing or estimating the cost-to-go (the h-value) from the current state before expanding it. But in minimax, that cost is not something you can estimate from heuristics alone ‚Äî it must be computed from the actual successor states. As long as those successor states are not evaluated yet, the minimax backup cannot be computed correctly.</p>

<p>Backward A* solves this dependency. By starting from the goal and working backward, the planner always expands states after all their successors‚Äô values have been determined. Because the minimax backup is:</p>

\[g(s) = \min_{a \in A(s)} \max_{u \in succ(s,a)} \big( c(s,u) + g(u) \big)\]

<p>the value of $s$ requires knowing the values of all its successors $u$. Backward search ensures that when we compute $g(s)$, all the $g(s)$ values are already finalized or improved optimally. This maintains the same correctness and optimality guarantees that A* has in deterministic planning.</p>

<h3 id="pros-and-cons-of-minimax-planning">Pros and Cons of Minimax Planning</h3>

<p>Minimax planning is powerful but expensive.</p>

<p><strong>Pros:</strong></p>

<ul>
  <li>Extremely robust to worst-case disturbances</li>
  <li>Guarantees finite cost regardless of execution randomness</li>
  <li>Provides strong safety guarantees</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li>Overly pessimistic: assumes worst-case outcome happens <em>always</em></li>
  <li>More expensive than deterministic A*</li>
  <li>Heuristics are less informative because minimax backup uses max, not sum</li>
  <li>Backward minimax A* may not apply to all MDPs</li>
  <li>Even when it applies, the search is often slower because uncertainty creates branching</li>
</ul>

<p>Thus minimax is most appropriate for safety-critical or adversarial domains where robustness outweighs efficiency.</p>

<h2 id="expected-cost-formulation">Expected Cost Formulation</h2>

<p>Minimax assumes that every time an action is taken, the world will produce the worst possible stochastic outcome. In many domains ‚Äî navigation on slippery surfaces, noisy actuation, imperfect sensing ‚Äî this assumption dramatically overestimates risk. For example, let‚Äôs say in a hill-climbing scenario, the robot has a 10% chance of slipping when moving uphill. If we treat this as an adversarial worst-case outcome, minimax will conclude that climbing is effectively impossible, because in the worst case the robot slips on every attempt. In reality, slipping occasionally is acceptable if the expected time or cost remains low.</p>

<p>Expected-cost planning instead models uncertainty probabilistically, so each action outcome contributes to future cost proportionally to its probability. This produces policies that are safer and more realistic than minimax, yet not overly pessimistic.</p>

<h3 id="defining-the-expected-cost-objective">Defining the Expected-Cost Objective</h3>

<p>The policy that is optimal under the expected-cost model minimizes:</p>

\[\pi^* = \arg\min_{\pi}  \mathbb{E}\{\text{cost-to-goal} \mid \pi\}\]

<p><img src="/assets/images/Search_algorithms/image%2013.png" alt="image.png" /></p>

<p>Expectation is taken over the stochastic outcomes of each action. To illustrate, consider the following example MDP:</p>

<ul>
  <li>From $s_1$, action $a_1$ transitions
    <ul>
      <li>to $s_{goal}$ with probability $0.9$ and cost $2$</li>
      <li>to $s_{2}$ with probability $0.1$ and cost $2$</li>
    </ul>
  </li>
</ul>

<p>Two competing strategies exist:</p>

<ul>
  <li>
    <p>$\pi_1:$ Go through $s_4$ deterministically</p>

    <p>Expected cost $= 1 + 1+3+1=6$</p>
  </li>
  <li>
    <p>$\pi_2:$ Keep trying to reach the goal through the stochastic $s_1$ edge</p>

    <p>The expected cost sums an infinite geometric series of attempts:</p>

\[0.9 \cdot (1+2+2)+
  0.9\cdot0.1 \cdot (1+2+2+2+2)+
  0.9\cdot0.1^2 \cdot (1+2+2+2+2+2+2)
  \cdots = 5.444\dots\]
  </li>
</ul>

<p>Thus, under expected-cost reasoning, $\pi_2$ is better than $\pi_1$ ‚Äî even though minimax preferred $\pi_1$. This highlights the fundamental philosophical difference: minimax protects against worst-case scenarios, while expected-cost planning optimizes performance on average.</p>

<h3 id="value-function">Value Function</h3>

<p>Let $v^<em>(s)$ denote the optimal expected cost-to-goal from state $s.$ $v^</em>(s)$ satisfies the core equation of expected-cost MDPs:</p>

\[v^*(s) =

\min_{a \in A(s)}

 \mathbb{E}\left[

c(s,a,s') + v^*(s')

\right]

\quad \text{for } s \neq s_{goal}\]

\[v^*(s_{goal}) = 0\]

<p>This is the Bellman optimality equation for stochastic shortest-path problems.</p>

<p>Given the optimal value function, the optimal policy is simply the greedy policy:</p>

\[\pi^*(s)

= \arg\min_{a \in A(s)}

 \mathbb{E}\left[c(s,a,s') + v^*(s')\right]\]

<p>This equation formalizes the idea that an optimal action is one whose immediate cost plus expected future cost is minimal.</p>

<h3 id="value-iteration">Value Iteration</h3>

<p>Value Iteration solves the Bellman optimality equations through fixed-point iteration. It starts with an initial guess for all $v(s)$, then repeatedly applies the Bellman backup:</p>

\[v(s)\leftarrow

\min_{a}

\mathbb{E}_{s'}\left[

c(s,a,s') + v(s')

\right]\]

<p>Important details:</p>

<ul>
  <li>$v(s_{goal})$ is always fixed at $0$.</li>
  <li>It is best to initialize to admissible values (underestimates of the actual costs-to-goal).</li>
  <li>All other values gradually improve (monotonically increasing) until convergence.</li>
  <li>VI converges in a finite number of iterations. In the worst case we take $N$ sweeps, and in each sweep we take $N$ backups. So the worst case will have atmost $N^2$ backups, compared to only $N$ in A* because the order there is optimal.</li>
  <li>
    <p>Convergence means:</p>

\[|v(s) - \min_{a} \mathbb{E}[c(s,a,s') + v(s')]| &lt; \Delta\]
  </li>
</ul>

<p>The order in which states are backed up affects speed but not correctness. For stochastic shortest-path problems in which every state can eventually reach the goal, VI always converges to the optimal value function.</p>

<p>The expected cost of executing greedy policy is at most:</p>

\[v^*(s_{start})c_{min}/(c_{min}-\Delta)\]

<p>VI has a drawback: it updates every state in the MDP on every iteration, even states that will never be reached by the optimal policy. This creates inefficiency in large MDPs.</p>

<p><img src="/assets/images/Search_algorithms/image%2014.png" alt="image.png" /></p>

<h2 id="real-time-dynamic-programming-rtdp">Real-Time Dynamic Programming (RTDP)</h2>

<p>While Value Iteration updates every state repeatedly, RTDP focuses computation only on states that actually matter for the optimal policy. Instead of sweeping through the entire state space, RTDP simulates execution and updates only the states encountered along greedy trajectories.</p>

<p>RTDP proceeds as follows:</p>

<ol>
  <li>Initialize all values to admissible (optimistic) underestimates.</li>
  <li>Starting from the real initial state, follow a greedy policy (choosing the action that minimizes expected cost) picking outcomes at random.</li>
  <li>
    <p>At each visited state, apply a Bellman backup.</p>

\[v(s)\leftarrow
    
 \min_{a}
    
 \mathbb{E}_{s'}\left[
    
 c(s,a,s') + v(s')
    
 \right]\]
  </li>
  <li>If the goal is reached, restart from the start and repeat.</li>
  <li>Stop when Bellman errors along the greedy path fall below $\Delta$</li>
</ol>

<p>For the first step:</p>

<ul>
  <li>
    <p>For any state $s$, pick an action $a$ that minimizes</p>

\[E\{c(s,a,s')+v(s')\}\]
  </li>
  <li>
    <p>Pick a successor $s‚Äô$ at random according to probability</p>

\[P(s'|a,s)\]
  </li>
</ul>

<p>RTDP is guaranteed to converge in a finite number of iterations (given admissible initial values and a reachable goal), and like VI, it leads to an optimal greedy policy.</p>

<p>The expected cost of executing greedy policy is at most:</p>

\[v^*(s_{start})c_{min}/(c_{min}-\Delta)\]

<h2 id="rewards">Rewards</h2>

<p>The reward formulation is central whenever tasks do not terminate after reaching a goal, or when the robot continually interacts with the environment ‚Äî patrolling, surveillance, balancing, manipulation, or any behavior that runs indefinitely. Instead of ending at a terminal state (like reaching a goal cell), the system keeps accruing rewards over time. Planning, therefore, must evaluate the long-term desirability of each state.</p>

<p>In a reward-based MDP, each transition gives the agent a reward $r(s,a,s‚Äô).$</p>

<p>The task becomes: Choose actions to maximize the expected total reward received over time.</p>

<p>Unlike the cost formulation, where lower values are better, here higher values are better.
The optimal value function satisfies:</p>

\[v^*(s) = \max_{a \in A(s)}  \mathbb{E}_{s'} \big[r(s,a,s') + \gamma  v^*(s') \big]\]

<p>This is the Bellman optimality equation for reward-MDPs. The appearance of the discount factor $\gamma$ is crucial ‚Äî without it, the sum of rewards could diverge, and the value function would be undefined.</p>

<h3 id="why-discounting">Why discounting?</h3>

<p>Discounting adjusts how the robot values immediate rewards relative to future ones:</p>

\[0 &lt; \gamma \le 1\]

<p>If $\gamma \le 1:$</p>

<ul>
  <li>Future rewards count less than immediate rewards</li>
  <li>Values remain finite, even if the system never terminates</li>
  <li>VI and PI converge rapidly, because the Bellman operator becomes a contraction</li>
</ul>

<p>If $\gamma=1:$</p>

<ul>
  <li>Rewards are not discounted</li>
  <li>Value iteration may not converge</li>
  <li>Total reward can become infinite if the robot loops through positive-reward cycles</li>
</ul>

<p>Thus discounting is not merely a mathematical trick ‚Äî it enforces stability and convergence in infinite-horizon tasks.</p>

<h3 id="difference-in-problem-structure">Difference in Problem Structure</h3>

<p>In cost-based MDPs:</p>

<ul>
  <li>There is always a terminal goal with $v^*(goal)=0$</li>
  <li>All states eventually must reach the goal for the formulation to be valid</li>
  <li>Value iteration converges because costs accumulate positively</li>
</ul>

<p>In reward-based MDPs:</p>

<ul>
  <li>There may be no terminal goal</li>
  <li>The robot may remain in the system forever (e.g., balancing a pole)</li>
  <li>Discounting controls infinite reward accumulation</li>
  <li>Policies are evaluated by their stationary, long-term behavior</li>
</ul>

<p>The reward formulation is therefore strictly more general.</p>

<h3 id="bellman-equations-now-maximize-not-minimize">Bellman Equations Now Maximize, Not Minimize</h3>

<p>The Bellman backup in the reward formulation becomes:</p>

\[v(s) \leftarrow \max_{a} \mathbb{E}_{s'} \big[ r(s,a,s') + \gamma v(s') \big]\]

<p>Compare this to the cost version:</p>

\[v(s) \leftarrow \min_{a} \mathbb{E}_{s'} \big[ c(s,a,s') + v(s') \big]\]

<p>The structure is the same ‚Äî min becomes max, cost becomes reward, and discounting appears in the infinite-horizon setting. But the planning intuition changes dramatically: instead of trying to reach a goal cheaply, the robot tries to accumulate as much positive reward as possible.</p>

<p>Given the optimal value function, the optimal policy is simply the greedy policy:</p>

\[\pi^*(s)

= \arg\max_{a \in A(s)}

 \mathbb{E}\left[r(s,a,s') + \gamma v^*(s')\right]\]

<h3 id="value-iteration-for-reward-mdps">Value Iteration for Reward MDPs</h3>

<p>Value iteration still works ‚Äî but with slightly modified behavior:</p>

<ol>
  <li>Initialize all values (optimistic or arbitrary).</li>
  <li>Apply Bellman backups using maximization and discounting.</li>
  <li>
    <p>Values converge to the unique fixed point of the Bellman operator:</p>

\[v(s) = \max_a \mathbb{E}[ r + \gamma v(s') ]\]
  </li>
</ol>

<p>Because $\gamma &lt;1,$ the operator is a contraction ‚Äî guaranteed to converge.</p>

<p><img src="/assets/images/Search_algorithms/image%2015.png" alt="image.png" /></p>

<p><img src="/assets/images/Search_algorithms/image%2016.png" alt="image.png" /></p>

<h2 id="partially-observable-markov-decision-processes-pomdp">Partially Observable Markov Decision Processes (POMDP)</h2>

<p>A simple path-planning example highlights the differences. In a classical graph search problem, the robot knows its exact state and every action deterministically leads to a single successor. A graph is essentially the tuple <code class="language-plaintext highlighter-rouge">{S, A, C}</code>: states, actions, and deterministic costs.</p>

<p>Once we allow action uncertainty (e.g., a drone drifting left or right with probability 0.5), the problem becomes an MDP because the effect of an action is no longer deterministic. The transition model must specify a probability over successor states:</p>

\[T(s,a,s') = P(s' \mid s,a)\]

<p>However, even in an MDP, the robot still assumes perfect localization: it always knows which state it lands in after acting.</p>

<p>In a POMDP, we remove this assumption. The robot now has uncertain state and uncertain action outcomes. It only receives limited observations that do not uniquely identify the true state. This situation arises in many robotics settings, such as UAVs navigating without GPS, mobile robots in ambiguous hallways, underwater robots with drifting odometry, or manipulation tasks where object identity is uncertain.</p>

<p>Thus,</p>

<ul>
  <li>Graphs model perfect knowledge + deterministic actions.</li>
  <li>MDPs model full observability + stochastic actions.</li>
  <li>POMDPs generalize both by allowing stochastic actions and partial state observability.</li>
</ul>

<h3 id="what-is-pomdp">What is POMDP?</h3>

<p>A POMDP is defined as:</p>

\[{S,\ A,\ T,\ R,\ \Omega,\ O}\]

<p>where:</p>

<ul>
  <li>$S:$ set of all possible world states</li>
  <li>$A:$ set of all actions</li>
  <li>$T(s,a,s‚Äô):$ transition model</li>
  <li>$R(s,a):$ reward (or cost) function</li>
  <li>$\Omega:$ set of possible observations</li>
  <li>$O(s‚Äô,a,o) = P(o|s‚Äô,a):$ probability of seeing observation $o$ after taking action $a$ and landing in $s‚Äô$</li>
</ul>

<p>The critical new component is the observation model. The robot only perceives partial information about the true state.</p>

<p><img src="/assets/images/Search_algorithms/image%2017.png" alt="image.png" /></p>

<p>A POMDP has the following causality chain:</p>

\[s \xrightarrow{a} s' \xrightarrow{o} \text{observation}\]

<p>The observation does not reveal $s‚Äô$ exactly, but it narrows down possible states.</p>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="cmu" /><category term="planning" /><category term="manipulators" /><category term="cbs" /><summary type="html"><![CDATA[In progress]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/Thumbnail/search_2.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/Thumbnail/search_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Case Studies of Planning and Decision-Making in Robotics</title><link href="https://bhaswanth-a.github.io//posts/planning-case-studies/" rel="alternate" type="text/html" title="Case Studies of Planning and Decision-Making in Robotics" /><published>2025-11-30T11:00:00-05:00</published><updated>2025-12-08T16:17:11-05:00</updated><id>https://bhaswanth-a.github.io//posts/planning-case-studies</id><content type="html" xml:base="https://bhaswanth-a.github.io//posts/planning-case-studies/"><![CDATA[<p><em>In progress</em></p>

<h1 id="autonomous-driving">Autonomous Driving</h1>

<h1 id="mobile-manipulators">Mobile Manipulators</h1>

<h1 id="legged-robots">Legged Robots</h1>

<h1 id="coverage-mapping-and-surveyal">Coverage, Mapping, and Surveyal</h1>

<h2 id="frontier-based-planning">Frontier-based Planning</h2>

<p>A frontier is any region that lies at the boundary between explored space and unexplored space.
Moving toward a frontier guarantees that the robot will gather new information.</p>

<p>This idea applies equally well to both mapping and coverage, even though the environments differ in how much initial knowledge the robot has.</p>

<p>In coverage, the map is already known, but the robot must visit every area to ‚Äúcover‚Äù it with its sensors. In mapping, the environment is unknown beforehand, and the robot‚Äôs sensors reveal the map gradually. Yet in both scenarios, the frontier represents the next most informative or unobserved area for the robot to visit.</p>

<h2 id="coverage">Coverage</h2>

<p>In the coverage scenario, the robot has access to a known map but must physically move to all parts of the environment to observe or ‚Äúcover‚Äù it. Coverage tasks include vacuum cleaning, lawn mowing, painting a surface with a sprayer, or security patrolling.</p>

<p>Initially, simple patterns like lawn-mower (boustrophedon) patterns are commonly used. These are efficient in open areas, but they degrade quickly in environments with irregular shapes, tight corridors, or obstacles. The robot may skip regions, require manual stitching of patterns, or need a complex global plan.</p>

<p>Frontier-based planning offers a more adaptive and principled alternative. The robot maintains a record of which cells or regions it has already covered, and at every step, it identifies frontier cells‚Äîthose cells that have not yet been visited but are adjacent to visited ones. The robot then chooses the nearest such frontier and computes a path to it. Once it reaches that frontier, new cells become observed, the set of frontiers changes, and the robot repeats the process. Eventually, all free cells will have been visited, guaranteeing complete coverage.</p>

<h3 id="why-it-works">Why it works?</h3>

<p>Frontier-based coverage works well because it always moves the robot toward ‚Äúnew‚Äù portions of the environment. Since frontier cells are, by definition, the closest areas that remain unobserved, the robot makes steady progress toward the coverage goal without repeatedly visiting the same areas. This reduces unnecessary overlap and ensures that the robot moves in a purposeful manner.</p>

<p>Moreover, frontier-based coverage naturally adapts to obstacles and environmental geometry. Unlike lawn-mowing patterns, frontiers reorganize themselves around walls, corners, and narrow passages. This allows the robot to navigate complex spaces efficiently, making the method widely used in real-world applications such as vacuum robots or cleaning drones.</p>

<h2 id="mapping">Mapping</h2>

<p>Mapping differs from coverage in one crucial way: the robot begins with no prior knowledge of the environment. As the robot moves and senses its surroundings, it gradually constructs an occupancy grid or map of free and occupied space. However, at any moment, most of the environment remains unknown.</p>

<p>Frontier-based planning for mapping uses the same principle ‚Äî Always move to the closest frontier: the boundary between what has already been scanned and what is still unknown.</p>

<p>In mapping, every frontier visit provides maximum information gain, because the robot‚Äôs sensors reveal new cells beyond the frontier. As the robot moves and senses, the set of frontiers changes. Some frontiers disappear as the robot explores them, and others appear as new unknown spaces are exposed.</p>

<p>The process continues until no frontiers remain, meaning the entire reachable environment has been mapped.</p>

<h2 id="using-multi-goal-a-for-frontier-navigation">Using Multi-Goal A* for Frontier Navigation</h2>

<p>One subtle but important detail is how to choose the closest frontier. Since the robot may have many frontier cells at once, finding the nearest one requires solving a multi-goal shortest-path problem.</p>

<p>Rather than running A* separately for each frontier (which would be inefficient), we perform a graph transformation that allows A* to check all frontiers in a single search. This is based on the multi-goal A* technique.</p>

<p>The idea is:</p>

<ul>
  <li>Treat all frontier cells as goal nodes in one A* search.</li>
  <li>Expand the search until the first frontier is reached.</li>
  <li>This frontier is guaranteed to be the closest in terms of path cost.</li>
</ul>

<h2 id="surveyal">Surveyal</h2>

<p>Surveyal refers to a class of robotics tasks in which the robot must visit a set of predefined points of interest in the environment and perform sensing actions at each one, such as taking photos, scanning objects, or collecting data. Unlike simple navigation problems, surveyal does not ask the robot to reach a single goal state. Instead, it requires the robot to construct a least-cost route that visits all required waypoints, in any order, while respecting motion constraints and avoiding obstacles. This makes surveyal a combination of combinatorial reasoning (deciding the order in which to visit waypoints) and motion planning (finding feasible paths between them).</p>

<p>The core difficulty in surveyal lies in the fact that the robot cannot teleport between waypoints ‚Äî it must travel along feasible trajectories. This means that the true ‚Äúcost‚Äù of going from waypoint A to waypoint B is not simply Euclidean distance, but the cost of a real motion-planning solution that accounts for obstacles, robot dynamics, vehicle orientation, and constraints such as minimum turning radius. Therefore, surveyal cannot be solved by simply treating it as a Traveling Salesman Problem (TSP) over Euclidean distances. Instead, we must combine low-level continuous motion planning and high-level discrete graph search into one unified planning pipeline.</p>

<h3 id="constructing-the-surveyal-search-space">Constructing the Surveyal Search Space</h3>

<p>To solve the surveyal problem using graph search, we must construct a discrete search space that captures two key aspects of the robot‚Äôs progress:</p>

<ol>
  <li>Which waypoints have already been visited, and</li>
  <li>Where the robot currently is (and possibly its orientation)</li>
</ol>

<p>To encode this, we define the search state as:</p>

\[v = \{ \alpha, \Omega \}\]

<p>Here, $\alpha$ is a vector of $M$ bits, where each bit corresponds to one waypoint. A bit is set to <code class="language-plaintext highlighter-rouge">1</code> if the corresponding waypoint has been visited, and <code class="language-plaintext highlighter-rouge">0</code> otherwise. This bitmask representation allows the planner to concisely describe all possible ‚Äúprogress states‚Äù of the mission. For example, if the robot has three waypoints to visit, then $\alpha=$ <code class="language-plaintext highlighter-rouge">101</code> means the robot has visited waypoints <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">2</code> but not <code class="language-plaintext highlighter-rouge">1</code>.</p>

<p>$\Omega$ encodes the robot‚Äôs current position in the waypoint graph. For holonomic robots (which can move in any direction), $\Omega$ is simply the index of the current waypoint. For non-holonomic robots (cars, drones with yaw constraints, differential-drive robots), $\Omega$ may also include the robot‚Äôs orientation at that waypoint. Orientation is important because the robot‚Äôs ability to travel to the next waypoint may depend strongly on how it is oriented when it leaves its current waypoint.</p>

<p>This representation guarantees that each search state contains all relevant information that affects future motion, which means that the state space satisfies the Markov property. From any state <code class="language-plaintext highlighter-rouge">{Œ±, Œ©}</code>, the planner can determine exactly which next waypoints are allowed and what costs are involved in reaching them.</p>

<h3 id="goal-states-in-surveyal">Goal States in Surveyal</h3>

<p>The goal of the surveyal problem is to visit all required waypoints. Therefore, the goal condition is simply:</p>

\[\alpha = [1, 1, \dots, 1]\]

<p>Once all bits in $\alpha$ are set to one, the robot has successfully surveyed every waypoint, and the search can terminate. Importantly, the robot‚Äôs orientation at the final waypoint does not affect goal satisfaction ‚Äî only the fact that every waypoint has been visited.</p>

<h3 id="role-of-low-level-motion-planning">Role of Low-Level Motion Planning</h3>

<p>Before graph search can be performed, the system must know the cost of traveling between each pair of waypoints. These costs cannot be assumed ‚Äî they must be computed by invoking a continuous motion planner (e.g., A* on a grid, Hybrid A<em>, RRT</em>, lattice planning).</p>

<p>For every ordered pair of waypoints (A, B), a low-level planner is used to determine:</p>

<ol>
  <li>Whether a feasible path exists between A and B</li>
  <li>The cost of this feasible path</li>
  <li>The resulting orientation of the robot at waypoint B</li>
</ol>

<p>Thus, edges in the high-level surveyal graph correspond to actual robot trajectories, not simple straight lines. This is what makes surveyal fundamentally richer than classical TSP.</p>

<h3 id="high-level-graph-search-where-a-comes-in">High-Level Graph Search: Where A* Comes In</h3>

<p>Once feasible motion edges are computed, surveyal becomes a graph search problem over the state space <code class="language-plaintext highlighter-rouge">{Œ±, Œ©}</code>. Each node in this graph represents a partially completed mission. Each outgoing edge from that node corresponds to:</p>

<ul>
  <li>Selecting an unvisited waypoint $i$</li>
  <li>Following a feasible low-level path from the current waypoint to $i$</li>
  <li>Updating the visitation bitmask $\alpha$</li>
  <li>Updating orientation (if applicable)</li>
</ul>

<p>Graph search algorithms such as A* or Dijkstra explore this state space to find the minimum-cost path from:</p>

\[{ \alpha = 0\ldots0, \space \Omega = \text{start waypoint} }

\quad \rightarrow \quad

{ \alpha = 1\ldots1, \space \Omega = \text{any waypoint} }\]

<p>This is where graph search actually happens. The search determines the optimal sequence of waypoints to visit, accounting for true motion constraints and path costs.</p>

<p>This formulation elegantly integrates:</p>

<ul>
  <li>Combinatorial reasoning about waypoint order</li>
  <li>Continuous geometric motion planning</li>
  <li>Robot kinematic/dynamic constraints</li>
  <li>Obstacle avoidance</li>
  <li>Optimality of the final surveyal tour</li>
</ul>

<p>By encoding both ‚Äúwhat has been visited‚Äù and ‚Äúwhere the robot is now,‚Äù the search graph becomes fully Markovian. The planner can reason about the mission using a well-structured state space while relying on low-level planners to guarantee physical feasibility.</p>

<p>Because of this, surveyal planning becomes a clean example of how discrete search and continuous planning must work together to solve real robotic tasks.</p>]]></content><author><name>&lt;author_id&gt;</name></author><category term="Blog" /><category term="Robotics" /><category term="cmu" /><category term="planning" /><category term="manipulators" /><category term="cbs" /><summary type="html"><![CDATA[In progress]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bhaswanth-a.github.io//assets/images/Thumbnail/car_2.png" /><media:content medium="image" url="https://bhaswanth-a.github.io//assets/images/Thumbnail/car_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>