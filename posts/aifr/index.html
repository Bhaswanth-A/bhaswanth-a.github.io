<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Probabilistic Robotics" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="In progress" /><meta property="og:description" content="In progress" /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/aifr/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/aifr/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/slam2.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-28T12:00:00-04:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/slam2.png" /><meta property="twitter:title" content="Probabilistic Robotics" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2024-12-13T17:24:47-05:00","datePublished":"2023-03-28T12:00:00-04:00","description":"In progress","headline":"Probabilistic Robotics","image":"https://bhaswanth-a.github.io//assets/images/Thumbnail/slam2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/aifr/"},"url":"https://bhaswanth-a.github.io//posts/aifr/"}</script><title>Probabilistic Robotics | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Perception | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Probabilistic Robotics</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Probabilistic Robotics</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1680019200" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Mar 28, 2023 </em> </span> <span> Updated <em class="" data-ts="1734128687" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Dec 13, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3669 words"> <em>20 min</em> read</span></div></div></div><div class="post-content"><p><em>In progress</em></p><p><a href="https://drive.google.com/drive/folders/13yY6DdbB76Q_KRWUjS1nBsmHc5FmW9uX?usp=share_link">Resources</a></p><p><a href="https://atsushisakai.github.io/PythonRobotics/index.html">Programming</a></p><h1 id="basic-concepts-of-probability">Basic Concepts of Probability</h1><h1 id="terminology">Terminology</h1><p>The environment, or world, is a dynamical system that possesses internal state. The robot can acquire information about its environment using its sensors. However, sensors are noisy, and there are usually many things that cannot be sensed directly. As a consequence, the robot maintains an internal belief with regards to the state of its environment.</p><h2 id="state"><span class="mr-2">State</span><a href="#state" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>State - It is a collection of all aspects of the robot and its environment that can impact the future. Dynamic states are state variables that tend to change over time, while static states are non-changing. The state at time $t$ is denoted as $x_t.$<li>Pose - The robot pose comprises of its location and orientation relative to a global reference frame. In the context of mobile robots, the pose is usually given by three variables, two location coordinates in the plane and one orientation w.r.t the vertical axis normal to the plane.<li>The location and features of the surrounding objects in the environment are also state variables. Based on the quality of the state, robot environments can have even hundreds to billions of state variables.<li>Some objects in the environment can be recognized reliably and are referred to as landmarks.<li>Other state variables can include whether or not a sensor is broken, the level of battery charge, etc. These take discrete values.<li>Markov chains - A state $x_t$ is said to be complete if the knowledge of past states, measurements or controls carry no additional information to help predict the future more accurately. The future may be stochastic but no variables prior to $x_t$ may influence the future states, unless this dependence is mediated through the state $x_t.$</ul><h2 id="environment-interaction"><span class="mr-2">Environment Interaction</span><a href="#environment-interaction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li><p>Sensor Measurements - Perception is the process by which the robot uses its sensors to obtain information about the state of its environment. Typically, sensor measurements have some delay and provide information about the state a few moments ago. However, we will assume that measurements corresponds to a specific point in time. Measurement data at point $t$ will be denoted by $z_t.$</p>\[z_{t1:t2} = z_{t1},z_{t1+1},z_{t1+2},....,z_{t2}\]<li><p>Control Actions - They carry information about the change of state of the environments and they change the state by actively asserting forces on the robot’s environment. It is denoted by $u_t.$</p>\[u_{t1:t2} = u_{t1},u_{t1+1},u_{t1+2},....,u_{t2}\]</ul><h2 id="hidden-markov-model"><span class="mr-2">Hidden Markov Model</span><a href="#hidden-markov-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The emergence of state $x_t$ might be conditioned on all past states, measurements and controls. This gives us a probability distribution of the following form:</p>\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t})\]<p>We assume that the robot executes the control action $u_1$ first and then takes the measurement $z_1$.</p><p>If the state $x$ is complete, then</p>\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t}) = p(x_t|x_{t-1},u_t) \implies State \space transition \space probability\]<p>Similarly, if $x_t$ is complete, we have</p>\[p(z_t | x_{0:t},z_{1:t-1},u_{1:t}) = p(z_t|x_{t}) \implies Measurement \space probability\]<p>The state at time $t$ is stochastically dependent on the state at time $t-1$ and the control $u_t.$ The measurement $z_t$ depends stochastically on the state at time $t.$</p><p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled.png" alt="Untitled" data-proofer-ignore></p><h2 id="belief-distributions"><span class="mr-2">Belief Distributions</span><a href="#belief-distributions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Belief represents the robot’s internal knowledge about the state of the environment. We denote the belief over a state variable $x_t$ by $bel(x_t),$</p>\[bel(x_t) = p(x_t|z_{1:t},u_{1:t})\]<p>We assume that the belief is taken after incorporating the measurement $z_t$. Sometimes it is also useful to calculate the belief before incorporating $z_t$, just after executing $u_t.$</p>\[\bar{bel(x_t)} = p(x_t|z_{1:t-1},u_{1:t})\]<p>Calculating $bel(x_t)$ from $\bar{bel(x_t)}$ is called correction or measurement update.</p><h1 id="bayes-filter">Bayes Filter</h1><h2 id="pseudo-code"><span class="mr-2">Pseudo-code</span><a href="#pseudo-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Algorithm Bayes_filter ($bel(x_{t-1}),u_t,z_t):$</p><p>for all $x_t$ do:</p>\[\bar{bel(x_t)} = \int{p(x_t|u_t,x_{t-1}) bel(x_{t-1})dx_{t-1}}\] \[bel(x_t) = \eta p(z_t|x_t)\bar{bel(x_t)}\]<p>endfor</p><p>return $bel(x_t)$</p><h2 id="markov-assumption"><span class="mr-2">Markov Assumption</span><a href="#markov-assumption" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Also called the complete state assumption, it postulates that the past and future data are independent if one knows the current state $x_t$. However, the following factors may have a systematic effect on sensor readings and induce violations to the Markov assumption:</p><ul><li>Unmodeled dynamics in the environment are not included in $x_t$<li>Inaccuracies in probabilistic models - state transition probability and measurement probability<li>Approximation errors when using approximate representations of belief functions<li>Software variables in the robot control software that influence multiple controls</ul><blockquote><p>Estimation is after the occurrence of the event i.e. posterior probability. Prediction is a kind of estimation before the occurrence of the event i.e. apriori probability.</p></blockquote><p><a href="https://stats.stackexchange.com/questions/17773/what-is-the-difference-between-estimation-and-prediction">What is the difference between estimation and prediction?</a></p><h1 id="the-alpha-beta-gamma-filter">The $\alpha-\beta-\gamma$ Filter</h1><p><a href="https://www.kalmanfilter.net/alphabeta.html">Online Kalman Filter Tutorial</a></p><h2 id="notation"><span class="mr-2">Notation</span><a href="#notation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$x$ is the true value of the weight</p><p>$z_n$ is the measured value of the weight at time n</p><p>$\hat{x}_{n,n}$ is the estimate of $x$ at time n (the estimate is made after taking the measurement $z_n$)</p><p>\(\hat{x}_{n+1,n}\) is the estimate of the future state ( n+1 ) of $x$. The estimate is made at the time n. In other words, $\hat{x}_{n+1,n}$ is a predicted state or extrapolated state</p><p>\(\hat{x}_{n-1,n-1}\) is the estimate of xx at time n−1 (the estimate is made after taking the measurement $z_{n-1}$)</p><p>$\hat{x}_{n,n-1}$ is a prior prediction - the estimate of the state at time n. The estimate is made at the time n−1</p>\[For \space constant \space dynamics: \newline \hat{x}_{n+1,n}= \hat{x}_{n,n}\] \[State \space Update \space Equation: \newline \hat{x}_{n,n} = \hat{x}_{n,n-1} + \alpha_n \left( z_{n} - \hat{x}_{n,n-1} \right)\]<p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled%201.png" alt="Untitled" data-proofer-ignore></p><p>$\alpha_n$ is called the Kalman Gain</p><p>$(z_n - \hat{x}_{n,n-1})$  is the “measurement residual”, also called innovation. The innovation contains new information.</p><p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled%202.png" alt="Untitled" data-proofer-ignore></p><h2 id="alpha---beta-filter"><span class="mr-2">$\alpha - \beta$ Filter</span><a href="#alpha---beta-filter" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>\[State \space Extrapolation \space Equations: \newline\] \[x_{n+1}= x_{n}+ \Delta t\dot{x}_{n} \newline \dot{x}_{n+1}= \dot{x}_{n}\]<p>The above system of equations is also called Transition Equation or Prediction Equation, and is true for constant velocity dynamics.</p><p>$\alpha-\beta$ track update equations or $\alpha-\beta$ track filtering equations -</p>\[State \space Update \space Equation \space for \space position: \newline\] \[\hat{x}_{n,n} = \hat{x}_{n,n-1}+ \alpha \left( z_{n}- \hat{x}_{n,n-1} \right) \newline\] \[State \space Update \space Equation \space for \space velocity: \newline\] \[\hat{\dot{x}}_{n,n} = \hat{\dot{x}}_{n,n-1}+ \beta \left( \frac{z_{n}-\hat{x}_{n,n-1}}{ \Delta t} \right)\]<p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled%203.png" alt="Untitled" data-proofer-ignore></p><h2 id="alpha-beta-gamma-filter"><span class="mr-2">$\alpha-\beta-\gamma$ Filter</span><a href="#alpha-beta-gamma-filter" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>\[State \space Extrapolation \space Equations: \newline\] \[\hat{x}_{n+1,n}= \hat{x}_{n,n}+ \hat{\dot{x}}_{n,n} \Delta t+ \hat{\ddot{x}}_{n,n}\frac{ \Delta t^{2}}{2} \newline\] \[\hat{\dot{x}}_{n+1,n}= \hat{\dot{x}}_{n,n}+ \hat{\ddot{x}}_{n,n} \Delta t \newline\] \[\hat{\ddot{x}}_{n+1,n}= \hat{\ddot{x}}_{n,n}\] \[State \space Update \space Equations: \newline\] \[\hat{x}_{n,n}= \hat{x}_{n,n-1}+ \alpha \left( z_{n}- \hat{x}_{n,n-1} \right) \newline\] \[\hat{\dot{x}}_{n,n}= \hat{\dot{x}}_{n,n-1}+ \beta \left( \frac{z_{n}-\hat{x}_{n,n-1}}{ \Delta t} \right) \newline\] \[\hat{\ddot{x}}_{n,n}= \hat{\ddot{x}}_{n,n-1}+ \gamma \left( \frac{z_{n}-\hat{x}_{n,n-1}}{0.5 \Delta t^{2}} \right)\]<p>The main difference between these filters is the selection of weighting coefficients $\alpha-\beta-(\gamma)$. Some filter types use constant weighting coefficients; others compute weighting coefficients for every filter iteration (cycle).</p><p>The choice of the $\alpha,\beta,\gamma$ is crucial for proper functionality of the estimation algorithm.</p><h1 id="kalman-filter">Kalman Filter</h1><p><a href="https://www.kalmanfilter.net/kalman1d.html">Online Kalman Filter Tutorial</a></p><p>The Kalman Filter is an optimal filter. It combines the prior state estimate with the measurement in a way that minimizes the uncertainty of the current state estimate.</p>\[For \space constant \space dynamics: \newline p_{n+1,n}= p_{n,n}\] \[Covariance \space Extrapolation \space Equation: \newline p_{n+1,n}^{x}= p_{n,n}^{x} + \Delta t^{2} \cdot p_{n,n}^{v} \newline p_{n+1,n}^{v}= p_{n,n}^{v}\]<p>$p^x$ is the position estimate uncertainty, $p^v$ is the velocity estimate uncertainty, and the above is true for constant velocity dynamics.</p><p>Note that for any normally distributed random variable $x$ with variance ${\sigma}^2$, $kx$ is distributed normally with variance $k^2{\sigma}^2$, therefore the time term in the uncertainty extrapolation equation is squared.</p>\[\hat{x}_{n,n} = \hat{x}_{n,n-1} + \frac{p_{n,n-1}}{p_{n,n-1} + r_{n}}\left( z_{n} - \hat{x}_{n,n-1} \right)\]<h3 id="kalman-gain"><span class="mr-2">Kalman Gain:</span><a href="#kalman-gain" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>\[K_{n}= \frac{Uncertainty \quad in \quad Estimate}{Uncertainty \quad in \quad Estimate \quad + \quad Uncertainty \quad in \quad Measurement}= \frac{p_{n,n-1}}{p_{n,n-1}+r_{n}}\] \[0 \leq K_{n} \leq 1\] \[\left( 1 - K_{n} \right) = \left( 1 - \frac{p_{n,n-1}}{p_{n,n-1} + r_{n}} \right) = \left( \frac{p_{n,n-1} + r_{n} - p_{n,n-1}}{p_{n,n-1} + r_{n}} \right) = \left( \frac{r_{n}}{p_{n,n-1} + r_{n}} \right)\] \[Covariance \space Update \space Equation: \newline p_{n,n} = \left( 1 - K_{n} \right)p_{n,n-1}\]<p>This equation updates the estimate uncertainty of the current state. It is clear from the equation that the estimate uncertainty is constantly decreasing with each filter iteration, since $\left( 1-K_{n} \right) \leq 1$. When the measurement uncertainty is high, the Kalman gain is low. Therefore, the convergence of the estimate uncertainty would be slow. However, the Kalman gain is high when the measurement uncertainty is low. Therefore, the estimate uncertainty would quickly converge toward zero.</p><p>The Kalman Gain is close to zero when the measurement uncertainty is high and the estimate uncertainty is low. Hence we give significant weight to the estimate and a small weight to the measurement.</p><p>On the other hand, when the measurement uncertainty is low, and the estimate uncertainty is high, the Kalman Gain is close to one. Hence we give a low weight to the estimate and a significant weight to the measurement.</p><p>If the measurement uncertainty equals the estimate uncertainty, then the Kalman gain equals 0.5.</p><p>The Kalman Gain Defines the measurement’s weight and the prior estimate’s weight when forming a new estimate. It tells us how much the measurement changes the estimate.</p><p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled%204.png" alt="Untitled" data-proofer-ignore></p><h2 id="summary-of-all-5-kalman-equations"><span class="mr-2">Summary of all 5 Kalman Equations</span><a href="#summary-of-all-5-kalman-equations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/AI%20for%20Robotics%20a7b628c206f349b4b9b9798885f99df1/Untitled%205.png" alt="Untitled" data-proofer-ignore></p><p>The State Extrapolation Equation and the Covariance Extrapolation Equation depend on the system dynamics.</p><h1 id="slam">SLAM</h1><h1 id="localization">Localization</h1><p>Navigation by odometry is prone to errors and can only give an estimate of the real pose of the robot. Further the robot moves, the larger is the error in the estimation of the pose. It can be compared to walking with eyes closed and counting our steps until we reach the destination. The further we walk with our eyes closed, the more uncertain we are about our location.</p><p>For moving short distances odometry is good enough, but when moving longer distances the robot must determine its position relative to an external reference called a landmark. This process is called localization.</p><h2 id="landmarks"><span class="mr-2">Landmarks</span><a href="#landmarks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Landmarks, such as lines on the ground or doors in a corridor can be detected and identified by the robot and used for localization.</p><h2 id="determining-position-from-objects-whose-position-is-known"><span class="mr-2">Determining Position from Objects Whose Position Is Known</span><a href="#determining-position-from-objects-whose-position-is-known" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The following are two methods by which a robot can determine its position by measuring angles and distances to an object whose position is known.</p><h3 id="from-an-angle-and-a-distance"><span class="mr-2">From an Angle and a Distance</span><a href="#from-an-angle-and-a-distance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Suppose an object is placed at the origin $(x_0,y_0)$ of the coordinate system. The azimuth of the robot $\theta$ is the angle between the north and the forward direction of the robot. A laser scanner is used to measure the distance s from the object and the angle $\phi$ between the forward direction of the robot and the object.</p>\[\Delta x = s \sin(\theta-\phi) \newline \Delta y = s \cos(\theta-\phi)\]<p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled.png" alt="Untitled" data-proofer-ignore></p><h3 id="triangulation"><span class="mr-2">Triangulation</span><a href="#triangulation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Triangulation is based on the principle that from two angles of a triangle and the length of the included side, the lengths of the other sides can be computed.</p><p>The robot measures the angles $\alpha$ and $\beta$ to the object from two positions separated by a distance $c$. This distance can be measured by odometry as the robot moves from one position to another, although this may be less accurate.</p><p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%201.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%202.png" alt="Untitled" data-proofer-ignore></p><h3 id="global-positioning-system"><span class="mr-2">Global Positioning System</span><a href="#global-positioning-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>GPS navigation is based upon orbiting satellites. Each satellite knows its precise position in space and its local time. The position is sent to the satellite by ground stations and the time is measured by a highly accurate atomic clock on the satellite.</p><p>A GPS receiver must be able to receive data from 4 satellites. For this reason, a large number of satellites (24-32) is needed so that there is always a line-of-sight between any location and at least four satellites. From the time signals sent by a satellite, the distances from the satellites to the receiver can be computed by multiplying the times of travel by the speed of light. These distances and the known locations of the satellites enable the computation of the three-dimensional position of the receiver: latitude, longitude and elevation.</p><p>Advantages:</p><ul><li>Accurate<li>Easily available - require only an electronic component which is small and inexpensive</ul><p>Disadvantages:</p><ul><li>Positional error is roughly 10m. Not sufficient to perform tasks that need higher accuracy.<li>GPS signals are not strong enough for indoor navigation and are subject to interference in dense urban environments.</ul><h1 id="probabilistic-localization">Probabilistic Localization</h1><ul><li>Consider a robot that is navigating within a known environment for which it has a map.<li>Suppose the map shows 5 doors (dark gray) and 3 areas where there is no door (light gray). The task of the robot is to enter a specific door.<li>By odometry, the robot can determine its current position given a known starting position.</ul><h1 id="mapping">Mapping</h1><p>A robot can use its capability to detect objects to localize itself, and this information is usually provided by a map. However, building a map requires the robot to localize itself, but at the same time, solving the localization problem requires a map. Hence, we are now presented with a chicken-and-egg problem. This problem is overcome by using SLAM algorithms.</p><h2 id="discrete-and-continuous-maps"><span class="mr-2">Discrete and Continuous Maps</span><a href="#discrete-and-continuous-maps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>A robot requires a non-visual representation of a map that it can store in its memory. There are 2 techniques for storing maps: discrete maps (or grid maps) and continuous maps.</p><h3 id="sonar-sensor-model"><span class="mr-2">Sonar Sensor Model</span><a href="#sonar-sensor-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h3 id="frontier-algorithm"><span class="mr-2">Frontier Algorithm</span><a href="#frontier-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h3 id="mapping-using-knowledge-of-the-environment"><span class="mr-2">Mapping using Knowledge of the Environment</span><a href="#mapping-using-knowledge-of-the-environment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Even with bad odometry, the robot can construct a better map if it has some information on the structure of the environment. Suppose that the robot tries to construct the plan of a room by following its walls. If the robot knows in advance that the walls are straight and perpendicular to each other, the robot can correctly construct the map.</p><p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%203.png" alt="Untitled" data-proofer-ignore></p><p>There will also be an error when measuring the lengths of the walls and this can lead to the gap shown in the figure between the first and the last walls. If the robot is mapping a large area, the problem of closing a loop in a map is hard to solve because the robot has only a local view of the environment.</p><p>Map correction can be improved by using sensor data that can give information on regular features in the environment. The regular features can be lines on the ground, a global orientation, or the detection of features that overlap with other measurements.</p><p>Large area measurements facilitate identifying overlaps between the local maps that are constructed at each location as the robot moves through the environment. By comparing local maps, the localization can be corrected and the map can be updated accurately.</p><h1 id="slam-perception">SLAM Perception</h1><p>Due to odometry and perception error, there is a mismatch between the current map and the sensor data which should correspond to the known part of the map.</p><p><strong>How is this mismatch corrected?</strong></p><ul><li>We assume that the odometry does give a reasonable estimation of the pose of the robot.<li>For each relatively small possible error in the pose, we compute what the perception of the current map would be and compare it with the actual perception computed from the sensor data.<li>The pose that gives the best match is chosen as the actual pose of the robot and the current map is updated accordingly.</ul><p>The similarity matrix is computed, and once we have this result, we correct the pose of the robot and use data from the perception map to update the current map stored in the robot’s memory.</p><h1 id="probabilistic-slam">Probabilistic SLAM</h1><p>The SLAM problem asks if it is possible for a mobile robot to be placed at an unknown location in an unknown environment and for the robot to incrementally build a consistent map of this environment while simultaneously determining its location within this map.</p><p>Both the trajectory of the platform and the location of all landmarks are estimated on-line without the need for any <em>a priori</em> knowledge of location.</p><p>SLAM problems possess a continuous and a discrete component:</p><ul><li>Continuous estimation problem: Deals with location of the objects in the map and the robot’s own pose variables.<li>Discrete estimation problem: Deals with correspondence, i.e., the relation of the object to previously detected objects. The reasoning is discrete - whether the object is same as a previously detected object or not.</ul><p><strong>Online SLAM:</strong> Involves estimating the posterior over the momentary pose along with the map.</p>\[p(x_t,m | z_{1:t}, u_{1:t})\]<p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%204.png" alt="Untitled" data-proofer-ignore></p><p><strong>Full SLAM:</strong> Involves calculating the posterior over the entire path along with the map, instead of just the current pose.</p>\[p(x_{1:t},m | z_{1:t}, u_{1:t})\]<p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%205.png" alt="Untitled" data-proofer-ignore></p><p>The online SLAM problem is the result of integrating out past poses from the full SLAM problem:</p>\[p\left(x_t, m \mid z_{1: t}, u_{1: t}\right)=\iint \cdots \int p\left(x_{1: t}, m \mid z_{1: t}, u_{1: t}\right) d x_1 d x_2 \ldots d x_{t-1}\]<p>Solving the SLAM problem requires that a <strong>state transition model</strong> and an <strong>observation model</strong> be defined describing the effect of the control input and observation respectively.</p><ul><li><div class="table-wrapper"><table><tbody><tr><td>The <strong>observation model</strong> describes the probability of making an observation \(z_k\) when the vehicle location and landmark locations are known $$ P(z_k<td>x_k,m) $$.</table></div><li><div class="table-wrapper"><table><tbody><tr><td>The <strong>motion model</strong> for the vehicle described in terms of a probability distribution on state transitions $$ P(x_k<td>x_{k-1},u_k) $$.</table></div></ul><p>The SLAM algorithm is now implemented in a recursive (sequential) prediction (time-update) correction (measurement-update) form.</p><p><strong>Time update:</strong></p>\[\begin{aligned}&amp; P\left(\mathbf{x}_k, \mathbf{m} \mid \mathbf{Z}_{0: k-1}, \mathbf{U}_{0: k}, \mathbf{x}_0\right) \\&amp; =\int P\left(\mathbf{x}_k \mid \mathbf{x}_{k-1}, \mathbf{u}_k\right) \times P\left(\mathbf{x}_{k-1}, \mathbf{m} \mid \mathbf{Z}_{0: k-1}, \mathbf{U}_{0: k-1}, \mathbf{x}_0\right) \mathrm{d} \mathbf{x}_{k-1}\end{aligned}\]<p><strong>Measurement update:</strong></p>\[\begin{aligned}&amp; P\left(\mathbf{x}_k, \mathbf{m} \mid \mathbf{Z}_{0: k}, \mathbf{U}_{0: k}, \mathbf{x}_0\right) \\&amp; =\frac{P\left(\mathbf{z}_k \mid \mathbf{x}_k, \mathbf{m}\right) P\left(\mathbf{x}_k, \mathbf{m} \mid \mathbf{Z}_{0: k-1}, \mathbf{U}_{0: k}, \mathbf{x}_0\right)}{P\left(\mathbf{z}_k \mid \mathbf{Z}_{0: k-1}, \mathbf{U}_{0: k}\right)}\end{aligned}\]<h1 id="ekf-slam">EKF SLAM</h1><p><a href="https://youtu.be/X30sEgIws0g">EKF-SLAM (Cyrill Stachniss)</a></p><p><a href="http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/slam04-ekf-slam.pdf"></a></p><p>It takes in observed landmarks from the environment and compares them with the known landmarks to find associations and new landmarks. It then uses the association to correct the state and state covariance matricies.</p><p>Maps in EKF SLAM are feature-based, which means they are composed of point landmarks. It tends to work well the less ambiguous the landmarks are, and hence requires significant engineering of feature detectors, sometimes using artificial beacons as features.</p><p>EKF SLAM makes a Gaussian noise assumption for robot motion and perception.</p><h3 id="slam-with-known-correspondence"><span class="mr-2">SLAM with Known Correspondence</span><a href="#slam-with-known-correspondence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>It addresses only the continuous portion of the SLAM problem. In addition to estimating the robot pose, the algorithm also estimates the coordinates of all landmarks encountered along the way.</p><p>Let the <em>combined state vector</em> comprising the robot pose and the map be denoted by $y_t$.</p>\[\begin{aligned}&amp; y_t=\left(\begin{array}{l}x_t \\m\end{array}\right) \\&amp; =\left(\begin{array}{llllllllllll}x &amp; y &amp; \theta &amp; m_{1, x} &amp; m_{1, y} &amp; s_1 &amp; m_{2, x} &amp; m_{2, y} &amp; s_2 &amp; \ldots &amp; m_{N, x} &amp; m_{N, y} &amp; s_N\end{array}\right)^T \\&amp;\end{aligned}\]<p>where $x,y,\theta$ denote the robot’s coordinates at time t.$m_{i,x},m_{i,y}$ are the coordinates of the i-th landmark for i=1,2,…,N and $s_i$ is its signature. The dimension of this state vector is 3N+3, where N is the number of landmarks.</p><h3 id="extended-kalman-filter-algorithm"><span class="mr-2">Extended Kalman Filter Algorithm</span><a href="#extended-kalman-filter-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/SLAM%20b11f30f58d2f44bdbe067ec1d6b7ff6b/Untitled%206.png" alt="Untitled" data-proofer-ignore></p><p>Inputs to the above algorithm are - the corrected estimate from the previous iteration, covariance or uncertainties, control input and measurements.</p><p>Line 2 represents the motion model. Line 4 is calculating the Kalman Gain. Lines 5 and 6 are the update equations.</p><p><strong>Disadvantages of EKF SLAM:</strong></p><ul><li>Linearization errors: The EKF SLAM algorithm relies on linearizing the nonlinear motion and measurement models, which can lead to approximation errors. These errors can accumulate over time, leading to inaccurate state estimates.<li>Computational complexity: EKF SLAM is computationally intensive, and its complexity grows with the number of landmarks and the size of the state vector. This can make it difficult to use in real-time applications or on resource-limited devices.<li>Limited observability: EKF SLAM assumes that all landmarks are observable, but in practice, this may not be the case. Landmarks that are not observed cannot be included in the state estimate, which can lead to inaccuracies.<li>Sensitivity to initialization: The EKF SLAM algorithm is sensitive to the initial estimates of the robot’s pose and the landmark locations. Poor initial estimates can result in the algorithm converging to the wrong solution or getting stuck in a local minimum.<li>Difficulty in handling loop closures: EKF SLAM assumes that the robot’s path is acyclic, which means it cannot handle loop closures. Loop closures occur when the robot revisits a previously visited location, and they are essential for accurate mapping. Handling loop closures requires more complex algorithms such as GraphSLAM.</ul><h3 id="loop-closing"><span class="mr-2">Loop Closing</span><a href="#loop-closing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>In loop closing, the SLAM algorithm attempts to identify and correct errors in the estimated robot trajectory by detecting and closing loops in the generated map. A loop is created when the robot revisits a previously mapped location after having moved through some other parts of the environment. When a loop is detected, the SLAM algorithm tries to reconcile the previously mapped location with the current robot pose estimate, usually by optimizing the robot’s trajectory using bundle adjustment or similar techniques.</p><p>Loop closing is important in SLAM because it enables the creation of consistent and accurate maps of the environment. Without loop closing, SLAM algorithms can suffer from drift, in which errors accumulate over time, causing the estimated robot pose to diverge from the true pose. Loop closing helps to correct such errors and ensure that the generated map is globally consistent.</p><ul><li>Loop closing means revisiting and recognizing an already mapped area. It reduces uncertainty in robot and landmark estimates.<li>Uncertainties collapse after a loop closure, whether the closure was correct or not.<li>This can be exploited when exploring an environment for the sake of better and more accurate maps.<li>However, wrong loop closures lead to filter divergence.</ul><h1 id="graphslam">GraphSLAM</h1><p><a href="https://youtu.be/uHbRKvD8TWg">Graph-based SLAM using Pose Graphs (Cyrill Stachniss)</a></p><p>GraphSLAM extracts from the data a set of soft constraints represented by a sparse graph. It obtains the map and the robot path by resolving these constraints into a globally consistent estimate.</p><p>The constraints are generally nonlinear, but in the process of resolving them they are linearized and transformed into an information matrix.</p><p>Each edge in the graph corresponds to an event: a motion event generates an edge between two robot poses, and a measurement event creates a link between a pose and a feature in the map.</p><p>For a linear system, these constraints are equivalent to entries in an information matrix and an information vector of a large system of equations.</p><h2 id="adding-confidence-measures"><span class="mr-2">Adding Confidence Measures</span><a href="#adding-confidence-measures" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Linear Least Squares allows us to include a weighting of each linear constraint.<li>If we know something about how confident a measure is, we can include that in the computation.<li>We weight each constraint by a diagonal matrix where the weights are 1/(variance for each constraint).<li>Highly confident constraints have low variance; 1/variance is large weight, and vice-versa.</ul><h2 id="cyclic-dependence"><span class="mr-2">Cyclic Dependence</span><a href="#cyclic-dependence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Features that are observed multiple times, with large time delays in between.<li>This might be the case because the robot goes back and forth through a corridor, or because the world possesses cycles.<li>In either situation, there will exist features that are seen at drastically different time steps.</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog/'>Blog</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/slam/" class="post-tag no-text-decoration" >slam</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Probabilistic+Robotics+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Faifr%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Probabilistic+Robotics+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Faifr%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Faifr%2F&text=Probabilistic+Robotics+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Faifr%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/intro-to-rl/">Introduction to Reinforcement Learning</a><li><a href="/posts/reinforcement-learning/">Reinforcement Learning</a><li><a href="/posts/deep-rl/">Deep Reinforcement Learning</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/cmu-blog/">Coursework at CMU</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/all-about-search-algorithms/"><div class="card-body"> <em class="small" data-ts="1764518400" data-df="ll" > Nov 30, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>All About Search Algorithms</h3><div class="text-muted small"><p> In progress TO-DO: Refine RRT, RRT-Connect, RRT* POMDP Multi-robot planning Graph Search Problem Once a robot converts the environment into a discrete representation, whether by grid de...</p></div></div></a></div><div class="card"> <a href="/posts/planning-case-studies/"><div class="card-body"> <em class="small" data-ts="1764518400" data-df="ll" > Nov 30, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Case Studies of Planning and Decision-Making in Robotics</h3><div class="text-muted small"><p> In progress Autonomous Driving Mobile Manipulators Legged Robots Coverage, Mapping, and Surveyal Frontier-based Planning A frontier is any region that lies at the boundary between explored sp...</p></div></div></a></div><div class="card"> <a href="/posts/diffusion-flow-matching/"><div class="card-body"> <em class="small" data-ts="1767283200" data-df="ll" > Jan 1, 2026 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Diffusion and Flow Matching</h3><div class="text-muted small"><p> Diffusion and Flow Matching</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/chebyshev/" class="btn btn-outline-primary" prompt="Older"><p>Third-order Tchebyshev Low-pass Filter</p></a> <a href="/posts/chess-engine-rl/" class="btn btn-outline-primary" prompt="Newer"><p>Chess Engine using Reinforcement Learning</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "Bhaswanth-A/bhaswanth-a.github.io", "data-repo-id": "R_kgDOHu5z_w", "data-category": "General", "data-category-id": "DIC_kwDOHu5z_84C0yVx", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
