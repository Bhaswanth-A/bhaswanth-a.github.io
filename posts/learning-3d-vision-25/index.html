<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Learning for 3D Vision" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="Assignment 1: Rendering Basics with PyTorch3D" /><meta property="og:description" content="Assignment 1: Rendering Basics with PyTorch3D" /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/learning-3d-vision-25/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/learning-3d-vision-25/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/ldr.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-03-09T21:30:00+05:30" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/ldr.png" /><meta property="twitter:title" content="Learning for 3D Vision" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2025-05-12T07:49:05+05:30","datePublished":"2025-03-09T21:30:00+05:30","description":"Assignment 1: Rendering Basics with PyTorch3D","headline":"Learning for 3D Vision","image":"https://bhaswanth-a.github.io//assets/images/ldr.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/learning-3d-vision-25/"},"url":"https://bhaswanth-a.github.io//posts/learning-3d-vision-25/"}</script><title>Learning for 3D Vision | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Vision | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Learning for 3D Vision</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Learning for 3D Vision</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1741536000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Mar 9, 2025 </em> </span> <span> Updated <em class="" data-ts="1747016345" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 11, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5259 words"> <em>29 min</em> read</span></div></div></div><div class="post-content"><h1 id="assignment-1-rendering-basics-with-pytorch3d">Assignment 1: Rendering Basics with PyTorch3D</h1><p>Questions: <a href="https://github.com/learning3d/assignment1">Github Assignment 1</a></p><h2 id="1-practicing-with-cameras"><span class="mr-2">1. Practicing with Cameras</span><a href="#1-practicing-with-cameras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="11-360-degree-renders"><span class="mr-2">1.1 360-degree Renders</span><a href="#11-360-degree-renders" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.render_360 <span class="nt">--num_frames</span> 100 <span class="nt">--fps</span> 15 <span class="nt">--output_file</span> submissions/360-render.gif
</pre></table></code></div></div><p>360-degree gif video that shows many continuous views of the provided cow mesh:</p><p><img data-src="/assets/images/L3D/a1/submissions/360-render.gif" alt="image" data-proofer-ignore></p><h3 id="12-re-creating-the-dolly-zoom"><span class="mr-2">1.2 Re-creating the Dolly Zoom</span><a href="#12-re-creating-the-dolly-zoom" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> starter.dolly_zoom <span class="nt">--num_frames</span> 20 <span class="nt">--output_file</span> submissions/dolly.gif
</pre></table></code></div></div><p>My recreated Dolly Zoom effect:</p><p><img data-src="/assets/images/L3D/a1/submissions/dolly.gif" alt="image" data-proofer-ignore></p><h2 id="2-practicing-with-meshes"><span class="mr-2">2. Practicing with Meshes</span><a href="#2-practicing-with-meshes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="21-constructing-a-tetrahedron"><span class="mr-2">2.1 Constructing a Tetrahedron</span><a href="#21-constructing-a-tetrahedron" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.mesh_practice <span class="nt">--shape</span> tetrahedron <span class="nt">--num_frames</span> 50 <span class="nt">--fps</span> 15 <span class="nt">--output_file</span> submissions/tetrahedron.gif <span class="nt">--image_size</span> 512
</pre></table></code></div></div><p>360-degree gif animation of tetrahedron:</p><p><img data-src="/assets/images/L3D/a1/submissions/tetrahedron.gif" alt="image" data-proofer-ignore></p><p>Number of vertices = 4 <br /> Number of faces = 4</p><h3 id="22-constructing-a-cube"><span class="mr-2">2.2 Constructing a Cube</span><a href="#22-constructing-a-cube" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.mesh_practice <span class="nt">--shape</span> cube <span class="nt">--num_frames</span> 50 <span class="nt">--fps</span> 15 <span class="nt">--output_file</span> submissions/cube.gif <span class="nt">--image_size</span> 512
</pre></table></code></div></div><p>360-degree gif animation of cube:</p><p><img data-src="/assets/images/L3D/a1/submissions/cube.gif" alt="image" data-proofer-ignore></p><p>Number of vertices = 8 <br /> Number of faces = 12</p><h2 id="3-re-texturing-a-mesh"><span class="mr-2">3. Re-texturing a mesh</span><a href="#3-re-texturing-a-mesh" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Chosen colors: <br /> color1 = [0, 1, 1] <br /> color2 = [1, 1, 0]</p><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.retexturing_mesh <span class="nt">--num_frames</span> 50 <span class="nt">--fps</span> 15 <span class="nt">--output_file</span> submissions/retexture_mesh.gif <span class="nt">--image_size</span> 512
</pre></table></code></div></div><p>Gif of rendered mesh:</p><p><img data-src="/assets/images/L3D/a1/submissions/retexture_mesh.gif" alt="image" data-proofer-ignore></p><h2 id="4-camera-transformations"><span class="mr-2">4. Camera Transformations</span><a href="#4-camera-transformations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$1)$ Rotate about z-axis by -90 degrees:</p><p>$R_{relative} = [[\cos(-\pi/2), -\sin(-\pi/2), 0], [\sin(-\pi/2), \cos(-\pi/2), 0], [0, 0, 1]]$</p><p>Use original translation matrix: $T_{relative} = [0, 0, 0]$</p><p><img data-src="/assets/images/L3D/a1/submissions/transformed_cow_1.jpg" alt="image" data-proofer-ignore></p><p>$2)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$</p><p>Move along z-axis by 2: $T_{relative} = [0, 0, 2]$</p><p><img data-src="/assets/images/L3D/a1/submissions/transformed_cow_2.jpg" alt="image" data-proofer-ignore></p><p>$3)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$</p><p>Move along x-axis by 0.5 and along y-axis by -0.5: $T_{relative} = [0.5, -0.5, 0]$</p><p><img data-src="/assets/images/L3D/a1/submissions/transformed_cow_3.jpg" alt="image" data-proofer-ignore></p><p>$4)$ Rotate along y-axis by 90 degrees: $R_{relative} = [[\cos(\pi/2), 0, \sin(\pi/2)], [0, 1, 0], [-\sin(\pi/2), 0, \cos(\pi/2)]]$</p><p>Move along x-axis by -3 and along z-axis by 3: $T_{relative} = [-3, 0, 3]$</p><p><img data-src="/assets/images/L3D/a1/submissions/transformed_cow_4.jpg" alt="image" data-proofer-ignore></p><h2 id="5-rendering-generic-3d-representations"><span class="mr-2">5. Rendering Generic 3D Representations</span><a href="#5-rendering-generic-3d-representations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="51-rendering-point-clouds-from-rgb-d-images"><span class="mr-2">5.1 Rendering Point Clouds from RGB-D Images</span><a href="#51-rendering-point-clouds-from-rgb-d-images" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.pcl_render <span class="nt">--image_size</span> 512
</pre></table></code></div></div><p>Gif of point cloud corresponding to the first image:</p><p><img data-src="/assets/images/L3D/a1/submissions/pcl1.gif" alt="image" data-proofer-ignore></p><p>Gif of point cloud corresponding to the second image:</p><p><img data-src="/assets/images/L3D/a1/submissions/pcl2.gif" alt="image" data-proofer-ignore></p><p>Gif of point cloud formed by the union of the first 2 point clouds:</p><p><img data-src="/assets/images/L3D/a1/submissions/pcl3.gif" alt="image" data-proofer-ignore></p><h3 id="52-parametric-functions"><span class="mr-2">5.2 Parametric Functions</span><a href="#52-parametric-functions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.torus_render <span class="nt">--function</span> parametric <span class="nt">--image_size</span> 512 <span class="nt">--num_samples</span> 500
</pre></table></code></div></div><p>Parametric equations of Torus:</p>\[x = (R + r\cos\theta)\cos\phi \\ y = (R + r\cos\theta)\sin\phi \\ z = r\sin\theta\]<p>where \(\theta \in [0,2\pi) \\ \phi \in [0,2\pi)\)</p><p>The major radius $R$ is the distance from the center of the tube to the center of the torus and the minor radius $r$ is the radius of the tube</p><p>360-degree gif of torus, with visible hole:</p><p><img data-src="/assets/images/L3D/a1/submissions/parametric_torus.gif" alt="image" data-proofer-ignore></p><p>Parametric equations of Superquadric Surface:</p>\[x = a(\cos\theta)^m(\cos\phi)^n \\ y = b(\cos\theta)^m(\sin\phi)^n \\ z = c(\sin\theta)^m\]<p>where \(\theta \in [-\frac{\pi}{2}, \frac{\pi}{2}] \\ \phi \in [0,2\pi)\)</p><p>360-degree gif of Superquadric Surface:</p><p><img data-src="/assets/images/L3D/a1/submissions/parametric_shape.gif" alt="image" data-proofer-ignore></p><h3 id="53-implicit-surfaces"><span class="mr-2">5.3 Implicit Surfaces</span><a href="#53-implicit-surfaces" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.torus_render <span class="nt">--function</span> implicit <span class="nt">--image_size</span> 512
</pre></table></code></div></div><p>Implicit equation of torus:</p>\[F(X,Y,Z) = (R - \sqrt{X^2+Y^2})^2 + Z^2 - r^2\]<p>360-degree gif of torus, with visible hole:</p><p><img data-src="/assets/images/L3D/a1/submissions/implicit_torus.gif" alt="image" data-proofer-ignore></p><p>Implicit equation of Superquadric Surface:</p>\[F(X,Y,Z) = \bigg(\bigg(\bigg\rvert\frac{X}{a}\bigg\rvert \bigg)^\frac{2}{n} + \bigg(\bigg\rvert\frac{Y}{b}\bigg\rvert \bigg)^\frac{2}{n} \bigg)^\frac{n}{m} + \bigg(\bigg\rvert\frac{Z}{c}\bigg\rvert \bigg)^\frac{2}{m} - 1\]<p>360-degree gif of Superquadric Surface:</p><p><img data-src="/assets/images/L3D/a1/submissions/implicit_shape.gif" alt="image" data-proofer-ignore></p><p>Tradeoffs between rendering as a mesh vs a point cloud:</p><ul><li>Method of Generation:<ul><li>Point Clouds: Formed by directly sampling a parametric function.<li>Meshes: Built by voxelizing a 3D space, sampling an implicit function, and then extracting surfaces using the Marching Cubes algorithm.</ul><li>Rendering Speed:<ul><li>Point Clouds: Faster to render since they simply use sampled points without extra processing.<li>Meshes: Slower because they need additional steps like voxelization and surface extraction before rendering.</ul><li>Accuracy &amp; Visual Quality:<ul><li>Point Clouds: More accurate at capturing fine details because each point represents a sampled location. However, they don’t have surfaces, making shading and texturing more difficult.<li>Meshes: Can be less accurate due to voxelization, but increasing the resolution can improve precision. They also provide continuous surfaces, which makes them easier to texture and shade.</ul><li>Computational Efficiency:<ul><li>Point Clouds: Easier to rotate, scale, and modify since they are just a collection of points.<li>Meshes: More computationally expensive to modify because updating a mesh requires adjusting vertex positions and their connections.</ul></ul><h2 id="6-do-something-fun"><span class="mr-2">6. Do Something Fun</span><a href="#6-do-something-fun" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Here is a 360 degree view of a cottage and also a dolly zoom view:</p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.fun <span class="nt">--function</span> full <span class="nt">--image_size</span> 512 <span class="nt">--output_file</span> submissions/cottage_render_360.gif
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a1/submissions/cottage_render_360.gif" alt="image" data-proofer-ignore></p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> submissions.src.fun <span class="nt">--function</span> dolly <span class="nt">--image_size</span> 512 <span class="nt">--output_file</span> submissions/cottage_dolly.gif
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a1/submissions/cottage_dolly.gif" alt="image" data-proofer-ignore></p><h2 id="extra-credit-7-sampling-points-on-meshes"><span class="mr-2">(Extra Credit) 7. Sampling Points on Meshes</span><a href="#extra-credit-7-sampling-points-on-meshes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/L3D/a1/submissions/ec_cow_mesh.gif" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a1/submissions/ec_sample10.gif" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a1/submissions/ec_sample100.gif" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a1/submissions/ec_sample1000.gif" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a1/submissions/ec_sample10000.gif" alt="image" data-proofer-ignore></p><hr /><h1 id="assignment-2-single-view-to-3d">Assignment 2: Single View to 3D</h1><p>Questions: <a href="https://github.com/learning3d/assignment2">Github Assignment 2</a></p><h2 id="0-setup"><span class="mr-2">0. Setup</span><a href="#0-setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Downloaded the shapenet single-class dataset. Unzipped the dataset and set the appropriate path in <code class="language-plaintext highlighter-rouge">dataset_location.py</code>.</p><h2 id="1-exploring-loss-functions"><span class="mr-2">1. Exploring loss functions</span><a href="#1-exploring-loss-functions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="11-fitting-a-voxel-grid"><span class="mr-2">1.1. Fitting a voxel grid</span><a href="#11-fitting-a-voxel-grid" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>To align a predicted voxel grid with a target shape, I used a binary cross-entropy (BCE) loss function. A 3D voxel grid consists of 0 (empty) and 1 (occupied) values, making this a binary classification problem where we predict occupancy probabilities of each voxel.</p><p>Implementation:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">voxel_loss</span><span class="p">(</span><span class="n">voxel_src</span><span class="p">,</span><span class="n">voxel_tgt</span><span class="p">):</span>
	<span class="c1"># voxel_src: b x h x w x d
</span>	<span class="c1"># voxel_tgt: b x h x w x d
</span>
	<span class="n">voxel_src</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">voxel_tgt</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">)</span>
	
	<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">voxel_src</span><span class="p">,</span> <span class="n">voxel_tgt</span><span class="p">)</span>

	<span class="k">return</span> <span class="n">loss</span>
</pre></table></code></div></div><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python fit_data.py <span class="nt">--type</span> <span class="s1">'vox'</span>
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Ground Truth<th style="text-align: center">Optimized Voxel<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_1_tgt.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_1_src.gif" alt="image" data-proofer-ignore></table></div><p>I trained the data for <code class="language-plaintext highlighter-rouge">10000</code> iterations.</p><h3 id="12-fitting-a-point-cloud"><span class="mr-2">1.2. Fitting a point cloud</span><a href="#12-fitting-a-point-cloud" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python fit_data.py <span class="nt">--type</span> <span class="s1">'point'</span>
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Ground Truth<th style="text-align: center">Optimized Point Cloud<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_2_tgt.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_2_src.gif" alt="image" data-proofer-ignore></table></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">chamfer_loss</span><span class="p">(</span><span class="n">point_cloud_src</span><span class="p">,</span><span class="n">point_cloud_tgt</span><span class="p">):</span>
	<span class="c1"># point_cloud_src, point_cloud_src: b x n_points x 3  
</span>
	<span class="n">p1</span> <span class="o">=</span> <span class="n">knn_points</span><span class="p">(</span><span class="n">point_cloud_src</span><span class="p">,</span> <span class="n">point_cloud_tgt</span><span class="p">)</span>
	<span class="n">p2</span> <span class="o">=</span> <span class="n">knn_points</span><span class="p">(</span><span class="n">point_cloud_tgt</span><span class="p">,</span> <span class="n">point_cloud_src</span><span class="p">)</span>
	<span class="n">loss_chamfer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">p1</span><span class="p">.</span><span class="n">dists</span> <span class="o">+</span> <span class="n">p2</span><span class="p">.</span><span class="n">dists</span><span class="p">))</span>	

	<span class="k">return</span> <span class="n">loss_chamfer</span>
</pre></table></code></div></div><h3 id="13-fitting-a-mesh"><span class="mr-2">1.3. Fitting a mesh</span><a href="#13-fitting-a-mesh" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python fit_data.py <span class="nt">--type</span> <span class="s1">'mesh'</span>
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Ground Truth<th style="text-align: center">Optimized Mesh<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_3_tgt.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/Q1_3_src.gif" alt="image" data-proofer-ignore></table></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">smoothness_loss</span><span class="p">(</span><span class="n">mesh_src</span><span class="p">):</span>

	<span class="n">loss_laplacian</span> <span class="o">=</span> <span class="n">mesh_laplacian_smoothing</span><span class="p">(</span><span class="n">mesh_src</span><span class="p">)</span>

	<span class="k">return</span> <span class="n">loss_laplacian</span>
</pre></table></code></div></div><h2 id="2-reconstructing-3d-from-single-view"><span class="mr-2">2. Reconstructing 3D from single view</span><a href="#2-reconstructing-3d-from-single-view" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="21-image-to-voxel-grid"><span class="mr-2">2.1. Image to voxel grid</span><a href="#21-image-to-voxel-grid" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Mesh<th style="text-align: center">Ground Truth Voxel<th style="text-align: center">Predicted 3D Voxel<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/16_vox.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/16_vox_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/16_vox_truth_voxel.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/16_vox.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/20_vox.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/20_vox_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/20_vox_truth_voxel.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/20_vox.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/12_vox.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/12_vox_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/12_vox_truth_voxel.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/12_vox.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/0_vox.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/0_vox_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/0_vox_truth_voxel.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/voxels/0_vox.gif" alt="image" data-proofer-ignore></table></div><p>I implemented the decoder architecture from the paper <a href="https://arxiv.org/abs/1901.11153">Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images</a>.</p><p>For the encoder, I used the pre-trained ResNet-18 model, which computes a set of features for the decoder to recover the 3D shape of the object.</p><p>The decoder is responsible for transforming information of 2D feature maps into 3D volumes. I specifically implemented a slightly modified version of the Pix2Vox-F architecture from the above paper. The input of the decoder is of size <code class="language-plaintext highlighter-rouge">[batch_size x 512]</code> and the output is <code class="language-plaintext highlighter-rouge">[batch_size x 32 x 32 x 32]</code>. The decoder contains five 3D transposed convolutional layers. The first four transposed convolutional layers are of kernel size $4^3$, with stride of $2$ and padding of $1$. The last layer has a kernel of size $1^3$. Each transposed convolutional layer is followed by a LeakyReLU activation function, except for the last layer which is followed by a sigmoid activation function. The number of output channels for each layer follows the Pix2Vox-F configuration: 128 -&gt; 64 -&gt; 32 -&gt; 8 -&gt; 1.</p><p>I trained the model for <code class="language-plaintext highlighter-rouge">10000</code> iterations, with the default batch size of <code class="language-plaintext highlighter-rouge">32</code> and learning rate of <code class="language-plaintext highlighter-rouge">4e-4</code>.</p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train_model.py <span class="nt">--type</span> <span class="s1">'vox'</span> <span class="nt">--max_iter</span> 10000 <span class="nt">--save_freq</span> 500 

python eval_model.py <span class="nt">--type</span> <span class="s1">'vox'</span> <span class="nt">--load_checkpoint</span>
</pre></table></code></div></div><h3 id="22-image-to-point-cloud"><span class="mr-2">2.2. Image to point cloud</span><a href="#22-image-to-point-cloud" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Mesh<th style="text-align: center">Ground Truth Point Cloud<th style="text-align: center">Predicted 3D Point Cloud<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/10_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/10_point_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/10_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/10_point.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/40_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/40_point_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/40_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/40_point.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point.gif" alt="image" data-proofer-ignore></table></div><p>I used an approach similar to the Pix2Vox-F decoder that I implemented above. The ResNet-18 model encodes the input images into feature maps, and a decoder reconstructs the 3D shape of the object from them.</p><p>The decoder takes in an input of size <code class="language-plaintext highlighter-rouge">[batch_size x 512]</code> and gives an output of size <code class="language-plaintext highlighter-rouge">[batch_size x n_points x 3]</code>. The decoder architecture comprises of 4 fully connected layers, three of which are followed by a LeakyReLU activation function.</p><p>I trained the model for <code class="language-plaintext highlighter-rouge">2000</code> iterations, with the default batch size of <code class="language-plaintext highlighter-rouge">32</code> and learning rate of <code class="language-plaintext highlighter-rouge">4e-4</code>.</p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train_model.py <span class="nt">--type</span> <span class="s1">'point'</span> <span class="nt">--max_iter</span> 2000 <span class="nt">--save_freq</span> 500 <span class="nt">--n_points</span> 5000 

python eval_model.py <span class="nt">--type</span> <span class="s1">'point'</span> <span class="nt">--load_checkpoint</span> <span class="nt">--n_points</span> 5000
</pre></table></code></div></div><h3 id="23-image-to-mesh"><span class="mr-2">2.3. Image to mesh</span><a href="#23-image-to-mesh" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Mesh<th style="text-align: center">Predicted Mesh<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/51_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/51_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/51_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/65_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/65_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/65_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/75_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/75_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/75_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/99_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/99_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/99_mesh.gif" alt="image" data-proofer-ignore></table></div><p>Instead of encoding an image like I did in case of image to voxel and image to point cloud, the meshes are constructed from an icosphere mesh. The purpose of the decoder is to refine this initial mesh by giving per-vertex displacement vector as an ouput.</p><p>The decoder architecture that I implemented is very similar to that in case of image to point cloud, as mentioned above. It takes an input of size <code class="language-plaintext highlighter-rouge">[batch_size x 512]</code> and gives an output of size <code class="language-plaintext highlighter-rouge">[batch_size x num_vertices x 3]</code>. It comprises of 4 fully connected layers, three of which are followed by a ReLU activation function.</p><p>I trained the model for <code class="language-plaintext highlighter-rouge">2000</code> iterations, with the default batch size of <code class="language-plaintext highlighter-rouge">32</code>, learning rate of <code class="language-plaintext highlighter-rouge">4e-4</code>.</p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train_model.py <span class="nt">--type</span> <span class="s1">'mesh'</span> <span class="nt">--max_iter</span> 2000 <span class="nt">--save_freq</span> 500

python eval_model.py <span class="nt">--type</span> <span class="s1">'mesh'</span> <span class="nt">--load_checkpoint</span>
</pre></table></code></div></div><h3 id="24-quantitative-comparisions"><span class="mr-2">2.4. Quantitative comparisions</span><a href="#24-quantitative-comparisions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>F1-score curves at different thresholds:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Voxel Grid<th style="text-align: center">Point Cloud<th style="text-align: center">Mesh<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_vox.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh.png" alt="image" data-proofer-ignore></table></div><p>From the above plots, we can infer that the point cloud model performed the best, giving the highest F1-score, followed by the mesh model and the voxel model.</p><p>Intuitively, I think the reason the point cloud outperformed the voxel and mesh models is because it aligns well with the evaluation method, which compares points directly from the network output to the ground truth. Since point clouds don’t need to define surfaces or connections, they are more flexible and avoid errors caused by surface sampling. This makes them easier to optimize and more accurate in reconstruction.</p><p>The mesh model performed slightly worse primarily due to the challenges associated with sampling points from a continuous surface. Unlike point clouds, where each output point is directly predicted, meshes require proper face orientation and connectivity. Due to this, the sampled points might not always align perfectly with the ground truth, especially when dealing with complex geometries like thin structures (legs of the chair).</p><p>The voxel model has the lowest F1-score because representing 3D space as voxels limits a lot of detail and accuracy. Fine details can be lost due to this fixed resolution and sampled points may not always align perfectly with the object’s actual surface, affecting evaluation results.</p><h3 id="25-analyse-effects-of-hyperparams-variations"><span class="mr-2">2.5. Analyse effects of hyperparams variations</span><a href="#25-analyse-effects-of-hyperparams-variations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>I have chosen to vary the <code class="language-plaintext highlighter-rouge">w_smooth</code> hyperparameter and analyze the changes in the mesh model prediction. The default value of <code class="language-plaintext highlighter-rouge">w_smooth</code> is <code class="language-plaintext highlighter-rouge">0.1</code>, and its results have been shown in Section 2.3. I sampled 4 other values of <code class="language-plaintext highlighter-rouge">w_smooth</code> - <code class="language-plaintext highlighter-rouge">0.001</code>, <code class="language-plaintext highlighter-rouge">0.01</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">10</code>.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Mesh<th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.001</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.01</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.1</code> (Default)<th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=1</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=10</code><tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/51_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/51_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_0001/51_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/51_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/51_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_1/51_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_10/51_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/75_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/75_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_0001/75_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/75_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/75_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_1/75_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_10/75_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/99_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/99_mesh_truth_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_0001/99_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_001/99_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/meshes/99_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_1/99_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/mesh_10/99_mesh.gif" alt="image" data-proofer-ignore></table></div><p>F1-score curves for different variations in <code class="language-plaintext highlighter-rouge">w_smooth</code> for the mesh model:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.001</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.01</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=0.1</code> (Default)<th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=1</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">w_smooth=10</code><tbody><tr><td style="text-align: center">Avg F1-score@0.05 = <code class="language-plaintext highlighter-rouge">72.977</code><td style="text-align: center">Avg F1-score@0.05 = <code class="language-plaintext highlighter-rouge">72.133</code><td style="text-align: center">Avg F1-score@0.05 = <code class="language-plaintext highlighter-rouge">70.951</code> (Default)<td style="text-align: center">Avg F1-score@0.05 = <code class="language-plaintext highlighter-rouge">71.834</code><td style="text-align: center">Avg F1-score@0.05 = <code class="language-plaintext highlighter-rouge">72.337</code><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh_w0001.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh_w001.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh_w1.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_mesh_w10.png" alt="image" data-proofer-ignore></table></div><p>For low values of <code class="language-plaintext highlighter-rouge">w_smooth = 0.001, 0.01</code>:</p><ul><li>They preserve the fine details but introduce noise and distortions<li>It results in rough and fragmented surfaces<li>They show a slightly higher F1-score because they retain geometric details</ul><p>For high values of <code class="language-plaintext highlighter-rouge">w_smooth = 1, 10</code>:</p><ul><li>They produce cleaner and more smooth meshes with reduced artifacts<li>It over-smooths the surface, causing loss of sharp details<li>The F1-score improves slightly likely because they more likely fall within the threshold radius of the ground truth</ul><p>The default value of <code class="language-plaintext highlighter-rouge">w_smooth = 0.1</code> falls in between the above two categories.</p><h3 id="26-interpret-your-model"><span class="mr-2">2.6. Interpret your model</span><a href="#26-interpret-your-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="per-point-error-visualization"><span class="mr-2">Per-Point Error Visualization</span><a href="#per-point-error-visualization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Point Cloud<th style="text-align: center">Predicted 3D Point Cloud<th style="text-align: center">Per-Point Error<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/53_point.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_per_point_error/per_point_53.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points/85_point.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_per_point_error/per_point_85.gif" alt="image" data-proofer-ignore></table></div><p>I used per-point error to gauge how well each predicted 3D point matches its corresponding point in the ground truth. In my approach, I compute the distance between each point in my reconstructed point cloud and its nearest neighbor in the ground truth. Then, I color-code these distances such that points with very small errors appear in cool colors (blue), while those with larger errors show up in warm colors (red).</p><p>From the above gifs, we can clearly see that some regions are rendered in cool tones, which tells me that my model is accurately capturing those parts of the object, such as the seat surface or the main body of the chair. On the other hand, areas highlighted in warm colors reveal where the model struggles, like along the thin chair legs or at complex curves of the backrest.</p><p>This visualization pinpoints the exact regions that need improvement.</p><h4 id="failure-case-analysis"><span class="mr-2">Failure Case Analysis</span><a href="#failure-case-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>In analyzing my 3D voxel model’s predictions, I noticed that while it reconstructs the backrest of chairs quite well, it struggles significantly with legs, seats, and unusual shapes. These failure cases provide valuable insight into the model’s learning behavior and what its limitations are.</p><ol><li><p>Legs: Chair legs vary widely in shape, thickness, and placement across different samples in the dataset. Some chairs have four standard legs, while others may have a single central support or a complex curved base. Because the model tries to generalize patterns across the dataset, it struggles to reconstruct legs consistently. Additionally, legs are usually thin and small compared to the rest of the chair, and this makes them more prone to voxelization errors.</p><li><p>Seats: Many chair designs have gaps in them, which makes it challenging for the model to learn and also more prone to voxelization errors. Since the model tries to reconstruct a smoothed version of objects, it often fails to represent holes correctly, either closing them off entirely or introducing unexpected artifacts.</p><li><p>Unusual shapes: Some chairs in the dataset have very unique designs. Since the model is trained on a limited dataset, it may not have seen enough similar examples to generalize well.</p></ol><h2 id="3-exploring-other-architectures--datasets"><span class="mr-2">3. Exploring other architectures / datasets</span><a href="#3-exploring-other-architectures--datasets" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="33-extended-dataset-for-training"><span class="mr-2">3.3 Extended dataset for training</span><a href="#33-extended-dataset-for-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">dataset_location.py</code> updated to include the extended dataset containing three classes (chair, car, and plane).</p><div class="table-wrapper"><table><thead><tr><th>category<th>#instances<tbody><tr><td>airplane<td>36400<tr><td>car<td>31460<tr><td>chair<td>61000<tr><td>total<td>128860</table></div><p>I trained and evaluated the point cloud model with <code class="language-plaintext highlighter-rouge">n_points = 5000</code>.</p><p>Quantitative evaluation:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Point Cloud trained on one class<th style="text-align: center">Point Cloud trained on three classes<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/eval_point_full_dataset.png" alt="image" data-proofer-ignore></table></div><p>Qualitative evlautaion by comparing “training on one class” vs “training on three classes”:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input RGB<th style="text-align: center">Ground Truth Point Cloud<th style="text-align: center">Predicted 3D Point Cloud for 1 Class Training<th style="text-align: center">Predicted 3D Point Cloud for 3 Classes Training<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/65_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/65_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/65_point_old.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/65_point.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/500_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/500_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/500_point_old.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/500_point.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/800_point.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/800_point_truth_pointcloud.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/800_point_old.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a2/submissions/points_full/800_point.gif" alt="image" data-proofer-ignore></table></div><p>3D consistency and diversity of output samples:</p><p>Training the model on a single class, like chairs, results in more consistent and refined reconstructions. Since the model only sees one object type during training, it gets really good at capturing the details and structure unique to that class. However, this also means that the model becomes highly specialized. So when it is faced with a completely new object type (such as an airplane or car), it struggles because it hasn’t learned to handle the variation.</p><p>On the other hand, training on multiple classes (airplanes, cars, and chairs) allows the model to adapt better to different object shapes. Instead of focusing on one type, it learns general patterns that apply across different categories. This makes it more versatile when reconstructing new objects.</p><p>So in conclusion, single-class models tend to produce more uniform outputs because they have learned a very specific structural representation but lack adaptability. Multi-class models generate more diverse outputs because they have seen various object types and have learned to adapt to different shapes but at the cost of some fine-grained details.</p><hr /><h1 id="assignment-3-part-1-neural-volume-rendering">Assignment 3: Part-1 Neural Volume Rendering</h1><p>Questions: <a href="https://github.com/learning3d/assignment3">Github Assignment 3</a></p><h2 id="0-transmittance-calculation"><span class="mr-2">0. Transmittance Calculation</span><a href="#0-transmittance-calculation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/L3D/a3/images/part_0_transmittance.png" alt="image" data-proofer-ignore></p><h2 id="1-differentiable-volume-rendering"><span class="mr-2">1. Differentiable Volume Rendering</span><a href="#1-differentiable-volume-rendering" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="13-ray-sampling"><span class="mr-2">1.3. Ray sampling</span><a href="#13-ray-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>box
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/1.3_xygrid.png" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a3/images/1.3_rays.png" alt="image" data-proofer-ignore></p><h3 id="14-point-sampling"><span class="mr-2">1.4. Point sampling</span><a href="#14-point-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>box
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/1.4_render_point_samples.png" alt="image" data-proofer-ignore></p><h3 id="15-volume-rendering"><span class="mr-2">1.5. Volume rendering</span><a href="#15-volume-rendering" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>box
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/part_1.gif" alt="image" data-proofer-ignore></p><p><img data-src="/assets/images/L3D/a3/images/1.5_depth_visualization.png" alt="image" data-proofer-ignore></p><h2 id="2-optimizing-a-basic-implicit-volume"><span class="mr-2">2. Optimizing a basic implicit volume</span><a href="#2-optimizing-a-basic-implicit-volume" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="21-random-ray-sampling"><span class="mr-2">2.1. Random ray sampling</span><a href="#21-random-ray-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">get_random_pixels_from_image</span><span class="p">(</span><span class="n">n_pixels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">xy_grid</span> <span class="o">=</span> <span class="n">get_pixels_from_image</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    
    <span class="c1"># Random subsampling of pixel coordinaters
</span>    <span class="n">xy_grid_sub</span> <span class="o">=</span> <span class="n">xy_grid</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">xy_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_pixels</span><span class="p">)].</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">xy_grid_sub</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:</span><span class="n">n_pixels</span><span class="p">]</span>
</pre></table></code></div></div><h3 id="22-loss-and-training"><span class="mr-2">2.2. Loss and training</span><a href="#22-loss-and-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">loss = torch.nn.functional.mse_loss(rgb_gt, out['feature'])</code></p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>train_box
</pre></table></code></div></div><p>Box center: <code class="language-plaintext highlighter-rouge">(0.2502, 0.2506, -0.0005)</code> <br /> Box side lengths: <code class="language-plaintext highlighter-rouge">(2.0051, 1.5036, 1.5034)</code></p><h3 id="23-visualization"><span class="mr-2">2.3. Visualization</span><a href="#23-visualization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>train_box
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/part_2.gif" alt="image" data-proofer-ignore></p><h2 id="3-optimizing-a-neural-radiance-field-nerf"><span class="mr-2">3. Optimizing a Neural Radiance Field (NeRF)</span><a href="#3-optimizing-a-neural-radiance-field-nerf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>nerf_lego
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/part_3.gif" alt="image" data-proofer-ignore></p><h2 id="4-nerf-extras"><span class="mr-2">4. NeRF Extras</span><a href="#4-nerf-extras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="41-view-dependence"><span class="mr-2">4.1 View Dependence</span><a href="#41-view-dependence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python volume_rendering_main.py <span class="nt">--config-name</span><span class="o">=</span>nerf_materials
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/part_4.gif" alt="image" data-proofer-ignore></p><p>Trade-offs between increased view dependence and generalization quality:</p><ul><li>Adding view dependence allows the model to capture complex lighting effects like reflections, and translucency. But excessive view dependence can create inconsistencies when interpolating between unique views, which will make the rendering look unnatural.<li>If the network heavily relies on viewing direction, it may overfit to the specific camera angles in the training data. This can lead to poor generalization to unseen viewpoints.<li>It can increase the network’s complexity, requiring more parameters and training time.</ul><h1 id="assignment-3-part-2-neural-surface-rendering">Assignment 3: Part-2 Neural Surface Rendering</h1><h2 id="5-sphere-tracing"><span class="mr-2">5. Sphere Tracing</span><a href="#5-sphere-tracing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>My implementation of the <code class="language-plaintext highlighter-rouge">SphereTracingRenderer</code> class uses the sphere tracing algorithm to find intersections between rays and an implicit surface of a torus defined by a signed distance field. The algorithm iteratively updates points along each ray by moving in the direction of the ray by the amount of the SDF value at the current point. This process continues until the maximum number of iterations is reached or the SDF value becomes very close to zero (threshold of <code class="language-plaintext highlighter-rouge">1e-6</code>), indicating a surface intersection.</p><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> surface_rendering_main <span class="nt">--config-name</span><span class="o">=</span>torus_surface
</pre></table></code></div></div><p><img data-src="/assets/images/L3D/a3/images/part_5.gif" alt="image" data-proofer-ignore></p><h2 id="6-optimizing-a-neural-sdf"><span class="mr-2">6. Optimizing a Neural SDF</span><a href="#6-optimizing-a-neural-sdf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> surface_rendering_main <span class="nt">--config-name</span><span class="o">=</span>points_surface
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Input<th style="text-align: center"><code class="language-plaintext highlighter-rouge">lr=0.0001</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">lr=0.001</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">lr=0.00001</code><tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_6_input.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_6.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_6_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_6_3.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Loss<td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.001279</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.000428</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.001635</code></table></div><p><code class="language-plaintext highlighter-rouge">eikonal_loss = ((gradients.norm(2, dim=1) - 1.0) ** 2).mean()</code></p><p>Implementation:</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>Input (XYZ points) -&gt; Harmonic Embedding -&gt; Layer 1 (Linear + ReLU)
                      -&gt; Layer 2 (Linear + ReLU)
                      -&gt; Layer 3 (Linear + ReLU)
                      -&gt; ...
                      -&gt; Layer N (Linear + ReLU)
                      -&gt; Linear SDF (Output: Signed Distance Function)
</pre></table></code></div></div><h2 id="7-volsdf"><span class="mr-2">7. VolSDF</span><a href="#7-volsdf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Usage:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> surface_rendering_main <span class="nt">--config-name</span><span class="o">=</span>volsdf_surface
</pre></table></code></div></div><ul><li>Alpha: Scales the overall density. A higher value increases the density, while a lower value reduces it.<li>Beta: Controls how quickly the density changes with distance from the surface. A smaller beta results in a sharper transition, while a larger beta smooths the transition.</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sdf_to_density</span><span class="p">(</span><span class="n">signed_distance</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">signed_distance</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
            <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">signed_distance</span> <span class="o">/</span> <span class="n">beta</span><span class="p">),</span>
            <span class="mi">1</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">signed_distance</span> <span class="o">/</span> <span class="n">beta</span><span class="p">),</span>
        <span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Geometry<th style="text-align: center">Result<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_1.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Loss<td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.006958</code></table></div><p>The above renders are for values <code class="language-plaintext highlighter-rouge">alpha=10.0</code> and <code class="language-plaintext highlighter-rouge">beta=0.05</code>.</p><p>When <code class="language-plaintext highlighter-rouge">alpha=10.0</code> and <code class="language-plaintext highlighter-rouge">beta</code> is changed:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"><code class="language-plaintext highlighter-rouge">beta</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">beta=0.05</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">beta=0.1</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">beta=0.5</code><tbody><tr><td style="text-align: center">Geometry<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_3.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Render<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_3.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Loss<td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.006958</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.010227</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.020789</code></table></div><p>When <code class="language-plaintext highlighter-rouge">beta=0.05</code> and <code class="language-plaintext highlighter-rouge">alpha</code> is changed:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"><code class="language-plaintext highlighter-rouge">alpha</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">alpha=1</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">alpha=10</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">alpha=50</code><tbody><tr><td style="text-align: center">Geometry<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_6.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_geometry_7.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Render<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_6.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_7_7.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Loss<td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.022317</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.006958</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.004329</code></table></div><p><strong>How does high beta bias your learned SDF? What about low beta?</strong></p><p>High <code class="language-plaintext highlighter-rouge">beta</code> makes the transition between occupied and free space more gradual, leading to a smoother SDF. This can cause a bias where surfaces appear more diffused rather than sharp.</p><p>Low <code class="language-plaintext highlighter-rouge">beta</code> results in a sharper transition, meaning the SDF will be more precise in distinguishing surfaces, but it can also lead to unstable gradients and more difficult optimization.</p><p><strong>Would an SDF be easier to train with volume rendering and low beta or high beta? Why?</strong></p><p>An SDF is generally easier to train with volume rendering when using a high <code class="language-plaintext highlighter-rouge">beta</code>. This is because high beta values cause a larger number of points along each ray to have non-zero density, allowing gradients to be backpropagated through more points simultaneously. This leads to denser gradients and faster convergence during training.</p><p>Training with a low <code class="language-plaintext highlighter-rouge">beta</code> can be more challenging because it forces the network to learn very sharp transitions, which means only points very close to the surface contributes significantly to the rendering. This can lead to sparse gradients and slower convergence.</p><p><strong>Would you be more likely to learn an accurate surface with high beta or low beta? Why?</strong></p><p>You are more likely to learn an accurate surface with a low <code class="language-plaintext highlighter-rouge">beta</code>. A low beta encourages sharp boundaries and a more precise surface representation, as the density function closely approximates a step function. High <code class="language-plaintext highlighter-rouge">beta</code> values, on the other hand, lead to smoother surfaces, which can be less accurate.</p><p>Implementation:</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>Input (SDF Feature + XYZ Embedding) -&gt; Layer 1 (Linear + ReLU)  
                      -&gt; Layer 2 (Linear + ReLU)  
                      -&gt; Layer 3 (Linear + ReLU)  
                      -&gt; ...  
                      -&gt; Layer N (Linear + ReLU)  
                      -&gt; Linear RGB (Output: 3D Color Prediction)  
</pre></table></code></div></div><h2 id="8-neural-surface-extras"><span class="mr-2">8. Neural Surface Extras</span><a href="#8-neural-surface-extras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="83-alternate-sdf-to-density-conversions"><span class="mr-2">8.3 Alternate SDF to Density Conversions</span><a href="#83-alternate-sdf-to-density-conversions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Logistic density distribution function:</p>\[\phi_s(x) = \frac{se^{-sx}}{(1+e^{-sx})^2}\]<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">neus_sdf_to_density</span><span class="p">(</span><span class="n">signed_distance</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span> <span class="o">*</span> <span class="n">signed_distance</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span> <span class="o">*</span> <span class="n">signed_distance</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> 
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center"><code class="language-plaintext highlighter-rouge">s</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">s=10</code><th style="text-align: center"><code class="language-plaintext highlighter-rouge">s=50</code><tbody><tr><td style="text-align: center">Geometry<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_8_geometry_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_8_geometry.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Render<td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_8_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a3/images/part_8.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Loss<td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.005590</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">0.006529</code></table></div><p>Low <code class="language-plaintext highlighter-rouge">s</code> results in a more blurry render, while a higher value of <code class="language-plaintext highlighter-rouge">s</code> makes it look sharp.</p><hr /><h1 id="assignment-4">Assignment 4</h1><p>Questions: <a href="https://github.com/learning3d/assignment4">Github Assignment 4</a></p><h2 id="1-3d-gaussian-splatting"><span class="mr-2">1. 3D Gaussian Splatting</span><a href="#1-3d-gaussian-splatting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="11-3d-gaussian-rasterization"><span class="mr-2">1.1 3D Gaussian Rasterization</span><a href="#11-3d-gaussian-rasterization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python render.py <span class="nt">--gaussians_per_splat</span> 1024
</pre></table></code></div></div><p>Output GIF:</p><p><img data-src="/assets/images/L3D/a4/Q1/output//q1_render.gif" alt="image" data-proofer-ignore></p><h3 id="12-training-3d-gaussian-representations"><span class="mr-2">1.2 Training 3D Gaussian Representations</span><a href="#12-training-3d-gaussian-representations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python train.py <span class="nt">--gaussians_per_splat</span> 2048 <span class="nt">--device</span> cuda
</pre></table></code></div></div><p>I modified the <code class="language-plaintext highlighter-rouge">run_training</code> function to improve performance and reduce CUDA memory usage. Specifically, I:</p><ul><li><p>Reduced the number of Gaussians from 10k to 7k by subsampling the points file, lowering the overall GPU memory footprint.</p><li><p>Ensured key Gaussian parameters (opacities, scales, colours, and means) were trainable and set up an optimizer with different learning rates for each parameter group (as mentioned in 1.2.1)</p><li><p>Wrapped the forward pass in <code class="language-plaintext highlighter-rouge">autocast</code> and used <code class="language-plaintext highlighter-rouge">GradScaler</code> to scale the loss, which both reduced memory usage and accelerated computation</p></ul><p>I called the <code class="language-plaintext highlighter-rouge">scene.render</code> method to generate the predicted image from the input camera parameters, using the specified image size, background color, and the number of Gaussians per splat. I then computed the L1 loss between the rendered (predicted) image and the ground truth image.</p><p>Learning rates that I used for each parameter:</p><ul><li><code class="language-plaintext highlighter-rouge">pre_act_opacities</code> = <code class="language-plaintext highlighter-rouge">0.001</code><li><code class="language-plaintext highlighter-rouge">pre_act_scales</code> = <code class="language-plaintext highlighter-rouge">0.001</code><li><code class="language-plaintext highlighter-rouge">colours</code> = <code class="language-plaintext highlighter-rouge">0.02</code><li><code class="language-plaintext highlighter-rouge">means</code> = <code class="language-plaintext highlighter-rouge">0.0002</code></ul><p>Number of iterations that I trained the model for = <code class="language-plaintext highlighter-rouge">1000</code></p><p>Mean PSNR: 27.356</p><p>Mean SSIM: 0.915</p><p>Training final render GIF:</p><p><img data-src="/assets/images/L3D/a4/Q1/output//q1_training_final_renders.gif" alt="image" data-proofer-ignore></p><p>Training progress GIF:</p><p><img data-src="/assets/images/L3D/a4/Q1/output//q1_training_progress.gif" alt="image" data-proofer-ignore></p><h3 id="13-extensions"><span class="mr-2">1.3 Extensions</span><a href="#13-extensions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="131-rendering-using-spherical-harmonics"><span class="mr-2">1.3.1 Rendering Using Spherical Harmonics</span><a href="#131-rendering-using-spherical-harmonics" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python render.py <span class="nt">--gaussians_per_splat</span> 1024
</pre></table></code></div></div><p>GIF:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Original<th style="text-align: center">Spherical Harmonics<tbody><tr><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q1_render.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q13_render.gif" alt="image" data-proofer-ignore></table></div><p>RGB image comparisons of the renderings obtained from both the cases:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Frame<th style="text-align: center">Original<th style="text-align: center">Spherical Harmonics<tbody><tr><td style="text-align: center">000<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q1_render/000.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q13_render/000.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">015<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q1_render/015.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q13_render/015.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">021<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q1_render/018.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q13_render/018.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">030<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q1_render/030.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q1/output//q13_render/030.png" alt="image" data-proofer-ignore></table></div><p>Differences that can be observed:</p><ul><li>Frame 000 and 015: The spherical harmonics rendering looks more photorealistic because the angular variations in color better capture how the material responds to illumination from different directions.<li>Frames 021 and 030: The spherical harmonics rendering looks more glossy and reflective because the added directional sensitivity leads to more dynamic and detailed shading.</ul><h2 id="2-diffusion-guided-optimization"><span class="mr-2">2. Diffusion-guided Optimization</span><a href="#2-diffusion-guided-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="21-sds-loss--image-optimization"><span class="mr-2">2.1 SDS Loss + Image Optimization</span><a href="#21-sds-loss--image-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q21_image_optimization.py <span class="nt">--sds_guidance</span> 1
</pre></table></code></div></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Prompt<th style="text-align: center">Without Guidance<th style="text-align: center">With Guidance<tbody><tr><td style="text-align: center">Iterations<td style="text-align: center">400<td style="text-align: center">2000<tr><td style="text-align: center">“a hamburger”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image_0/a_hamburger/output.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image/a_hamburger/output.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">“a standing corgi dog”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image_0/a_standing_corgi_dog/output.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image/a_standing_corgi_dog/output.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">“a fish in a pan”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image_0/a_fish_in_a_pan/output.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image/a_fish_in_a_pan/output.png" alt="image" data-proofer-ignore><tr><td style="text-align: center">“a mansion”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image_0/a_mansion/output.png" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//image/a_mansion/output.png" alt="image" data-proofer-ignore></table></div><h3 id="22-texture-map-optimization-for-mesh"><span class="mr-2">2.2 Texture Map Optimization for Mesh</span><a href="#22-texture-map-optimization-for-mesh" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python python Q22_mesh_optimization.py
</pre></table></code></div></div><p>In order to reduce the CUDA memory footprint, I reduced the image size to <code class="language-plaintext highlighter-rouge">256x256</code>.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Prompt<th style="text-align: center">Initial Mesh GIF<th style="text-align: center">Final Mesh GIF<tbody><tr><td style="text-align: center">“a tiger”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//mesh/a_tiger/initial_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//mesh/a_tiger/final_mesh.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">“a zebra”<td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//mesh/a_zebra/initial_mesh.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a4/Q2/output//mesh/a_zebra/final_mesh.gif" alt="image" data-proofer-ignore></table></div><h3 id="23-nerf-optimization"><span class="mr-2">2.3 NeRF Optimization</span><a href="#23-nerf-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>I perfomed no CUDA memory optimization here. All values were the same as default as pulled from GitHub.</p><p>Prompt: “a standing corgi dog”</p><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q23_nerf_optimization.py <span class="nt">--prompt</span> <span class="s2">"a standing corgi dog"</span> <span class="nt">--lambda_entropy</span> 1e-2 <span class="nt">--lambda_orient</span> 1e-2 <span class="nt">--latent_iter_ratio</span> 0.1 
</pre></table></code></div></div><p>Rendered RGB video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_standing_corgi_dog/videos/rgb_ep_99.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Rendered depth video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_standing_corgi_dog/videos/depth_ep_99.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Prompt: “a hamburger”</p><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q23_nerf_optimization.py <span class="nt">--prompt</span> <span class="s2">"a hamburger"</span> <span class="nt">--iters</span> 2500 <span class="nt">--lambda_entropy</span> 1e-3 <span class="nt">--lambda_orient</span> 1e-2 <span class="nt">--latent_iter_ratio</span> 0.2 
</pre></table></code></div></div><p>Rendered RGB video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_hamburger/videos/rgb_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Rendered depth video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_hamburger/videos/depth_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Prompt: “a mansion”</p><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q23_nerf_optimization.py <span class="nt">--prompt</span> <span class="s2">"a mansion"</span> <span class="nt">--iters</span> 2500 <span class="nt">--lambda_entropy</span> 1e-2 <span class="nt">--lambda_orient</span> 1e-2 <span class="nt">--latent_iter_ratio</span> 0.1 
</pre></table></code></div></div><p>Rendered RGB video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_mansion/videos/rgb_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Rendered depth video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/a_mansion/videos/depth_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><h3 id="24-extensions"><span class="mr-2">2.4 Extensions</span><a href="#24-extensions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="241-view-dependent-text-embedding"><span class="mr-2">2.4.1 View-dependent text embedding</span><a href="#241-view-dependent-text-embedding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Prompt: “a standing corgi dog”</p><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q23_nerf_optimization.py <span class="nt">--prompt</span> <span class="s2">"a standing corgi dog"</span> <span class="nt">--lambda_entropy</span> 1e-2 <span class="nt">--lambda_orient</span> 1e-2 <span class="nt">--latent_iter_ratio</span> 0.1 <span class="nt">--view_dep_text</span> 1
</pre></table></code></div></div><p>Rendered RGB video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/2.4/a_standing_corgi_dog/videos/rgb_ep_99.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Rendered depth video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/2.4/a_standing_corgi_dog/videos/depth_ep_99.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Prompt: “a hamburger”</p><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python Q23_nerf_optimization.py <span class="nt">--prompt</span> <span class="s2">"a hamburger"</span> <span class="nt">--iters</span> 2500 <span class="nt">--lambda_entropy</span> 1e-3 <span class="nt">--lambda_orient</span> 1e-2 <span class="nt">--latent_iter_ratio</span> 0.2 <span class="nt">--view_dep_text</span> 1
</pre></table></code></div></div><p>Rendered RGB video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/2.4/a_hamburger/videos/rgb_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Rendered depth video:</p><video width="640" height="360" controls="" loop=""> <source src="/assets/images/L3D/a4/Q2/output//nerf/2.4/a_hamburger/videos/depth_ep_24.mp4" type="video/mp4" /> Your browser does not support the video tag. </video><p>Comparing with 2.3:</p><p>In 2.3, the NeRF model used one fixed text embedding for all views. This led to consistent but somewhat flat results and sometimes produced artifacts like multiple front faces (standing corgi dog example) because the model didn’t know how to adjust for different angles.</p><p>With view-dependent text conditioning, different embeddings (like front, side, and back) are used based on the camera angle. This helps the model adjust lighting, highlights, and reflections for each view, resulting in more realistic and 3D-consistent images.</p><p>The “hamburger” looks really good with view-dependent conditioning, while the “standing corgi dog” did not turn out as well, likely because it needs more training to capture all its details.</p><hr /><h1 id="assignment-5">Assignment 5</h1><p>Questions: <a href="https://github.com/learning3d/assignment5">Github Assignment 5</a></p><h2 id="1-classification-model"><span class="mr-2">1. Classification Model</span><a href="#1-classification-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train.py <span class="nt">--task</span> cls

python eval_cls.py
</pre></table></code></div></div><p>The model was trained for 250 epochs, and the best model was saved at epoch 115.</p><p>Train loss = 1.1288</p><p>Test accuracy of best model = 0.9696</p><p>Visualization of a few random test point clouds and their predicted classes:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Correct Prediction<th style="text-align: center">Chairs<th style="text-align: center">Vases<th style="text-align: center">Lamps<tbody><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num51_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num638_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num763_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num134_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num630_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_s_num877_2_2.gif" alt="image" data-proofer-ignore></table></div><p>Visualization of a failure prediction for each class:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Correct Label<th style="text-align: center">Vase<th style="text-align: center">Lamp<th style="text-align: center">Vase<tbody><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_f_num673_1_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_f_num777_2_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q1/cls_f_num714_1_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Incorrect Prediction<td style="text-align: center">Lamp<td style="text-align: center">Vase<td style="text-align: center">Lamp</table></div><p>The successful results show that the model is able to pick out important features. For example, it correctly identifies the distinct structure of chairs, vases, and lamps. But in some cases, the shapes can be ambiguous or similar, which causes the model to misclassify objects. This can happen when the point sampling misses some important details or when the features are too similar between classes.</p><h2 id="2-segmentation-model"><span class="mr-2">2. Segmentation Model</span><a href="#2-segmentation-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train.py <span class="nt">--task</span> seg

python eval_seg.py
</pre></table></code></div></div><p>The model was trained for 250 epochs, and the best model was saved at epoch 225.</p><p>Train loss = 13.0442</p><p>Test accuracy of best model = 0.8966</p><p>Visualization of correct segmentation predictions along with their corresponding ground truth:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Accuracy<th style="text-align: center">0.9317<th style="text-align: center">0.9147<th style="text-align: center">0.9308<tbody><tr><td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_gt_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Good Segmentation Predictions<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/success_pred_2.gif" alt="image" data-proofer-ignore></table></div><p>Visualization of incorrect segmentation predictions along with their corresponding ground truth:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Accuracy<th style="text-align: center">0.5604<th style="text-align: center">0.5572<th style="text-align: center">0.4702<tbody><tr><td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_gt_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Bad Segmentation Predictions<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q2/failure_pred_2.gif" alt="image" data-proofer-ignore></table></div><p>The good segmentation predictions show that the model can distinguish the chair’s seat, back, and legs fairly accurately. In the failure cases, we can see that certain parts like the seat or back merges with the legs or other regions. A possible reason could be that if the training data includes ambiguous or poorly differentiated boundaries, the model may struggle to learn to differentiate segments in these areas.</p><h2 id="3-robustness-analysis"><span class="mr-2">3. Robustness Analysis</span><a href="#3-robustness-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="31-rotate-input-point-clouds"><span class="mr-2">3.1 Rotate Input Point Clouds</span><a href="#31-rotate-input-point-clouds" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="311-classification-model"><span class="mr-2">3.1.1 Classification Model</span><a href="#311-classification-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Visualization of a few random test point clouds and their predicted classes:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Angle<th style="text-align: center">Accuracy<th style="text-align: center">Chairs<th style="text-align: center">Vases<th style="text-align: center">Lamps<tbody><tr><td style="text-align: center">10 degrees<td style="text-align: center">0.958<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num114_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num620_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num730_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">10 degrees<td style="text-align: center">0.958<td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num396_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">20 degrees<td style="text-align: center">0.894<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_s_num114_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_s_num620_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_s_num730_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">20 degrees<td style="text-align: center">0.894<td style="text-align: center">Wrong predicted as lamp <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_f_num396_0_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">30 degrees<td style="text-align: center">0.679<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_s_num114_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_s_num620_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_s_num730_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">30 degrees<td style="text-align: center">0.679<td style="text-align: center">Wrong predicted as lamp <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_f_num396_0_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Wrongly predicted as lamp <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_f_num675_1_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">60 degrees<td style="text-align: center">0.299<td style="text-align: center">Wrongly predicted as vase <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_f_num114_0_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_s_num620_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Wrongly predicted as chair <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_f_num730_2_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">60 degrees<td style="text-align: center">0.299<td style="text-align: center">Wrong predicted as lamp <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_f_num396_0_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Wrongly predicted as lamp <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_f_num675_1_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore></table></div><p>From the observations above, we see that the model classifies most rotated objects correctly when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), the accuracy drops and the model starts confusing objects. This happens because the model wasn’t trained with any rotational variations and so the learned features become orientation-dependent.</p><h4 id="312-segmentation-model"><span class="mr-2">3.1.2 Segmentation Model</span><a href="#312-segmentation-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Angle<th style="text-align: center">Model<th style="text-align: center">X<th style="text-align: center">X<th style="text-align: center">X<th style="text-align: center">X<tbody><tr><td style="text-align: center">10 degrees<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">10 degrees<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.9185 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9134 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9055 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.4798 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/10/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">20 degrees<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">20 degrees<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.9396 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9191 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9307 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.5906 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/20/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">30 degrees<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"> <tr><td style="text-align: center">30 degrees<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.9093 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9429 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.4707 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/30/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"> <tr><td style="text-align: center">60 degrees<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"> <tr><td style="text-align: center">60 degrees<td style="text-align: center">Prediction<td style="text-align: center">Failure: 0.4707 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.5283 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.5478 <img data-src="/assets/images/L3D/a5/output/Q3/rotate/60/seg/failure_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"> </table></div><p>From the observations above, we see that the model segments most portions of the rotated objects well when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), we see a massive accuracy drop. This happens because the model wasn’t trained with any rotational variations and so the learned features become orientation-dependent. The rotations make it difficult for the network to correctly differentiate between segments.</p><h3 id="32-different-number-of-point-points-per-object"><span class="mr-2">3.2 Different Number of Point Points per Object</span><a href="#32-different-number-of-point-points-per-object" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="321-classification-model"><span class="mr-2">3.2.1 Classification Model</span><a href="#321-classification-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Number of Points<th style="text-align: center">Accuracy<th style="text-align: center">Chairs<th style="text-align: center">Vases<th style="text-align: center">Lamps<tbody><tr><td style="text-align: center">5000<td style="text-align: center">0.968<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/points/5000/cls/cls_s_num396_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">1000<td style="text-align: center">0.9695<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/points/1000/cls/cls_s_num396_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">500<td style="text-align: center">0.9622<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/points/500/cls/cls_s_num396_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/500/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/500/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">100<td style="text-align: center">0.882<td style="text-align: center">Successful<img data-src="/assets/images/L3D/a5/output/Q3/points/100/cls/cls_s_num396_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/100/cls/cls_s_num675_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Successful <img data-src="/assets/images/L3D/a5/output/Q3/points/100/cls/cls_s_num929_2_2.gif" alt="image" data-proofer-ignore></table></div><p>We see that the classification model performs well even with fewer points. The drop in accuracy when the number of points is reduced to 100 or below is expected because fewer points reduce geometric detail, which makes it harder to distinguish specific features. Still, even at 100 points, the outline structure is preserved, allowing the model to classify successfully in many cases.</p><h4 id="322-segmentation-model"><span class="mr-2">3.2.2 Segmentation Model</span><a href="#322-segmentation-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Number of Points<th style="text-align: center">Model<th style="text-align: center">X<th style="text-align: center">X<th style="text-align: center">X<th style="text-align: center">X<tbody><tr><td style="text-align: center">5000<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">5000<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.9448 <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.992 <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.9658 <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.5366 <img data-src="/assets/images/L3D/a5/output/Q3/points/5000/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">1000<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">1000<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.952 <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.988 <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.904 <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.515 <img data-src="/assets/images/L3D/a5/output/Q3/points/1000/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">500<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">500<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.934 <img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.998 <img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.926 <img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.484 <img data-src="/assets/images/L3D/a5/output/Q3/points/500/seg/failure_pred_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">100<td style="text-align: center">Ground Truth<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_gt_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_gt_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_gt_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/failure_gt_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">100<td style="text-align: center">Prediction<td style="text-align: center">Success: 0.92 <img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_pred_0.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.96 <img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_pred_1.gif" alt="image" data-proofer-ignore><td style="text-align: center">Success: 0.91 <img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/success_pred_2.gif" alt="image" data-proofer-ignore><td style="text-align: center">Failure: 0.58 <img data-src="/assets/images/L3D/a5/output/Q3/points/100/seg/failure_pred_0.gif" alt="image" data-proofer-ignore></table></div><p>We see that the segmentation model performs well even with fewer points. While the model performs well even with 500 points, the performance becomes more unstable at 100 points, especially in complex or ambiguous regions. This is because fewer points reduce the structural detail, making it harder for the model to distinguish fine boundaries and specific object features like thin legs and armrests.</p><h2 id="4-bonus-question---locality"><span class="mr-2">4. Bonus Question - Locality</span><a href="#4-bonus-question---locality" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I implemented a general Transformer framework for both the classification model and the segmentation model.</p><p>The classification model predicts a single label for an entire point cloud. Each point’s 3D coordinates are passed through two linear layers: one to embed the input features, and another to learn positional information. These two embeddings are added together and fed into a standard Transformer Encoder. After processing, I apply max pooling across all points to extract a global feature vector. This is then passed through fully connected layers with batch normalization, ReLU activation, and dropout. The log softmax over the class scores is then returned.</p><p>The segmentation model /assets/images/L3D/a5/outputs a class label for each point. I embed the 3D points and add positional information before passing them through a Transformer Encoder. Instead of pooling the features, I use 1D convolution layers to generate per-point predictions. These /assets/images/L3D/a5/outputs are also passed through a log softmax to get per-point class probabilities.</p><h3 id="41-classification-model-using-transformers"><span class="mr-2">4.1 Classification Model using Transformers</span><a href="#41-classification-model-using-transformers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Run:</p><div class="language-zsh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python train.py <span class="nt">--task</span> cls

python eval_cls.py <span class="nt">--load_checkpoint</span> best_model <span class="nt">--</span>/assets/images/L3D/a5/output_dir <span class="s2">".//assets/images/L3D/a5/output/Q4/cls"</span>
</pre></table></code></div></div><p>The model was trained for 100 epochs, and the best model was saved at epoch 18.</p><p>Train loss = 30.6177</p><p>Test accuracy of best model = 0.9454</p><p>Visualization of a few random test point clouds and their predicted classes using Transformers:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Correct Prediction<th style="text-align: center">Chairs<th style="text-align: center">Vases<th style="text-align: center">Lamps<tbody><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num349_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num676_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num835_2_2.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num6_0_0.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num653_1_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_s_num771_2_2.gif" alt="image" data-proofer-ignore></table></div><p>Visualization of a failure prediction for each class using Transformers:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Correct Label<th style="text-align: center">Vase<th style="text-align: center">Lamp<th style="text-align: center">Lamp<tbody><tr><td style="text-align: center">Point Cloud<td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_f_num699_1_2.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_f_num777_2_1.gif" alt="image" data-proofer-ignore><td style="text-align: center"><img data-src="/assets/images/L3D/a5/output/Q4/cls/cls_f_num806_2_0.gif" alt="image" data-proofer-ignore><tr><td style="text-align: center">Incorrect Prediction<td style="text-align: center">Lamp<td style="text-align: center">Vase<td style="text-align: center">Chair</table></div><p>The model was run only for 100 epochs because it was taking a lot of time. I also did not have the time to run the segmentation Transformer model. However, there were no errors in starting the training and it only had to be left running for ~8 hours on my laptop.</p><p>Moreover, the Transformer model gave a very high accuracy of 94% for just training for 100 epochs. The classification model without Transformer gave an accuracy of 97% for training for 250 epochs.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/cmu-mrsd/'>CMU MRSD</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/computer-vision/" class="post-tag no-text-decoration" >computer vision</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep learning</a> <a href="/tags/gaussian-splatting/" class="post-tag no-text-decoration" >gaussian splatting</a> <a href="/tags/nerf/" class="post-tag no-text-decoration" >nerf</a> <a href="/tags/multi-view-geometry/" class="post-tag no-text-decoration" >multi-view geometry</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Learning+for+3D+Vision+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Flearning-3d-vision-25%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Learning+for+3D+Vision+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Flearning-3d-vision-25%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Flearning-3d-vision-25%2F&text=Learning+for+3D+Vision+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Flearning-3d-vision-25%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/cmu-blog/">Coursework at CMU</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/learning-3d-vision-25/">Learning for 3D Vision</a><li><a href="/posts/robot-autonomy-25/">Robot Autonomy</a><li><a href="/posts/intro-robotics-business-25/">Introduction to Robotics Business</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/simulation/">simulation</a> <a class="post-tag" href="/tags/matlab/">matlab</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/computer-vision-24/"><div class="card-body"> <em class="small" data-ts="1733414400" data-df="ll" > Dec 5, 2024 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introduction to Computer Vision</h3><div class="text-muted small"><p> Image Processing and Representation Your browser does not support PDFs. Please download the report here. Cameras and Image Formation Your browser does not support PDFs. Please downloa...</p></div></div></a></div><div class="card"> <a href="/posts/traffic-signs-recognition/"><div class="card-body"> <em class="small" data-ts="1660233600" data-df="ll" > Aug 11, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Recognizing Traffic Signs using CNNs</h3><div class="text-muted small"><p> Introduction The following project shows the implementation of a simple convolutional neural network (CNN). The model will be able to identify which signal it is when presented with a colour ima...</p></div></div></a></div><div class="card"> <a href="/posts/transnomous/"><div class="card-body"> <em class="small" data-ts="1660233600" data-df="ll" > Aug 11, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Autonomous Ground Vehicle</h3><div class="text-muted small"><p> To be Updated</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/intro-robotics-business-25/" class="btn btn-outline-primary" prompt="Older"><p>Introduction to Robotics Business</p></a> <a href="/posts/robot-autonomy-25/" class="btn btn-outline-primary" prompt="Newer"><p>Robot Autonomy</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/simulation/">simulation</a> <a class="post-tag" href="/tags/matlab/">matlab</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
