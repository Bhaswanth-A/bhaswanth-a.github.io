<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Deep Learning - Perceptrons" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="Neural Networks" /><meta property="og:description" content="Neural Networks" /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/deep-learning-perceptrons/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/deep-learning-perceptrons/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/perceptron2.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-06-09T12:00:00-04:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/perceptron2.png" /><meta property="twitter:title" content="Deep Learning - Perceptrons" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2025-11-05T10:32:49-05:00","datePublished":"2025-06-09T12:00:00-04:00","description":"Neural Networks","headline":"Deep Learning - Perceptrons","image":"https://bhaswanth-a.github.io//assets/images/Thumbnail/perceptron2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/deep-learning-perceptrons/"},"url":"https://bhaswanth-a.github.io//posts/deep-learning-perceptrons/"}</script><title>Deep Learning - Perceptrons | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Perception | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Deep Learning - Perceptrons</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Deep Learning - Perceptrons</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1749484800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 9, 2025 </em> </span> <span> Updated <em class="" data-ts="1762356769" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 5, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4820 words"> <em>26 min</em> read</span></div></div></div><div class="post-content"><h1 id="neural-networks">Neural Networks</h1><p>Depth - length of longest path from source to sink Layer - Set of all neurons which are all at the same depth with respect to the source</p><h2 id="gradient"><span class="mr-2">Gradient</span><a href="#gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>For a scalar function $f(x)$ with a multivariate input $x$:</p>\[df\left( x\right) =\nabla _{x}f\left( x\right) dx\] \[\nabla _{x}f\left( x\right) =\left[ \dfrac{\partial f}{\partial x_{1}}\dfrac{\partial f}{\partial x_{2}}\ldots \dfrac{\partial f}{\partial x_{n}}\right]\]<p>Because it is a dot product, for an increment $dX$ of any given length, $df$ is maximum if the increment $dX$ is aligned with the gradient direction $\nabla _{x}f\left( x\right)^T$. So the gradient is the direction of steepest ascent.</p><p>So if you want a function to decrease, your increment should be in the direction of $-\nabla _{x}f\left( x\right)^T$.</p><p>To find a maximum move in the direction of gradient:</p>\[x^{k+1} = x^k + \eta^k \nabla_x f(x^k)^T \\[1em]\]<p>To find a minimum move in the direction of gradient:</p>\[x^{k+1} = x^k - \eta^k \nabla_x f(x^k)^T \\[1em]\]<p>There are many solutions to choosing step size $\eta^k$.</p><h2 id="hessian"><span class="mr-2">Hessian</span><a href="#hessian" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>\[\nabla^2_{x} f(x_1, \ldots, x_n) = \begin{bmatrix}\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}\end{bmatrix}\]<h2 id="network"><span class="mr-2">Network</span><a href="#network" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>A continuous activation function applied to an affine function of the inputs</p>\[y = f\left( \sum_i w_i x_i + b \right) \\[1em] y = f(x_1, x_2, \ldots, x_N; W)\]<h3 id="activation-functions-and-derivatives"><span class="mr-2">Activation Functions and Derivatives</span><a href="#activation-functions-and-derivatives" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Sigmoid:</p>\[f(z) = \frac{1}{1 + \exp(-z)} \\f'(z) = f(z)(1 - f(z)) \\[1.5em]\]<p>Tanh:</p>\[f(z) = \tanh(z) \\f'(z) = 1 - f^2(z)\]<p>ReLU:</p>\[f(z) = \begin{cases}z, &amp; z \geq 0 \\0, &amp; z &lt; 0\end{cases} \\f'(z) = \begin{cases}1, &amp; z \geq 0 \\0, &amp; z &lt; 0\end{cases} \]<p>Softplus:</p>\[f(z) = \log(1 + \exp(z)) \\f'(z) = \frac{1}{1 + \exp(-z)}\]<p>Softmax:</p>\[f(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \\ f'(z_i) = \begin{cases} f(z_i)(1 - f(z_i)), &amp; \text{if } i = j \\ - f(z_i) f(z_j), &amp; \text{if } i \ne j \end{cases}\]<h2 id="training-by-back-propagation"><span class="mr-2">Training by Back Propagation</span><a href="#training-by-back-propagation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image.png" alt="image.png" data-proofer-ignore></p><h3 id="forward-pass"><span class="mr-2">Forward Pass</span><a href="#forward-pass" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%201.png" alt="image.png" data-proofer-ignore></p><p>Setting $y_i^{(0)} = x_i$ and $w_{0j}^{(k)} = b_j^{(k)}$. Let the bias equal $1$ for simplicity.</p><p>For layer 1,</p>\[z_j^{(1)} = \sum_i w_{ij}^{(1)}y_i^{(0)} \\ y_j^{(1)} = f_1(z_j^{(1)})\]<p>For layer 2,</p>\[z_j^{(2)} = \sum_i w_{ij}^{(2)}y_i^{(1)} \\ y_j^{(2)} = f_2(z_j^{(2)})\]<p>Similarly,</p>\[z_j^{(N)} = \sum_i w_{ij}^{(N)}y_i^{(N-1)} \\ y_j^{(N)} = f_N(z_j^{(N)})\]<h3 id="backward-pass"><span class="mr-2">Backward Pass</span><a href="#backward-pass" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Step 1: Initialize Gradient at Output Layer</strong></p><p>We start by computing the gradient of the loss w.r.t. the network output:</p>\[\frac{\partial Div}{\partial y^{(N)}_i} = \frac{\partial Div(Y, d)}{\partial y_i}\]<p>Then, propagate this to the pre-activation output $z^{(N)}$:</p>\[\frac{\partial Div}{\partial z^{(N)}_i} = \frac{\partial y_i^{(N)}}{\partial z_i^{(N)}} \cdot \frac{\partial Div}{\partial y^{(N)}_i} =f_N'\left(z^{(N)}_i\right) \cdot \frac{\partial Div}{\partial y^{(N)}_i}\]<p>In case of a vector activation function, such as the softmax function, $y_i^{(N)}$ is influenced by every $z_i^{(N)}$:</p>\[\frac{\partial Div}{\partial z_i^{(N)}} = \sum_j \frac{\partial y_j^{(N)}}{\partial z_i^{(N)}} \cdot \frac{\partial Div}{\partial y_j^{(N)}}\]<p><strong>Step 2: Backpropagation Through Layers</strong></p><p>Loop from layers $k = (N-1) \rightarrow 0$:</p><p>For each layer $k$ and for each neuron $i$ in that layer:</p><p>Compute gradient of loss w.r.t. activation:</p>\[\frac{\partial Div}{\partial y^{(k)}_i} = \sum_j w_{ij}^{(k+1)} \cdot \frac{\partial Div}{\partial z^{(k+1)}_j}\]<p>Chain through activation function:</p>\[\frac{\partial Div}{\partial z^{(k)}_i} = \frac{\partial y_i^{(k)}}{\partial z_i^{(k)}} \cdot \frac{\partial Div}{\partial y^{(k)}_i} = f_k'\left(z^{(k)}_i\right) \cdot \frac{\partial Div}{\partial y^{(k)}_i}\]<p><strong>Step 3: Gradient w.r.t. Weights</strong></p><p>For each weight connecting neuron $i$ in layer $k$ to neuron $j$ in layer $k$:</p>\[\frac{\partial Div}{\partial w_{ij}^{(k)}} = y^{(k-1)}_i \cdot \frac{\partial Div}{\partial z^{(k)}_j}\]<p><strong>Step 4: Updating Weights</strong></p><p>Actual loss is the sum of the divergence over all training instances:</p>\[Loss = \frac{1}{|\{X\}|}\sum_X Div(Y(X), d(X))\]<p>Actual gradient is the average of the derivatives computed for each training instance:</p>\[\nabla_W Loss = \frac{1}{|\{X\}|}\sum_X \nabla_W Div(Y(X), d(X))\] \[W \leftarrow W - \eta \nabla_W Loss^T\]<h3 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%202.png" alt="image.png" data-proofer-ignore></p><h3 id="vector-formulation"><span class="mr-2">Vector Formulation</span><a href="#vector-formulation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%203.png" alt="image.png" data-proofer-ignore></p>\[\mathbf{z}_k = \begin{bmatrix}z^{(k)}_1 \\z^{(k)}_2 \\\vdots \\z^{(k)}_{D_k}\end{bmatrix}\qquad\mathbf{y}_k = \begin{bmatrix}y^{(k)}_1 \\y^{(k)}_2 \\\vdots \\y^{(k)}_{D_k}\end{bmatrix}\] \[\mathbf{W}_k =\begin{bmatrix}w^{(k)}_{11} &amp; w^{(k)}_{21} &amp; \cdots &amp; w^{(k)}_{D_{k-1}1} \\w^{(k)}_{12} &amp; w^{(k)}_{22} &amp; \cdots &amp; w^{(k)}_{D_{k-1}2} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\w^{(k)}_{1D_k} &amp; w^{(k)}_{2D_k} &amp; \cdots &amp; w^{(k)}_{D_{k-1}D_k}\end{bmatrix}\qquad\mathbf{b}_k = \begin{bmatrix}b^{(k)}_1 \\b^{(k)}_2 \\\vdots \\b^{(k)}_{D_k}\end{bmatrix}\] \[\mathbf{z_k} = \mathbf{W_k y_{k-1} + b_k} \\\] \[\mathbf{y_k} = \mathbf{f_k (z_k)}\]<p><strong>Setup:</strong></p><ul><li>Let $\mathbf{y_n = Y}$, the network output<li>Let $\mathbf{y_0 = X}$, the input<li>Initialize:</ul>\[\nabla_{\mathbf{y}_N} Div = \nabla_{\mathbf{Y}} Div\]<ul><li><strong>For each layer $k = N \rightarrow 1$:</strong><ul><li><strong>Compute the Jacobian of activation:</strong></ul>\[J_{\mathbf{y}_k}(\mathbf{z}_k) = \frac{\partial \mathbf{y}_k}{\partial \mathbf{z}_k}\]<ul><li>This is a matrix of partial derivatives:</ul>\[J_{\mathbf{y}}(\mathbf{z}) =\begin{bmatrix}\frac{\partial y_1}{\partial z_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial z_D} \\\vdots &amp; \ddots &amp; \vdots \\\frac{\partial y_M}{\partial z_1} &amp; \cdots &amp; \frac{\partial y_M}{\partial z_D}\end{bmatrix}\]<ul><li><strong>Backward recursion step:</strong></ul>\[\nabla_{\mathbf{z}_k} Div = \nabla_{\mathbf{y}_k} Div \cdot J_{\mathbf{y}_k}(\mathbf{z}_k)\] \[\nabla_{\mathbf{y}_{k-1}} Div = \nabla_{\mathbf{z}_k} Div \cdot \mathbf{W}_k\]<li><p><strong>Gradient Computation for all $k$:</strong></p>\[\nabla_{\mathbf{W}_k} Div = \mathbf{y}_{k-1} \cdot \nabla_{\mathbf{z}_k} Div\] \[\nabla_{\mathbf{b}_k} Div = \nabla_{\mathbf{z}_k} Div\]</ul><h3 id="summary-1"><span class="mr-2">Summary</span><a href="#summary-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%204.png" alt="image.png" data-proofer-ignore></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre>Algorithm: Batch Gradient Descent with Backpropagation

Initialize all weights and biases: W1, b1, W2, b2, ..., WN, bN
Loss ← 0

For all k:
    ∇Wk Loss ← 0
    ∇bk Loss ← 0

For t = 1 to T:     # Loop through training examples
    Compute output Y(Xt)
    Compute divergence Div(Yt, dt)

    For all k:      # Backward pass
        ∇Wk Div(Yt, dt), ∇bk Div(Yt, dt)
        ∇Wk Loss ← ∇Wk Loss + ∇Wk Div(Yt, dt)
        ∇bk Loss ← ∇bk Loss + ∇bk Div(Yt, dt)

For all k:          # Gradient Descent Update
    Wk ← Wk - (η / T) * (∇Wk Loss)^T
    bk ← bk - (η / T) * (∇bk Loss)^T

Repeat until Loss has converged
</pre></table></code></div></div><h3 id="important-points"><span class="mr-2">Important Points</span><a href="#important-points" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Backpropagation will often not find a separating solution even though the solution is within the class of functions learnable by the network, because the separable solution is not a feasible optimum for the loss function.<ul><li>It is minimally changed by new training instances — doesn’t swing wildly in response to small changes to the input<li>It prefers consistency over perfection, due to which it works better even if there are outliers<li>It is a low-variance estimator, at the potential cost of bias</ul><li>Minimizing the differentiable loss function does not imply minimizing the classification error.</ul><h2 id="convergence-of-gradient-descent"><span class="mr-2">Convergence of Gradient Descent</span><a href="#convergence-of-gradient-descent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="covergence-rate"><span class="mr-2">Covergence Rate</span><a href="#covergence-rate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>It measures <strong>how quickly</strong> an iterative optimization algorithm approaches the solution.</p>\[R = \left| \frac{f(x^{(k+1)}) - f(x^*)}{f(x^{(k)}) - f(x^*)} \right|\]<p>Where:</p><ul><li>$x^{(k)}$: point at iteration $k$<li>$x^*$: optimal point (solution)<li>$f(x)$: objective function</ul><p>If $R&lt;1$, that means that the function value is getting closer to the optimum and is hence converging. The smaller the $R$, the faster the convergence.</p><p>If $R$ is a constant or upper-bounded, then the algorithm has linear convergence. This means that the difference between the function value and the optimum shrinks exponentially with iterations.</p>\[|f(x^{(k)}) - f(x^*)| \leq R^k \cdot |f(x^{(0)}) - f(x^*)|\]<h3 id="convergence-for-quadratic-surfaces"><span class="mr-2">Convergence for Quadratic Surfaces</span><a href="#convergence-for-quadratic-surfaces" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>What is the optimal step size $\eta$ to reach the minimum value of the error fastest?</strong></p><ul><li>Consider a general quadratic function:</ul>\[E(w) = \frac{1}{2}aw^2 + bw + c\]<ul><li>Gradient descent update:</ul>\[w^{(k+1)} = w^{(k)} - \eta \frac{dE(w^{(k)})}{dw}\]<ul><li>Taylor expansion of $E(w)$ around $w^{(k)}$:</ul>\[E(w) \approx E(w^{(k)}) + E'(w^{(k)})(w - w^{(k)}) + \frac{1}{2} E''(w^{(k)})(w - w^{(k)})^2\]<ul><li>Newton’s method gives:</ul>\[w_{\text{min}} = w^{(k)} - \left(E''(w^{(k)})\right)^{-1} E'(w^{(k)})\]<ul><li>Optimal learning rate:</ul>\[\eta_{\text{opt}} = \left(E''(w^{(k)})\right)^{-1} = a^{-1}\]<p><strong>Effect of step size $\eta$:</strong></p><ul><li>$\eta &lt; \eta_{opt} \rightarrow$ monotonic convergence<li>$\eta = \eta_{opt} \rightarrow$ fast convergence in one step<li>$\eta_{opt}&lt;\eta &lt; 2\eta_{opt} \rightarrow$ oscillating convergence<li>$\eta \geq 2\eta_{opt} \rightarrow$ divergence</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%205.png" alt="image.png" data-proofer-ignore></p><h3 id="convergence-for-multivariate-quadratic-functions"><span class="mr-2">Convergence for Multivariate Quadratic Functions</span><a href="#convergence-for-multivariate-quadratic-functions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>General form of a quadratic convex function:</ul>\[\mathbf{w} = [w_1, w_2, ..., w_N] \\ E(\mathbf{w}) = \frac{1}{2} \mathbf{w}^T A \mathbf{w} + \mathbf{w}^T \mathbf{b} + c\]<ul><li>If $A$ is diagonal:</ul>\[E(\mathbf{w}) = \frac{1}{2} \sum_i (a_{ii} w_i^2 + b_i w_i) + c\]<p>This means the cost function is a sum of independent univariate quadratics. Each direction $w_i$ is uncoupled.</p><p><strong>Equal-Value Contours:</strong></p><ul><li>For diagonal $A$, the contours are ellipses aligned with coordinate axes.<li>Each coordinate’s behavior is independent of the others.</ul><p><strong>Optimal Step Size:</strong></p><p>For each dimension $i$:</p>\[\eta_{i,\text{opt}} = \frac{1}{a_{ii}} = \left( \frac{\partial^2 E}{\partial w_i^2} \right)^{-1}\]<p>Each coordinate has a different optimal learning rate.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%206.png" alt="image.png" data-proofer-ignore></p><p><strong>Problem with Vector Update Rule:</strong></p><p>Conventional update applies the same step size $\eta$ to all coordinates. An issue with this is that one direction might converge optimally while another direction might diverge due to too large a step.</p>\[\mathbf{w}^{(k+1)} = \mathbf{w}^{(k)} - \eta \nabla_{\mathbf{w}} E\]<p><strong>Safe Learning Rate Rule to Avoid Divergence:</strong></p>\[\eta &lt; 2 \cdot \min_i \eta_{i,\text{opt}}\]<p>This guarantees convergence but slows down overall learning.</p><h3 id="generic-convex-functions"><span class="mr-2">Generic Convex Functions</span><a href="#generic-convex-functions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>We can apply Taylor expansion to approximate any smooth convex function:</ul>\[E(\mathbf{w}) \approx E(\mathbf{w}^{(k)}) + \nabla_{\mathbf{w}} E(\mathbf{w}^{(k)})(\mathbf{w} - \mathbf{w}^{(k)})+ \frac{1}{2} (\mathbf{w} - \mathbf{w}^{(k)})^T H_E(\mathbf{w}^{(k)})(\mathbf{w} - \mathbf{w}^{(k)})\]<ul><li>$H_E$ is the Hessian of second derivatives and measures the curvature of loss function<li>Optimal step size is inversely proportional to eigenvalues of the Hessian<ul><li>The eigenvalues give curvature in orthogonal directions<li>For the smoothest convergence, the eigenvalues must all be equal</ul></ul><h3 id="convergence-challenges"><span class="mr-2">Convergence Challenges</span><a href="#convergence-challenges" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>In high dimensions, convergence becomes harder to control<li>Ideally, the step size $\eta$ should work for both:</ul>\[\max_i \eta_{i,\text{opt}}, \quad \min_i \eta_{i,\text{opt}}\]<ul><li>If the following condition number is large, then convergence is slow and unstable.</ul>\[\frac{\max_i \eta_{i,\text{opt}}}{\min_i \eta_{i,\text{opt}}}\]<h2 id="decaying-learning-rate"><span class="mr-2">Decaying Learning Rate</span><a href="#decaying-learning-rate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The loss surface has many saddle points and gradient descent can stagnate on saddle points</p><ul><li>Start with a large learning rate to explore fast<li>Gradually reduce step size to fine-tune near the minimum<li>Prevents overshooting and bouncing around the optimum</ul><p><strong>Common Decay Schedules:</strong></p>\[\text{Linear:} \quad \eta_k = \frac{\eta_0}{k + 1}\] \[\text{Quadratic:} \quad \eta_k = \frac{\eta_0}{(k + 1)^2}\] \[\text{Exponential:} \quad \eta_k = \eta_0 \cdot e^{-\beta k}, \quad \beta &gt; 0\]<p>Common Approach for Neural Networks:</p><ul><li>Train with a fixed learning rate until the validation loss stagnates<li>Reduce learning rate:</ul>\[\eta \leftarrow \alpha \eta \quad \text{(e.g., } \alpha = 0.1 \text{)}\]<ul><li>Resume training from the same weights, repeat as needed</ul><h3 id="rprop"><span class="mr-2">RProp</span><a href="#rprop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Resilient Propagation is a first-order optimization algorithm that adjusts step size independently for each parameter<li>It doesn’t rely on the gradient magnitude, rather only on the sign of the gradient<li>It is more robust than vanilla gradient descent<li>Does not need a global learning rate<li>Doesn’t require Hessian or curvature information and doesn’t assume convexity</ul><p>The core idea is that at each step:</p><ul><li>If the gradient sign has not changed → increase step size in the same direction<li>If the gradient sign has flipped → reduce step size and reverse direction (overshoot detected)</ul><p><strong>Algorithm:</strong></p><ul><li>For each layer $l$, for each parameter $w_{l,i,j}$:</ul>\[\Delta w_{l, i, j} &gt; 0\] \[\text{prevD}(l, i, j) = \frac{d \, \text{Loss}(w_{l, i, j})}{d w_{l, i, j}}\] \[\Delta w_{l, i, j} = \text{sign(prevD}(l, i, j)) \cdot \Delta w_{l, i, j}\]<ul><li>While not converged:<ul><li>Update parameter:</ul>\[w_{l, i, j} = w_{l, i, j} - \Delta w_{l, i, j}\]<ul><li>Recompute gradient:</ul>\[D(l, i, j) = \frac{d \, \text{Loss}(w_{l, i, j})}{d w_{l, i, j}}\]<ul><li><p>Check sign consistency:</p><p>If:</p>\[\text{sign(prevD}(l, i, j)) == \text{sign}(D(l, i, j))\]<p>Then,</p>\[\Delta w_{l, i, j} = \min(\alpha \cdot \Delta w_{l, i, j}, \Delta_{\max})\] \[\text{prevD}(l, i, j) = D(l, i, j)\]<p>Else undo step:</p>\[w_{l, i, j} = w_{l, i, j} + \Delta w_{l, i, j} \quad\] \[\Delta w_{l, i, j} = \max(\beta \cdot \Delta w_{l, i, j}, \Delta_{\min})\]</ul></ul><h3 id="quickprop"><span class="mr-2">QuickProp</span><a href="#quickprop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h3 id="momentum"><span class="mr-2">Momentum</span><a href="#momentum" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h2 id="incremental-stochastic-gradient-descent"><span class="mr-2">Incremental Stochastic Gradient Descent</span><a href="#incremental-stochastic-gradient-descent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="table-wrapper"><table><thead><tr><th><strong>Feature</strong><th><strong>Batch Gradient Descent</strong><th><strong>Incremental Gradient Descent (SGD)</strong><tbody><tr><td>Update Frequency<td>Once per full dataset<td>After each training point<tr><td>Speed per Update<td>Slow<td>Fast<tr><td>Stability<td>High (smooth updates)<td>Noisy but flexible<tr><td>Memory Usage<td>High (needs all data at once)<td>Low (uses one sample at a time)<tr><td>Suitable for Big Data<td>Not ideal<td>Very efficient<tr><td>Escaping Local Minima<td>Less likely<td>More likely (due to randomness/noise)</table></div><p>Batch Gradient Descent computes gradients for all training samples and makes one big update after processing the full batch.</p><p>Incremental Gradient Descent will update one sample at a time without waiting for the full dataset.</p><h3 id="algorithm"><span class="mr-2">Algorithm</span><a href="#algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Given training data $(X_1, d_1), (X_2, d_2), \ldots, (X_T, d_T)$<li>Initialize weights $W_1, W_2, \ldots, W_K \space; \space j=0$<li>Repeat (over multiple epochs)<ul><li>Randomly permute $(X_1, d_1), (X_2, d_2), \ldots, (X_T, d_T)$ $\rightarrow$ <em>stochastic</em><li>For all $t = 1 \rightarrow T:$<ul><li>$j = j + 1$<li>For each layer $k$:<ul><li>Compute Gradient $\nabla_{W_k} \text{Div}(Y_t, d_t)$<li>Update weights</ul>\[W_k = W_k - \eta_j \nabla_{W_k} \text{Div}(Y_t, d_t)^T\]</ul></ul><li>Until loss converged</ul><h3 id="important-points-1"><span class="mr-2">Important Points</span><a href="#important-points-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>If we loop through the samples in the same order, we may get a cyclic behavior and can oscillate. We must go through them randomly to get more convergent behavior. Hence the “stochastic” nature.<li>Since batch gradient descent operates over $T$ training instances to get a single update, while SGD gets $T$ updates for the same computation, the advantage of SGD is more pronounced when the data points are very similar. But as data gets increasingly diverse, the benefits of incremental update decreases, but do not entirely vanish.<li>Risk of chasing the latest input: Since SGD updates the model after every point, it could swing drastically towards the latest input and never converge.<ul><li>To tackle this, we shrink the learning rate with each iteration.<li>Caveat: If learning rate shrinks too much, model becomes unresponsive to new data.<li>$\eta_k$ reduces with $k$ and must satisfy the conditions:<ul><li>Ensure full parameter space is explored:</ul>\[\sum_k \eta_k = \infty\]<ul><li>Ensure steps shrink over time:</ul>\[\sum_k \eta_k^2 &lt; \infty\]<ul><li>The fastest converging series that satisfies both above requirements is:</ul>\[\eta_k \propto \frac{1}{k}\]</ul><li>If the loss is convex, SGD converges to the optimal solution. For non-convex losses, SGD converges to a local minimum.<li>SGD converges faster than batch gradient descent, but arrives at a poorer optima.</ul><h3 id="variance"><span class="mr-2">Variance</span><a href="#variance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Variance refers to how much our estimated loss (or error) fluctuates depending on which samples we use. When we approximate the true expected error using a finite number of training samples (empirical risk), the result is an unbiased estimate, meaning it’s correct on average, but it has variance, which affects how reliable the estimate is.</p><p><strong>Empirical Risk Variance:</strong></p><p>We estimate the expected divergence using:</p>\[\text{Loss}(W) = \frac{1}{N} \sum_{i=1}^{N} \text{div}(f(X_i; W), d_i) \\ \mathbb{E}[\text{Loss}(W)] = \mathbb{E}[\text{div}(f(X; W), g(X))]\]<p>The variance of this estimate is:</p>\[\text{Var}(\text{Loss}) = \frac{1}{N} \, \text{Var}(\text{div})\]<ul><li>More samples (larger $N$) → lower variance → more stable learning.<li>Fewer samples → higher variance → model may learn unstable or misleading patterns.</ul><p><strong>Why Variance Matters:</strong></p><ul><li>High variance means your model could “swing” wildly depending on the exact training data.<li>If you only use one or two samples, the estimated error could drastically change just by replacing one data point. This affects optimization — the model might overcorrect in response to noisy updates.</ul><p><strong>In SGD:</strong></p><p>When using Stochastic Gradient Descent, each update uses only one sample:</p>\[\text{div}(f(X_i; W), d_i)\]<p>This is still an unbiased estimate of the expected error:</p>\[\mathbb{E}[\text{div}(f(X_i; W), d_i)] = \mathbb{E}[\text{div}(f(X; W), g(X))]\]<p>But the variance is:</p>\[\text{Var}_{\text{SGD}} = \text{Var}(\text{div})\]<p>So SGD has $N$ times more variance per update than full batch training.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%207.png" alt="image.png" data-proofer-ignore></p><p>In the above image, we are trying to fit the red line to the blue line. Having more samples makes the estimate more robust to changes in the position of samples (smaller variance). But having very few samples makes the estimate swing wildly with the sample position (high variance).</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%208.png" alt="image.png" data-proofer-ignore></p><h2 id="mini-batch-stochastic-gradient-descent"><span class="mr-2">Mini-Batch Stochastic Gradient Descent</span><a href="#mini-batch-stochastic-gradient-descent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>SGD uses the gradient from only one sample at a time, and is consequently high variance. But it also provides significantly quicker updates than batch gradient descent. So to get the best of both worlds, we use mini-batch incremental updates.</p><h3 id="algorithm-1"><span class="mr-2">Algorithm</span><a href="#algorithm-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Given training data $(X_1, d_1), (X_2, d_2), \ldots, (X_T, d_T)$<li>Initialize weights $W_1, W_2, \ldots, W_K \space; \space j=0$<li>Repeat (over multiple epochs)<ul><li>Randomly permute $(X_1, d_1), (X_2, d_2), \ldots, (X_T, d_T)$ $\rightarrow$ <em>stochastic</em><li>For all $t = 1:b:T$<ul><li>$j = j + 1$<li>For each layer $k$:<ul><li>$\nabla W_k = 0$</ul><li>For $t’ = t:t+b-1$<ul><li>For every layer $k:$<ul><li>Compute Gradient $\nabla_{W_k} \text{Div}(Y_t, d_t)$<li>Cumulative loss for the mini-batch:</ul>\[\nabla W_k = \nabla W_k + \frac{1}{b}\nabla_{W_k} \text{Div}(Y_t, d_t)^T\]</ul><li>Update weights<ul><li>For every layer $k:$</ul></ul>\[W_k = W_k - \eta_j \nabla{W_k}\]</ul><li>Until loss converged</ul><h3 id="important-points-2"><span class="mr-2">Important Points</span><a href="#important-points-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>The mini-batch size is generally set to the largest that your hardware will support (in memory) without compromising overall compute time.<li>Larger minibatches implies less variance and fewer updates per epoch.<li>Simple technique for convergence: fix learning rate until the error plateaus, then reduce learning rate by a fixed factor.<li>Estimates have lower variance than SGD.<li>Convergence rate is theoretically worse than SGD<li><p>But we compensate by being able to perform batch processing</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%209.png" alt="image.png" data-proofer-ignore></p></ul><h2 id="choosing-a-divergence"><span class="mr-2">Choosing a Divergence</span><a href="#choosing-a-divergence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="l2-divergence"><span class="mr-2">L2 Divergence</span><a href="#l2-divergence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Used in regression</p>\[\text{Div} = \frac{1}{2}(y - d)^2 \\ \text{Div} = \frac{1}{2} \sum_i (y_i - d_i)^2\] \[\frac{\partial \mathcal{L}}{\partial z_k} = y_k - d_k\]<h3 id="kl-divergence"><span class="mr-2">KL Divergence</span><a href="#kl-divergence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Used in classification tasks, especially with softmax outputs. It measures how different two probability distributions are. In classification, $Y$ is the predicted distribution and $d$ is the ground truth, usually a one-hot encoding.</p>\[KL(Y, d) = \sum_i d_i \log \frac{d_i}{y_i} \\ KL(Y, d) = \sum_i d_i \log(d_i) - \sum_i d_i \log(y_i)\]<p>For one-hot, $d$ simplifies to</p>\[KL(Y, d) = -\log y_c\]<p>This means that it is minimized when correct class is predicted with the highest probability $y_c \rightarrow 1$.</p><p>Gradient:</p>\[\frac{d}{dy_c} KL(Y, d) = -\frac{1}{y_c}\]<p>For scalar binary classification:</p>\[\text{Div} = -d \log(y) - (1 - d)\log(1 - y)\]<p><strong>Deriving Gradient of KL Divergence with Softmax Output:</strong></p><p>KL Divergence Loss:</p>\[\mathcal{L} = KL(\mathbf{d} \| \mathbf{y}) = \sum_{i=1}^C d_i \log \left( \frac{d_i}{y_i} \right)\]<p>Softmax function:</p>\[y_i = \frac{e^{z_i}}{\sum_{j=1}^C e^{z_j}}\]<p>Gradient of loss w.r.t $z_k$:</p>\[\frac{\partial \mathcal{L}}{\partial z_k} = - \sum_{i=1}^C d_i \cdot \frac{\partial}{\partial z_k} \log y_i\]<p>Derivative of log-softmax:</p>\[\log y_i = z_i - \log \sum_j e^{z_j}\] \[\frac{\partial \log y_i}{\partial z_k} = \frac{\partial z_i}{\partial z_k} - \frac{\partial}{\partial z_k} \log \left( \sum_j e^{z_j} \right)\] \[\frac{\partial z_i}{\partial z_k} = \begin{cases}1 &amp; \text{if } i = k \\0 &amp; \text{if } i \neq k\end{cases}= \delta_{ik}\] \[\frac{\partial}{\partial z_k} \log \left( \sum_j e^{z_j} \right) = \frac{e^{z_k}}{\sum_j e^{z_j}} = y_k\]<p>Therefore,</p>\[\frac{\partial \log y_i}{\partial z_k} = \delta_{ik} - y_k\] \[\frac{\partial \mathcal{L}}{\partial z_k} = - \sum_i d_i \left( \delta_{ik} - y_k \right) \\ = - \left( d_k (1 - y_k) + \sum_{i \ne k} d_i (-y_k) \right) = - \left( d_k - d_k y_k - y_k \sum_{i \ne k} d_i \right)\]<p>Note that,</p>\[\sum_i d_i = 1 \Rightarrow \sum_{i \ne k} d_i = 1 - d_k\]<p>So,</p>\[\frac{\partial \mathcal{L}}{\partial z_k} = - \left( d_k - d_k y_k - y_k (1 - d_k) \right) \\ = y_k - d_k\]<p>Hence,</p>\[\frac{\partial \mathcal{L}}{\partial z_k} = y_k - d_k\]<h3 id="how-to-choose"><span class="mr-2">How to choose?</span><a href="#how-to-choose" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>If we plot the L2 and KL divergences as a function of $y$ (one-dimensional input), we find that both are convex, and L2 appears more bowl-like and nicer compared to KL which appears to be very steep at the extremes and flatten badly near the minimum. This would show that L2 is a better divergence function for classification applications. But why do we still use KL?</p><p>Because now when we plot the divergence as a function of $z$, only the KL is nice and convex.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2010.png" alt="image.png" data-proofer-ignore></p><p>Their plots as a function of weights for a two-dimensional input look like this:</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2011.png" alt="image.png" data-proofer-ignore></p><h2 id="batch-normalization"><span class="mr-2">Batch Normalization</span><a href="#batch-normalization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Training assumes the training data are all similarly distributed, i.e., that minibatches have similar distribution. But in practice, each minibatch may have a different distribution. So there is essentially a covariance shift and sometimes this shift could be large.</p><p>So the solution to this is to move all the minibatches to a standard location. We do this by:</p><ul><li>Shifting all batches to the origin by subtracting each data point in the batch with their mean<li>Normalize each batch by their standard deviation</ul><p>This will move the entire collection to an appropriate location and is called batch normalization.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2012.png" alt="image.png" data-proofer-ignore></p><p>Batch normalization is done during training and can be done independently for each unit.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2013.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2014.png" alt="image.png" data-proofer-ignore></p><p>For a minibatch size $B$, we have the mean $\mu_B$ and standard deviation $\sigma_B^2$,</p>\[\mu_B = \frac{1}{B}\sum_{i=1}^B z_i \\ \sigma_B^2 = \frac{1}{B}\sum_{i=1}^B(z_i - \mu_B)^2\]<p>After batch normalization,</p>\[u_i = \frac{z_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\] \[\hat{z_i} = \gamma u_i + \beta\]<p>where $\gamma, \beta$ are learnable parameters.</p><p>In a normal minibatch gradient descent, we compute the loss of gradients as,</p>\[\text{Loss(minibatch)} = \frac{1}{B}\nabla_{W_k} \text{Div}(Y_t(X_t), d_t(X_t))\]<p>But now with batch normalization, the outputs are also functions of $\mu_B, \sigma_B^2$</p>\[\text{Loss(minibatch)} = \frac{1}{B}\nabla_{W_k} \text{Div}(Y_t(X_t, \mu_B, \sigma_B^2), d_t(X_t))\]<p>Since batch normalization is a vector function applied over all inputs from a minibatch, every $z_i$ affects every $\hat{z_j}$.</p><p>Here’s how we backpropagate,</p>\[\frac{dLoss}{d\hat{z}} = f'(\hat{z})\frac{dLoss}{dy}\] \[\frac{dLoss}{d\gamma} = \frac{d\hat{z}}{d\gamma}\frac{dLoss}{d\hat{z}} = u \frac{dLoss}{d\hat{z}} \\ \frac{dLoss}{d\beta} = \frac{d\hat{z}}{d\beta}\frac{dLoss}{d\hat{z}} = \frac{dLoss}{d\hat{z}}\] \[\frac{dLoss}{du_i} = \frac{d\hat{z_i}}{du_i}\frac{dLoss}{d\hat{z_i}} = \gamma \frac{dLoss}{d\hat{z_i}}\]<p>Now we need to compute $\frac{dLoss}{du_i}$ for every $u_i$.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2015.png" alt="image.png" data-proofer-ignore></p><p>The complete derivative of the mini-batch loss w.r.t. $z_i$</p>\[\frac{dLoss}{dz_i} = \sum_j\frac{dLoss}{du_j}\frac{du_j}{dz_i}\]<p>The first part of this has been computed above. We now need to compute $\frac{du_i}{dz_i}$ for every pair $i,j$.</p><p>First when $i=j$,</p>\[\frac{du_i}{dz_i} = \frac{\partial u_i}{\partial z_i} + \frac{\partial u_i}{\partial \mu_B}\frac{\partial \mu_B}{\partial z_i} + \frac{\partial u_i}{\partial \sigma_B^2}\frac{d \sigma_B^2}{d z_i}\]<p>First term:</p>\[\frac{\partial u_i}{\partial z_i} = \frac{1}{\sqrt{\sigma_B^2 + \epsilon}}\]<p>Second term:</p>\[\frac{\partial u_i}{\partial \mu_B} = \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}} \\ \frac{\partial \mu_B}{\partial z_i} = \frac{1}{B}\]<p>Third term:</p>\[\frac{\partial u_i}{\partial \sigma_B^2} = \frac{-(z_i - \mu_B)}{2(\sigma_B^2 + \epsilon)^{3/2}}\] \[\frac{d \sigma_B^2}{dz_i} = \frac{\partial d\sigma_B^2}{\partial z_i} + \frac{\partial \sigma_B^2}{\partial \mu_B}\frac{\partial \mu_B}{\partial z_i}\] \[\frac{\partial \sigma_B^2}{\partial z_i} = \frac{2(z_i - \mu_B)}{B} \\ \frac{\partial \sigma_B^2}{\partial \mu_B} = 0\] \[\frac{\partial u_i}{\partial \sigma_B^2} \cdot \frac{\partial \sigma_B^2}{\partial z_i} = \frac{-(z_i - \mu_B)}{2(\sigma_B^2 + \epsilon)^{3/2}} \cdot \frac{2(z_i - \mu_B)}{B} \\ = \frac{-(z_i - \mu_B)^2}{B(\sigma_B^2 + \epsilon)^{3/2}}\]<p>So, for $i=j$,</p>\[\frac{d u_i}{d z_i} = \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{-1}{B \sqrt{\sigma_B^2 + \epsilon}} + \frac{-(z_i - \mu_B)^2}{B(\sigma_B^2 + \epsilon)^{3/2}}\]<p>Now when $i \neq j$,</p>\[\frac{d u_j}{d z_i} = \frac{\partial u_j}{\partial \mu_B} \frac{\partial \mu_B}{\partial z_i} + \frac{\partial u_j}{\partial \sigma_B^2} \frac{\partial \sigma_B^2}{\partial z_i}\]<p>This is similar to the case when $i=j$ but without the first term. So,</p>\[\frac{d u_j}{d z_i} = \frac{-1}{B \sqrt{\sigma_B^2 + \epsilon}} + \frac{-(z_i - \mu_B)(z_j - \mu_B)}{B(\sigma_B^2 + \epsilon)^{3/2}}\]<p><strong>Full derivative summary:</strong></p>\[\frac{d u_j}{d z_i} =\begin{cases}\frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{-1}{B \sqrt{\sigma_B^2 + \epsilon}} + \frac{-(z_i - \mu_B)^2}{B(\sigma_B^2 + \epsilon)^{3/2}} &amp; \text{if } i = j \\\frac{-1}{B \sqrt{\sigma_B^2 + \epsilon}} + \frac{-(z_i - \mu_B)(z_j - \mu_B)}{B(\sigma_B^2 + \epsilon)^{3/2}} &amp; \text{if } i \ne j\end{cases}\] \[\frac{dLoss}{dz_i} = \sum_j\frac{dLoss}{du_j}\frac{du_j}{dz_i}\] \[\frac{d \mathcal{L}}{d z_i} = \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} \cdot \frac{d \mathcal{L}}{d u_i} + \frac{-1}{B \sqrt{\sigma_B^2 + \epsilon}} \sum_j \frac{d \mathcal{L}}{d u_j} + \frac{-(z_i - \mu_B)}{B(\sigma_B^2 + \epsilon)^{3/2}} \sum_j \frac{d \mathcal{L}}{d u_j} (z_j - \mu_B)\]<p>The rest of the backpropagation continues normally from $\frac{d \mathcal{L}}{d z_i}$</p><h3 id="inference"><span class="mr-2">Inference</span><a href="#inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>On test data, we still do batch normalization. But don’t have minibatches and we perform inference over individual instances. So we will use the average over all training batches, as follows:</p>\[\mu_{BN} = \frac{1} \sum_{batch} \mu_B({batch})\] \[\sigma_{BN}^2 = \frac{B}{(B-1){Nbatches}} \sum_{batch} \sigma_B^2({batch})\]<p>$\mu_B(batch), \sigma_B^2(batch)$ are obtained from the final converged network.</p><h3 id="important-points-3"><span class="mr-2">Important Points</span><a href="#important-points-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Batch normalization may only be applied to some layers or even only selected neurons in a layer<li>Improves both convergence rate and neural network performance<li>Anecdotal evidence suggests that BN eliminates the need for dropout<li>To get maximum benefit from BN, learning rates must be increased and learning rate decay can be faster<li>Also needs better randomization of training data order</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2016.png" alt="image.png" data-proofer-ignore></p><h2 id="regularization"><span class="mr-2">Regularization</span><a href="#regularization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2017.png" alt="image.png" data-proofer-ignore></p><p>For a simple binary classifier, the desired output would be the smooth blue curve. But the perceptron network is also fully capable of learning the purple curve. This is because the perceptrons in the network are individually capable of sharp changes in output.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2018.png" alt="image.png" data-proofer-ignore></p><p>If you consider a sigmoid activation function, the function becomes steeper with increasing magnitude of weights. So the model can fully learn large weights so that the output has steep edges.</p><p>So the solution to preventing this is contrain the weights $w$ to be low, which will force perceptrons to learn slowly and give a smoother output response.</p><p><strong>Regularized training loss:</strong></p>\[L(W_1, W_2, \ldots, W_K) = \frac{1}{T} \sum_t \text{Div}(Y_t, d_t) + \frac{1}{2} \lambda \sum_k \| W_k \|_F^2\]<p>Batch Mode Gradient:</p>\[\nabla_{W_k} L = \frac{1}{T} \sum_t \nabla_{W_k} \text{Div}(Y_t, d_t) + \lambda W_k^T\]<p>Stochastic Gradient Descent:</p>\[\nabla_{W_k} L = \nabla_{W_k} \text{Div}(Y_t, d_t) + \lambda W_k^T\]<p>Mini-batch Gradient:</p>\[\nabla_{W_k} L = \frac{1}{b} \sum_{\tau = t}^{t + b - 1} \nabla_{W_k} \text{Div}(Y_{\tau}, d_{\tau}) + \lambda W_k^T\]<p><strong>Weight update:</strong></p>\[W_k \leftarrow (1 - \lambda) W_k - \eta \frac{1}{b} \sum_{\tau = t}^{t + b - 1} \nabla_{W_k} \text{Div}(Y_{\tau}, d_{\tau})\]<p>$\lambda$ gives a measure of how important it is to prioritize regularization.</p><h3 id="important-points-4"><span class="mr-2">Important Points</span><a href="#important-points-4" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>For a given number of parameters, deeper networks impose more smoothness than shallow ones. This is because each layer works on the already smooth surface output by the previous layer.</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2019.png" alt="image.png" data-proofer-ignore></p><ul><li>Data underspecification can result in overfitted models and must be handled by regularization and more constrained (generally deeper) network architectures</ul><h2 id="dropout"><span class="mr-2">Dropout</span><a href="#dropout" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong>Bagging:</strong></p><ul><li>This is a method that samples training data and trains several different classifiers<li>Then we classify the test instances with the entire ensemble of classifiers, after which we vote across classifiers for final decision<li>It is empirically shown to improve significantly over training a single classifier from combined data</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2020.png" alt="image.png" data-proofer-ignore></p><p>In dropout, during training for each input, for every iteration, we turn of each neuron with a probability $1-\alpha$. In practice, set them to 0 according to the failure of a Bernoulli random number generator with success probability $\alpha$. Backpropagation is effectively performed only over the remaining network.</p><p>So each training instance or epoch, essentially sees a different training network.</p><p><strong>Statistical interpretation:</strong></p><ul><li>For a network with a total $N$ neurons, there are $2^N$ possible sub-networks obtained by randomly turning on and off the neurons.<li>Since $2^N$ is a very large number, dropout samples over all these possible networks.<li>Then by bagging, we learn a network that averages over all possible networks.</ul><p><strong>Another interpretation:</strong></p><ul><li>In a rich and dense network, there is nothing stopping the network from learning a pass-through, i.e., just cloning the input to its output. Because of this, we would essentially lose the neuron and maybe a whole layer.<li>Dropout forces the neurons to learn denser patterns.</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2021.png" alt="image.png" data-proofer-ignore></p><h3 id="algorithm-2"><span class="mr-2">Algorithm</span><a href="#algorithm-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Forward Pass:</strong></p><ul><li>Given inputs $\mathbf{x} = [x_j, \quad j = 1 \ldots D]$<li>Set:<ul><li>$D_0 = D$, is the width of the $0^{th}$ (input) layer<li>$y_j^{(0)} = x_j, \quad j = 1 \ldots D$<li>$y_0^{(k=1\ldots N)} = x_0 = 1$</ul><li>For layers $k = 1 \ldots N$<ul><li>Apply dropout mask: $\text{mask}(k - 1, j) = \text{Bernoulli}(\alpha), \quad j = 1 \ldots D_{k-1}$<li>$\tilde{y}<em>j^{(k-1)} = y_j^{(k-1)} \cdot \text{mask}(k - 1, j), \quad j = 1 \ldots D</em>{k-1}$<li><p>For each neuron $j = 1 \ldots D_k$:</p>\[z_j^{(k)} = \sum_{i=0}^{D_{k-1}} w_{ij}^{(k)} \cdot \tilde{y}_i^{(k-1)} + b_j^{(k)}\] \[y_j^{(k)} = f_k \left( z_j^{(k)} \right)\]</ul><li>Output: $Y = y_j^{(N)}, \quad j = 1 \ldots D_N$</ul><p><strong>Backward Pass:</strong></p><ul><li>Output layer $(k=N)$:</ul>\[\frac{\partial D}{\partial y_i} = \frac{\partial \text{Div}(Y, d)}{\partial y_i^{(N)}}\] \[\frac{\partial \text{Div}}{\partial z_i^{(k)}} = f_k' \left( z_i^{(k)} \right) \cdot \frac{\partial \text{Div}}{\partial y_i^{(k)}}\]<ul><li>For layers $k = (N-1) \ldots 0$:<ul><li><p>For each neuron $i = 1 \ldots D_k$:</p>\[\frac{\partial \text{Div}}{\partial y_i^{(k)}} = \text{mask}(k, i) \cdot \sum_j w_{ij}^{(k+1)} \cdot \frac{\partial \text{Div}}{\partial z_j^{(k+1)}}\] \[\frac{\partial \text{Div}}{\partial z_i^{(k)}} = f_k' \left( z_i^{(k)} \right) \cdot \frac{\partial \text{Div}}{\partial y_i^{(k)}}\] \[\frac{\partial \text{Div}}{\partial w_{ij}^{(k+1)}} = y_i^{(k)} \cdot \frac{\partial \text{Div}}{\partial z_j^{(k+1)}}\]</ul></ul><p><strong>Inference:</strong></p><ul><li>Given inputs $\mathbf{x} = [x_j, \quad j = 1 \ldots D]$<li>Set:<ul><li>$D_0 = D$, is the width of the $0^{th}$ (input) layer<li>$y_j^{(0)} = x_j, \quad j = 1 \ldots D$<li>$y_0^{(k=1\ldots N)} = x_0 = 1$</ul><li>For layers $k = 1 \ldots N$<ul><li><p>For each neuron $j = 1 \ldots D_k$:</p>\[z_j^{(k)} = \sum_{i=0}^{D_{k-1}} w_{ij}^{(k)} \cdot {y}_i^{(k-1)} + b_j^{(k)}\] \[y_j^{(k)} = \alpha f_k \left( z_j^{(k)} \right)\]</ul><li>Output: $Y = y_j^{(N)}, \quad j = 1 \ldots D_N$</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Perceptrons/image%2022.png" alt="image.png" data-proofer-ignore></p><h2 id="exploding--vanishing-gradients"><span class="mr-2">Exploding &amp; Vanishing Gradients</span><a href="#exploding--vanishing-gradients" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>A Multilayer Perceptron (MLP) with multiple hidden layers is essentially a nested composition of functions:</p>\[Y = f_N\left(W_N f_{N-1}\left(W_{N-1} \cdots f_1(W_1 X) \right) \right)\]<p>We define the error or divergence on the output prediction as:</p>\[Div(X) = D\left(f_N\left(W_N f_{N-1}\left(W_{N-1} \cdots f_1(W_1 X) \right) \right)\right)\]<p>To perform backpropagation through this nested structure, we compute the gradient of the divergence with respect to the output of each hidden layer:</p>\[\nabla_{f_k} Div = \nabla D \cdot \nabla f_N \cdot W_N \cdot \nabla f_{N-1} \cdot W_{N-1} \cdots \nabla f_{k+1} \cdot W_{k+1}\]<h3 id="why-deep-networks-are-hard-to-train"><span class="mr-2">Why Deep Networks are Hard to Train</span><a href="#why-deep-networks-are-hard-to-train" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The challenge arises because of repeated multiplication of Jacobian matrices during backpropagation:</p><ul><li>For typical activations like ReLU, sigmoid, tanh:<ul><li>The Jacobian is a diagonal matrix.<li><p>Each diagonal entry is the derivative of the activation, i.e., $f’_k(z_i)$</p>\[\nabla f_k(z) =\begin{bmatrix}f_k'(z_1) &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; f_k'(z_2) &amp; \cdots &amp; 0 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\0 &amp; 0 &amp; \cdots &amp; f_k'(z_n)\end{bmatrix}\]</ul><li>These derivatives are typically $&lt;1$<li>Hence, multiplying by Jacobians shrinks the gradient at each layer<li>So after a few layers, the derivative of the divergence at any time is totally forgotten</ul><p><strong>Effect of weights:</strong></p><p>Each weight matrix $W_n$ performs a linear transformation of the gradient. Specifically:</p><ul><li>It scales the gradient in different directions.<li>The amount of scaling is determined by the singular values (or eigenvalues for symmetric matrices) of the matrix.</ul><p>Let’s say the singular values of a matrix $W_n$ are $\sigma_1, \sigma_2, \ldots, \sigma_m$</p><ul><li>If all $\sigma_i&lt;1 \rightarrow$ gradient will shrink in all directions<li>If any $\sigma_i &gt; 1 \rightarrow$ gradient may grow in that direction</ul><p>If many weight matrices have singular values greater than 1, then the gradients will explode.</p><p>If many weight matrices have singular values lesser than 1, then the gradients will vanish.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog/'>Blog</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/learning/" class="post-tag no-text-decoration" >learning</a> <a href="/tags/nnets/" class="post-tag no-text-decoration" >nnets</a> <a href="/tags/backpropagation/" class="post-tag no-text-decoration" >backpropagation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+Perceptrons+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-perceptrons%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+Perceptrons+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-perceptrons%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-perceptrons%2F&text=Deep+Learning+-+Perceptrons+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-perceptrons%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/intro-to-rl/">Introduction to Reinforcement Learning</a><li><a href="/posts/reinforcement-learning/">Reinforcement Learning</a><li><a href="/posts/deep-rl/">Deep Reinforcement Learning</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/cmu-blog/">Coursework at CMU</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deep-learning-cnn-rnn-lang/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - CNNs, RNNs, & Language Models</h3><div class="text-muted small"><p> In Progress To-Do CNNs — all LSTM, GRU Connectionist Temporal Classification Convolutional Neural Networks https://youtu.be/kYeeB3CNcx8?si=otHbiKRITjJZk9S8 Forward Pass 1 2 3 4 5 6 7 ...</p></div></div></a></div><div class="card"> <a href="/posts/deep-learning-advanced/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - Attention & Transformers</h3><div class="text-muted small"><p> Attention Models Problem with vanilla Seq2Seq Models In the vanilla sequence-to-sequence (Seq2Seq) model with an encoder–decoder setup: The encoder reads the entire input sequence (e.g., I ...</p></div></div></a></div><div class="card"> <a href="/posts/mmml/"><div class="card-body"> <em class="small" data-ts="1768838400" data-df="ll" > Jan 19, 2026 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-Modal Machine Learning</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/robot-autonomy-25/" class="btn btn-outline-primary" prompt="Older"><p>Robot Autonomy</p></a> <a href="/posts/deep-learning-cnn-rnn-lang/" class="btn btn-outline-primary" prompt="Newer"><p>Deep Learning - CNNs, RNNs, & Language Models</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "Bhaswanth-A/bhaswanth-a.github.io", "data-repo-id": "R_kgDOHu5z_w", "data-category": "General", "data-category-id": "DIC_kwDOHu5z_84C0yVx", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
