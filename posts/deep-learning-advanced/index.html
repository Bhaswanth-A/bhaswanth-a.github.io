<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Deep Learning - Attention &amp; Transformers" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="Attention Models" /><meta property="og:description" content="Attention Models" /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/deep-learning-advanced/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/deep-learning-advanced/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/transformer2.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-06-09T12:00:00-04:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/transformer2.png" /><meta property="twitter:title" content="Deep Learning - Attention &amp; Transformers" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2025-11-05T10:32:49-05:00","datePublished":"2025-06-09T12:00:00-04:00","description":"Attention Models","headline":"Deep Learning - Attention &amp; Transformers","image":"https://bhaswanth-a.github.io//assets/images/Thumbnail/transformer2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/deep-learning-advanced/"},"url":"https://bhaswanth-a.github.io//posts/deep-learning-advanced/"}</script><title>Deep Learning - Attention & Transformers | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Perception | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Deep Learning - Attention & Transformers</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Deep Learning - Attention & Transformers</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1749484800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 9, 2025 </em> </span> <span> Updated <em class="" data-ts="1762356769" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 5, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4841 words"> <em>26 min</em> read</span></div></div></div><div class="post-content"><h1 id="attention-models">Attention Models</h1><h2 id="problem-with-vanilla-seq2seq-models"><span class="mr-2">Problem with vanilla Seq2Seq Models</span><a href="#problem-with-vanilla-seq2seq-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image.png" alt="image.png" data-proofer-ignore></p><p>In the vanilla sequence-to-sequence (Seq2Seq) model with an encoder–decoder setup:</p><ul><li>The encoder reads the entire input sequence (e.g., I ate an apple) and compresses all information into a single fixed-length vector (the final hidden state).<li>The decoder then uses this single vector to generate the entire output sequence (e.g., Ich habe einen Apfel gegessen).</ul><p><strong>Why is this a problem?</strong></p><ol><li>Bottleneck at the final hidden state<ul><li>All the input information is forced into the last encoder hidden state (red box in the image above).<li>This creates an information bottleneck, especially for long sentences.</ul><li>Decoder only gets the encoder hidden state once<ul><li>At time step 0, the decoder receives the encoder’s final hidden state.<li>After that, the decoder only relies on its own recurrent updates, carrying forward information from previous steps.</ul><li>Information dilution<ul><li>As decoding progresses, the hidden state is increasingly dominated by the decoder’s own recurrence.<li>The contribution from the encoder’s final hidden state becomes weaker and “diluted.”<li>At later steps, the decoder may lose track of crucial information from the input sequence.</ul></ol><p>Because of this, for long and complex sentences, the translations or predictions degrade in quality. Moreover, important words from the beginning of the sentence may be forgotten when generating later outputs. Fundamentally, the model is assuming that a single compressed summary of the entire input is sufficient — which is not true for natural language, where each output word often depends on a specific part of the input.</p><h2 id="potential-solution-1"><span class="mr-2">Potential Solution 1</span><a href="#potential-solution-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%201.png" alt="image.png" data-proofer-ignore></p><p>A simple solution is to have the same encoder representation feed directly into all decoder states as input. Now the decoder’s recurrence is local, and the encoder signal isn’t washed out as we move forward.</p><p><strong>Why a single encoder vector is still not enough</strong></p><p>Even with that change, we are still squeezing the entire input into one vector. That single vector becomes overloaded, especially for long inputs. In reality, all encoder hidden states carry information, with each one tied to its corresponding input token plus surrounding context. By relying on a single compressed summary, we inevitably lose detail, and some information from earlier tokens has already been attenuated by the time the encoder reaches its final state.</p><p><strong>What the decoder really needs</strong></p><p>Every output token is related to the input directly. Giving the decoder only one fixed encoder vector (even at all steps) misses the more granular relationship between individual outputs and specific parts of the input. The decoder should be able to draw, at each step, on the set of encoder states rather than only on a single summary, so it can use the portions of the input that are most relevant to the current output token.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%202.png" alt="image.png" data-proofer-ignore></p><h2 id="potential-solution-2"><span class="mr-2">Potential Solution 2</span><a href="#potential-solution-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%203.png" alt="image.png" data-proofer-ignore></p><p>A simple approach is to compute the average of all encoder hidden states:</p>\[\text{Average} = \frac{1}{N} \sum_{i=1}^{N} h_i\]<p>This average vector is then given to the decoder at every step. This way, the decoder receives a representation that reflects the entire sequence, not just the final hidden state.</p><p><strong>Limitations of simple averaging</strong></p><p>While better than relying on one state, averaging has a clear drawback:</p><ul><li>It applies the same weight to every encoder state. Every decoder step receives the same averaged vector, regardless of which output word is being generated.<li>In reality, different outputs depend on different parts of the input. For example: “Ich” is most closely related to “I”, while “habe” and “gegessen” are more related to “ate”.</ul><p><strong>Towards Weighted Averages</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%204.png" alt="image.png" data-proofer-ignore></p><p>The solution is to let each decoder step compute its own weighted average of the encoder hidden states.</p>\[c_t = \frac{1}{N} \sum_{i=1}^{N} w_i(t) h_i\]<p>$c_t$ is the context vector at decoding step $t$.</p><p>$w_i(t)$ are the attention weights and are dynamically computed as functions of the decoder state. The expectation is that if this model is well-trained, this will automatically highlight the correct inputs.</p><h2 id="attention-weights"><span class="mr-2">Attention Weights</span><a href="#attention-weights" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="requirements"><span class="mr-2">Requirements</span><a href="#requirements" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The attention weights depend on the decoder’s current needs. At time step $t$, they are computed using the decoder’s state from the previous step $s_{t-1}$:</p>\[w_i(t) = a(h_i, s_{t-1})\]<p>where $a(.)$ is a function (called the alignment model or score function) that measures how well encoder state $h_i$ matches the decoder state $s_{t-1}$.</p><p>For the system to work properly, the attention weights must satisfy two conditions:</p><ol><li>Each weight must be positive<li><p>All weights for step $t$ must sum to $1$, forming a probability distribution</p>\[\sum_{i=1}^{N} w_i(t) = 1\]</ol><p>To meet these requirements, we compute weights in two stages:</p><ol><li><p>Compute a set of raw scores that can be positive or negative</p>\[e_i(t) = g(h_i, s_{t-1})\]<p>where $g$ is the scoring function.</p><li><p>Convert these scores into valid weights using the softmax function</p>\[w_i(t) = \frac{\exp(e_i(t))}{\sum_j \exp(e_j(t))}\]</ol><p>This guarantees that $w_i(t) \ge 0, \space \sum_i w_i(t) = 1.$</p><p><strong>Options for scoring function:</strong></p><ol><li>Dot Product Attention</ol>\[g(h_i, s_{t-1}) = h_i^T s_{t-1}\]<ol><li><p>General Attention</p>\[g(h_i, s_{t-1}) = h_i^T W_g s_{t-1}\]<p>$W_g$ needs to be learned.</p><li><p>Concat (Additive) Attention</p>\[g(h_i, s_{t-1}) = v_g^T \tanh ( W_g \begin{bmatrix} h_i \\ s_{t-1}\end{bmatrix} \big)\]<p>$v_g, W_g$ need to be learned.</p><li><p>MLP-based Attention</p>\[g(h_i, s_{t-1}) = \text{MLP}([h_i, s_{t-1}])\]<p>$\text{MLP}$ needs to be learned.</p></ol><p>We will be using the general attention scoring function.</p><h3 id="query-key-value"><span class="mr-2">Query-Key-Value</span><a href="#query-key-value" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%205.png" alt="image.png" data-proofer-ignore></p><p>Query–Key–Value (QKV) introduces a more flexible and general framework:</p><ul><li>Each input is represented by a key and a value.<li>Keys represent how an input is searched or matched. Values represent the actual content/information to be passed.<li>Each decoder step generates a query. Queries represent what the decoder is currently looking for.<li>Attention weights are computed as a function of query and keys, and the final context is a weighted sum of values.</ul><p>Keys and values are derived from encoder hidden states:</p>\[k_i = W_kh_i\] \[v_i = W_vh_i\]<p>Query is derived from the decoder state:</p>\[q_t = W_qs_{t-1}\]<p>Attention weights are computed as a function of query and keys:</p>\[e_i(t) = g(k_i,q_t)\] \[w_i(t) = softmax(e_i(t))\]<p>Context is a weighted sum of values:</p>\[c_i(t) = \sum_iw_i(t)v_i\]<p><strong>Pseudocode:</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre># Assuming encoded input is available
# (K,V) = [k_enc[0], k_enc[1], ..., k_enc[T]], [v_enc[0], v_enc[1], ..., v_enc[T]]

t = -1
h_out[-1] = 0   # Initial Decoder hidden state
q[0] = 0        # Initial query 

Y_out[0] = &lt;sos&gt;
do 
	t = t+1
	C = compute_context_with_attention(q[t], K, V)
	y[t],h_out[t],q[t+1] = RNN_decode_step(h_out[t-1], y_out[t-1], C)
	y_out[t] = generate(y[t]) # Random, or greedy
until y_out[t] == &lt;eos&gt;

function compute_context_with_attention(q, K, V)
	# First compute attention
	e = []
	for t = 1:T # Length of input
	e[t] = raw_attention(q, K[t])
	end
	maxe = max(e) # subtract max(e) from everything to prevent underflow
	a[1..T] = exp(e[1..T] - maxe) # Component-wise exponentiation
	suma = sum(a) # Add all elements of a
	a[1..T] = a[1..T]/suma
	
	C = 0
	for t = 1..T
		C += a[t] * V[t]
	end
	
	return C
</pre></table></code></div></div><h3 id="beam-search"><span class="mr-2">Beam Search</span><a href="#beam-search" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Our general goal is to generate the most likely output sequence that ends with <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>. If we simply pick the most likely word at each step (greedy decoding), it can lead to suboptimal sequences. Example: Choosing “Ich → habe → einen → apfel” step-by-step may fail if another slightly less probable choice early on leads to a better overall translation.</p><p>So, instead of committing to just one choice at each step, we retain multiple candidate paths. Each word choice forks the network, creating parallel hypotheses. This allows the model to explore different continuations of the sequence.</p><p>However, keeping all possible continuations would be computationally infeasible. So we prune by keeping only the top K most likely candidate sequences at each time step,</p><p><strong>Process:</strong></p><ul><li><p>At each step, compute the probability of partial sequences:</p>\[P(O_1, O_2, \ldots, O_t \mid I_1, I_2, \ldots, I_N)\]<li>Retain the Top-K scoring sequences (based on the product of probabilities)<li>Continue extending them until <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> is reached</ul><p>Different paths may reach <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> at different lengths. The final output will be the likely complete sequence ending in <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%206.png" alt="image.png" data-proofer-ignore></p><p><strong>Pseudocode:</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre># Assuming encoder output H = hin[1]… hin[T] is available
path = &lt;sos&gt;
beam = {path}
pathscore = [path] = 1
state[path] = h[0] # computed using your favorite method
context[path] = compute_context_with_attention(h[0], H)
do # Step forward
	nextbeam = {}
	nextpathscore = []
	nextstate = {}
	nextcontext = {}
	for path in beam:
		cfin = path[end] 
		hpath = state[path]
		C = context[path]
		y,h = RNN_decode_step(hpath, cfin, C)
		nextC = compute_context_with_attention(h, H)
		for c in Symbolset
			newpath = path + c
			nextstate[newpath] = h
			nextcontext[newpath] = nextC
			nextpathscore[newpath] = pathscore[path]*y[c]
			nextbeam += newpath # Set addition
		end
	end
	beam, pathscore, state, context, bestpath = prune (nextstate, nextpathscore, nextbeam, nextcontext)
until bestpath[end] = &lt;eos&gt;
</pre></table></code></div></div><h3 id="training"><span class="mr-2">Training</span><a href="#training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>In the forward pass, the encoder reads the input sentence and produces a sequence of hidden states that capture contextual meaning of each word.<li>At each step, the decoder looks at its own previous hidden state, the last generated word, and a context vector formed by attention over encoder states. This gives a probability distribution over possible next words in the target language.<li>Once we have the predicted distributions, we compare them with the actual ground truth sequence.<li>We compute the divergence, and this loss is then backpropagated through the decoder, attention module, and encoder, so that every parameter learns to adjust in a way that improves translation accuracy next time.</ul><p>However, if we always feed the predicted word into the next decoder hidden state, the early training predictions will be poor because these is no one-to-one correspondence between the input and the ouput. So instead, since we already know the target or the true next word, we feed that in directly. This is called teacher forcing, and it makes training much easier because the decoder always has the right context, and the model converges faster.</p><p>The problem with pure teacher forcing is that at inference time, the decoder won’t have access to ground truth, and its own mistakes can snowball (a mismatch called exposure bias). So instead, we find the middle ground where sometimes we feed the ground truth, and sometimes feed the predicted word. We could even gradually shift from more teacher forcing early on to less later in training.</p><p>Another trick is sampling from the model’s predicted distribution instead of always taking the most likely word. This makes training closer to inference, because at inference we don’t have ground truth and the model must rely on its own probabilities. But sampling is not naturally differentiable (you can’t backpropagate through the act of choosing one word). To solve this, we use the Gumbel-Softmax trick.</p><p><strong>Gumbel Noise Trick:</strong></p><p>The Gumbel trick allows differentiable sampling.</p><ol><li><p>Given decoder logits $l_i$, we add Gumbel noise $g_i$:</p>\[z_i = l_i + g_i, \quad g_i \sim \text{Gumbel(0,1)}\]<ul><li>This transforms logits into a set of noisy scores.<li>The max of these corresponds to a true categorical sample.</ul><li><p>Instead of taking a hard max, apply a softmax with temperature $\tau$:</p>\[y_i = \frac{\exp(z_i / \tau)}{\sum_j \exp(z_j / \tau)}\]<ul><li>$y$ is now a <strong>continuous vector</strong>, approximating a one-hot sample.<li>As $\tau \rightarrow 0:$ output becomes nearly one-hot<li>As $\tau \rightarrow \inf:$ output becomes uniform<li>High $\tau$ implies soft &amp; exploratory, low $\tau$ implies sharp &amp; close to true sampling</ul></ol><h2 id="multi-head-attention"><span class="mr-2">Multi-Head Attention</span><a href="#multi-head-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Instead of a single attention function, we use multiple sets of projections of keys, queries, and values.</p>\[k_i^l = W_k^l h_i, \quad v_i^l = W_v^l h_i, \quad q_j^l = W_q^l s_{j-1}\]<ul><li>$l$ is the head index<li>Each head has its own learnable $W_k^l, W_v^l, W_q^l$</ul><p>So for each head:</p><ul><li>It computes its own attention scores $e_i^l(t)$<li>Produces its own context vector $c_t^l$</ul><p>After all heads compute their contexts, we concatenate and project them:</p>\[\text{MultiHead}(Q,K,V) = W_o \big[ c_t^1 ; c_t^2 ; \dots ; c_t^h \big]\]<p><strong>Purpose of Multi-Head:</strong></p><ul><li>Each head learns to focus on different aspects of the input.<ul><li>One head might align with word position.<li>Another might capture semantic meaning.<li>Another might handle long-range dependencies.</ul><li>Improves representation power by letting the model look at the same sequence from different subspaces.</ul><h1 id="self-attention">Self-Attention</h1><h2 id="problem-with-recurrent-encoders"><span class="mr-2">Problem with Recurrent Encoders</span><a href="#problem-with-recurrent-encoders" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>In standard encoder-decoder models with RNNs, each hidden state $h_i$ is computed sequentially and contains information from all previous words.<li>When the decoder attends to these $h_i$ vectors, it is implicitly attending to the entire sequence again, not just the specific word.<li>This raises a question: If the decoder is already selecting relevant words through attention, do we really need recurrence in the encoder at all?</ul><p>Instead of using recurrence, we can directly compute embeddings for all words in parallel:</p>\[h_i = f(x_i)\]<p>where $f$ could be a simple MLP applied to the word embedding $x_i$.</p><p>But this removes the context-specificity:</p><ul><li>Example: the word <em>“an”</em> may map to <em>“ein”</em>, <em>“einer”</em>, or <em>“eines”</em> in German depending on context.<li>Purely local embeddings $h_i$ lose this adaptability.</ul><p>So the solution is to use the attention framework itself to introduce contextspecificity in embeddings.</p><h2 id="self-attention-1"><span class="mr-2">Self-Attention</span><a href="#self-attention-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Use the attention mechanism itself to inject context into embeddings.<li>Each word’s representation is updated by attending to all other words in the sequence (including itself).<li>This creates context-sensitive embeddings without recurrence.</ul><h3 id="computing-self-attention"><span class="mr-2">Computing Self-Attention</span><a href="#computing-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%207.png" alt="image.png" data-proofer-ignore></p><ol><li><p><strong>Initial hidden representations</strong></p><p>Each word gets an initial representation:</p>\[h_i = f(x_i)\]<li><p><strong>Compute Queries, Keys, and Values</strong></p><p>For each word embedding $h_i$:</p>\[q_i = W_qh_i, \quad k_i=W_kh_i, \quad v_i = W_vh_i\]<p>where $W_q, W_k,W_v$ are learnable matrices.</p><li><p><strong>Attention scores</strong></p><p>The attention score between word $i$ and word $j$ is a function of query $q_i$ and key $k_j$:</p>\[e_{ij} = q_i^Tk_j\]<li><p><strong>Normalize via Softmax</strong></p>\[w_{ij} = \frac{\exp(e_{ij})}{\sum_{j=1}^N\exp(e_{ij})}\]<p>This gives the attention weight of word $i$ on word $j$.</p>\[w_{i0}, \ldots, w_{iN} = softmax(e_{i0}, \ldots e_{iN})\] \[w_{ij}=attn(q_i, k_{0:N})\]<li><p><strong>Compute New Representation</strong></p><p>The updated embedding for word $i$ is:</p>\[o_i = \sum_{j=1}^Nw_{ij}v_j\]<p>This means each word now carries information from all other words in proportion to attention weights.</p></ol><h2 id="multi-head-self-attention"><span class="mr-2">Multi-Head Self-Attention</span><a href="#multi-head-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%208.png" alt="image.png" data-proofer-ignore></p><p>To capture different aspects of relationships simultaneously, we can also have multiple attention heads.</p><p>For each head $a$, we project hidden state $h_i$ into queries, keys, and values using learnable matrices:</p>\[q_i^a = W_q^a h_i \] \[k_i^a = W_k^a h_i\] \[v_i^a = W_v^a h_i\]<p>Each head has its own $W_q^a, W_k^a, W_v^a.$</p><p>For each head $a$, compute attention weights and outputs:</p>\[w_{ij}^a = \text{attn}(q_i^a, k_{0:N}^a)\] \[o_i^a = \sum_j w_{ij}^a v_j^a\]<p>Each head produces its own output representation $o_i^a$, and different heads focus on different aspects of the input. Once all heads compute their outputs, we concatenate them:</p>\[o_i = [o_i^1;o_i^2;o_i^3;\ldots;o_i^H]\]<p>where $H$ is the number of heads.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%209.png" alt="image.png" data-proofer-ignore></p><p>The concatenated vector is passed through an MLP (feedforward block):</p>\[y_i = \text{MLP}(o_i)\]<p>Typically, this is a linear transformation + nonlinearity (ReLU/GeLU), and helps mix the information across heads.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2010.png" alt="image.png" data-proofer-ignore></p><p>The encoder can include many layers of suck multi-head self-attention blocks, and there is no need for recurrence.</p><h3 id="positional-encoding"><span class="mr-2">Positional Encoding</span><a href="#positional-encoding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Need to know relative position</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2011.png" alt="image.png" data-proofer-ignore></p><p>Attention mechanisms treat the input sequence as a set of tokens without inherent order. Unlike RNNs or CNNs, there is no built-in sense of sequence or distance between words. But in language, relative position matters:</p><ul><li>“dog bites man” ≠ “man bites dog”<li>Words closer together often influence each other more strongly than words far apart.</ul><p>Without positional information, self-attention compares every token with every other token equally — the attention score between tokens does not depend on their relative distance. For example, the influence of “an” on “apple” should be much stronger when they are adjacent, and weaker when separated by many words.</p><p>Thus, we add positional encodings to word embeddings to inject sequence order into the model, giving the network awareness of word positions and distances.</p><p><strong>Properties:</strong></p><p>We define a sequence of vectors $P_t$ for each position $t$. These vectors have the following properties:</p><ol><li><p>Each position has a unique encoding:</p>\[P_{t_1} \neq P_{t_2}, \quad \text{for } \space t_1 \neq t_2\] \[|P_t|^2 = C\]<li><p>The correlation between two position encodings falls off with distance:</p>\[|P_t \cdot P_{t+\tau_1}| &gt; |P_t \cdot P_{t+\tau_2}|, \quad \text{if } |\tau_1| &lt; |\tau_2|\]<li><p>The correlation is stationary, meaning it depends only on the distance, not the absolute positions:</p>\[P_{t_1} \cdot P_{t_1+\tau} = P_{t_2} \cdot P_{t_2+\tau}\]</ol><p><strong>Constructing Positional Encodings:</strong></p><p>The most widely used approach (from the original Transformer paper) is to use sinusoids of different frequencies. For a token at position $t$ and embedding dimension $d$:</p>\[P_t = \begin{bmatrix} \sin(\omega_1 t) \\ \cos(\omega_1 t) \\ \sin(\omega_2 t) \\ \cos(\omega_2 t) \\ \vdots \\ \sin(\omega_{d/2} t) \\ \cos(\omega_{d/2} t) \end{bmatrix}\] \[\omega_l = \frac{1}{10000^{2l/d}}, \quad l = 1, \dots, d/2\]<p>Thus, even and odd dimensions alternate between sine and cosine functions at exponentially decreasing frequencies.</p>\[P_t[2i] = \sin \left( \omega_i t \right), \quad P_t[2i+1] = \cos \left( \omega_i t \right)\]<p>The encoding preserves relative distances. For any position $t$ and offset $\tau$:</p>\[P_{t+\tau} = M_\tau P_t\]<p>where</p>\[M_\tau = \text{diag} \left( \begin{bmatrix} \cos(\omega_l \tau) &amp; \sin(\omega_l \tau) \\ -\sin(\omega_l \tau) &amp; \cos(\omega_l \tau) \end{bmatrix}, \quad l = 1 \dots d/2 \right)\]<p>This means the positional encoding shifts in a predictable way when moving from one position to another, allowing the model to learn shift-invariant (translation-invariant) relationships.</p><p>So now, with positional encoding, we have a mechanism where the attention that we pay to a word also depends on how far away it is.</p><h2 id="masked-self-attention"><span class="mr-2">Masked Self-Attention</span><a href="#masked-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2012.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2013.png" alt="image.png" data-proofer-ignore></p><p>The idea of removing recurrence using self-attention can be extended to the decoder as well. However, in the decoder, generation is sequential. At step $t$, we only know outputs up to $t$ (previous words). So we must prevent looking ahead at future tokens.</p><p>This is done by masking: when computing attention weights, we set $e_{ij} = -\inf$ if $j&gt;i.$ This forces the softmax to ignore future words.</p>\[w_{ij} = \begin{cases} \frac{\exp(e_{ij})}{\sum_{m=1}^i \exp(e_{im})}, &amp; j \leq i \\ 0, &amp; j &gt; i \end{cases}\]<p>And the output is still:</p>\[o_i = \sum_{j=1}^i w_{ij} v_j\]<p>This is called masked self-attention, ensuring that each word prediction depends only on past outputs.</p><p>Just like in the encoder, we stack multiple masked self-attention layers followed by feedforward layers (MLPs).</p><p>A block looks like:</p><ol><li>Masked self-attention<li>Feedforward MLP with nonlinearity (e.g., ReLU)<li>Residual connections + normalization</ol><h3 id="masked-mullti-head-self-attention"><span class="mr-2">Masked Mullti-Head Self-Attention</span><a href="#masked-mullti-head-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2014.png" alt="image.png" data-proofer-ignore></p><p>Instead of a single set of $Q,K,V,$ we use multiple heads, just like we did before:</p>\[q_i^a = W_q^a h_i, \quad k_i^a = W_k^a h_i, \quad v_i^a = W_v^a h_i\]<p>Each head computes its own weighted sum:</p>\[o_i^a = \sum_{j=1}^i w_{ij}^a v_j^a\]<p>The outputs of all heads are concatenated:</p>\[o_i = [o_i^1; o_i^2; \dots; o_i^H]\]<p>This allows different heads to capture different aspects of dependencies (e.g., short-range vs. long-range).</p><p>We can also add positional encoding, like we did for the encoders.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2015.png" alt="image.png" data-proofer-ignore></p><h1 id="transformers">Transformers</h1><h2 id="architecture"><span class="mr-2">Architecture</span><a href="#architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2016.png" alt="image.png" data-proofer-ignore></p><p><strong>Components:</strong></p><ol><li>Tokenization – Convert text into discrete tokens<li>Embedding Layer – Map tokens into continuous vectors<li>Self-Attention (Multi-Headed) – Compute contextual representations<li>Position Encoding – Add word order information<li>Feed-Forward Network (FFN) – Apply non-linear transformations<li>Residual Connections + Layer Normalization – Stabilize and improve training<li>Output Projection Layer – Map to vocabulary logits</ol><h3 id="tokenization"><span class="mr-2">Tokenization</span><a href="#tokenization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Splits text into smaller units (words, subwords, characters).<li>Each token is represented by an index in the vocabulary.</ul><p>Example: The sentence <em>“CMU’s 11785 is the best deep learning course”</em> becomes token IDs <code class="language-plaintext highlighter-rouge">[3, 5, 100, 57, ..., 1]</code>.</p><h3 id="embeddings"><span class="mr-2">Embeddings</span><a href="#embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Converts discrete token IDs into dense vectors using an embedding matrix. In Pytorch, it is the function <code class="language-plaintext highlighter-rouge">nn.Embedding</code>, and is essentially a linear layer.</p><div class="table-wrapper"><table><tbody><tr><td>If vocabulary size $<td>V<td>$, embedding dimension $D$, and sequence length $L$, then:</table></div>\[X ∈ ℝ^{L × |V|}, \quad W ∈ ℝ^{|V| × D}\] \[Y = XW, \quad Y ∈ ℝ^{L × D}\]<p>$X$ is the one-hot vector, $W$ is the weight matrix, and $Y$ is the token embedding.</p><p>So, each token index is mapped to a continuous embedding vector.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2017.png" alt="image.png" data-proofer-ignore></p><h3 id="self-attention-2"><span class="mr-2">Self-Attention</span><a href="#self-attention-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>For input $X \in ℝ^{L×D}$:</p>\[Q = XW_Q, \quad K = XW_K, \quad V = XW_V\]<p>where $W_Q, W_K, W_V \in ℝ^{D×d_k}$.</p><p>Compute similarity between queries and keys:</p>\[e_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}\] \[w_{ij} = softmax(e_{ij})\] \[O_i = \sum_j w_{ij} V_j\]<h3 id="multi-head-attention-1"><span class="mr-2">Multi-Head Attention</span><a href="#multi-head-attention-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Instead of one attention map, use multiple heads.<li>Each head projects Q, K, V differently, capturing diverse relationships.<li>Outputs are concatenated:</ul>\[MHA(Q,K,V) = Concat(head_1, ..., head_h)W_O\]<h3 id="attention-masking"><span class="mr-2">Attention Masking</span><a href="#attention-masking" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2018.png" alt="image.png" data-proofer-ignore></p><ul><li>Bidirectional attention (encoder): Each token attends to all tokens.<li>Causal attention (decoder): Masks future tokens to prevent cheating during training.</ul><h3 id="cross-attention"><span class="mr-2">Cross-Attention</span><a href="#cross-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Used in encoder-decoder models.<li>Decoder queries come from previous outputs, keys and values come from encoder outputs.</ul><p>We do not need to mask in cross-attention because the encoder sequence is fully known from the start.</p><ul><li>The input sentence (source) is fixed and complete.<li>There’s no notion of “future” tokens in the source — every encoder hidden state can be used freely.<li>So when the decoder attends to the encoder, it’s fine to use all encoder tokens.</ul><p>Masking is only needed where there’s autoregressive generation (decoder self-attention), not when attending to a fully known sequence (encoder outputs).</p><h3 id="positional-encoding-1"><span class="mr-2">Positional Encoding</span><a href="#positional-encoding-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Since attention is permutation-invariant, we add positional encodings to capture order.</p>\[p_t^{(i)} = f(t)^{(i)} := \begin{cases} \sin(\omega_k \cdot t), &amp; \text{if } i = 2k \\ \cos(\omega_k \cdot t), &amp; \text{if } i = 2k+1 \end{cases} \quad \text{where} \quad \omega_k = \frac{1}{10000^{2k/d}}\]<p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2019.png" alt="image.png" data-proofer-ignore></p><h3 id="feed-forward-block"><span class="mr-2">Feed-Forward Block</span><a href="#feed-forward-block" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>A two-layer MLP applied to each position independently.</p>\[FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2\]<h3 id="residual-connections--layer-normalization"><span class="mr-2">Residual Connections &amp; Layer Normalization</span><a href="#residual-connections--layer-normalization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Residual connections, also called skip connections, allow each block to learn a refinement over its input instead of learning a transformation from scratch.</p>\[x_{t+1} = x_t + F(x_t)\]<p>where $F(x_t)$ is the function of the block (like Multi-Head Attention or Feed-Forward Network).</p><p>This prevents vanishing gradients and allows information to flow directly across layers.</p><p>Normalization stabilizes training by scaling inputs to have mean 0 and variance 1. In Layer Norm, the normalization is applied across the embedding dimension of each token and each token vector is normalized independently:</p>\[\text{LN}(h) = \frac{h - \mu}{\sigma} \cdot \gamma + \beta\]<p>The position of LayerNorm relative to the residual connection changes training dynamics.</p><ol><li>Post-Norm (original Transformer)<ul><li><p>Apply residual connection first, then normalize:</p>\[x_{t+1} = \text{LN}(x_t + F(x_t))\]<li>Pros: empirically achieves slightly higher performance when tuned well.<li>Cons: harder to train, gradients may explode/vanish for deep models.</ul><li>Pre-Norm (modern variants, e.g., GPT-2, BERT)<ul><li><p>Normalize input before applying the block function:</p>\[x_{t+1} = x_t + F(\text{LN}(x_t))\]<li>Pros: much more stable and easier to train, especially for very deep networks.<li>Cons: may cap performance a bit if not tuned, but usually negligible.</ul></ol><h2 id="improvements-on-transformers"><span class="mr-2">Improvements on Transformers</span><a href="#improvements-on-transformers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="types-of-architectures"><span class="mr-2">Types of Architectures</span><a href="#types-of-architectures" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2020.png" alt="image.png" data-proofer-ignore></p><p><strong>Encoder-Decoder:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2021.png" alt="image.png" data-proofer-ignore></p><ul><li>Same design as in the original Transformer paper.<li>Input (prompt or source text) goes into the encoder.<li>The decoder generates the output sequence step by step, attending both to previous outputs (masked self-attention) and encoder outputs (cross-attention).<li>Used for tasks that map one text to another (translation, summarization, etc.).</ul><p><strong>Encoder-Only (BERT):</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2022.png" alt="image.png" data-proofer-ignore></p><ul><li>Uses only the encoder stack.<li>Training objectives:<ul><li>Masked Language Modeling (MLM): randomly mask tokens and predict them.<li>Next Sentence Prediction (NSP): predict if two sentences follow each other.</ul><li>Produces contextualized representations of tokens that can be fine-tuned for downstream tasks (classification, QA, etc.).</ul><p><strong>Decoder-Only (GPT):</strong></p><ul><li>Uses only the decoder stack with causal masking.<li>Trained with next token prediction (standard language modeling).<li>Naturally suited for text generation tasks.</ul><h3 id="positional-encoding-2"><span class="mr-2">Positional Encoding</span><a href="#positional-encoding-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Absolute Position Encoding:</strong></p>\[PE_{(t, 2i)} = \sin\left(\frac{t}{10000^{2i/d}}\right) \ PE_{(t, 2i+1)} = \cos\left(\frac{t}{10000^{2i/d}}\right)\]<p><strong>Relative Position Encoding:</strong></p><p>Instead of encoding absolute index, encode relative distance between tokens.</p><p>Attention score modification:</p>\[e_{ij} = \frac{q_i \cdot k_j}{\sqrt{d}} + b_{i-j}\]<p>where $b_{i-j}$ is a learnable bias depending on relative distance.</p><p>The advantage of this is that it has better generalization to longer sequences than trained on.</p><p><strong>Rotary Position Encoding (RoPE):</strong></p><ul><li>Standard sinusoidal positional encodings (used in original Transformers) add position information to token embeddings.<li>RoPE instead represents position by <em>rotating embeddings in a 2D plane</em>.<li>This technique is used in modern LLMs such as LLaMA.<li>Advantage: Encodes <em>relative positions</em> naturally and improves generalization to longer contexts.</ul><p>Each query and key vector is multiplied by a complex exponential to encode its position:</p><ul><li>Rotation in 2D space corresponds to multiplying by $e^{i\theta}$<li>Thus, embeddings are rotated depending on their sequence position.</ul><p>For a query token at position $m$ and a key at position $n$:</p>\[f_q(x_m, m) = (W_q x_m) e^{i m \theta}\] \[f_k(x_n, n) = (W_k x_n) e^{i n \theta}\]<p>The dot product (attention score) between them is:</p>\[g(x_m, x_n, m-n) = \text{Re}\Big[ f_q(x_m, m) \cdot f_k(x_n, n)^* \Big]\] \[g(x_m, x_n, m-n) = \text{Re} \left[ (W_q x_m)(W_k x_n)^{*} e^{i (m-n)\theta} \right]\]<p>The exponential factor $e^{i(m-n)\theta}$ encodes the relative distance between positions $m,n$. This allows the model to directly incorporate relative positional information into attention scores.</p><p>Instead of using complex numbers, RoPE can be expressed as a rotation matrix applied to even–odd index pairs of the embedding vector:</p>\[f_{q,k}(x_m, m) = R^d_{\Theta, m} W_{q,k} x_m\] \[R^d_{\Theta, m} = \begin{bmatrix} \cos(m\theta_1) &amp; -\sin(m\theta_1) &amp; &amp; &amp; &amp; \\ \sin(m\theta_1) &amp; \cos(m\theta_1) &amp; &amp; &amp; &amp; \\ &amp; &amp; \cos(m\theta_2) &amp; -\sin(m\theta_2) &amp; &amp; \\ &amp; &amp; \sin(m\theta_2) &amp; \cos(m\theta_2) &amp; &amp; \\ &amp; &amp; &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; &amp; &amp; \cos(m\theta_{d/2}) &amp; -\sin(m\theta_{d/2}) \\ &amp; &amp; &amp; &amp; &amp; \sin(m\theta_{d/2}) &amp; \cos(m\theta_{d/2}) \end{bmatrix}\]<p>Each pair of dimensions undergoes a rotation by an angle proportional to $m\theta_j$.</p><h3 id="efficient-attention-mechanism"><span class="mr-2">Efficient Attention Mechanism</span><a href="#efficient-attention-mechanism" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The vanilla self-attention mechanism has quadratic time and memory complexity with respect to sequence length $L.$</p>\[Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]<p>This requires:</p><ul><li>Time complexity: $O(L^2d)$ FLOPs<li>Memory complexity: $O(L^2)$</ul><p>For long sequences (e.g., 4k–32k tokens), this quadratic cost becomes a major bottleneck.</p><p><strong>Linear Attention:</strong></p><p>Idea: Replace the softmax with a kernel function that allows factorization.</p><p>Standard softmax attention:</p>\[\text{Softmax}(QK^T)V = \frac{\exp(QK^T)}{\sum_{i=1}^L \exp(QK_i^T)}V\]<p>Instead, approximate similarity using a kernel function $\phi(.):$</p>\[\text{sim}(Q,K) = \phi(Q)\phi(K)^T\]<p>Thus attention becomes linearized:</p>\[\frac{\phi(Q)\phi(K)^T}{\sum_{i=1}^L \phi(Q)\phi(K_i)^T}V = \frac{\phi(Q) (\phi(K)^TV)}{\phi(Q)\sum_{i=1}^L\phi(K_i)^T}\]<p>Complexity drops from $O(L^2)$ to $O(Ld’^2)$.</p><p><strong>Flash Attention:</strong></p><p>Standard attention stores the huge $L \times L$ matrix in memory. Flash Attention avoids materializing it. Instead it computes attention block by block, directly in GPU high-bandwidth memory (HBM).</p><p>The trick is to:</p><ul><li>Fuse operations into a single GPU kernel (matrix multiplication + softmax + scaling).<li>Use tiling: split matrices into small blocks that fit into SRAM.<li>Reduces memory reads/writes by orders of magnitude.</ul><p>Issues with softmax:</p><ul><li>Naive Softmax requires 2 loops:<ol><li>Compute normalizer $\sum_je^{x_j}$<li>Divide each exponential</ol><li>Safe Softmax adds a max-subtraction step for numerical stability.</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2023.png" alt="image.png" data-proofer-ignore></p><ul><li>Online Softmax combines steps to reduce passes.</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2024.png" alt="image.png" data-proofer-ignore></p><p>Flash Attention fuses this into one loop:</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2025.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2026.png" alt="image.png" data-proofer-ignore></p><p>This keeps the normalization stable without ever storing the full matrix.</p><p>IO complexity:</p><ul><li>Standard attention: $\Omega(Nd+N^2)$<li>Flash attention: $O(\frac{N^2d^2}{M})$, where $M$ is SRAM size</ul><p>This results in e-3x speedup and ~10x less memory usage.</p><p><strong>KV Caching:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2027.png" alt="image.png" data-proofer-ignore></p><p>When decoding autoregressively (LLMs), recomputing all past $K,V$ at each step is wasteful.</p><p>So the trick is to:</p><ul><li>Cache previously computed Key and Value vectors.<li>For the next token, only compute new $K_t,V_t$ and append to cache.</ul><p>This avoids recomputation, reducing complexity per token from $O(L^2)$ to $O(L)$.</p><p><strong>Multi and Grouped Query Attention:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2028.png" alt="image.png" data-proofer-ignore></p><p>Standard Multi-head Attention (MHA):</p><ul><li>In normal multi-head attention, we have H independent sets of queries, keys, and values.<li>This means memory + compute cost grows with H.</ul><p>Multi-query Attention (MQA):</p><ul><li>Shares the same keys and values across all query heads.<li>Only queries are separate per head.<li>This reduces memory cost, since we only store one set of K and V, instead of H.</ul><p>Grouped-query Attention (GQA):</p><ul><li>A middle ground between MHA and MQA.<li>Queries are divided into groups, where each group shares a single key and value set.<li>Reduces KV memory footprint more than MHA but not as aggressively as MQA.<li>Better accuracy than MQA since it doesn’t collapse all queries into one KV set.</ul><p><strong>Multi-Head Latent Attention:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2029.png" alt="image.png" data-proofer-ignore></p><p>MLA improves on MQA/GQA by:</p><ul><li>Low-rank compression of K and V: Instead of directly caching K and V, MLA projects them into a latent space.<li>Stores compressed latent representations (much smaller).<li>During inference, only compressed KV pairs are cached, reducing memory usage while maintaining accuracy.</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2030.png" alt="image.png" data-proofer-ignore></p><h3 id="parameter-efficient-tuning"><span class="mr-2">Parameter Efficient Tuning</span><a href="#parameter-efficient-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Traditional fine-tuning involves updating all parameters of a large pre-trained transformer for each downstream task. However, this is expensive in memory and compute.</p><p>Parameter Efficient Tuning methods update only a small proportion of parameters, while keeping most of the pre-trained model frozen.</p><p><strong>Prefix Tuning:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2031.png" alt="image.png" data-proofer-ignore></p><ul><li>Idea: Learn a small set of continuous prefix vectors (virtual tokens) that are prepended to the input at each layer.<li>The transformer parameters remain frozen; only these prefix vectors are trained.<li>Works well for sequence generation tasks (translation, summarization, table-to-text).</ul><p><strong>Prompt Tuning:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2032.png" alt="image.png" data-proofer-ignore></p><ul><li>Similar to prefix tuning but even lighter: Learns a small set of soft prompt embeddings (tokens) specific to each task.<li>Instead of modifying all layers, only the input embedding space is modified.<li>Allows a single pre-trained model to adapt to many tasks by swapping task-specific prompts.</ul><p><strong>Adapter:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20Advanced/image%2033.png" alt="image.png" data-proofer-ignore></p><ul><li>Introduce small MLP “adapter” layers inside each transformer layer (usually after feedforward or attention).<li>During training, pre-trained weights stay frozen, and only adapter parameters are updated.<li>Advantage: Easy to plug in; allows good transfer across tasks.</ul><p><strong>LoRA (Low-Rank Adaptation):</strong></p><ul><li>Replace large weight update matrices with a low-rank decomposition.<li><p>Instead of updating $W,$ represent update as:</p>\[h = W_0 x + \Delta W x = W_0 x + B A x\]</ul><p>where:</p><ul><li>$W_0:$ frozen pre-trained weight<li>$A,B:$ small trainable low-rank matrices<li>Much fewer trainable parameters</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog/'>Blog</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/learning/" class="post-tag no-text-decoration" >learning</a> <a href="/tags/nnets/" class="post-tag no-text-decoration" >nnets</a> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a> <a href="/tags/attention/" class="post-tag no-text-decoration" >attention</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+Attention+%26+Transformers+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-advanced%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+Attention+%26+Transformers+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-advanced%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-advanced%2F&text=Deep+Learning+-+Attention+%26+Transformers+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-advanced%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/intro-to-rl/">Introduction to Reinforcement Learning</a><li><a href="/posts/reinforcement-learning/">Reinforcement Learning</a><li><a href="/posts/deep-rl/">Deep Reinforcement Learning</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/cmu-blog/">Coursework at CMU</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deep-learning-cnn-rnn-lang/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - CNNs, RNNs, & Language Models</h3><div class="text-muted small"><p> In Progress To-Do CNNs — all LSTM, GRU Connectionist Temporal Classification Convolutional Neural Networks https://youtu.be/kYeeB3CNcx8?si=otHbiKRITjJZk9S8 Forward Pass # Input: # Y...</p></div></div></a></div><div class="card"> <a href="/posts/deep-learning-perceptrons/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - Perceptrons</h3><div class="text-muted small"><p> Neural Networks Depth - length of longest path from source to sink Layer - Set of all neurons which are all at the same depth with respect to the source Gradient For a scalar function $f(x)$ wit...</p></div></div></a></div><div class="card"> <a href="/posts/mmml/"><div class="card-body"> <em class="small" data-ts="1768838400" data-df="ll" > Jan 19, 2026 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-Modal Machine Learning</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/visual-learning-recognition/" class="btn btn-outline-primary" prompt="Older"><p>Visual Learning and Recognition</p></a> <a href="/posts/cbs-multi-arm/" class="btn btn-outline-primary" prompt="Newer"><p>Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "Bhaswanth-A/bhaswanth-a.github.io", "data-repo-id": "R_kgDOHu5z_w", "data-category": "General", "data-category-id": "DIC_kwDOHu5z_84C0yVx", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
