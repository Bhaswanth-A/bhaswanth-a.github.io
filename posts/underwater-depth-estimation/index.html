<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Underwater Depth Estimation and Localization" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="Robotics engineer and researcher focused on learning-based autonomy, perception, and 3D vision, with work spanning mobile robots, space robotics, and foundation models for robotics." /><meta property="og:description" content="Robotics engineer and researcher focused on learning-based autonomy, perception, and 3D vision, with work spanning mobile robots, space robotics, and foundation models for robotics." /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/underwater-depth-estimation/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/underwater-depth-estimation/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/cam.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-12-11T11:00:00-05:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/cam.png" /><meta property="twitter:title" content="Underwater Depth Estimation and Localization" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2024-12-13T17:24:47-05:00","datePublished":"2022-12-11T11:00:00-05:00","description":"Robotics engineer and researcher focused on learning-based autonomy, perception, and 3D vision, with work spanning mobile robots, space robotics, and foundation models for robotics.","headline":"Underwater Depth Estimation and Localization","image":"https://bhaswanth-a.github.io//assets/images/Thumbnail/cam.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/underwater-depth-estimation/"},"url":"https://bhaswanth-a.github.io//posts/underwater-depth-estimation/"}</script><title>Underwater Depth Estimation and Localization | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Perception | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Underwater Depth Estimation and Localization</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Underwater Depth Estimation and Localization</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1670774400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Dec 11, 2022 </em> </span> <span> Updated <em class="" data-ts="1734128687" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Dec 13, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1845 words"> <em>10 min</em> read</span></div></div></div><div class="post-content"><p><a href="https://github.com/Bhaswanth-A/Underwater-Depth-Estimation-and-Localization"><img data-src="https://gh-card.dev/repos/Bhaswanth-A/Underwater-Depth-Estimation-and-Localization.svg" alt="Bhaswanth-A/Underwater-Depth-Estimation-and-Localization - GitHub" data-proofer-ignore></a></p><h1 id="abstract"><strong>Abstract</strong></h1><p>Sophisticated robots operating in an underwater environment require vision to perform different tasks. This project involves developing a reliable vision system by employing a depth camera, rather than a conventional binocular-stereo camera, for underwater depth estimation and localization. The project’s initial phase, until midsemester, consisted of testing the depth camera’s ability to estimate depth in the air. This report covers the second phase of the project, which involves underwater experimentation with objects for depth measurements.</p><h1 id="depth-estimation-and-localization"><strong>DEPTH ESTIMATION AND LOCALIZATION</strong></h1><h2 id="camera-calibration"><span class="mr-2"><strong>Camera Calibration</strong></span><a href="#camera-calibration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Camera calibration is the process of estimating the parameters of a lens and camera image sensor. These calibrated camera parameters can be used to rectify the image by correcting lens distortion, estimating the depth of objects, and localization of the camera in the environment.</p><p>The camera parameters, given by a camera matrix, includes the intrinsic, extrinsic and distortion parameters of the camera. The 3D world coordinates of the objects and their corresponding 2D image points are required to compute the camera matrix.</p><p>The following are the components of the camera calibration matrix:</p><ol><li>Intrinsic parameters - The camera internal parameters such as the focal length, optical center, and the skew coefficient.<li>Extrinsic parameters - The pose parameters of the camera given by a rotation and a translation vector.<li>Distortion parameters -<ol><li>Radial distortion - When light rays bend differently near a lens’s edges and in its optical center, the result is radial distortion. The radial distortion coefficients model this type of distortion.<li>Tangential distortion - Occurs when the lens and the image plane are not parallel. The tangential distortion coefficients model this type of distortion.</ol><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled.png" alt="Untitled" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled9.png" alt="Untitled" width="380" height="594" class="center" data-proofer-ignore></p></ol><p>Radial distortion:</p>\[\begin{aligned}&amp; x_{\text {corrected }}=x\left(1+k_1 r^2+k_2 r^4+k_3 r^6\right) \\&amp; y_{\text {corrected }}=y\left(1+k_1 r^2+k_2 r^4+k_3 r^6\right)\end{aligned}\]<p>Tangential distortion:</p>\[\begin{aligned}&amp; x_{\text {corrected }}=x+\left[2 p_1 x y+p_2\left(r^2+2 x^2\right)\right] \\&amp; y_{\text {corrected }}=y+\left[p_1\left(r^2+2 y^2\right)+2 p_2 x y\right]\end{aligned}\]<p>where, $x$, $y$ = undistorted pixel locations</p><p>$r^2=x^2+y^2$</p><p>$k_1,k_2,k_3 \space -$ radial distortion coefficients</p><p>$p_1,p_2 \space -$ tangential distortion coefficients</p><p>There exist quite a few methods for camera calibration, the most popular one being the Zhang Zhengyou calibration method.</p><h3 id="zhangs-method"><span class="mr-2"><strong>Zhang’s method:</strong></span><a href="#zhangs-method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The Zhang Zhengyou technique is a calibration method that uses a checkerboard pattern. It runs a corner detector to find the points of interest and their positions on the checkerboard. The world coordinate system is set to the corner of the checkerboard, and all points are assumed to lie on the XY-plane.</p><p>The method requires test patterns for camera calibration. The following shows the set of all test images used for calibration -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1.jpeg" alt="1.jpeg" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2.jpeg" alt="2.jpeg" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3.jpeg" alt="3.jpeg" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4.jpeg" alt="4.jpeg" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/5.jpeg" alt="5.jpeg" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/6.jpeg" alt="6.jpeg" data-proofer-ignore></p><p>The corners of the checkerboard are found using OpenCV, as shown below.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1.png" alt="1.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3.png" alt="3.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2.png" alt="2.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/5.png" alt="5.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4.png" alt="4.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/6.png" alt="6.png" data-proofer-ignore></p><p>Once the calibration is done, we can take an image and undistort it. The camera matrix and the distortion coefficients can then be stored. The following are the results obtained after running the code -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/image.png" alt="image.png" data-proofer-ignore></p><p>Camera Matrix:</p>\[\begin{bmatrix} 1.07443280e+03 &amp; 0.00000000 \mathrm{e}+00 &amp; \quad 4.32669818 \mathrm{e}+02 \\ 0.00000000 \mathrm{e}+00 &amp; \quad 1.01862189 \mathrm{e}+03 &amp; \quad 2.63179677 \mathrm{e}+02\\ 0.00000000 \mathrm{e}+00 &amp; \quad 0.00000000 \mathrm{e}+00 &amp; \quad 1.00000000 \mathrm{e}+00\\ \end{bmatrix}\]<p>Distortion Parameters:</p>\[\begin{bmatrix} -0.09265683 &amp; 0.80628656 &amp; -0.00354457 &amp; -0.00947235 &amp; -1.59645404 \end{bmatrix}\]<p>Total Error: $0.06509290538498035$</p><h3 id="depth-camera"><span class="mr-2"><strong>Depth camera:</strong></span><a href="#depth-camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The Intel Realsense D415 Depth camera uses stereo vision to calculate depth, and consists of a pair of depth sensors, RGB sensor, and an infrared projector.</p><ol><li>Image sensors - The set of image sensors enable capturing of disparity between images up to 1280 x 720 resolution.<li>RGB sensor - Dedicated color image signal processor for image adjustments and scaling color data.<li>Infrared sensor - Active infrared projector to illuminate objects to enhance the depth data.</ol><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/im.png" alt="im.png" data-proofer-ignore></p><p>The camera data rectified in its Vision Processor D4 hardware component is streamed in most modes. The rectified data is then sent to the computer via the USB cable and made available for us to view. The rectified and unrectified images can be obtained by using the relevant IR stream modes, as follows -</p><p>1) Y8 IR mode - provides calibrated and rectified images.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%201.png" alt="1.png" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%201.png" alt="2.png" width="380" height="594" class="center" data-proofer-ignore></p><p>2) Y16 IR mode - provides unrectified images (closest to what you would get from a true rawstream without calibration)</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%201.png" alt="3.png" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4%201.png" alt="4.png" width="380" height="594" class="center" data-proofer-ignore></p><p>The Intel Realsense SDK, however, does not allow simultaneous RGB stream of the left and right cameras. Thus the process of camera calibration in the depth camera is done by the on-board chip, and the calibrated data can be obtained by looking into the Y8 IR mode stream. The same can also be obtained using OpenCV and the provided librealsense API, as shown below.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/5%201.png" alt="5.png" data-proofer-ignore></p><h2 id="depth-estimation"><span class="mr-2"><strong>Depth Estimation</strong></span><a href="#depth-estimation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="using-calibrated-ir-data"><span class="mr-2"><strong>Using calibrated IR data:</strong></span><a href="#using-calibrated-ir-data" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The calibrated IR image data from the depth camera can be used to perform depth estimation. The StereoSGBM class provided by OpenCV is used for computing the stereo correspondence using the Sum of Squared Differences (SSD) block-matching algorithm, and for disparity calculation by matching the blocks in left and right images as shown below.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled%201.png" alt="Untitled" data-proofer-ignore></p><p>The image below shows the left and right IR images that are used as input to the stereo-depth algorithm.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%202.png" alt="2.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%202.png" alt="3.png" data-proofer-ignore></p><p>The following is the depth map, plotted on matplotlib, obtained from the stereo-depth algorithm. The map resembles the image on the right side to some extent, but needs to be improved by playing around with the values of the number of disparities and block size, which are given as input to the StereoSGBM class.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled%202.png" alt="Untitled" data-proofer-ignore></p><h3 id="using-default-calibration-data"><span class="mr-2"><strong>Using default calibration data:</strong></span><a href="#using-default-calibration-data" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The entire process of camera calibration, rectification, block matching, disparity calculation, and depth estimation is carried out by the On-board Chip of the Intel Realsense depth camera by default. Thus, the simpler way of performing depth estimation is by listening to the relevant streams and using them to find the depth. The depth stream, Z16 mode, and the color stream, BGR8 mode, are used for this purpose.</p><p>The image on the left below shows the color frame and the corresponding distance at the point of placement of the cursor, while the image on the right shows the depth frame.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%202.png" alt="1.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%203.png" alt="2.png" width="380" height="594" class="center" data-proofer-ignore></p><h1 id="oak-d-camera">OAK-D Camera</h1><p>The OAK-D depth camera by Luxonis has three on-board cameras which can implement stereo and RGB vision used for depth and AI processing. The camera has a baseline length of 7.5cm, which is the distance between the left and the right stereo cameras.</p><ol><li>Stereo Cameras - The set of image sensors enable capturing of disparity between images up to 1280 x 800 (1MP) resolution.<li>Color Camera - Dedicated color image signal processor that provides a resolution of up to 4032 x 3040 (12MP).</ol><p>The camera is capable of performing stereo depth perception with filtering, post-processing, RGB-depth alignment, and high configurability.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 389 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%204.png" alt="2.png" width="389" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 370 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%203.png" alt="1.png" width="370" height="594" class="center" data-proofer-ignore></p><h1 id="underwater-testing"><strong>UNDERWATER TESTING</strong></h1><p>The depth camera calibrated in the air cannot be used for performing underwater experiments due to differences in disparities caused by the refraction of light. As a result, the depth camera needs to be calibrated underwater and then used for depth estimation.</p><h2 id="setup"><span class="mr-2"><strong>Setup</strong></span><a href="#setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The setup to perform underwater depth measurement involves placing the OAK-D depth camera in a transparent container and holding it inside water partially submerged, as shown in the images below. The entire setup and all the experiments were performed in the swimming pool.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%201.jpeg" alt="3.jpeg" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4%201.jpeg" alt="4.jpeg" width="380" height="594" class="center" data-proofer-ignore></p><h2 id="camera-calibration-1"><span class="mr-2"><strong>Camera Calibration</strong></span><a href="#camera-calibration-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The camera is calibrated using a charuco board, in a process very similar to calibrating using the Zhang Zhengyou technique. The charuco board is printed onto a flat surface, and 13 different images are captured in different orientations.</p><p>The following shows an image of the charuco board having a square size of 2.45 cm -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/image%201.png" alt="image.png" data-proofer-ignore></p><p>The following shows the set of all test images (left + right + rgb) used for calibration. The numbers that appear on some images in the middle show the timer while capturing the image -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%204.png" alt="1.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%205.png" alt="2.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%203.png" alt="3.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4%202.png" alt="4.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/5%202.png" alt="5.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/6%201.png" alt="6.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/7.png" alt="7.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/8.png" alt="8.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/9.png" alt="9.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/10.png" alt="10.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/11.png" alt="11.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/12.png" alt="12.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/13.png" alt="13.png" data-proofer-ignore></p><p>Once the camera calibration is complete, we obtain the distortion coefficients, intrinsic parameters, extrinsic parameters, and the stereorectification data of the camera.</p><div class="language-json highlighter-rouge"><div class="code-header"> <span data-label-text="JSON"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
</pre><td class="rouge-code"><pre><span class="nl">"extrinsics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                    </span><span class="nl">"rotationMatrix"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">0.9999324679374695</span><span class="p">,</span><span class="w">
                            </span><span class="mf">-0.008666034787893295</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.007746713235974312</span><span class="w">
                        </span><span class="p">],</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">0.008631718344986439</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.9999528527259827</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.0044522895477712154</span><span class="w">
                        </span><span class="p">],</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">-0.00778493145480752</span><span class="p">,</span><span class="w">
                            </span><span class="mf">-0.0043851216323673725</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.9999600648880005</span><span class="w">
                        </span><span class="p">]</span><span class="w">
                    </span><span class="p">],</span><span class="w">
                    </span><span class="nl">"specTranslation"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                        </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">-7.5</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"z"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="w">
                    </span><span class="p">},</span><span class="w">
                    </span><span class="nl">"toCameraSocket"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
                    </span><span class="nl">"translation"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                        </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">-7.581012725830078</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.0006467599887400866</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"z"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.040638044476509094</span><span class="w">
                    </span><span class="p">}</span><span class="w">
                </span><span class="p">}</span><span class="err">,</span><span class="w">

</span><span class="nl">"intrinsicMatrix"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                    </span><span class="p">[</span><span class="w">
                        </span><span class="mf">799.7395629882813</span><span class="p">,</span><span class="w">
                        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                        </span><span class="mf">662.93359375</span><span class="w">
                    </span><span class="p">],</span><span class="w">
                    </span><span class="p">[</span><span class="w">
                        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                        </span><span class="mf">799.383056640625</span><span class="p">,</span><span class="w">
                        </span><span class="mf">382.2420654296875</span><span class="w">
                    </span><span class="p">],</span><span class="w">
                    </span><span class="p">[</span><span class="w">
                        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                        </span><span class="mf">1.0</span><span class="w">
                    </span><span class="p">]</span><span class="w">
                </span><span class="p">]</span><span class="err">,</span><span class="w">

</span><span class="nl">"rectifiedRotationLeft"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">0.9999605417251587</span><span class="p">,</span><span class="w">
                </span><span class="mf">-0.008557096123695374</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.002386769512668252</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">0.00855177454650402</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.9999609589576721</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.0022310472559183836</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">-0.002405767561867833</span><span class="p">,</span><span class="w">
                </span><span class="mf">-0.002210548147559166</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.9999946355819702</span><span class="w">
            </span><span class="p">]</span><span class="w">
        </span><span class="p">]</span><span class="err">,</span><span class="w">
        </span><span class="nl">"rectifiedRotationRight"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">0.9999856352806091</span><span class="p">,</span><span class="w">
                </span><span class="mf">8.531191269867122e-05</span><span class="p">,</span><span class="w">
                </span><span class="mf">-0.005360426381230354</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">-9.721628157421947e-05</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.9999975562095642</span><span class="p">,</span><span class="w">
                </span><span class="mf">-0.0022205670829862356</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="p">[</span><span class="w">
                </span><span class="mf">0.005360223352909088</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.0022210562601685524</span><span class="p">,</span><span class="w">
                </span><span class="mf">0.9999831914901733</span><span class="w">
            </span><span class="p">]</span><span class="w">
        </span><span class="p">]</span><span class="err">,</span><span class="w">

</span><span class="nl">"distortionCoeff"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                    </span><span class="mf">9.491045951843262</span><span class="p">,</span><span class="w">
                    </span><span class="mf">-102.06877136230469</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0008228731458075345</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.001999291591346264</span><span class="p">,</span><span class="w">
                    </span><span class="mf">401.19476318359375</span><span class="p">,</span><span class="w">
                    </span><span class="mf">9.264853477478027</span><span class="p">,</span><span class="w">
                    </span><span class="mf">-100.42776489257813</span><span class="p">,</span><span class="w">
                    </span><span class="mf">394.56182861328125</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
                    </span><span class="mf">0.0</span><span class="w">
                </span><span class="p">]</span><span class="err">,</span><span class="w">
</span></pre></table></code></div></div><h2 id="depth-estimation-1"><span class="mr-2"><strong>Depth Estimation</strong></span><a href="#depth-estimation-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The StereoDepth node, provided by the OAK-D API, is used to calculate the disparity and depth from the stereo camera pair.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled%203.png" alt="Untitled" data-proofer-ignore></p><p>The generation of the depth map from the StereoDepth node can be visualized using the following images, which are taken in air -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%206.png" alt="1.png" data-proofer-ignore></p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%207.png" alt="2.png" data-proofer-ignore></p><p>When the object is placed closer to the camera -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%205.png" alt="3.png" data-proofer-ignore></p><p>When the object is placed farther away from the camera -</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4%204.png" alt="4.png" data-proofer-ignore></p><p>The same experiments are performed underwater, but now using the newly calibrated camera matrix. We obtain the depth values and the disparity map as follows -</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%207.png" alt="1.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/2%208.png" alt="2.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/3%206.png" alt="3.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/4%205.png" alt="4.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/5%203.png" alt="5.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled%204.png" alt="Untitled" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/7%201.png" alt="7.png" width="380" height="594" class="center" data-proofer-ignore></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 380 594'%3E%3C/svg%3E" data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/Untitled%205.png" alt="Untitled" width="380" height="594" data-proofer-ignore></p><p>From the above images, we see that as the bottle is placed farther away from the depth camera setup, the value of the depth increases, and the disparity map changes accordingly. From the experiments performed above, we find that the depth camera can measure a depth of up to 1m (~3ft) underwater.</p><h2 id="camera-localization"><span class="mr-2"><strong>Camera Localization</strong></span><a href="#camera-localization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>As done earlier, the position of the depth camera can be estimated using the extrinsic parameters of the camera.</p><p><img data-src="/assets/images/Underwater%20Perception%20and%20Navigation%207b07d806054842b887b63405dd37ddab/1%208.png" alt="1.png" data-proofer-ignore></p><p>The following are the rotation and translation vectors obtained after calibration -</p><div class="language-json highlighter-rouge"><div class="code-header"> <span data-label-text="JSON"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="nl">"rotationMatrix"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">0.9999324679374695</span><span class="p">,</span><span class="w">
                            </span><span class="mf">-0.008666034787893295</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.007746713235974312</span><span class="w">
                        </span><span class="p">],</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">0.008631718344986439</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.9999528527259827</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.0044522895477712154</span><span class="w">
                        </span><span class="p">],</span><span class="w">
                        </span><span class="p">[</span><span class="w">
                            </span><span class="mf">-0.00778493145480752</span><span class="p">,</span><span class="w">
                            </span><span class="mf">-0.0043851216323673725</span><span class="p">,</span><span class="w">
                            </span><span class="mf">0.9999600648880005</span><span class="w">
                        </span><span class="p">]</span><span class="w">
                    </span><span class="p">]</span><span class="err">,</span><span class="w">
    
</span><span class="nl">"translation"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                        </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">-7.581012725830078</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.0006467599887400866</span><span class="p">,</span><span class="w">
                        </span><span class="nl">"z"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.040638044476509094</span><span class="w">
                    </span><span class="p">}</span><span class="w">
</span></pre></table></code></div></div><h1 id="conclusion"><strong>Conclusion</strong></h1><p>The work done so far includes camera calibration, block matching, disparity calculation, and depth estimation in an underwater environment. While performing the experiments, the depth camera could measure the depth of obstacles only up to a distance of 1m. This can be further improved by building a better canister for housing the depth camera and by improving the lighting conditions using subsea LED lights.</p><p>One major issue that I faced while working with the underwater camera setup was the problem of fogging. Due to the difference in temperatures at the back of the camera, which heats up, and the front of the camera, which is cold, fog begins to form on the left and right stereo cameras. This issue can also be solved by building a properly insulated canister.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/projects/'>Projects</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/perception/" class="post-tag no-text-decoration" >perception</a> <a href="/tags/underwater/" class="post-tag no-text-decoration" >underwater</a> <a href="/tags/sauvc/" class="post-tag no-text-decoration" >sauvc</a> <a href="/tags/depth/" class="post-tag no-text-decoration" >depth</a> <a href="/tags/camera/" class="post-tag no-text-decoration" >camera</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Underwater+Depth+Estimation+and+Localization+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Funderwater-depth-estimation%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Underwater+Depth+Estimation+and+Localization+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Funderwater-depth-estimation%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Funderwater-depth-estimation%2F&text=Underwater+Depth+Estimation+and+Localization+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Funderwater-depth-estimation%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/intro-to-rl/">Introduction to Reinforcement Learning</a><li><a href="/posts/reinforcement-learning/">Reinforcement Learning</a><li><a href="/posts/deep-rl/">Deep Reinforcement Learning</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/cmu-blog/">Coursework at CMU</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/aug-simulator/"><div class="card-body"> <em class="small" data-ts="1701705600" data-df="ll" > Dec 4, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Development of Python-Based Simulator for Analyzing Autonomous Underwater Glider Motions and Performance</h3><div class="text-muted small"><p> The following project was completed as part of my undergraduate thesis at the Institute for Systems and Robotics, Lisbon under the supervision of Dr. David Cabecinhas and Dr. Pedro Batista Abstr...</p></div></div></a></div><div class="card"> <a href="/posts/asv/"><div class="card-body"> <em class="small" data-ts="1701705600" data-df="ll" > Dec 4, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Controller for Autonomous Surface Vehicle</h3><div class="text-muted small"><p> The following project was completed as part of a competition at the Institute for Systems and Robotics, Lisbon under the supervision of Dr. David Cabecinhas and Dr. Pedro Batista</p></div></div></a></div><div class="card"> <a href="/posts/cbs-multi-arm/"><div class="card-body"> <em class="small" data-ts="1751472000" data-df="ll" > Jul 2, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences</h3><div class="text-muted small"><p> Techincal Report Your browser does not support PDFs. Please download the report here. Results xECBS Scene 1 Scene 2 Scene 3 Scene 4 ECBS Scene 1 Scene 2 Scene 3 Sc...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/sauvc-intro/" class="btn btn-outline-primary" prompt="Older"><p>The Singapore AUV Challenge!</p></a> <a href="/posts/chebyshev/" class="btn btn-outline-primary" prompt="Newer"><p>Third-order Tchebyshev Low-pass Filter</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "Bhaswanth-A/bhaswanth-a.github.io", "data-repo-id": "R_kgDOHu5z_w", "data-category": "General", "data-category-id": "DIC_kwDOHu5z_84C0yVx", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
