<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Deep Learning - CNNs, RNNs, &amp; Language Models" /><meta name="author" content="<author_id>" /><meta property="og:locale" content="en" /><meta name="description" content="In Progress" /><meta property="og:description" content="In Progress" /><link rel="canonical" href="https://bhaswanth-a.github.io//posts/deep-learning-cnn-rnn-lang/" /><meta property="og:url" content="https://bhaswanth-a.github.io//posts/deep-learning-cnn-rnn-lang/" /><meta property="og:site_name" content="Bhaswanth Ayapilla" /><meta property="og:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/cnn2.jpg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-06-09T12:00:00-04:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bhaswanth-a.github.io//assets/images/Thumbnail/cnn2.jpg" /><meta property="twitter:title" content="Deep Learning - CNNs, RNNs, &amp; Language Models" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@<author_id>" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<author_id>"},"dateModified":"2025-11-05T10:32:49-05:00","datePublished":"2025-06-09T12:00:00-04:00","description":"In Progress","headline":"Deep Learning - CNNs, RNNs, &amp; Language Models","image":"https://bhaswanth-a.github.io//assets/images/Thumbnail/cnn2.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://bhaswanth-a.github.io//posts/deep-learning-cnn-rnn-lang/"},"url":"https://bhaswanth-a.github.io//posts/deep-learning-cnn-rnn-lang/"}</script><title>Deep Learning - CNNs, RNNs, & Language Models | Bhaswanth Ayapilla</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Bhaswanth Ayapilla"><meta name="application-name" content="Bhaswanth Ayapilla"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/prfl.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Bhaswanth Ayapilla</a></div><div class="site-subtitle font-italic">Perception | Reinforcement Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/cmu/" class="nav-link"> <i class="fa-fw fas fa-school ml-xl-3 mr-xl-3 unloaded"></i> <span>CMU MRSD</span> </a><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-blog ml-xl-3 mr-xl-3 unloaded"></i> <span>BLOG</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-book ml-xl-3 mr-xl-3 unloaded"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Bhaswanth-A" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bhaswanthayapilla','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.instagram.com/bhaswanth_a/" aria-label="instagram" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.linkedin.com/in/bhaswanth-a/" aria-label="linkedin" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://bhaswanth-a.github.io/cv/" aria-label="cv" class="order-7" target="_blank" rel="noopener"> <i class="fas fa-file"></i> </a> <span id="mode-toggle-wrapper" class="order-1"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> </span></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Deep Learning - CNNs, RNNs, & Language Models</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Deep Learning - CNNs, RNNs, & Language Models</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1749484800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 9, 2025 </em> </span> <span> Updated <em class="" data-ts="1762356769" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 5, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> Bhaswanth Ayapilla </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="7795 words"> <em>43 min</em> read</span></div></div></div><div class="post-content"><p><em>In Progress</em></p><h1 id="to-do">To-Do</h1><ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />CNNs — all<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />LSTM, GRU<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Connectionist Temporal Classification</ul><h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1><p><a href="https://youtu.be/kYeeB3CNcx8?si=otHbiKRITjJZk9S8">https://youtu.be/kYeeB3CNcx8?si=otHbiKRITjJZk9S8</a></p><h2 id="forward-pass"><span class="mr-2">Forward Pass</span><a href="#forward-pass" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre># Input:
#   Y(0, :, :, :) = Image     # The input image (or feature map) is stored at layer 0
#                             # Dimensions: (channels, width, height)

# Iterate over each convolutional layer
for l = 1:L                          # L = total number of layers

    # Iterate over the spatial positions (x, y) of the output feature map
    for x = 1:(W_{l-1} - K_l + 1)    # Slide window horizontally
        for y = 1:(H_{l-1} - K_l + 1) # Slide window vertically

            # Iterate over the output channels (filters)
            for j = 1:D_l
                z(l, j, x, y) = 0     # Initialize pre-activation (accumulator)

                # Accumulate contributions from all input channels
                for i = 1:D_{l-1}        # D_{l-1} = # input channels to layer l
                    for x' = 1:K_l       # K_l = kernel width
                        for y' = 1:K_l   # K_l = kernel height

                            # Perform convolution at this location:
                            # weight * input value
                            z(l, j, x, y) += w(l, j, i, x', y') * Y(l-1, i, x + x' - 1, y + y' - 1)
                            # Note: (x + x' - 1) and (y + y' - 1) are for correct indexing

                # Apply activation function (e.g. ReLU) after convolution
                Y(l, j, x, y) = activation(z(l, j, x, y))

# Final prediction Y is obtained by flattening the last layer's output and applying softmax
Y = softmax(Y(L, :, 1, 1) .. Y(L, :, W - K + 1, H - K + 1))
</pre></table></code></div></div><h2 id="backpropagation"><span class="mr-2">Backpropagation</span><a href="#backpropagation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I don’t think I can do a better job in explaining CNN backpropagation than this video. Just watch it. But here’s the pseudo code for quick reference.</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre># Start with gradient of the loss w.r.t. the final layer output
dY(L) = dDiv / dY(L)    # From loss function 

# Loop backward through layers
for l = L : downto : 1                             # Backprop layer-by-layer
    dw(l) = zeros(D_l × D_{l-1} × K_l × K_l)       # Initialize weight gradient
    dY(l - 1) = zeros(D_{l-1} × W_{l-1} × H_{l-1}) # Initialize input gradient

    # Loop over spatial positions in the output feature map
    for x = W_{l-1} - K_l + 1 : downto : 1
        for y = H_{l-1} - K_l + 1 : downto : 1
            for j = D_l : downto : 1              # For each output channel

                # Backprop through activation
                dz(l, j, x, y) = dY(l, j, x, y) * f′( z(l, j, x, y) )

                # Backpropagate error and compute weight gradients
                for i = D_{l-1} : downto : 1          # Input channels
                    for x′ = K_l : downto : 1         # Kernel width
                        for y′ = K_l : downto : 1     # Kernel height

                            # Chain rule: propagate error to input
                            dY(l - 1, i, x + x′ - 1, y + y′ - 1) += \
                                w(l, j, i, x′, y′) * dz(l, j, x, y)

                            # Compute gradient of weight
                            dw(l, j, i, x′, y′) += \
                                dz(l, j, x, y) * Y(l - 1, i, x + x′ - 1, y + y′ - 1)

</pre></table></code></div></div><h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image.png" alt="image.png" data-proofer-ignore></p><h2 id="state-space-model"><span class="mr-2">State-Space Model</span><a href="#state-space-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>State-space model is a model for infinite response systems.</p>\[h_t = f(x_t,h_{t-1}) \\\] \[y_t = g(h_t)\]<p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%201.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%202.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%203.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%204.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%205.png" alt="image.png" data-proofer-ignore></p><p>There are all different types of RNNs, having varying number of layers.</p><p>Considering an RNN with just one hidden layer,</p>\[h_i^{(1)}(t) = f_1 \left( \sum_j w_{ji}^{(1)} x_j(t) + \sum_j w_{ji}^{(11)} h_j^{(1)}(t-1) + b_i^{(1)} \right) \\\] \[Y_k(t) = f_2 \left( \sum_j w_{jk}^{(2)} h_j^{(1)}(t) + b_k^{(2)} \right), \quad k = 1..M\]<p>In the vector form:</p>\[\textbf{h}^{(1)}(t) = f_1(\textbf{W}^{(1)}\textbf{X}(t)+\textbf{W}^{(11)}\textbf{h}^{(1)}(t-1)+\textbf{b}^{(1)}) \\\] \[\textbf{Y}(t) = f_2(\textbf{W}^{(2)}\textbf{h}^{(1)}(t)+\textbf{b}^{(2)})\]<p>$f_1()$ is usually the $\tanh$ activation function.</p><ul><li>The hidden state captures both new input and temporal memory via the recurrent connection.<li>Output depends only on the current hidden state, but that hidden state has a history encoded in it through $h(t-1), h(t-2),$ etc.<li>The initial values of hidden states are generally learnable paramteres as well.</ul><h3 id="variants-on-rnns"><span class="mr-2">Variants on RNNs</span><a href="#variants-on-rnns" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%206.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%207.png" alt="image.png" data-proofer-ignore></p><ul><li><strong>One to one:</strong> Conventional MLP<li><strong>One to many:</strong> Sequence generation, e.g. image to caption<li><strong>Many to one:</strong> Sequence based prediction or classification, e.g. Speech recognition, text classification<li><strong>Many to many:</strong> Delayed sequence to sequence, e.g. machine translation<li><strong>Many to many:</strong> Sequence to sequence, e.g. stock problem, label prediction</ul><h2 id="training"><span class="mr-2">Training</span><a href="#training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="forward-pass-1"><span class="mr-2">Forward Pass</span><a href="#forward-pass-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>for t = 0:T-1:
	# Initialize input vectors
	h(t,0) = x(t)
	for l = 1:L:
		z(t,l) = Wc(l) * h(t,l-1) + Wr(l) * h(t-1,l) + b(l) # c for current, r for recurrent
		h(t,l) = tanh(z(t,l))
	zo(t,L) = Wo(L) * h(t,L) + bo
	Y(t) = softmax(zo(t,L))
</pre></table></code></div></div><ul><li>Assuming $h(-1,^*)$ is known<li>Assuming $L$ hidden-state layers and an output layer<li>$W_c, W_r$ are matrices, $b$ are vectors<li>$W_c$ are weights for inputs from current time<li>$W_r$ are recurrent weights applied from previous time<li>$W_o$ are output layer weights</ul><h3 id="back-propagation-through-time-bptt"><span class="mr-2">Back Propagation Through Time (BPTT)</span><a href="#back-propagation-through-time-bptt" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%208.png" alt="image.png" data-proofer-ignore></p><p>The divergence computed is between the sequence of outputs by the network and the desired sequence of outputs. So $DIV$ is a scalar function of a series of vectors.</p><p>The $DIV$ is not necessarily the sum of divergences at individual times, unless we explicitly define it to be that way.</p><p><strong>Local Gradients from Output:</strong></p><p>We want:</p>\[\frac{\partial DIV}{\partial Y_i(t)} \space \space \text{for all i,t}\]<p>If $DIV$ is the sum of all divergences, then</p>\[\frac{\partial DIV}{\partial Y_i(t)} = \frac{\partial Div(t)}{\partial Y_i(t)}\]<p>In any case, let’s assume we know how to compute this gradient. We will tackle what exact divergence helps us do this later.</p><p><strong>Backprop for the last time sequence:</strong></p><ul><li><p>Final layer</p>\[\nabla_{Z^{(2)}(T)} DIV = \nabla_{Y(T)} DIV \cdot \nabla_{Z^{(2)}(T)} Y(T)\]<p>For element-wise output activation:</p>\[\frac{dDIV}{dZ_i^{(2)}(T)} = \frac{dDIV}{dY_i(T)} \cdot \frac{dY_i(T)}{dZ_i^{(2)}(T)}\]<p>For vector output activation:</p>\[\frac{dDIV}{dZ_i^{(2)}(T)} = \sum_j \frac{dDIV}{dY_j(T)} \cdot \frac{dY_j(T)}{dZ_i^{(2)}(T)}\]<li><p>For final layer weights</p>\[\nabla_{W^{(2)}} DIV = h(T) \nabla_{Z^{(2)}(T)} DIV\] \[\frac{dDIV}{dw_{ij}^{(2)}} = \frac{dDIV}{dZ_j^{(2)}(T)} \cdot h_i(T)\]<li><p>To hidden state</p>\[\nabla_{h(T)} DIV = \nabla_{Z^{(2)}(T)} DIV \cdot W^{(2)}\] \[\frac{dDIV}{d h_i(T)} = \sum_j \frac{dDIV}{dZ_j^{(2)}(T)} \cdot \frac{dZ_j^{(2)}(T)}{d h_i(T)} = \sum_j w_{ij}^{(2)} \cdot \frac{dDIV}{dZ_j^{(2)}(T)}\]</ul>\[\nabla_{Z^{(1)}(T)} DIV = \nabla_{h(T)} DIV \cdot \nabla_{Z^{(1)}(T)} h(T)\] \[\frac{dDIV}{dZ_i^{(1)}(T)} = \frac{dDIV}{dh_i(T)} \cdot \frac{dh_i(T)}{dZ_i^{(1)}(T)}\]<ul><li><p>Weights to hidden state</p>\[\nabla_{W^{(1)}} DIV = X(T) \cdot \nabla_{Z^{(1)}(T)} DIV\] \[\frac{dDIV}{dw_{ij}^{(1)}} = \frac{dDIV}{dZ_j^{(1)}(T)} \cdot X_i(T)\] \[\nabla_{W^{(11)}} DIV = h(T - 1) \cdot \nabla_{Z^{(1)}(T)} DIV\] \[\frac{dDIV}{dw_{ij}^{(11)}} = \frac{dDIV}{dZ_j^{(1)}(T)} \cdot h_i(T - 1)\]</ul><p><strong>Progressing backward in time:</strong></p><ul><li><p>Final layer</p>\[\nabla_{Z^{(2)}(T-1)} DIV = \nabla_{Y(T-1)} DIV \cdot \nabla_{Z^{(2)}(T-1)} Y(T-1)\] \[\frac{dDIV}{dZ_i^{(2)}(T-1)} = \frac{dDIV}{dY_i(T-1)} \cdot \frac{dY_i(T-1)}{dZ_i^{(2)}(T-1)}\] \[\frac{dDIV}{dZ_i^{(2)}(T-1)} = \sum_j \frac{dDIV}{dY_j(T-1)} \cdot \frac{dY_j(T-1)}{dZ_i^{(2)}(T-1)}\]<li><p>For final layer weights</p>\[\frac{dDIV}{dw_{ij}^{(2)}} += \frac{dDIV}{dZ_j^{(2)}(T-1)} \cdot h_i(T-1)\] \[\nabla_{W^{(2)}} DIV += h(T-1) \cdot \nabla_{Z^{(2)}(T-1)} DIV\]<p>Since the RNN uses the same weights at each time step (weight sharing), the total gradient of the loss with respect to $\textbf{W}^{(2)}$ is the sum of the gradients over all time steps. Hence we have the increment</p><li><p>To hidden state</p>\[\nabla_{h(T-1)} DIV = \nabla_{Z^{(2)}(T-1)} DIV \cdot W^{(2)} + \nabla_{Z^{(1)}(T)} DIV \cdot W^{(11)}\] \[\frac{dDIV}{d h_i(T-1)} = \sum_j w_{ij}^{(2)} \frac{dDIV}{d Z_j^{(2)}(T-1)} + \sum_j w_{ij}^{(11)} \frac{dDIV}{d Z_j^{(1)}(T)}\] \[\nabla_{Z^{(1)}(T-1)} DIV = \nabla_{h(T-1)} DIV \cdot \nabla_{Z^{(1)}(T-1)} h(T-1)\] \[\frac{dDIV}{d Z_i^{(1)}(T-1)} = \frac{dDIV}{d h_i(T-1)} \cdot \frac{d h_i(T-1)}{d Z_i^{(1)}(T-1)}\]<li><p>Weights to hidden state</p>\[\nabla_{W^{(1)}} DIV \mathrel{+}= X(T-1) \cdot \nabla_{Z^{(1)}(T-1)} DIV\] \[\frac{dDIV}{dw_{ij}^{(1)}} \mathrel{+}= \frac{dDIV}{dZ_j^{(1)}(T-1)} \cdot X_i(T-1)\] \[\nabla_{W^{(11)}} DIV \mathrel{+}= h(T-2) \cdot \nabla_{Z^{(1)}(T-1)} DIV\] \[\frac{dDIV}{dw_{ij}^{(11)}} \mathrel{+}= \frac{dDIV}{dZ_j^{(1)}(T-1)} \cdot h_i(T-2)\]</ul><p>We continue this until</p>\[\nabla_{h_{-1}} DIV = \nabla_{Z^{(1)}(0)} DIV \cdot W^{(11)}\] \[\frac{dDIV}{dh_i(-1)} = \sum_j w_{ij}^{(11)} \cdot \frac{dDIV}{dZ_j^{(1)}(0)}\]<div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre><td class="rouge-code"><pre># Assuming forward pass has been completed
# Jacobian(x, y) is the Jacobian of x w.r.t. y
# Assuming dY(t) = ∇_{Y(t)}DIV is available for all t
# Assuming all dZ, dH, dW and db are initialized to 0

for t = T-1 : downto : 0
    # ∇_{Z^{(2)}(t)}DIV = ∇_{Y(t)}DIV × ∇_{Z^{(2)}(t)}Y(t)
    dZ_o(t) = dY(t) × Jacobian(Y(t), Z_o(t))  

    # ∇_{W^{(2)}}DIV += h(t, L)^T × ∇_{Z^{(2)}(t)}DIV
    dW_o += h(t, L)ᵀ × dZ_o(t)

    # db_o += ∇_{Z^{(2)}(t)}DIV
    db_o += dZ_o(t)

    # ∇_{h(t, L)}DIV += ∇_{Z^{(2)}(t)}DIV × W^{(2)}
    dH(t, L) += dZ_o(t) × W_oᵀ

    # --- Reverse through layers ---
    for l = L : downto : 1
        # ∇_{Z^{(1)}(t,l)}DIV = ∇_{h(t,l)}DIV × ∇_{Z^{(1)}(t,l)}h(t,l)
        dZ(t, l) = dH(t, l) × Jacobian(h(t, l), Z(t, l))

        # ∇_{h(t,l-1)}DIV += ∇_{Z^{(1)}(t,l)}DIV × W_c(l)
        dH(t, l-1) += dZ(t, l) × W_c(l)

        # ∇_{h(t-1,l)}DIV += ∇_{Z^{(1)}(t,l)}DIV × W_r(l)
        dH(t-1, l) += dZ(t, l) × W_r(l)

        # ∇_{W_c(l)}DIV += h(t,l-1)^T × ∇_{Z^{(1)}(t,l)}DIV
        dW_c(l) += h(t, l-1)ᵀ × dZ(t, l)

        # ∇_{W_r(l)}DIV += h(t-1,l)^T × ∇_{Z^{(1)}(t,l)}DIV
        dW_r(l) += h(t-1, l)ᵀ × dZ(t, l)

        # ∇_{b(l)}DIV += ∇_{Z^{(1)}(t,l)}DIV
        db(l) += dZ(t, l)
</pre></table></code></div></div><h2 id="bidirectional-rnn"><span class="mr-2">Bidirectional RNN</span><a href="#bidirectional-rnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%209.png" alt="image.png" data-proofer-ignore></p><h3 id="inference"><span class="mr-2">Inference</span><a href="#inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre># Subscript "f" → forward RNN, subscript "b" → backward RNN
# x(t) is input sequence at time t (could be output from a lower layer)
# Assume h_f(-1,*) and h_b(∞,*) are known initializations

# --- Forward recurrence (left to right in time) ---
for t = 0 : T-1                         # Looping forward through time
    h_f(t, 0) = x(t)                    # Input to first layer (layer 0)
    for l = 1 : L_f                     # L_f is depth of forward RNN
        # z_f(t,l) = W_fc × h_f(t, l-1) + W_fr × h_f(t-1, l) + b_f
        z_f(t, l) = W_fc(l) × h_f(t, l-1) + W_fr(l) × h_f(t-1, l) + b_f(l)

        # h_f(t,l) = tanh(z_f(t,l))      # Assuming tanh activation
        h_f(t, l) = tanh(z_f(t, l))

# --- Backward recurrence (right to left in time) ---
h_b(T, :, :) = h_b(∞, :, :)             # Initial hidden state for backward pass
for t = T-1 : downto : 0                # Looping backward through time
    h_b(t, 0) = x(t)                    # Input to first layer (layer 0)
    for l = 1 : L_b                     # L_b is depth of backward RNN
        # z_b(t,l) = W_bc × h_b(t, l-1) + W_br × h_b(t+1, l) + b_b
        z_b(t, l) = W_bc(l) × h_b(t, l-1) + W_br(l) × h_b(t+1, l) + b_b(l)

        # h_b(t,l) = tanh(z_b(t,l))
        h_b(t, l) = tanh(z_b(t, l))

# --- Output from bidirectional layer ---
for t = 0 : T-1
    # Concatenate forward and backward hidden states at final layer
    h(t) = [h_f(t, L_f); h_b(t, L_b)]
</pre></table></code></div></div><h3 id="forward-pass-2"><span class="mr-2">Forward Pass</span><a href="#forward-pass-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre># Inputs:
#   L     : Number of hidden layers
#   Wc    : Current weights of each layer (input-to-hidden)
#   Wr    : Recurrent weights of each layer (hidden-to-hidden)
#   b     : Biases for each layer
#   hinit : Initial hidden state (e.g., h(-1,*))
#   x     : Input vector sequence [T x input_dim]
#   T     : Length of the input sequence
#
# Output:
#   h     : Hidden activations after tanh
#   z     : Pre-activations (linear sums before tanh)

function RNN_forward(L, Wc, Wr, b, hinit, x, T)
    h(-1,:) = hinit                  # h(-1) = hinit for all layers

    for t = 0:T-1                    # Forward through time
        h(t,0) = x(t)                # Input goes into "layer 0"
        for l = 1:L                 # Forward through layers
            # Linear sum: zᶫₜ = Wc(l) * h(t, l-1) + Wr(l) * h(t-1, l) + b(l)
            z(t,l) = Wc(l) * h(t,l-1) + Wr(l) * h(t-1,l) + b(l)
            
            # Non-linearity: hᶫₜ = tanh(zᶫₜ)
            h(t,l) = tanh(z(t,l))   
        end
    end

    return h
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre># Subscript "f" = forward net
# Subscript "b" = backward net
# Assuming:
#   h_f(-1, :) is known as initial state
#   h_b(∞, :) is known as terminal state

# Forward pass: go left-to-right
h_f = RNN_forward(Lf, Wfc, Wfr, b_f, h_f(-1,:), x, T)

# Backward pass: go right-to-left
x_rev = fliplr(x)                   # Flip input in time
h_brev = RNN_forward(Lb, Wbc, Wbr, b_b, h_b(inf,:), x_rev, T)
h_b = fliplr(h_brev)                # Flip hidden sequence to match forward time

# Combine both forward and backward outputs for each time step:
for t = 0:T-1
    # Final output h(t) is a concat of last layer forward + backward
    h(t) = [h_f(t, Lf); h_b(t, Lb)]
    # You can also use another output layer like: y(t) = W_o * h(t)
end
</pre></table></code></div></div><h3 id="back-propagation-through-time"><span class="mr-2">Back Propagation Through Time</span><a href="#back-propagation-through-time" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre># Inputs:
#   L       : Number of hidden layers
#   W_c, W_r, b : current (feedforward) weights, recurrent weights, and biases
#   hinit   : Initial hidden state (corresponds to h(-1,*))
#   x       : Input sequence (T time steps)
#   T       : Length of input sequence
#   dh_top  : Gradient from the layer above (dDIV/dh(t,L) for all t)
#   h, z    : Stored values from the forward pass

# Output:
#   dx      : Gradient w.r.t input x(t)
#   dW_c    : Gradient w.r.t current weights
#   dW_r    : Gradient w.r.t recurrent weights
#   db      : Gradient w.r.t bias
#   dh(-1)  : Gradient w.r.t initial hidden state

function RNN_bptt(L, W_c, W_r, b, hinit, x, T, dh_top, h, z)
    dh = zeros                  # initialize full dh(t,l)
    
    for t = T-1:down to:0      # Backward through time
        dh(t, L) += dh_top(t)  # Add top gradient to dh(t, L)
        h(t, 0) = x(t)         # Initialize input to first layer
        
        for l = L:1            # Reverse through layers
            dz(t,l) = dh(t,l) * Jacobian(h(t,l), z(t,l))     # ∇h → ∇z using chain rule
            dh(t, l-1) += dz(t,l) * W_c(l)                   # Backprop through current weights
            dh(t-1, l) = dz(t,l) * W_r(l)                    # Backprop through recurrent weights

            dW_c(l) += h(t, l-1) * dz(t,l)                   # Accumulate ∂L/∂W_c
            dW_r(l) += h(t-1, l) * dz(t,l)                   # Accumulate ∂L/∂W_r
            db(l)   += dz(t,l)                               # Accumulate ∂L/∂b
        dx(t) = dh(t,0)                                      # Gradient w.r.t. input x(t)

    return dx, dW_c, dW_r, db, dh(-1)  # dh(-1) = dh(-1,1:L,:) → for use by previous layer
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre># Assumptions:
#   dh(t)     : Gradient from the upper layer, shape (T, Lf+Lb)
#   x(t)      : Input to the BRNN block
#   z_f, h_f  : Forward net activations (t=0...T-1)
#   z_b, h_b  : Backward net activations (t=0...T-1)
#   Lf, Lb    : Number of forward and backward layers

for t = 0:T-1
    dh_f(t) = dh(t, 1:Lf)              # Split forward part
    dh_b(t) = dh(t, Lf+1:Lf+Lb)        # Split backward part

# === Forward Net Backprop ===
[dx_f, dW_fc, dW_fr, db_f, dh_f(-1)] = 
    RNN_bptt(L, W_fc, W_fr, b_f, h_f(-1,:), x, T, dh_f, h_f, z_f)

# === Backward Net Backprop ===
x_rev = fliplr(x)                      # Reverse input time order
dh_brev = fliplr(dh_b)                 # Reverse dh_b
h_brev = fliplr(h_b)                   # Reverse activations
z_brev = fliplr(z_b)                   # Reverse preactivations

[dx_brev, dW_bc, dW_br, db_b, dh_b(inf)] = 
    RNN_bptt(L, W_bc, W_br, b_b, h_b(inf,:), x_rev, T, dh_brev, h_brev, z_brev)

dx_b = fliplr(dx_brev)                # Flip back the dx

# === Combine the gradients from both passes ===
for t = 0:T-1
    dx(t) = dx_f(t) + dx_b(t)         # Total gradient w.r.t input x(t)
</pre></table></code></div></div><h2 id="behavior-of-recurrence"><span class="mr-2">Behavior of Recurrence</span><a href="#behavior-of-recurrence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="bibo-stability"><span class="mr-2">BIBO Stability</span><a href="#bibo-stability" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Time-delay structures have bounded input if:</p><ul><li>The activation function $f()$ has a bounded output for a bounded input, and this is generally the case for every activaton function that we use<li>The input $X(t)$ is bounded</ul><p>This is called the “Bounded Input Bounded Output” stability, and is highly desirable.</p><p><strong>Is a Recurrent Network BIBO?</strong></p><p>If a non-linear activation is used, then the output obtained from it is always bounded and hence behaves as a BIBO.</p><p>But what if we use a linear activation function? To understand this, we can analyze only the behavior of the recurrent hidden layer $h_t$. This is called the Streetlight effect. Consider the following example of an identity activation:</p>\[z_t = W_hh_{t-1} + W_xx_t \\ h_t = z_t\]<p>Now,</p>\[h_t = W_hh_{t-1} + W_xx_t \\ h_{t-1} = W_hh_{t-2} + W_xx_{t-1}\]<p>This gives,</p>\[h_t = W_h^2h_{t-2} + W_hW_xx_{t-1} + W_xx_t\]<p>Repeating this for the entire time series, we get,</p>\[h_t = W_h^{t+1}h_{-1} + W_h^tW_xx_0 + W_h^{t-1}W_xx_1 + W_h^{t-2}W_xx_2 + \ldots + W_xx_t\]<p>This becomes,</p>\[h_t = H_t(h_{-1}) + H_t(x_0) + H_t(x_1) + ... H_t(x_t)\]<p>If $H_t(x_0)$ blows up, the entire system blows up. If this does not blow up, then the responses to later inputs will also not blow up.</p><p>So $h_k$ is a sum of weighted transformations of all past inputs and the initial state.</p><p>Suppose we have nothing else except the input $x(0)$.</p>\[h_0(t) = W^t C x(0)\]<p>So the response at time $t$ is governed by the matrix power $W^t$. Assume $W$ is diagonalisable. Then the power of $W$ is</p>\[W^t = U \Lambda^t U^{-1}\]<p>with</p>\[\Lambda^t = \begin{bmatrix}\lambda_1^t &amp; 0 &amp; 0 \\0 &amp; \lambda_2^t &amp; 0 \\0 &amp; 0 &amp; \ddots\end{bmatrix}\]<p>Lets say $\lambda_1$ is the largest eigenvalue of $W.$ Then by bringing it outside the matrix, we have</p>\[\Lambda^t = \lambda_1^t\begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; (\frac{\lambda_2}{\lambda_1})^t &amp; 0 \\0 &amp; 0 &amp; \ddots\end{bmatrix}\]<p>So this essentially becomes dominated by the largest eigenvalue $\lambda_1$.</p>\[h_0(t) = W^t C x(0) = U \Lambda^t U^{-1} C x(0)\]<p>The growth or decay of $h(t)$ as $t \rightarrow \inf$ is controlled by the largest eigenvalue of $W$:</p><ul><li>If $|\lambda_1| &gt; 1, h(t)$ grows exponentially — system blows up<li>If $|\lambda_1| &lt; 1, h(t)$ shrinks to zero — system vanishes<li>If $|\lambda_1| = 1, h(t)$ is marginally stable<li>Complex eigenvalues will cause oscillatory reponse but with the same overall trends</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2010.png" alt="image.png" data-proofer-ignore></p><p>With a non-linear activation function:</p><ul><li><p>Sigmoid</p>\[h(t) = sigmoid(wh(t-1) +cx(t)+b)\]<p>Final value depends only on $b, w$ and not $x$</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2011.png" alt="image.png" data-proofer-ignore></p><li><p>Tanh</p>\[h(t) = tanh(wh(t-1) +cx(t)+b)\]<p>Final value depends only on $b, w$ and not $x$</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2012.png" alt="image.png" data-proofer-ignore></p><li><p>ReLU</p>\[h(t) = relu(wh(t-1) +cx(t)+b)\]<p>Relu blows up if $w&gt;1$ for $x&gt;0$, and dies for $x&lt;0$. So it is pretty unstable and useless</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2013.png" alt="image.png" data-proofer-ignore></p></ul><p>For a vector processing:</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2014.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2015.png" alt="image.png" data-proofer-ignore></p><p>So Tanh activations are slightly more effective at storing memory, but they too not for very long (the inputs are eventually forgotten in an exponential manner).</p><h3 id="exploding--vanishing-gradients"><span class="mr-2">Exploding &amp; Vanishing Gradients</span><a href="#exploding--vanishing-gradients" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>We know about the vanishing gradients problem in MLPs — that for most common activation functions which have derivatives always less than 1, multiplication by the Jacobian is always a shrinking operation. Because of this, the derivative of divergence after a few layers at any time is totally forgotten.</p><p><strong>Effect of weights:</strong></p><p>Each weight matrix $W_n$ performs a linear transformation of the gradient. Specifically:</p><ul><li>It scales the gradient in different directions.<li>The amount of scaling is determined by the singular values (or eigenvalues for symmetric matrices) of the matrix.</ul><p>Let’s say the singular values of a matrix $W_n$ are $\sigma_1, \sigma_2, \ldots, \sigma_m$</p><ul><li>If all $\sigma_i&lt;1 \rightarrow$ gradient will shrink in all directions<li>If any $\sigma_i &gt; 1 \rightarrow$ gradient may grow in that direction</ul><p>If many weight matrices have singular values greater than 1, then the gradients will explode.</p><p>If many weight matrices have singular values lesser than 1, then the gradients will vanish.</p><p><strong>Important points:</strong></p><ul><li>The derivatives for most parameters will become vanishingly small as we backpropagate the loss gradient through deep networks<li>The derivatives for a small number of parameters will blow up and become large and unstable as we propagate the los gradient through deep networks<li>The derivatives would be more stable if the recurrent weight matrices had singular values equal to 1<li>The derivatives would be more stable if the recurrent activations were identity transforms (with identity Jacobian matrices)<li>The memory would be more stable if the recurrent weight matrix were an identity matrix (i.e. a diagonal matrix with diagonal values equal to “1”)<li>The memory would be more stable if the recurrent activations were identity transforms (which are linear and do not scale up or shrink the output)</ul><h2 id="lstm"><span class="mr-2">LSTM</span><a href="#lstm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We want the RNN to remember for extended periods of time and recall when necessary by tackling the expanding and vanishing gradients problem.</p><p>We do this by getting rid of the Jacobian and weight matrices, but we retain memories until a switch based on the input flags them as okay to forget, and recall them on demand.</p><p>This is called the <strong>constant error carousel</strong> — they have something in memory, and at each time $t$, what is in memory is modified based on what is encountered in the input.</p><p>The Long Short-Term Memory neuron allows us to explicitly latch information to prevent decay and blowing up of gradients.</p><h2 id="gru"><span class="mr-2">GRU</span><a href="#gru" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h2 id="time-synchronous-recurrence"><span class="mr-2">Time-Synchronous Recurrence</span><a href="#time-synchronous-recurrence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2016.png" alt="image.png" data-proofer-ignore></p><p>They have one output corresponding to every input. They can be bidirectional as well.</p><p><strong>Assumption:</strong> Sequence divergence is the sum of the divergence at individual instants</p>\[Div(Y_{target}(1 ... T), Y(1 ... T)) = \Sigma_t Div(Y_{target}(t), Y(t))\] \[∇_{Y(t)} Div(Y_{target}(1 ... T), Y(1 ... T)) = ∇_{Y(t)} Div(Y_{target}(t), Y(t))\]<p>Typical divergence for classification:</p>\[Div(Y_{target}(t), Y(t)) = KL(Y_{target}(t), Y(t))\]<h3 id="variants"><span class="mr-2">Variants</span><a href="#variants" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>These are all sequence to sequence models, and we will soon transition into a separate heading to tackle these problems, specifically on training them.</p><p><strong>Many to One:</strong></p><p>Consider a many to one RNN. In such networks:</p><ul><li>Even though we only read the final output at the end of the sequence, every hidden state at each time step does generate an output internally.<li><p>These outputs are typically ignored during inference, unless you’re doing something like attention or intermediate supervision.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2017.png" alt="image.png" data-proofer-ignore></p></ul><p>During training, the divergence (loss) is only computed at the final step.</p><p>We define the divergence as the KL divergence between the target phoneme and the predicted output at the final time step:</p>\[DIV(Y_{\text{target}}, Y) = KL(Y(T), \text{Phoneme})\]<p>This approach assumes that the only useful information is at the final step. But intermediate states may have rich information too! So we’re wasting learning potential by ignoring outputs from intermediate inputs.</p><p>So the solution to this is to use all outputs generated during the forward pass — not just the final one — to compute divergence. That is, assume that each hidden state output should try to predict the same target phoneme. So instead of just computing divergence at $t=T$, we compute it for all $t \in {0,1,2}:$</p>\[DIV(Y_{\text{target}}, Y) = \sum_t w_t \cdot KL(Y(t), \text{Phoneme})\]<p>$w_t$ is a weighting factor for each time step. You can keep them equal, or weigh later steps more if needed.</p><p><strong>Many to One:</strong></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2018.png" alt="image.png" data-proofer-ignore></p><p>Objective: Given a sequence of inputs, asynchronously output a sequence of symbols.</p><p>This is just a simple concatenation of many copies of the simple “output at the end of the input sequence” model discussed above.</p><p>But during inference, the network is producing an output at every time and we need to figure out when to read the outputs because we are not given this information. This process of obtaining an output from the network is called <strong>decoding</strong>.</p><p>Solution to decoding:</p><p>Option 1:</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2019.png" alt="image.png" data-proofer-ignore></p><p>At each time, the network outputs a probability for each output class given all inputs until that time.</p>\[y_k^D = prob(s_4=D|X_0\ldots X_k)\]<p>Just like we do in any neural network with a softmax or logistic output, we select the class with the highest probability results. Using the same principle here, we find the most likely symbol sequence at each time given the inputs. Then we merge adjacent repeated symbols, and place the actual emission of the symbol in the final instant.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2020.png" alt="image.png" data-proofer-ignore></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre># Pseudo-code
# Assuming y(t,i), t = 1...T, i = 1...N is already computed using the underlying RNN
# N is the number of classes in output

n = 1
best(1) = argmax_i y(1, i)
for t = 1 to T:
    best(t) = argmax_i y(t, i)
    if best(t) != best(t-1):
        out(n) = best(t-1)  # stores the previous class (the segment that just ended)
        time(n) = t - 1     # stores the time it ended
        n = n + 1
</pre></table></code></div></div><p>But the problem with this is that we cannot distuinguish between an extended symbol and repetitions of symbols.</p><h1 id="sequence-to-sequence-models">Sequence-to-Sequence Models</h1><h2 id="training-1"><span class="mr-2">Training</span><a href="#training-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>During training, if we have the input sequences and also the output sequence, i.e., at what exact time step we have a label, then training becomes easy. We can then easily convert it into a time-synchronous problem and train the model.</p><p>But the problem arises when no timing information of the output is provided. Only the sequence of output symbols is provided for the training data, but there is no indication of which one occurs where. This becomes a problem of “training without alignment”, and can be solved in two ways:</p><ol><li>Guess the alignment — Viterbi algorithm<li>Consider all possible alignments — Connectionist Temporal Classification</ol><p><strong>Formally, we want to solve:</strong></p><p>Given an unaligned $K$ length compressed symbol sequence $S = s_0, \dots, s_{K-1}$, and an $N$ length input $(N\geq K)$, we want to find the most likely alignment:</p>\[\argmax_{s_0, \dots, s_{N-1}} \Pr(s_0, \dots, s_{N-1} \mid S, X)\quad \text{such that} \quad \text{compress}(s_0, \dots, s_{N-1}) = S\]<p>The expression inside the $\text{argmax}$ represents the model’s belief about the alignment. The constraint ensures that the sequence, once compressed, matches the given label sequence.</p><h3 id="important-points"><span class="mr-2"><strong>Important points:</strong></span><a href="#important-points" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Alignment tells us which portion of the input aligns with what symbol in the sequence<li>An order-synchronous symbol sequence that is shorter than the input can be “aligned” to the input by repeating symbols until the expanded sequence is exactly as long as the input<li>The “alignment” of an order-synchronous symbol sequence to an input is a time synchronous symbol sequence<li>A symbol sequence that is time-synchronous with an input can be compressed to a shorter order-synchronous input by eliminating repetitions of symbols<li>Order-synchronous symbol sequences that are shorter than the input are compressed symbol sequences</ul><h2 id="training-guessing-the-alignment-using-viterbi-algorithm"><span class="mr-2">Training: Guessing the Alignment using Viterbi Algorithm</span><a href="#training-guessing-the-alignment-using-viterbi-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>When the alignment is not provided, one approach is to guess it and refine it iteratively. Procedure:</p><ol><li>Initialize: Assign an initial alignment (can be random, heuristic-based, uniform, etc.)<li>Train: Train the network using the current alignment<li>Re-estimate: Update the alignment based on the model’s improved predictions<li>Repeat steps 2-3 until convergence</ol><p>When a model outputs a distribution over symbols at each time step, greedy decoding (i.e., taking the highest probability symbol at each step) can produce invalid outputs.</p><p>To tackle this, we prune the model’s output grid and retain only the rows corresponding to symbols that occur in the target sequence. This gives us a reduced output table, eliminating unwanted paths. This assures that we only hypothesize symbols that are valid, but it still doesn’t enforce their correct order.</p><p>To ensure that the output expands the target sequence exactly, we:</p><ol><li>Create one row per symbol in the output sequence — If a symbol repeats, create multiple rows<li>Arrange the rows in the desired order<li><p>Build a table such that at each time step, you have probabilities for that symbol</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2021.png" alt="image.png" data-proofer-ignore></p></ol><p>This ensures the alignment can only traverse through the correct symbol order. Once the output table is arranged, we model it as a graph:</p><ul><li>Rows represent the positions of target symbols<li>Columns represent time steps $t=0,1,\ldots,T-1$<li><p>Nodes contain the network’s output scores for the corresponding symbol at time $t$:</p>\[y_t^{s_i} = \text{Pr}(s_i|x_0,\ldots,x_t)\]<li>Edges have a score of $1$ and connect a node to:<ul><li>The next time step on the same row (symbol is held)<li>The next row (next symbol) at the next time step</ul></ul><p>We define valid paths to:</p><ul><li>Start at top-left<li>End at bottom-right<li>Monotonically move downward or forward in time</ul><p><strong>Graph constraints:</strong></p><p>To ensure valid decoding, we impose these:</p><ol><li>The first symbol must begin at the top-left node.<li>The last symbol must end at the bottom-right node.<li>All paths must be monotonic:<ul><li>You can stay on the same row (repeat symbol)<li>Or move downward to the next symbol (progress in output)<li>But you can’t go backward</ul></ol><p>This guarantees that the path represents a valid expansion of the target sequence.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2022.png" alt="image.png" data-proofer-ignore></p><p><strong>Scoring Paths:</strong></p><p>Each path through the graph represents a possible alignment. The score of a path is the product of the probabilities of its nodes:</p>\[\text{Score(Path)} = \prod_{(t, i) \in \text{Path}} y_t^{s_i}\]<p>This score represents the model’s confidence in that specific alignment.</p><p>To pick the most likely alignment given the input and given this graph, we need to pick the path that has the highest probability.</p><p>Using this, we can find the most probable path from source to sink using any dynamic programming algorithm, such as breadth-first search, Dijkstra’s, A*, etc. One such algorithm is the Viterbi algorithm.</p><h3 id="viterbi-algorithm"><span class="mr-2">Viterbi Algorithm</span><a href="#viterbi-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The basic idea is that the best path to any node must be an extension of the best path to one of its parent nodes. Any other path would necessarily have a lower probability.</p><p>The best parent is simply the parent with the best scoring best path.</p><p><strong>Algorithm:</strong></p><ul><li>Dynamically track the best path (and the score of the best path) from the source node to every node in the graph<ul><li>At each node, keep track of:<ul><li>The best incoming parent edge<li>The score of the best path from the source to the node through this best parent edge</ul></ul><li>Eventually compute the best path from source to sink</ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2023.png" alt="image.png" data-proofer-ignore></p><p><strong>Pseudocode:</strong></p><ul><li><p>Initialization</p>\[BP(0, i) = \text{null}, \quad i = 0, \ldots, K-1\] \[Bscr(0,0) = y_0^{S(0)}, \quad Bscr(0,i) = -\infty, \quad i = 1, \ldots, K-1\]<li><p>For $t = 1, \ldots, T-1$</p>\[BP(t,0) = 0, \quad Bscr(t,0) = Bscr(t-1,0) \times y_t^{S(0)}\]<ul><li><p>For $l = 1, \ldots, K-1$</p>\[BP(t,l) = \begin{cases} l-1, &amp; \text{if } Bscr(t-1,l-1) &gt; Bscr(t-1,l) \\ l, &amp; \text{otherwise} \end{cases}\] \[Bscr(t,l) = Bscr(BP(t,l)) \times y_t^{S(l)}\]</ul></ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2024.png" alt="image.png" data-proofer-ignore></p><ul><li>$s(T-1) = s(K-1)$<li>for $t=T-1:1$<ul><li>$s(t-1) = BP(s(t))$</ul></ul><p><strong>Loss:</strong></p>\[DIV = \sum_t KL(Y_t, symbol_t^{bestpath}) = -\sum_t \log Y(t, symbol_t^{bestpath})\] \[\nabla_{Y_t} DIV = \begin{bmatrix} 0 &amp; 0 &amp; \cdots &amp; \frac{-1}{Y(t, symbol_t^{bestpath})} &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix}\]<p>The gradient is zero everywhere except at the component corresponding to the target in the estimated alignment. This shows that only the probabilities along the best path (red boxes) receive gradient updates.</p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2025.png" alt="image.png" data-proofer-ignore></p><p><strong>Problem with Iterative Update:</strong></p><ul><li>Approach is heavily dependent on initial alignment<li>Prone to poor local optima<li>The process works well if we have large amounts of training data</ul><p>Alternate solution: Do not commit to an alignment during any pass</p><h2 id="training-connectionist-temporal-classification"><span class="mr-2">Training: Connectionist Temporal Classification</span><a href="#training-connectionist-temporal-classification" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Instead of only selecting the most likely alignment, we use the statistical expectation over all possible alignments.</p>\[DIV = \mathbb{E} \left[ -\sum_t \log Y(t, s_t) \right]\]<p>We want to train a model that outputs, at each frame $t$, a probability distribution $Y_t(.)$ over symbols.</p><p>We are given an unaligned label sequence $\mathbf{S} = S_0, S_1, …, S_{K-1}$ and an input frame sequence $\mathbf{X} = X_0, X_1, …, X_{N-1}$.</p><p>Because alignment is unknown, there are many ways to align $\mathbf{S}$ to the frames. Instead of picking one alignment (best path), we average over all valid alignments. Concretely, we need the probability that a particular symbol $s$ is aligned to frame $t$, given $\mathbf{S,X}$. That probability is:</p>\[P(s_t=S|\mathbf{S},\mathbf{X})\]<p>Another way of explaining this is that this is the posterior (conditional) probability that the model is aligned with symbol $S$ at time $t$, given that we already know the model should produce the overall label sequence $\mathbf{S}$, and we have the input $\mathbf{X}$. Or, given that the model correctly produces the target sequence $\mathbf{S}$, what’s the chance that, at frame $t$, it’s currently aligned to symbol $S$.</p><p><strong>From expectation to a computable loss:</strong></p><p>Start from the expectation over alignments:</p>\[DIV = \mathbb{E} \left[ -\sum_t \log Y(t, s_t) \right]\]<p>This expectation can be written using the frame-label posterior $P(s_t=S|\mathbf{S},\mathbf{X})$:</p>\[DIV = -\sum_{t}\sum_{s\in{S_0,\dots,S_{K-1}}} P(s_t=S\mid \mathbf{S},\mathbf{X})\log Y(t,s_t={S})\]<p>So training reduces to two things:</p><ol><li>compute the posteriors $P(s_t=S|\mathbf{S},\mathbf{X})$ for every $t,s$<li>take a weighted cross-entropy using those posteriors as targets</ol><p><strong>How do we compute $P(s_t=S|\mathbf{S},\mathbf{X})$?</strong></p>\[P(s_t=S|\mathbf{S_r},\mathbf{X}) \propto P(s_t=S_r, \mathbf{S}|\mathbf{X})\]<p>$P(s_t=S_r, \mathbf{S}|\mathbf{X})$ is the joint probability that:</p><ol><li>At time $t$, the model is aligned with symbol $S_r$<li>and over the entire sequence, the model emits (or aligns to) the full target label sequence $\mathbf{S}$</ol><p>Intuitively, this answers what is the total probability that, while generating the entire target sequence $\mathbf{S}$, the model goes through symbol $S_r$ at time $t$.</p><p>To compute this efficiently, we split the trellis into two parts:</p>\[P(s_t = S_r, S \mid X) = P(S_0, ..., S_r, s_t = S_r \mid X) \times P(S_{t+1}, ..., S_{K-1} \mid S_0, ..., S_r, s_t = S_r, X)\]<p>The first path</p>\[P(S_0, ..., S_r, s_t = S_r \mid X)\]<p>represents the probability of all partial paths that lead up to state $S_r$ at time $t$. This is the forward probability, denoted by $\alpha_t$.</p>\[\alpha(t, r) = P(S_0, S_1, ..., S_r, s_t = S_r \mid X)\]<p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2026.png" alt="image.png" data-proofer-ignore></p><p>This is the total probability of all valid paths that align the prefix of the target sequence up to symbol $S_r$ to the first $t$ input frames, and that end exactly at symbol $S_r$ at time $t$.</p>\[\alpha(t, r) = \sum_{q : S_q \in pred(S_r)} \alpha(t-1, q) Y_t^{S(r)}\]<ul><li>$pred(S_r)$ is the set of symbols that are allowed to transition into $S_r$. It usually includes itself (stay transition) and the previous symbol (advance transition)<li>$q$ is its row index<li>$Y_t^{S(r)}$ is the network’s predicted probability of symbol $S_r$ at time $t$<li>$\alpha(t-1,q)$ is the total probability of reaching predecessor $S_q$ at the previous frame</ul><p>The second part</p>\[P(S_{t+1}, ..., S_{K-1} \mid S_0, ..., S_r, s_t = S_r, X)\]<p>represents the probability of all possible continuations after time $t$ that follow valid transitions and end in the final symbol. This is the backward probability, denoted by $\beta_t$.</p>\[\beta(t, r) = P(\text{rest of the path from } (t, r) \text{ to the end} \mid X)\]<p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2027.png" alt="image.png" data-proofer-ignore></p><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2028.png" alt="image.png" data-proofer-ignore></p><p>$\beta(t,r)$ is the total probability of all valid paths that start after time $t$, given that you are currently at state $S_r$ at time $t$, and that follow legal transitions until the sequence ends.</p><p>$\beta(t,r)$ is the probability of the exposed subgraph, not including the orange shaded box. For convenience, let us include the box in the graph, and factor it out later.</p><p>$\hat{\beta}(t,r)$ is the probability of graph including node at $(t,r)$.</p>\[\beta(t, r) = \frac{1}{y_t^{S_r}} \hat{\beta}(t, r)\] \[\hat{\beta}(t, r) = y_t^{S_r} \sum_{q \in succ(r)} \hat{\beta}(t+1, q)\]<ul><li>$succ(r)$ is the set of successor states that can follow $S_r$<li>$y_t^{S_r}$ emission probability of symbol $S_r$ at time $t$</ul>\[\beta(t, r) = \frac{\hat{\beta}(t, r)}{y_t^{S_r}}\]<p>To get the posterior probability (the fraction of total path probability passing through this node):</p>\[P(s_t = S_r \mid S, X) = \frac{\alpha(t,r)\beta(t,r)}{\sum_j \alpha(t,j)\beta(t,j)}\]<p>Finally, putting all this back into the expected divergence</p>\[DIV = -\sum_{t}\sum_{s\in{S_0,\dots,S_{K-1}}} P(s_t=S\mid \mathbf{S},\mathbf{X})\log Y(t,s_t={S})\]<h3 id="forward-algorithm"><span class="mr-2">Forward Algorithm</span><a href="#forward-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li><p>Initialization</p>\[\alpha(0,0) = y_0^{S(0)}, \quad \alpha(0,r) = 0, r &gt; 0\]<li><p>$\text{for } t = 1, \ldots, T-1$:</p>\[\alpha(t,0) = \alpha(t-1,0) y_t^{S(0)}\]<ul><li><p>$\text{for } l = 1, \ldots, K-1$:</p>\[\alpha(t,l) = \big(\alpha(t-1,l) + \alpha(t-1,l-1)\big) y_t^{S(l)}\]</ul></ul><h3 id="backward-algorithm"><span class="mr-2">Backward Algorithm</span><a href="#backward-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li><p>Initialization</p>\[\hat{\beta}(T-1, K-1) = y_{T-1}^{S(K-1)}, \quad \hat{\beta}(T-1, r) = 0, r &lt; K-1\]<li><p>$\text{for } t = T-2 \text{ downto } 0$</p><ul><li><p>$\text{for } r = K-1 \text{ downto } 0$</p>\[\hat{\beta}(t, r) = y_t^{S(r)} \sum_{q \in succ(r)} \hat{\beta}(t+1, q)\] \[\beta(t, r) = \frac{1}{y_t^{S(r)}} \hat{\beta}(t, r)\]</ul></ul><h3 id="tackling-repetitions"><span class="mr-2">Tackling Repetitions</span><a href="#tackling-repetitions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h3 id="inference-1"><span class="mr-2">Inference</span><a href="#inference-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h1 id="language-models">Language Models</h1><p>Language Models (LMs) model the probability distribution of token sequences in a language. For example, word sequences, if words are the tokens.</p><p>LMs can be used to:</p><ul><li>Compute the probability of a given token sequence<li>Generate new sequences from the learned distribution of the language</ul><p>A language model assigns a probability to an entire sequence of tokens (words, subwords, characters). If your sentence is $(w_1,w_2,w_3,…)$, the LM’s job is to say how likely that whole sequence is. Because directly modeling $P(w_1,w_2,…,w_T)$ is hard, we use the chain rule of probability to factor it into conditional next-token probabilities:</p>\[P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P\left(w_t \mid w_1,\ldots,w_{t-1}\right)\]<h2 id="representing-words"><span class="mr-2">Representing Words</span><a href="#representing-words" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Words are represented as one-hot vectors. We pre-specify a vocabulary $V$ of $N$ words in a fixed (e.g., lexical) order. Example: <code class="language-plaintext highlighter-rouge">V = [ A, AARDVARK, AARON, ABACK, ABACUS, …, ZZYP ]</code></p><p>Each word $w \in V$ is mapped to a vector $\mathbf{e}_w \in \mathbf{R}^N$ with all zeros except a single 1 at that word’s index. If “ARRON” is index 3,</p>\[\mathbf{e}_{ARRON} = [0, 0, 1, 0, ..., 0]\]<p>Characters can be encoded the same way with a smaller $N (\approx 100$ for English if you include case, punctuation, digits, symbols, and space).</p><p><strong>Why use one-hot representation?</strong></p><ul><li>It makes no assumptions about the relative importance of words. All word vectors are the same length<li>It makes no assumptions about the relationships between words<li>The distance between every pair of words is the same</ul><p><strong>Why one-hot is not enough?</strong></p><ol><li>Sparsity &amp; dimensionality<ul><li>Vectors are length $N$ and almost entirely zero.<li>Memory &amp; compute scale with $N$. With a 100k-word vocab, each vector is 100k-dimensional.<li>It uses only $N$ corners of the $2^N$ corners of a unit cube.</ul><li>No notion of similarity<ul><li>Any two distinct one-hots have cosine similarity $0$ and Euclidean distance $\sqrt{2}$. “cat” and “dog” are as dissimilar as “cat” and “banana.”<li>So the model must learn from scratch that some words are related.</ul><li>UNK problem<ul><li>It is not possible to include every possible word in your vocabulary $V$.<li>You fix $V$(say, 50 000 most frequent tokens). Anything not in $V$, such as a new name, misspelling, slang, or foreign word, is replaced by a special <strong>[UNK]</strong> token (“unknown”).</ul></ol><h3 id="solution-to-the-dimensonality-problem-embeddings"><span class="mr-2">Solution to the dimensonality problem: Embeddings</span><a href="#solution-to-the-dimensonality-problem-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The goal is to reduce the dimensonality of one-hot vectors while preserving semantic relationships among words.</p><p>Each work $W$ (a one-hot vector) is projected into a smaller, dense vector using a learned projection matrix $P$:</p>\[W \rightarrow P W\]<p>Here,</p><ul><li>$W$ is the one-hot vector of dimension $N$<li>$P$ is the projection matrix of size $M \times N$, where $M « N$<li>$PW$ is the new dense vector representation (embedding) of dimension $M$</ul><p><strong>Intuition:</strong></p><ul><li>This is a linear transformation into a lower-dimensional space<li>Each original axis (word) is now represented as a point in this smaller subspace<li>If trained properly, distances between projected points will correspond to semantic similarity between words</ul><p><strong>Benefits of projection:</strong></p><ul><li>Reduces computational complexity drastically<li>Converts sparse high-dimensional one-hots into dense, meaningful embeddings<li>Word similarity (e.g., “cat” and “dog”) becomes measurable via cosine similarity or Euclidean distance</ul><p>Embeddings, however, still don’t enable you to find representations for words that are not part of our training vocabulary.</p><p>Each word’s one-hot vector $W_i$ is multiplied by $P$ to get its embedding:</p>\[PW_i = \text{embedding of word } W_i\]<p>Then, a function $f(.)$ predicts the next word based on the embeddings of the previous words:</p>\[W_n = f(PW_0, PW_1, ..., PW_{n-1})\]<ul><li>$P$ is shared across all words — it’s a single learned matrix<li>The same transformation is applied to every word’s one-hot vector<li>$f$ can be a neural network that learns to combine embeddings from context words<li>$P$ is updated during training, so embeddings gradually become semantically meaningful</ul><p>Mathematically, $P$ acts as a linear layer with $M$ outputs and $N$ inputs:</p>\[\text{Embedding} = P W = E[w]\]<p>This can be implemented as:</p><ul><li>A linear transformation shared across all inputs, or<li>A lookup operation in an embedding table (each row is one projected vector).</ul><h2 id="beginnning-and-end-of-sentences"><span class="mr-2">Beginnning and End of Sentences</span><a href="#beginnning-and-end-of-sentences" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>A sequence of words by itself does not indicate if it is a complete sentence or not. To make it explicit, we will add two additional symbols (in addition to the words) to the base vocabulary.</p><ul><li><code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> Indicates start of a sentence<li><code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> Indicates end of a sentence</ul><p>In situations where the start of sequence is obvious, the <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> may not be needed, but <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> is required to terminate sequences.</p><p>So when predicting the next token, we feed the drawn word as the next word in the series by sampling from the output probability distribution. We continue this process until we draw an <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>, or if we decide to terminate generation based on some other criterion.</p><h2 id="delayed-sequence-to-sequence-models"><span class="mr-2">Delayed Sequence to Sequence Models</span><a href="#delayed-sequence-to-sequence-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2029.png" alt="image.png" data-proofer-ignore></p><p><strong>Pseudocode:</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre># --- Step 1: Encode the input sequence ---
# First run the inputs through the network  
# Assuming h(-1,1) is available for all layers  

for t = 0 : T-1                      # Including both ends of the index  
    [h(t), ...] = RNN_input_step(x(t), h(t-1), ...)  
H = h(T-1)                            # Final hidden state after reading all inputs  

# --- Step 2: Generate the output sequence ---
# Now generate the output y_out(1), y_out(2), ...  

t = 0  
h_out(0) = H  

do  
    t = t + 1  
    [y(t), h_out(t)] = RNN_output_step(h_out(t-1))  
    Y_out(t) = draw_word_from(y(t))  
until Y_out(t) == &lt;eos&gt;  

# --- Explanation ---
# x(t)        : input at time step t (e.g., word embedding)
# h(t)        : hidden state carrying memory of previous inputs
# H           : final hidden state summarizing the input sequence
# RNN_input_step()  : recurrence for encoding
# RNN_output_step() : recurrence for decoding
# draw_word_from()  : selects the most likely or sampled word
# &lt;eos&gt;       : end-of-sequence token signaling generation stop

</pre></table></code></div></div><ul><li>Encoder network:<ul><li>The input sequence feeds into a recurrent structure<li>The input sequence is terminated by an explicit <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> symbol<li>The hidden activation at the <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> “stores” all information about the sentence</ul><li>Decoder network:<ul><li>Subsequently a second RNN uses the hidden activation as initial state, and <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> as initial symbol, to produce a sequence of outputs<li>The output at each time becomes the input at the next time<li>Output production continues until an <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> is produced</ul></ul><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2030.png" alt="image.png" data-proofer-ignore></p><p>The one-hot word representations can also be compressed via embeddings. These embeddings will be learned along with the rest of the network.</p><p>At each time $k$ the network produces a probability distribution over the output vocabulary</p>\[y_k^w = P(O_k=w | O_{k-1}, ..., O_1, I_1, ..., I_N)\]<p>At each time a word is drawn from the output distribution and is provided as an input to the next time. Over the entire predicted sequence, we compute the total probability as</p>\[P(O_1, ..., O_L \mid I_1, ..., I_N) = P(O_1 \mid I_{1}, ..., I_N) P(O_2 \mid O_1, I_1, ..., I_N) P(O_3 \mid O_1, O_2, I_1, ..., I_N) ... P(O_L \mid O_1, ..., O_{L-1}, I_1, ..., I_N) = y_1^{O_1} , y_2^{O_2} , ... , y_L^{O_L}\]<h3 id="picking-the-next-token-given-the-probability-distribution"><span class="mr-2">Picking the next token given the probability distribution</span><a href="#picking-the-next-token-given-the-probability-distribution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The model gives a distribution $y_t$ at each time step $t$. The ideal decoder would pick the sequence</p>\[\arg\max_{o_{1:L}} \prod_{t=1}^{L} y_t^{(o_t)}\]<p>i.e., the most probable sequence as a whole, not just the best token at each step. That’s hard to search exactly, so we approximate.</p><p><strong>Greedy decoding:</strong></p><p>At each step, choose the token with the highest probability in $y_t$, feed it back, and repeat until <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>.</p><p>Why this often fails?</p><ul><li>Myopia: A locally best token can push the model into a worse future. Example: choosing “he” at $t=0$ might make “nose” at $t=1$ more likely than a better global path (“knows” later).<li>Distribution shaping: The token you pick changes the next distribution. Sometimes picking a slightly less likely token now makes the next distribution much “peakier,” yielding a higher overall product of probabilities.<li>Error compounding: One early poor choice can lock you into a suboptimal continuation.</ul><p><strong>Sampling:</strong></p><p>At each step, sample a token from the full distribution $y_t$. It introduces diversity and can escape greedy’s local traps. It also sometimes stochastically finds better (higher-probability) sequences than greedy.</p><p>Why it also fails?</p><ul><li>Unreliable: You may sample low-probability “junk” early, which can derail future steps<li>High variance: Two runs can differ wildly; average quality can drop without constraints.</ul><p><strong>What is actually used in practice?</strong></p><ol><li>Beam search: Keep the top-$B$ partial sequences at each step (by cumulative log-probability), expand each, prune again.<li>Top-k sampling: At each step, restrict to the $k$ most probable tokens (renormalize), then sample.<li>Nucleus (Top-p) Sampling: Choose the smallest set of tokens whose cumulative probability $\ge p$, renormalize, sample.</ol><h3 id="beam-search"><span class="mr-2">Beam Search</span><a href="#beam-search" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2031.png" alt="image.png" data-proofer-ignore></p><p>At each time step, the model can produce many possible next words. If we try to explore all of them, the search tree explodes exponentially (like “He knows…” vs. “The nose…”). So, the idea is to prune — keep only the top few promising paths instead of exploring everything.</p><p>We compute the cumulative probability for each partial sequence up to time $t$:</p>\[P(O_1, ..., O_t \mid I_1, ..., I_N) = \prod_{n=1}^{t} P(O_n \mid O_{1:n-1}, I_1, ..., I_N)\]<p>At every time step, we:</p><ol><li>Expand each of the current candidate sequences by all possible next words.<li>Compute their cumulative probabilities (product of conditional probabilities).<li>Retain only the top K candidates (where K is called the beam width).</ol><p>The search continues until one of the active hypotheses produces an <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> token. We can employ one of two strategies:</p><ol><li>Early termination: Stop as soon as the most likely overall path ends with <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>. This is used for fast inference.<li>N-best search: Continue expanding other beams even after some reach <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> to collect multiple high-probability completions. This is used in translation or summarization tasks.</ol><p>Once a path emits the <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> token, it cannot be extended further — it’s a completed sequence. Different beams may terminate at different lengths. The final output is the sequence ending with <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> that has the highest cumulative probability across all terminated beams.</p><p><strong>Pseudocode:</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
</pre><td class="rouge-code"><pre># Assuming encoder output H is available
path = &lt;sos&gt;
beam = {path}
pathscore = [path] = 1
state[path] = h[0]   # Output of encoder

do  # Step forward
    nextbeam = {}
    nextpathscore = {}
    nextstate = {}

    for path in beam:
        cfin = path[end]
        hpath = state[path]
        [y, h] = RNN_output_step(hpath, cfin)

        for c in Symbolset:
            newpath = path + c
            nextstate[newpath] = h
            nextpathscore[newpath] = pathscore[path] * y[c]
            nextbeam += newpath   # Set addition
        end
		end
    beam, pathscore, state, bestpath = prune(nextstate, nextpathscore, nextbeam, bw)
until bestpath[end] == &lt;eos&gt;

function prune(state, score, beam, beamwidth):
    sortedscore = sort(score)
    threshold = sortedscore[beamwidth]

    prunedstate = {}
    prunedscore = {}
    prunedbeam = {}

    bestscore = -inf
    bestpath = None

    for path in beam:
        if score[path] &gt; threshold:
            prunedbeam += path     # Set addition
            prunedstate[path] = state[path]
            prunedscore[path] = score[path]

            if score[path] &gt; bestscore:
                bestscore = score[path]
                bestpath = path
            end
        end
    end
    return prunedbeam, prunedscore, prunedstate, bestpath
</pre></table></code></div></div><h2 id="training-2"><span class="mr-2">Training</span><a href="#training-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="forward-pass-3"><span class="mr-2">Forward Pass</span><a href="#forward-pass-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2032.png" alt="image.png" data-proofer-ignore></p><p>Input the source and target sequences, sequentially. The output will be a probability distributon over target symbol set (vocabulary).</p><h3 id="backward-pass"><span class="mr-2">Backward Pass</span><a href="#backward-pass" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/images/IDL/Introduction%20to%20Deep%20Learning%20CNNs,%20RNNs%20&amp;%20Languag/image%2033.png" alt="image.png" data-proofer-ignore></p><p>We compute the divergence between the output distribution and target word sequence. Then we backpropagate the derivatives of the divergence through the network to learn.</p><p>In practice, if we apply SGD, we may randomly sample words from the output to actually use for the backprop and update.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog/'>Blog</a>, <a href='/categories/robotics/'>Robotics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/learning/" class="post-tag no-text-decoration" >learning</a> <a href="/tags/nnets/" class="post-tag no-text-decoration" >nnets</a> <a href="/tags/cnn/" class="post-tag no-text-decoration" >cnn</a> <a href="/tags/rnn/" class="post-tag no-text-decoration" >rnn</a> <a href="/tags/sq2sq/" class="post-tag no-text-decoration" >sq2sq</a> <a href="/tags/attention/" class="post-tag no-text-decoration" >attention</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+CNNs%2C+RNNs%2C+%26+Language+Models+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-cnn-rnn-lang%2F" data-toggle="tooltip" data-placement="top" title="Instagram" target="_blank" rel="noopener" aria-label="Instagram"> <i class="fa-fw fab fa-instagram"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep+Learning+-+CNNs%2C+RNNs%2C+%26+Language+Models+-+Bhaswanth+Ayapilla&u=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-cnn-rnn-lang%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-cnn-rnn-lang%2F&text=Deep+Learning+-+CNNs%2C+RNNs%2C+%26+Language+Models+-+Bhaswanth+Ayapilla" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbhaswanth-a.github.io%2F%2Fposts%2Fdeep-learning-cnn-rnn-lang%2F" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/intro-to-rl/">Introduction to Reinforcement Learning</a><li><a href="/posts/reinforcement-learning/">Reinforcement Learning</a><li><a href="/posts/deep-rl/">Deep Reinforcement Learning</a><li><a href="/posts/lunar-roadster-cmu/">Lunar ROADSTER</a><li><a href="/posts/cmu-blog/">Coursework at CMU</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deep-learning-advanced/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - Attention & Transformers</h3><div class="text-muted small"><p> Attention Models Problem with vanilla Seq2Seq Models In the vanilla sequence-to-sequence (Seq2Seq) model with an encoder–decoder setup: The encoder reads the entire input sequence (e.g., I ...</p></div></div></a></div><div class="card"> <a href="/posts/deep-learning-perceptrons/"><div class="card-body"> <em class="small" data-ts="1749484800" data-df="ll" > Jun 9, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning - Perceptrons</h3><div class="text-muted small"><p> Neural Networks Depth - length of longest path from source to sink Layer - Set of all neurons which are all at the same depth with respect to the source Gradient For a scalar function $f(x)$ wit...</p></div></div></a></div><div class="card"> <a href="/posts/mmml/"><div class="card-body"> <em class="small" data-ts="1768838400" data-df="ll" > Jan 19, 2026 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-Modal Machine Learning</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/deep-learning-perceptrons/" class="btn btn-outline-primary" prompt="Older"><p>Deep Learning - Perceptrons</p></a> <a href="/posts/deep-learning/" class="btn btn-outline-primary" prompt="Newer"><p>Deep Learning</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "Bhaswanth-A/bhaswanth-a.github.io", "data-repo-id": "R_kgDOHu5z_w", "data-category": "General", "data-category-id": "DIC_kwDOHu5z_84C0yVx", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/Bhaswanth-A">Bhaswanth Ayapilla</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/learning/">learning</a> <a class="post-tag" href="/tags/nnets/">nnets</a> <a class="post-tag" href="/tags/rl/">rl</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/arduino/">arduino</a> <a class="post-tag" href="/tags/computer-vision/">computer vision</a> <a class="post-tag" href="/tags/control/">control</a> <a class="post-tag" href="/tags/electronics/">electronics</a> <a class="post-tag" href="/tags/manipulators/">manipulators</a> <a class="post-tag" href="/tags/ml/">ml</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-CJ97GH1VYR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-CJ97GH1VYR'); }); </script>
