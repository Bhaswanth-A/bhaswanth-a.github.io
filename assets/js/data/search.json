[ { "title": "Advanced Deep Reinforcement Learning", "url": "/posts/advanced-deep-rl/", "categories": "Blog, Robotics", "tags": "learning, rl", "date": "2026-02-14 11:00:00 -0500", "snippet": "In Progress" }, { "title": "Deep Reinforcement Learning", "url": "/posts/deep-rl/", "categories": "Blog, Robotics", "tags": "learning, rl", "date": "2026-02-09 11:00:00 -0500", "snippet": "Finite Markov Decision ProcessA Finite Markov Decision Process (MDP) is a mathematical framework used to model sequential decision-making problems where an agent interacts with an environment over time. It is defined as a tuple $(S, A, p, r, \\gamma)$, where each component captures one aspect of this interaction. The state space $S$ is a finite set of all possible situations the agent can be in. The action space $A$ is a finite set of choices available to the agent. The transition function $p$ describes how the environment evolves: given the current state and action, it specifies the probability of transitioning to the next state. The reward function $r$ assigns a scalar feedback signal that tells the agent how desirable a state‚Äìaction pair is. The discount factor $\\gamma \\in [0,1]$ controls how much the agent values future rewards relative to immediate ones.Agent, Policy, Model, and PlanningAn agent is the decision-making entity. It perceives the environment through sensors, acts through actuators, and has goals it wants to achieve. A policy is the agent‚Äôs behavior: it specifies what action to take in each state. Formally, a policy is a probability distribution over actions given states.\\[\\pi(a \\mid s) = \\Pr(A_t = a \\mid S_t = s), \\forall t\\]A model describes how the environment works ‚Äî it predicts the next state and reward given the current state and action. Planning means using this model to look ahead into the future and choose actions that achieve a goal. A plan is simply a sequence of actions. In reinforcement learning, the agent often does not know the model in advance and must learn through interaction.The goal of reinforcement learning is to learn a policy. Policies are typically assumed to be stationary, meaning the same policy is used at every time step. During learning, the agent updates its policy based on experience.Markovian States (Why History Can Be Thrown Away)A state is called Markovian if it contains all the information needed to predict the future. This is known as the Markov property: given the current state and action, the future is independent of the past. Intuitively, this means the state is a sufficient summary of everything that has happened so far.\\[\\Pr(R_{t+1}, S_{t+1} \\mid S_0, A_0, \\dots, S_t, A_t)=\\Pr(R_{t+1}, S_{t+1} \\mid S_t, A_t)\\]Rewards and ReturnsRewards are scalar signals provided by the environment that indicate progress toward a goal. Importantly, rewards define what the agent should accomplish, not how to do it. Almost all goal-directed behavior can be framed as maximizing the expected cumulative reward.The reward function is often written as an expectation:\\[r(s, a) = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\\]In episodic tasks, interaction naturally breaks into episodes with a clear end, such as a game or navigating a maze. The return is the total reward accumulated until the terminal state:\\[G_t = R_{t+1} + R_{t+2} + \\dots + R_T\\]Here, $T$ is the final time step of the episode. The agent‚Äôs objective is to maximize this return.In continuing tasks, there is no natural episode boundary (e.g., robot control or stock trading). In this case, we use discounted returns to ensure the total reward remains finite and to prioritize near-term outcomes:\\[G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\]The discount factor $\\gamma$ determines how farsighted the agent is.Value-Based vs Policy-Based Reinforcement LearningValue-based methods focus on learning a value function: a function that tells us how good a state or state-action pair is in terms of expected future reward. Once this value function is learned, the policy is not learned explicitly; instead, it is implicit, typically derived by choosing the action with the highest value (for example, using $\\epsilon-$greedy exploration). Algorithms like Q-learning fall into this category.Policy-based methods flip this idea around. Instead of learning values and deriving actions from them, the agent directly learns the policy itself, which is a mapping from states to actions or action distributions. There is no requirement to maintain or estimate a value function. The policy parameters are adjusted so that actions sampled from the policy lead to higher long-term reward. This makes policy-based methods especially natural for continuous action spaces, where choosing an action by ‚Äúargmax over values‚Äù is awkward or ill-defined.Actor-Critic methods sit exactly at the intersection of these two ideas. They learn both a policy (the actor) and a value function (the critic). The critic evaluates how good the actor‚Äôs actions are, and this feedback is used to update the policy more efficiently and with lower variance.Policy-Based RLPolicy-based reinforcement learning can be cleanly framed as a direct optimization problem. We define an objective function $U(\\theta)$, which measures how good a policy parameterized by $\\theta$ is. This is typically the expected cumulative reward when following that policy.Learning then proceeds by iteratively improving the policy parameters. At each iteration, we first roll out the current policy in the environment to collect trajectories (sequences of states and actions generated by following the policy). Using these trajectories, we estimate how changing the policy parameters would affect the objective. Finally, we update the parameters in the direction that increases the objective. Intuitively, this is just like standard gradient ascent.\\[\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\alpha \\nabla_\\theta U(\\theta)\\]Why are they useful?One major advantage of policy-based methods is that they handle high-dimensional and continuous action spaces naturally. Instead of evaluating many possible actions and selecting the best one, the policy directly outputs an action (or a distribution over actions). This is especially important in robotics, where actions are often continuous (e.g., torques, velocities, steering angles).Another key benefit is the ability to learn stochastic policies. Rather than committing to a single action in each state, the policy can represent uncertainty or intentional randomness, which is often crucial for exploration or for modeling multimodal behaviors.REINFORCE (or Monte Carlo Policy Gradient)In policy-based reinforcement learning, the agent does not try to estimate values first and then derive a policy. Instead, it directly optimizes the policy itself. To do this, we need to define a clear objective that tells us how good a policy is.A trajectory $\\tau$ is one full rollout of the agent interacting with the environment. It is simply a sequence of states and actions:\\[\\tau = (s_0, a_0, s_1, a_1, \\dots, s_H, a_H)\\]where $H$ is the horizon (episode length). Intuitively, a trajectory is ‚Äúone attempt‚Äù by the agent to solve the task from start to finish.Since the environment is stochastic (and the policy may be stochastic too), the agent does not experience the same trajectory every time. Instead, trajectories are sampled from a probability distribution induced by the policy. Therefore, a reasonable objective is to maximize the expected trajectory reward:üí°$$U(\\theta) = \\mathbb{E}_{\\tau \\sim P(\\tau; \\theta)}[R(\\tau)]$$Intuition: ‚ÄúOn average, how much reward does my policy get when I deploy it in the environment?‚ÄùThe expectation can also be written explicitly as a sum over all possible trajectories:\\[U(\\theta) = \\sum_{\\tau} P(\\tau; \\theta)\\, R(\\tau)\\]Probability of a Trajectory:A trajectory is generated by two components: Environment dynamics: how states evolve Policy: how actions are chosenThe probability of a trajectory factorizes as:üí°$$P(\\tau; \\theta) = \\prod_{t=0}^{H} P(s_{t+1} \\mid s_t, a_t)\\, \\pi_\\theta(a_t \\mid s_t)$$ The dynamics term $P(s_{t+1} \\mid s_t,a_t)$ is fixed and unknown The policy term $\\pi_{\\theta}(a_t \\mid s_t)$ is what we control and parameterizeDerivationOur goal is not to compute the objective itself, but to compute its gradient with respect to the policy parameters:\\[\\nabla_\\theta U(\\theta)= \\nabla_\\theta \\mathbb{E}_{\\tau \\sim P(\\tau; \\theta)}[R(\\tau)]\\]\\[\\nabla_\\theta U(\\theta)= \\sum_{\\tau} \\nabla_\\theta P(\\tau; \\theta)\\, R(\\tau)\\]Now apply the log-derivative trick:\\[\\nabla_\\theta P(\\tau; \\theta)= P(\\tau; \\theta)\\, \\nabla_\\theta \\log P(\\tau; \\theta)\\]Substitute:\\[\\nabla_\\theta U(\\theta)= \\sum_{\\tau} P(\\tau; \\theta)\\, \\nabla_\\theta \\log P(\\tau; \\theta)\\, R(\\tau)\\]This can be written as an expectation:\\[\\nabla_\\theta U(\\theta)= \\mathbb{E}_{\\tau \\sim P(\\tau; \\theta)}\\left[ \\nabla_\\theta \\log P(\\tau; \\theta)\\, R(\\tau) \\right]\\]Recall the trajectory probability:\\[\\log P(\\tau; \\theta)= \\sum_{t=0}^{H} \\log P(s_{t+1} \\mid s_t, a_t)+ \\sum_{t=0}^{H} \\log \\pi_\\theta(a_t \\mid s_t)\\]The environment dynamics do not depend on $\\theta$, so their gradient is zero. This leaves:\\[\\nabla_\\theta \\log P(\\tau; \\theta)= \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\]Substitute back into the gradient:üí°$$\\nabla_\\theta U(\\theta)= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^{H}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, R(\\tau)\\right]$$Using Monte-Carlo sampling with $N$ trajectories:üí°$$\\nabla_\\theta U(\\theta)\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=0}^{H}\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\,R(\\tau^{(i)})$$Intuition:Each term $\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})$ asks: ‚ÄúHow should I change the policy parameters to make this action more likely in this state?‚ÄùThat change is scaled by the total reward $R(\\tau)$: If the trajectory was good $\\rightarrow$ reinforce the actions If the trajectory was bad $\\rightarrow$ suppress the actionsSo learning boils down to: Increase the probability of actions that led to good outcomes, decrease the probability of actions that led to bad ones.Assigning the full trajectory return to every action is wasteful. An action at time $t$ cannot affect rewards that happened before it. So we define the return from time $t$ onward:\\[G_t = \\sum_{k=t}^{H} R(s_k, a_k)\\]Replace $R(\\tau)$ with $G_t$:üí°$$\\nabla_\\theta U(\\theta)\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=0}^{H}\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\,G_t^{(i)}$$Gaussian PolicyAssume a continuous action space. The policy outputs the parameters of a Gaussian:\\[\\pi_\\theta(a \\mid s) = \\mathcal{N}\\big(a \\mid \\mu_\\theta(s), \\sigma^2\\big)\\]To keep things simple: Mean $\\mu_\\theta(s)$ depends on $\\theta$ Variance $\\sigma^2$ is fixedSo at state $s_t$: The network outputs $\\mu_\\theta(s_t)$ The action is sampled:\\[a_t \\sim \\mathcal{N}(\\mu_\\theta(s_t), \\sigma^2)\\] For a 1D Gaussian:\\[\\log \\pi_\\theta(a_t \\mid s_t)= -\\frac{1}{2\\sigma^2}(a_t - \\mu_\\theta(s_t))^2 + C\\]We ignore constants because they vanish under gradients.\\[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)= \\frac{(a_t - \\mu_\\theta(s_t))}{\\sigma^2}\\nabla_\\theta \\mu_\\theta(s_t)\\]For a 2D Gaussian:The policy outputs a mean vector:\\[\\mu_\\theta(s)=\\begin{bmatrix}\\mu_1(s) \\\\\\mu_2(s)\\end{bmatrix}\\]We assume a Gaussian policy with fixed covariance:\\[\\pi_\\theta(a \\mid s)=\\mathcal{N}\\big(a \\mid \\mu_\\theta(s), \\Sigma\\big)\\]\\[a \\sim \\mathcal{N}(\\mu_\\theta(s), \\Sigma)\\]\\[\\log \\pi_\\theta(a \\mid s)=-\\frac{1}{2}(a - \\mu_\\theta(s))^\\top\\Sigma^{-1}(a - \\mu_\\theta(s))+ C\\]\\[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)=\\Sigma^{-1}(a - \\mu_\\theta(s))\\;\\nabla_\\theta \\mu_\\theta(s)\\]This is the multi-dimensional generalization of the 1D case.Geometric Intuition:The vector $(a - \\mu_\\theta(s))$ points from the mean to the sampled action.Then: $\\Sigma^{-1}$ scales that vector Directions with low variance get amplified Directions with high variance get dampedSo the gradient says: ‚ÄúMove the mean toward the sampled action, but trust directions where the policy is more confident.‚ÄùREINFORCE Update in 2D:\\[\\Delta \\theta\\propto\\Sigma^{-1}(a_t - \\mu_\\theta(s_t))\\nabla_\\theta \\mu_\\theta(s_t)\\; G_t\\]Softmax PolicyNow assume the action space is discrete. Instead of outputting a mean and variance, the policy outputs a score (logit) for each action. Let this score function be $h_\\theta(s,a)$.The policy converts scores into probabilities using softmax:\\[\\pi_\\theta(a \\mid s)=\\frac{\\exp(h_\\theta(s, a))}{\\sum_{b} \\exp(h_\\theta(s, b))}\\]Key idea: Higher score $\\rightarrow$ higher probability Scores are unconstrained real numbers Softmax turns them into a valid probability distribution\\[\\log \\pi_\\theta(a \\mid s)=h_\\theta(s, a)-\\log \\sum_{b} \\exp(h_\\theta(s, b))\\]\\[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)=\\nabla_\\theta h_\\theta(s,a)-\\nabla_\\theta \\log \\sum_{b} \\exp(h_\\theta(s,b))\\]\\[\\nabla_\\theta \\log \\sum_{b} \\exp(h_\\theta(s,b))=\\frac{1}{\\sum_{b} \\exp(h_\\theta(s,b))}\\nabla_\\theta \\sum_{b} \\exp(h_\\theta(s,b))\\]\\[\\nabla_\\theta \\sum_{b} \\exp(h_\\theta(s,b))=\\sum_{b} \\nabla_\\theta \\exp(h_\\theta(s,b)) =\\sum_{b} \\exp(h_\\theta(s,b)) \\nabla_\\theta h_\\theta(s,b)\\]\\[\\nabla_\\theta \\log \\sum_{b} \\exp(h_\\theta(s,b))=\\frac{\\sum_{b} \\exp(h_\\theta(s,b)) \\nabla_\\theta h_\\theta(s,b)}{\\sum_{b} \\exp(h_\\theta(s,b))} \\\\ =\\sum_{b}\\frac{\\exp(h_\\theta(s,b))}{\\sum_{b'} \\exp(h_\\theta(s,b'))}\\nabla_\\theta h_\\theta(s,b)\\]But this fraction is exactly the softmax probability:\\[\\pi_\\theta(b \\mid s)=\\frac{\\exp(h_\\theta(s,b))}{\\sum_{b'} \\exp(h_\\theta(s,b'))}\\]So the entire term becomes:\\[\\nabla_\\theta \\log \\sum_{b} \\exp(h_\\theta(s,b))=\\sum_{b} \\pi_\\theta(b \\mid s)\\, \\nabla_\\theta h_\\theta(s,b)\\]\\[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)=\\nabla_\\theta h_\\theta(s,a)-\\sum_{b} \\pi_\\theta(b \\mid s)\\, \\nabla_\\theta h_\\theta(s,b)\\]Intuition:The softmax policy gradient has two terms because learning in discrete action spaces is about redistributing probability mass, not independently increasing action scores. Increasing the score of a chosen action must be balanced by decreasing the scores of other actions to preserve normalization. The gradient therefore increases the score of the selected action while subtracting the probability-weighted average score gradient of all actions, ensuring that probability mass shifts toward better actions in a competitive manner.Algorithm:LimitationsMathematically, the REINFORCE update is:\\[\\nabla_\\theta U(\\theta)\\approx\\sum_{t}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, G_t\\]The return $G_t$ is a very blunt learning signal. It mixes together two fundamentally different effects: State quality: some states are inherently good or bad Action quality: some actions are better than others within the same stateREINFORCE cannot distinguish between these. If the agent is in a bad state, all actions receive low returns and are suppressed, even if one of them was the best possible choice. Conversely, in a good state, all actions are reinforced, even if some were poor.This is why REINFORCE updates are extremely noisy: it is trying to solve a fine-grained credit assignment problem using a coarse, trajectory-level signal.Actor-CriticBuilding upon the limitations of the REINFORCE algorithm, we infer a key conceptual insight: we should not judge an action by how good the episode was, but by how good the action was relative to the state in which it was taken.BaselineREINFORCE updates the policy using returns:\\[\\nabla_\\theta U(\\theta)\\approx\\sum_t\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, G_t\\]This estimator is unbiased, but extremely high variance. This is because the return $G_t$ contains a lot of irrelevant information. It depends on: the quality of the state randomness in the environment future actions that had nothing to do with the current actionSo the policy gradient ends up reacting strongly to things the action did not cause. Intuitively, the algorithm is asking ‚ÄúWas this episode good?‚Äù, when it should be asking ‚ÄúWas this action good for this state?‚ÄùIf the agent is in a bad state, all actions receive low returns and are suppressed, even if one of them was the best possible choice. Conversely, in a good state, all actions are reinforced, even if some were poor. This is why REINFORCE updates are extremely noisy: it is trying to solve a fine-grained credit assignment problem using a coarse, trajectory-level signal.The baseline exists to fix exactly this mismatch.A baseline is simply a reference value that we subtract from the return before using it as a learning signal. So instead of using $G_t$, we use $(G_t-b)$.Baseline choices: Constant baseline (single scalar): $b = \\mathbb{E}[R(\\tau)]$\\[\\hat{g}=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\left(G_t^{(i)} - b\\right)\\] This baseline removes global reward bias. If all trajectories tend to have large positive (or negative) returns, the policy gradient would otherwise push parameters strongly even when actions are not particularly informative. The constant baseline recenters the learning signal so that updates reflect relative success across trajectories. Time-dependent Baseline (a vector length $T$): $b_t = \\frac{1}{N} \\sum_{i=1}^{N} G_t^{(i)}$\\[\\hat{g}=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\left(G_t^{(i)} - b_t\\right)\\] Many tasks have systematic reward structure over time. Early actions tend to have larger remaining returns than late actions simply because more rewards are still ahead. A time-dependent baseline removes this predictable temporal effect, allowing the gradient to focus on deviations from the average behavior at that timestep. State-dependent Baseline (a function):\\[b(s)=\\mathbb{E}[r_t + r_{t+1} + \\dots + r_{T-1} \\mid s_t = s]=V^\\pi(s)\\]\\[\\hat{g}=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=0}^{T}\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\left(G_t^{(i)} - V^\\pi(s_t^{(i)})\\right)\\] Subtracting this baseline removes the effect of how easy or difficult the state is, leaving only information about the quality of the chosen action. The learning signal now answers a precise question: was this action better or worse than what I normally do in this state? This produces stable, low-variance updates and leads directly to the advantage function. We want to encourage an action not when it has high return, but when it has higher return than the other actions from that state, that is, when it has an advantage over the other actions. It may well be that a state is bad and all actions have low returns in that state, but we do not care. The actions that have higher returns than the rest are the ones we want to reinforce. Therefore, we need to calibrate the goodness of actions using state-dependent baselines.Why subtracting a baseline helps?Consider two states: A good state where all actions give high reward A bad state where all actions give low rewardWithout a baseline, good states reinforce everything, and bad states suppress everything. This is useless for learning which action is better.With a baseline, only actions that outperform the state average are reinforced, and only actions that underperform are suppressed.So the baseline removes the effect of state quality from the learning signal.Does subtracting a baseline change the gradient?The true policy gradient is:\\[\\nabla_\\theta U(\\theta)=\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_t\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, G_t\\right]\\]Now suppose we subtract a baseline $b(s_t)$:\\[\\mathbb{E}\\left[\\sum_t\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\,(G_t - b(s_t))\\right]\\]\\[=\\mathbb{E}\\left[\\sum_t\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, G_t\\right]-\\mathbb{E}\\left[\\sum_t\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, b(s_t)\\right]\\]Focus on the second term:\\[\\mathbb{E}\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, b(s_t)\\right]\\]Since $b(s_t)$ does not depend on the action, we can pull it out of the inner expectation:\\[=\\mathbb{E}_{s_t}\\left[b(s_t)\\;\\mathbb{E}_{a_t \\sim \\pi_\\theta(\\cdot \\mid s_t)}\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\right]\\right]\\]\\[\\mathbb{E}_{a \\sim \\pi}[\\nabla_\\theta \\log \\pi(a)]=\\sum_a \\pi(a) \\nabla_\\theta \\log \\pi(a)=\\sum_a \\nabla_\\theta \\pi(a)=\\nabla_\\theta \\sum_a \\pi(a)=\\nabla_\\theta 1=0\\]So the entire baseline term vanishes in expectation. This shows that subtracting any baseline that depends only on the state: does not change the expected gradient does reduce varianceSo the estimator remains unbiased, but becomes far more stable.How do we learn the state-dependent baseline $V^\\pi(s)$?The value function is defined as the expected future return starting from a state and following the current policy:\\[V^\\pi(s)=\\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\\;\\middle \\mid \\;s_t = s\\right]\\]In real environments, the true value function is unknown. We only observe sampled trajectories, not expectations. Therefore, the value function must be learned from experience, using the same data generated by the policy.There are two fundamental strategies for doing this: Monte Carlo (MC) estimation Temporal-Difference (TD) estimationBoth aim to approximate the same quantity $V^\\pi(s)$, but they differ in how much of the future they wait to observe before making an update.Monte Carlo Value Estimation:Monte Carlo methods estimate the value of a state by waiting until the episode finishes, then using the actual observed return as the training target.Intuition: ‚ÄúDon‚Äôt guess the future. Wait and see what really happens.‚ÄùFor a state visited at time $t$, the Monte Carlo return is:\\[G_t=r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_T\\]This return is treated as a sample of the true value:\\[V^\\pi(s_t) \\approx G_t\\]If the value function is parameterized by $\\phi$, it can be trained via regression:\\[\\mathcal{L}_{MC}(\\phi)=\\left( V_\\phi(s_t) - G_t \\right)^2\\]Monte Carlo estimation is: Unbiased, because it uses the true outcome High variance, because returns depend on everything that happens later Delayed, because updates occur only after the episode endsThis makes MC estimation conceptually clean but often impractical for long-horizon or sparse-reward problems.Temporal-Difference (TD) Value Estimation:Temporal-Difference methods update the value function before the episode ends by bootstrapping from the current value estimate of the next state.Intuition: ‚ÄúUse what I already believe about the future, and correct myself if I‚Äôm wrong.‚ÄùInstead of waiting for the full return, TD uses a one-step lookahead:\\[r_t + \\gamma V_\\phi(s_{t+1})\\]The TD error is defined as:\\[\\delta_t=r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\]This error measures how surprised the value function is by new experience. The value function is trained to minimize this prediction error:\\[\\mathcal{L}_{TD}(\\phi)=\\delta_t^2\\]Temporal-Difference estimation is: Lower variance, because it uses shorter-horizon targets Biased, because it relies on imperfect value estimates Online, allowing updates at every timestepIn practice, the reduced variance and faster learning usually outweigh the bias.Once a value function is available, it can be used as a baseline to compute advantages.Monte Carlo Advantage:\\[A_t^{MC}=G_t - V_\\phi(s_t)\\]TD Advantage:\\[A_t^{TD}=r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\]A positive advantage $A^{\\pi}(s,a) &gt; 0$ means that action $a$ in state $s$ leads to returns higher than the expected value of that state ‚Äî it‚Äôs better than what the policy typically does from that state. A negative advantage means it‚Äôs worse. So computing advantages is essentially identifying which actions should be made more likely (positive advantage) and which should be made less likely (negative advantage).The Core Actor-Critic FrameworkActor-Critic methods maintain two separate components working in tandem: an actor (the policy network) that decides which actions to take, and a critic (the value function network) that evaluates how good those actions are. Think of it like a performer and a coach: the actor performs actions in the environment, while the critic watches and provides feedback on performance. The actor uses this feedback to improve its policy parameters through gradient ascent, while the critic learns to better evaluate states by observing the actual rewards received.In the basic Actor-Critic algorithm, after collecting trajectories by running the current policy, you update the critic to fit a value function $V_{\\phi}^{\\pi}(s)$ using either Monte Carlo or Temporal Difference estimation. Then, for the policy gradient update, you compute:\\[\\nabla_\\theta U(\\theta) \\approx \\hat{g} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)}) A^\\pi(s_t^{i}, a_t^{i})\\]where the advantage $A^\\pi(s_t^i, a_t^i) = G_t^{(i)} - V_{\\phi}^{\\pi}(s_t^i)$ and $G_t^{(i)}$ is the sample return (the sum of all rewards from time $t$ onward in that trajectory).Advantage Actor-Critic (A2C)The key innovation in A2C is in how it computes the advantage. Instead of using the full Monte Carlo return $G_t$ and subtracting the value function baseline, it uses a bootstrapped estimate. The advantage becomes:\\[A^\\pi(s_t^i, a_t^i) = R(s_t^i, a_t^i) + \\gamma V_{\\phi}^{\\pi}(s_{t+1}^i) - V_{\\phi}^{\\pi}(s_t^i)\\]This is essentially a one-step TD error.Instead of waiting to collect the entire trajectory‚Äôs return, you‚Äôre using the immediate reward $R(s_t, a_t)$ plus the estimated value of the next state $\\gamma V(s_{t+1})$ and comparing that to your current state‚Äôs value $V(s_t)$.In general, it is useful to remember the following about advantage function:\\[A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)\\]\\[\\mathbb{E}_{a \\sim \\pi}[A^\\pi(s,a)] = 0\\]Why This Difference Matters: A2C vs ACThe distinction might seem subtle, but it has profound implications for learning efficiency and variance. To understand why, imagine you‚Äôre learning to play a game where each episode lasts 1000 steps. In basic Actor-Critic using full returns, the advantage at time step t=5 depends on everything that happens for the next 995 steps. This creates enormous variance. Maybe you made a great move at step 5, but 800 steps later something random happened that ruined the episode. Your advantage estimate would unfairly penalize that early good decision.Advantage Actor-Critic solves this by using the TD error, which essentially asks: ‚ÄúGiven what I expected the value of my current state to be, did taking this action and moving to the next state exceed or fall short of expectations?‚Äù This is a much more localized signal. You‚Äôre only looking one step ahead (or k steps if using k-step returns), and you‚Äôre bootstrapping the rest using your learned value function. This dramatically reduces variance at the cost of introducing some bias (since your value function estimate might be wrong).Think of it like getting feedback on your work. Monte Carlo returns (used in basic Actor-Critic) are like waiting until a project is completely finished before getting any evaluation ‚Äî you get an unbiased assessment, but it‚Äôs very noisy because many things could have gone wrong along the way. The TD approach (used in Advantage Actor-Critic) is like getting incremental feedback after each task: ‚ÄúYou did better/worse than I expected given where you started.‚Äù It‚Äôs slightly biased by your coach‚Äôs potentially imperfect expectations, but it‚Äôs much more consistent and immediate.From an implementation standpoint, both methods follow similar algorithmic structure ‚Äî they both initialize policy and critic parameters, sample trajectories, fit the value function, compute advantages, and update the policy. The crucial line that differs is step 3, where advantages are computed.Actor-Critic as Policy IterationPolicy iteration alternates between two phases in a loop: policy evaluation and policy improvement. During policy evaluation, you fix your current policy $\\pi$ and compute its value function, essentially answering ‚Äúhow good is each state under my current policy?‚Äù Then, during policy improvement, you create a new policy that‚Äôs greedy with respect to these values, meaning you switch to actions that have higher Q-values than the average. This two-step dance is guaranteed to converge to the optimal policy under certain conditions.Actor-critic methods implement the same fundamental structure as classical policy iteration, but in a continuous, gradient-based way rather than through discrete policy updates.The gradient update $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta U(\\theta)$ increases the probability of actions with positive advantages and decreases the probability of actions with negative advantages. This single update step is the policy improvement. You don‚Äôt need to do anything separately. It‚Äôs a soft, gradual version of the greedy policy switch in classical policy iteration.Actor-critic is an on-policy method: you‚Äôre evaluating your current policy $\\pi_\\theta$, computing how good actions are under that specific policy, and then improving that same policy. The data you collected is intrinsically tied to the policy that generated it, just like in policy iteration where $V^{\\pi}$ and $Q^{\\pi}$ are defined relative to a specific policy $\\pi$.Asynchronous Deep RL for On-Policy LearningThe Core ProblemWhen training neural networks for reinforcement learning, gradient updates need to be decorrelated for stable learning. However, this creates a fundamental problem for on-policy methods like REINFORCE and actor-critic. When you collect experience sequentially by running a single agent through an environment, consecutive states and actions are highly correlated ‚Äî each state naturally follows from the previous one. If you compute gradient updates from these sequential, correlated experiences, your value function approximator tends to oscillate and become unstable.The Asynchronous Solution: Parallel Experience CollectionThe key insight of asynchronous deep RL is to parallelize the collection of experience to break these correlations and stabilize training. Instead of running a single agent, you run multiple independent threads of experience simultaneously ‚Äî one agent per thread. Each agent explores a different part of the environment at the same time, contributing experience tuples (state, action, reward, next state) from diverse regions of the state space. Because these parallel agents are in different states and taking different actions, their experiences are much less correlated than sequential data from a single agent would be.This approach is particularly crucial for on-policy algorithms like actor-critic because they can‚Äôt simply use a replay buffer of old experiences (that would violate the on-policy constraint where data must come from the current policy). Instead, asynchronous parallel collection provides the decorrelation benefits you‚Äôd get from experience replay, while still maintaining the on-policy property ‚Äî all workers are running approximately the same current policy. The diversity comes from spatial exploration across parallel environments rather than temporal diversity from replaying old experiences.The result is much more stable and efficient training. Neural networks converge faster, value function estimates are more accurate, and policies learn more robustly without the wild oscillations that plague sequential single-agent on-policy learning.How It Works in PracticeThe algorithm runs multiple worker agents in parallel, each with their own copy of the environment. These workers collect trajectories independently and asynchronously. Each worker periodically computes gradients based on its local experience and sends these gradients to a central parameter server (or directly updates shared parameters with appropriate synchronization). The key architectural choice is whether to do this fully asynchronously (A3C - Asynchronous Advantage Actor-Critic) where workers update independently whenever they‚Äôre ready, or synchronously (A2C) where all workers wait for each other and their gradients are averaged before updating the central network.Asynchronous Advantage Actor-Critic (A3C)Evolutionary Methods for Policy SearchBlack-Box Policy OptimizationBlack-box policy optimization represents a fundamentally different approach to reinforcement learning compared to the gradient-based methods you‚Äôve studied. Instead of exploiting the structure of the problem (how states connect to each other, how rewards decompose over timesteps), black-box methods treat the entire RL problem as an opaque function to be optimized.You‚Äôre solving:\\[\\max_{\\theta} U(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau) \\mid \\pi_\\theta, \\mu_0(s_0)]\\]You only interact with this objective function by querying it. You try some parameters $\\theta$, run the policy, get back a scalar reward $R(\\tau)$, and that‚Äôs all you know. You don‚Äôt compute gradients, you don‚Äôt learn value functions, you don‚Äôt exploit the Markov structure of the MDP.Algorithm: Initialize policy parameters randomly Perturb the parameters (create variations, mutations, offspring) Evaluate each perturbed policy by running it in the environment Select which perturbations led to better performance Update your parameters toward the better-performing variations RepeatCross-Entropy Method (CEM)Principle: keep trying random variations of your policy, remember which ones worked best, and adjust your search distribution to generate more samples like the good ones.CEM models your policy parameters as random variables drawn from a probability distribution. Specifically, it uses a multivariate Gaussian with a diagonal covariance matrix:\\[\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2 I)\\]Algorithm: Initialization: Start with $Œº = 0$ and $œÉ¬≤ = 100I$. This means you‚Äôre initially searching with a lot of noise, exploring broadly. Sampling: Generate n random parameter vectors by sampling from your current Gaussian. Think of this as creating $n$ ‚Äúoffspring‚Äù or ‚Äúmutants‚Äù of your current best guess:\\[{\\theta_i}_{i=1}^n \\sim \\mathcal{N}(\\mu, \\sigma^2 I)\\] Evaluation: For each $\\theta_i$, run the policy in the environment for $L$ episodes and compute the average score. This is the fitness: how well did that particular parameter vector perform? Selection: You don‚Äôt use all $n$ samples. Instead, you select only the top $[\\rho n]$ performers (where $œÅ ‚â§ 1$, typically around 0.1 or 0.2). These are your winners, the parameter vectors that led to the best performance. Update: Now you shift your search distribution toward these elites. The new mean becomes the average of the elite samples:\\[\\mu(j) = \\frac{1}{[\\rho n]} \\sum_{i=1}^{[\\rho n]} \\theta'_i(j)\\] And the new variance is computed from how spread out the elites are:\\[¬†\\sigma^2(j) = \\frac{1}{[\\rho n]} \\sum_{i=1}^{[\\rho n]} [\\theta'_i(j) - \\mu(j)]^2 + \\eta\\] $\\eta$ is the added noise to prevent the variance from collapsing to zero, which would kill exploration. CEM struggles as dimensionality grows. With neural networks having thousands or millions of parameters, the diagonal Gaussian assumption becomes too restrictive, and the selection mechanism (only keeping top $\\rho$ fraction) throws away most of your sampled information.Covariance Matrix Adaptation Evolution Strategy (CMA-ES)While CEM used a diagonal covariance matrix (assuming parameter independence), CMA-ES uses a full covariance matrix. This means it learns and exploits correlations between parameters, making it dramatically more powerful, though also more complex.CEM sampled from:\\[¬†\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2 I)\\]CMA-ES generalizes this to:\\[\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)\\]Visual Intuition:With diagonal covariance (CEM), your search distribution is always axis-aligned ‚Äî you can only scale search along the coordinate axes. But the optimal search direction might be diagonal.CMA-ES adapts the ellipse to match the problem geometry. If the fitness landscape is like a valley running northeast-southwest, CMA-ES will stretch its sampling ellipse along that valley. This dramatically improves efficiency because you‚Äôre searching along the natural contours of the problem rather than fighting against them.Algorithm: Sample: Draw $n$ parameter vectors from your current Gaussian $\\mathcal{N}(\\mu_i, C_i)$, where $C$ is the covariance matrix at iteration $i$. Select Elites: Evaluate each sample‚Äôs fitness and keep the top performers, just like CEM. Update Mean: Move your center $\\mu$ toward the weighted average of the elite samples:\\[\\mu_{t+1} = \\mu_t + \\alpha \\sum_{i=1}^{n_{elit}} w_i(\\theta_i^{elit,t} - \\mu_t)\\] Update Covariance:\\[¬†\\Sigma_{t+1} = \\text{Cov}(\\theta_1^{elit,t}, \\theta_2^{elit,t}, \\ldots) + \\epsilon I\\] This tells you: ‚ÄúWhat directions in parameter space led to good performance?‚Äù The covariance matrix encodes these productive search directions. The $\\epsilon I$ term (small noise on the diagonal) prevents the matrix from becoming singular. Natural Evolutionary Strategies (NES)CEM and CMA-ES had a clear selection mechanism: evaluate samples, keep the elites, throw away the rest. NES takes a different approach: every offspring contributes to the update, weighted by its fitness.NES maintains a Gaussian distribution over policy parameters, but for computational tractability with neural networks, it uses a diagonal covariance with fixed variance:\\[\\theta \\sim P_\\mu(\\theta) = \\mathcal{N}(\\mu, \\sigma^2 I_d)\\]Where: $\\mu \\in ‚Ñù^d$ is the mean (what we‚Äôre optimizing) $œÉ¬≤I_d$ is a fixed diagonal covariance ($\\sigma$ is a hyperparameter) $d$ is the parameter dimensionThe goal is to maximize the expected fitness:\\[\\max_\\mu U(\\mu) = \\mathbb{E}_{\\theta \\sim P\\mu(\\theta)} [F(\\theta)]\\]where $F(\\theta)$ is the fitness (total return) when running policy $\\pi \\theta$.Derivation\\[\\nabla_\\mu U(\\mu) = \\nabla_\\mu \\mathbb{E}_{\\theta \\sim P\\mu(\\theta)} [F(\\theta)]\\]We can write the expectation as an integral:\\[\\nabla_\\mu U(\\mu) = \\nabla_\\mu \\int P_\\mu(\\theta) F(\\theta) d\\theta\\]\\[= \\int \\nabla_\\mu P_\\mu(\\theta) F(\\theta) d\\theta\\]Log-likelihood trick:\\[\\nabla_\\mu U(\\mu) = \\int P_\\mu(\\theta) \\frac{\\nabla_\\mu P_\\mu(\\theta)}{P_\\mu(\\theta)} F(\\theta) d\\theta\\]\\[= \\int P_\\mu(\\theta) \\nabla_\\mu \\log P_\\mu(\\theta) F(\\theta) d\\theta\\]\\[= \\mathbb{E}_{\\theta \\sim P\\mu(\\theta)} [\\nabla_\\mu \\log P_\\mu(\\theta) F(\\theta)]\\]This is exactly the same trick used in policy gradients! Just like REINFORCE transforms policy gradients into an expectation, NES transforms parameter distribution gradients into an expectation.Since we have an expectation, we can approximate it by sampling:\\[¬†\\nabla_\\mu U(\\mu) \\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\mu \\log P_\\mu(\\theta^{(i)}) F(\\theta^{(i)})\\]where each $\\theta^i \\sim P_\\mu(\\theta)$.For our Gaussian with mean $\\mu$ and fixed variance $\\sigma^2I$:\\[¬†\\log P_\\mu(\\theta) = -\\frac{|\\theta - \\mu|^2}{2\\sigma^2} + \\text{const}\\]Taking the gradient with respect to $\\mu$:\\[\\nabla_\\mu \\log P_\\mu(\\theta) = \\frac{\\theta - \\mu}{\\sigma^2}\\]So the NES update becomes:\\[\\mu_{t+1} = \\mu_t + \\frac{\\alpha}{N\\sigma} \\sum_{i=1}^N F(\\theta_i) \\epsilon_i\\]where $\\theta_i = \\mu_t + \\sigma \\epsilon_i$ and $\\epsilon_i \\sim \\mathcal{N}(0,I)$.Value-Based RLReview: Actor-Critic and the Advantage FunctionWe saw previously that in actor-critic methods, we‚Äôre optimizing a policy gradient objective that looks like this:\\[\\nabla_\\theta U(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot A^{\\pi}(s_t, a_t) \\right]\\]The advantage function plays a crucial role here, serving as our signal for which actions are better than average. The advantage is formally defined as:\\[A^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t)\\]Intuition: The term $r(s_t, a_t) + \\gamma V^\\pi(s_{t+1})$ represents the Q-value of the state-action pair, essentially asking ‚Äúwhat‚Äôs the total expected return if I take action $a_t$ in state $s_t$ and then follow $\\pi$ afterward?‚Äù By subtracting the value function $V^\\pi(s_t)$, which represents the average value of being in that state, we get a relative measure. The advantage tells us: ‚Äúhow much better is this specific action compared to what we‚Äôd typically expect in this state?‚ÄùWe can also express the advantage function in terms of Q-values and values:\\[A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t)\\]General policy-based RL workflow: We collect trajectories by rolling out our current policy in the environment We estimate how good each action was by computing returns or advantages through policy evaluation We improve the policy by adjusting it to favor actions with higher estimated advantages.The Value-Based ApproachNow we arrive at a fundamental question that motivates value-based RL: if we had a perfect estimate of the advantage function, would we even need to maintain an explicit policy? The answer is no, and this insight drives the entire value-based methodology.Think about it this way: in policy-based methods, we‚Äôre using the advantage estimates to nudge our policy parameters gradually in the direction of better actions. We collect data, estimate advantages using rollouts from our policy, compute gradients, and take small steps. But if we somehow had perfect knowledge of $A^\\pi(s,a)$ for all state-action pairs, we could simply construct the optimal policy by taking the argmax:\\[\\pi(s) \\leftarrow \\argmax_a A^\\pi(s,a)\\]This leads to the value-based RL skeleton, which differs from policy-based methods primarily in steps 2 and 3: Collect data in the environment using the current policy $\\pi$ (with some exploration noise to ensure we visit diverse states). Estimate the advantage function $A^\\pi(s,a)$ as perfectly as possible rather than just well enough to compute a policy gradient. This is where we invest most of our effort by building the most accurate model we can of which actions are truly better. Update the policy via argmax by setting $\\pi‚Äô(s) = \\argmax_a A^\\pi(s, a)$ for all states. Notice this is ‚Äúoff-policy‚Äù in the sense that we‚Äôre not computing gradients of the policy itself; we‚Äôre simply choosing the best action according to our learned advantage estimates.We don‚Äôt need to worry about policy parameterization, gradient estimation, or learning rates for the policy. If our advantage estimates are good, the policy update is trivial ‚Äî just pick the best action.Value Iteration and Q-LearningThe theoretical algorithm that formalizes this idea is called dynamic programming or value iteration. The core of value iteration is the Bellman equation for the Q-function:\\[Q(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s' \\sim p(\\cdot \\mid s, a)} [V(s')]\\]Intuition: ‚ÄúThe value of taking action $a$ in state $s$ equals the immediate reward plus the discounted expected value of wherever we end up next.‚ÄùThe value iteration algorithm alternates between two updates: Update 1: Update Q-values using the Bellman equation:\\[Q(s, a) \\leftarrow r(s, a) + \\gamma \\mathbb{E}_{s'} [V(s')]\\] Update 2: Update the value function by taking the max over actions:\\[V(s) \\leftarrow \\max_a Q(s, a)\\] These updates are performed iteratively until convergence. In tabular settings (where we have finite state and action spaces), we can literally maintain a table with entries $Q(s, a)$ for every possible state-action pair, and this algorithm is guaranteed to converge to the optimal Q-function.But here‚Äôs the practical challenge: how do we implement this when we have large or continuous state spaces? We need function approximation.Fitted Q-Iteration: Neural Networks for Value FunctionsTo scale value-based methods to complex domains, we use neural networks to approximate the Q-function. This leads to fitted Q-iteration, where instead of maintaining a table, we train a neural network $Q_\\theta(s,a)$ parameterized by weights $\\theta$.The training procedure draws inspiration from supervised learning, specifically, from temporal difference (TD) learning. Here‚Äôs how it works:We want to train $Q_\\theta$ to satisfy the Bellman equation, so we construct a target value:\\[y(s, a) = r(s, a) + \\gamma \\max_{a'} Q_\\theta(s', a')\\]The target itself involves our Q-function evaluated at the next state $s‚Äô$. This creates a ‚Äúbootstrap‚Äù situation where we‚Äôre using our current estimates to create training targets for future estimates.We then define a TD-error loss function which we want to minimize:\\[\\text{TD-Error}(\\theta) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} \\left[ \\left( Q_\\theta(s, a) - y(s, a) \\right)^2 \\right]\\]The gradient update becomes:\\[\\theta_{t+1} \\leftarrow \\theta_t - \\alpha \\nabla_\\theta \\text{TD-Error}(\\theta) \\big|_{\\theta = \\theta_t}\\]This is not gradient descent in the traditional sense. Why? Because our target $y(s,a)$ itself depends on $\\theta$ through the $\\max_{a‚Äô} Q_\\theta(s‚Äô, a‚Äô)$ term. When we compute the gradient, we‚Äôre treating the target as fixed (not differentiating through it), but the target is actually moving as $\\theta$ changes. This creates a moving target problem that can lead to instability.Think of it like trying to hit a bullseye that keeps shifting position as you aim. Traditional supervised learning has a fixed target. But in Q-learning, as our Q-estimates get better, the targets we‚Äôre aiming for also change, creating a feedback loop that can sometimes diverge.Despite this theoretical concern, fitted Q-iteration can work in practice with the right tricks. The key insight is that even though we‚Äôre not doing true gradient descent on a fixed objective, we‚Äôre still iteratively improving our Q-estimates to better satisfy the Bellman equation.Why the Q-Learning Target Makes Sense:The target $y(s, a) = r(s, a) + \\gamma \\max_{a‚Äô} Q(s‚Äô, a‚Äô)$ comes directly from the Bellman optimality equation. When you take action $a$ in state $s$, your total expected return naturally splits into two components: the immediate reward $r(s,a)$ you receive right now, and the best possible future value you can obtain from wherever you land next, which is $\\gamma \\max_{a‚Äô} Q(s‚Äô, a‚Äô)$. The max operator appears because we‚Äôre learning the optimal Q-function, which assumes we‚Äôll always choose the best action going forward, even if our current exploratory policy doesn‚Äôt actually do that yet.This creates a bootstrapping process where we use our current value estimates to improve themselves. Even though our Q-estimates start out imperfect, we can make progress by using the estimate at the next state $s‚Äô$ to improve the estimate at the current state $s$. The one-step reward $r(s,a)$ provides accurate immediate information, and combining it with our (gradually improving) estimate of future value pushes our Q-function toward satisfying the Bellman equation. Over many iterations, these improvements propagate backward through the state space, eventually converging to the optimal Q-function.The reason we take the max rather than averaging over our current policy‚Äôs action distribution is what makes Q-learning ‚Äúoff-policy‚Äù ‚Äî it learns about the optimal policy directly, regardless of how we‚Äôre actually behaving during data collection. This allows us to learn from exploratory or even random actions, since we‚Äôre always targeting what the best action would be, not what we actually did.Deep Q-Networks (DQN)The breakthrough that made deep Q-learning practical came with the Deep Q-Network (DQN) algorithm, which introduced two critical innovations to address the instability issues.Innovation 1: Replay BufferInstead of training on data as it comes from the agent‚Äôs current policy, DQN stores transitions (s, a, r, s') in a large replay buffer (essentially a dataset of experiences). During training, we sample random mini-batches from this buffer rather than using consecutive experiences.Why does this help?First, it breaks the correlation between consecutive samples. If we trained on experiences in the order they occurred, the network would see highly correlated data (e.g., many frames from the same episode of a game), which can lead to overfitting to recent experiences and catastrophic forgetting of earlier lessons. Random sampling from the replay buffer gives us more independent and identically distributed-like data, which neural networks handle much better.Second, the replay buffer provides data efficiency. Each experience can be used for multiple gradient updates rather than being discarded after a single use. This is particularly valuable in domains where collecting data is expensive.Innovation 2: Target NetworksTo address the moving target problem, DQN maintains two sets of Q-network parameters: the current network $\\theta_t$ that we‚Äôre actively training, and a target network $\\theta_t^{target}$ that we use to compute the targets in the Bellman equation.The TD-error loss now becomes:\\[\\min_\\theta \\mathbb{E}_{(s, a, r, s') \\in \\mathcal{D}} \\left[ \\left( Q_\\theta(s, a) - \\left( r(s, a) + \\gamma \\max_{a'} Q_{\\theta_t^{\\text{target}}}(s', a') \\right) \\right)^2 \\right]\\]The key idea is that we update $\\theta_t$ on every gradient step, but we only update the target network $\\theta_t^{target}$ occasionally (e.g., every few thousand steps) by copying the current network weights. This creates a period of stability where the targets remain fixed while we train, reducing the moving target problem. It‚Äôs like the bullseye staying still for a while so we can actually learn to aim at it before it moves again.AlgorithmThe complete DQN algorithm works as follows: Collect data using policy $\\pi_{\\theta_t}$ (typically $\\epsilon-$greedy: take argmax action with probability $1-\\epsilon$, and random action with probability $\\epsilon$ for exploration by sampling from a uniform distribution of actions $\\pi \\sim \\text{Unif}(\\mathcal{A})$ ).\\[\\pi(s) = \\arg\\max_{a'} Q_{\\theta_t}(s, a')\\] Store transitions (s, a, r, s') in replay buffer $\\mathcal{D}$ Sample mini-batch from $\\mathcal{D}$ and perform gradient descent on:\\[\\min_\\theta \\mathbb{E}_{(s, a, r, s') \\in \\mathcal{D}} \\left[ \\left( Q_\\theta(s, a) - \\left( r(s, a) + \\gamma \\max_{a'} Q_{\\theta_t^{\\text{target}}}(s', a') \\right) \\right)^2 \\right]\\] Periodically update target network: every $K$ steps, set $\\theta_t^{target} \\leftarrow \\theta_t$The use of soft targets (keeping the target network fixed for periods) combined with the replay buffer‚Äôs decorrelated sampling creates a much more stable training process than naive fitted Q-iteration. This allowed DQN to achieve superhuman performance on many Atari games, marking a major milestone in deep reinforcement learning.The fundamental insight of DQN is that even though we‚Äôre not doing true gradient descent (since we‚Äôre using target networks and treating parts of our objective as fixed), we can still make meaningful progress by carefully managing the optimization process to reduce instability.Key Hyperparameters and Design ChoicesUpdate-to-Data (UTD) Ratio:The UTD ratio measures how many gradient updates we perform per environment step. If UTD = 1, we take one gradient step for each new transition we collect. If UTD = 4, we perform four gradient updates (sampling four different mini-batches from the replay buffer) for each new transition.A higher UTD ratio means we‚Äôre extracting more learning from each piece of data, which can be sample-efficient. However, there‚Äôs a trade-off: if the UTD ratio is too high, we can overfit to the data in our replay buffer, and the rapid updates can destabilize training. The agent might start exploiting patterns in old data that are no longer relevant to the current policy. Selecting its value is problem-dependent, as there are cases where UTD is as high as 100, but sometimes as low as 0.001.Example scenario for UTD=4. So the process looks like this: Agent takes one action in environment $\\rarr$ gets one new transition $\\rarr$ add it to replay buffer Sample mini-batch #1 from replay buffer $\\rarr$ compute loss $\\rarr$ gradient step Sample mini-batch #2 from replay buffer $\\rarr$ compute loss $\\rarr$ gradient step Sample mini-batch #3 from replay buffer $\\rarr$ compute loss $\\rarr$ gradient step Sample mini-batch #4 from replay buffer $\\rarr$ compute loss $\\rarr$ gradient step Return to step 1, take another action in environmentThose 4 gradient steps in the middle are all training on old data from the replay buffer, not on the single new transition we just collected. The new transition goes into the buffer and might get sampled in future mini-batches, but we‚Äôre primarily learning from historical experiences.Hard vs Soft Target UpdatesThe target network update described above is called a ‚Äúhard update‚Äù, where we completely replace $\\theta^{target}$ with $\\theta$ all at once. An alternative approach is ‚Äúsoft updates,‚Äù where we gradually blend the target network toward the main network at every step:\\[\\theta^{\\text{target}}_{i+1} \\leftarrow (1 - \\alpha) \\theta^{\\text{target}}_i + \\alpha \\theta_i\\]Here $\\alpha$ is a small constant like 0.005. This means the target network slowly tracks the main network, providing continuously updating targets rather than sudden jumps. Hard updates every $K$ steps with $K$ large create discrete jumps, while soft updates with small $\\alpha$ create smooth tracking. Soft updates often provide more stable training because the targets change gradually rather than suddenly shifting every few thousand steps.Note: You pick one strategy and stick with it throughout training. They‚Äôre two different design choices for solving the same problem (keeping targets stable), not complementary techniques you‚Äôd combine.Challenges with DQNChallenge 1: Overestimation Bias in Q-ValuesOne of the major practical challenges with DQN is that it tends to systematically overestimate Q-values. This is a fundamental issue with how the algorithm compounds errors through bootstrapping.The Root Cause: Error Accumulation Through Max OperationsOur target is:\\[y(s,a) = r(s,a) + \\gamma \\max_{a'} Q_{\\theta^{\\text{target}}}(s', a')\\]The term $\\max_{a‚Äô} Q_{\\theta^{target}}(s‚Äô, a‚Äô)$ is supposed to represent the optimal state value $V^*(s‚Äô)$, which is the true value we‚Äôd get by following the optimal policy from state $s‚Äô$ onward. However, our Q-function $Q_{\\theta^{target}}$ is not perfect and has some errors. Some actions might have Q-values that are too high (overestimates), while others might be too low (underestimates).This problem becomes serious because we‚Äôre doing bootstrapping ‚Äî the Q-value we‚Äôre training becomes the target for earlier states. So if $Q_\\theta(s‚Äô, a‚Äô)$ is overestimated, then when we compute the target for state $s$, we use this overestimate. This means $Q_\\theta(s, a)$ will also become overestimated. Then when we compute targets for states that lead to $s$, they inherit this error, and so on. The overestimation compounds backward through the state space like a chain reaction.Double DQN: Decoupling Action Selection and EvaluationThe problem is that we‚Äôre using the same Q-function both to select which action looks best AND to evaluate how good that action is. This creates the bias‚Äîwe pick the action with the largest estimation error, then trust that error as our target.Double DQN fixes this by maintaining two Q-functions (initialized differently): $Q_{\\theta^A}$ and $Q_{\\theta^B}$. The key idea is to use one network to select the action and the other network to evaluate it. Here‚Äôs how the target computation changes: Use network A to determine which action looks best in state $s‚Äô$:\\[a^* = \\arg\\max_{a} Q_{\\theta^A}(s', a)\\] Use network B to evaluate that specific action:\\[y(s,a) = r(s,a) + \\gamma Q_{\\theta^B}(s', a^*)\\] Why does this help? If network A has a positive error on some action (thinks it‚Äôs better than it really is), it will select that action. But then network B evaluates it, and B‚Äôs errors are independent of A‚Äôs errors (since they were initialized differently and trained on different mini-batches). So B is unlikely to have the same overestimation for that same action. On average, this decorrelates the selection bias from the evaluation bias, dramatically reducing the systematic overestimation.In practice with DQN, we already have two networks‚Äîthe main network $\\theta$ and the target network $\\theta^{target}$ ‚Äî so we can implement Double DQN without adding any extra parameters. We use the main network to select actions and the target network to evaluate them:\\[a^* = \\arg\\max_{a} Q_{\\theta}(s', a)\\]\\[y(s,a) = r(s,a) + \\gamma Q_{\\theta^{\\text{target}}}(s', a^*)\\]Challenge 2: Reducing Error Compounding with N-Step ReturnsAnother approach to reducing error accumulation is to use N-step returns instead of one-step TD targets. The standard DQN target uses only one step of real reward before bootstrapping:\\[y(s,a) = r(s,a) + \\gamma \\max_{a'} Q_{\\theta^{\\text{target}}}(s', a')\\]This means we trust the immediate reward $r(s,a)$ (which comes from the environment and is accurate), but then immediately rely on our Q-function estimate for everything after. If our Q-estimates are poor, this propagates error quickly.With N-step returns, we instead accumulate N steps of actual rewards before bootstrapping. If we have a trajectory (s, a, r, s', a', r', s'', ...) in our replay buffer, we can construct an N-step target:\\[y_N(s,a) = r(s,a) + \\gamma r(s', a') + \\gamma^2 r(s'', a'') + \\cdots + \\gamma^N \\max_{a} Q_{\\theta^{\\text{target}}}(s_{N+1}, a)\\]The intuition is that we‚Äôre trusting real data for longer before falling back on our potentially erroneous estimates.The trade-off is that N-step returns introduce higher variance ‚Äî they depend on N different random rewards and state transitions ‚Äî and they can only be computed for transitions where we have N subsequent steps stored in the replay buffer. But when tuned properly, they often significantly improve learning by reducing the impact of Q-value errors on the targets. The most common value of N used in practice is 3.Challenge 3: Cold Start ProblemAt the very beginning of training, the replay buffer is empty, so DQN cannot perform any updates. The solution is a burn-in phase: before any Q-network training begins, the agent collects an initial dataset by acting in the environment for thousands of steps using either a random policy or highly exploratory $\\epsilon-$greedy policy. This fills the replay buffer with diverse experiences. Only after this warm-up period do gradient updates begin. The Q-network starts with random weights and poor predictions, but it now has real environmental data to learn from.Challenge 4: The Limited Exploration ProblemA more persistent challenge arises when the agent repeatedly visits similar states and fails to explore broadly. This can happen if the policy is too exploitative ($\\epsilon$ too small), if certain states act as ‚Äúattractors‚Äù where the agent gets stuck, or if the state space is so vast that the explored region remains limited. When the replay buffer lacks diversity, the Q-function becomes specialized to only the visited states and performs poorly elsewhere, trapping the agent in a local optimum where its policy keeps leading it back to the same familiar territory.This is why $\\epsilon-$greedy exploration is essential throughout training. With probability $\\epsilon$, the agent takes random actions instead of greedy ones, occasionally forcing it out of comfortable patterns and into new situations. In practice, $\\epsilon$ typically starts at 1.0 during burn-in, then gradually decays to a small constant like 0.1 or 0.05. This schedule prioritizes early exploration to build buffer diversity, then shifts toward exploitation as the Q-function improves, while maintaining baseline exploration indefinitely to prevent getting stuck. Even a small constant $\\epsilon$, over millions of steps, generates enough random detours to gradually expand the explored region and diversify the replay buffer.Practical Training Details for DQNA few additional implementation details that significantly impact DQN‚Äôs performance:Coverage Really Helps: Ensuring broad state-space coverage through effective exploration is crucial. If the replay buffer only contains data from a limited region of the state space, the Q-function will have poor estimates elsewhere, and the agent may fail to discover better behaviors. Techniques like decaying $\\epsilon$ over time (starting with high exploration, gradually reducing it as we learn) help balance initial exploration with later exploitation.High UTD Can Destabilize Training: While performing many gradient updates per environment step seems attractive for sample efficiency, setting the UTD ratio too high can cause instability. The agent essentially overfits to its replay buffer, and rapid Q-value changes can violate the assumptions that make target networks helpful. Conservative UTD ratios tend to be more robust.Improved Loss Functions: Instead of using standard squared error, practitioners often use the Huber loss, which is quadratic for small errors but linear for large errors:\\[L(x) = \\begin{cases}\\frac{x^2}{2}, &amp; |x| \\leq \\delta \\\\\\delta |x| - \\frac{\\delta^2}{2}, &amp; |x| &gt; \\delta\\end{cases}\\]In the context of DQN, $x$ represents the TD-error:\\[x = Q_{\\theta}(s,a) - \\left( r + \\gamma \\max_{a'} Q_{\\theta^{\\text{target}}}(s', a') \\right)\\]$\\delta(x)$ is a hyperparameter that we choose.This makes training more robust to outliers and large TD-errors, which can occur especially early in training when Q-estimates are poor. The Huber loss prevents single large errors from causing dramatic parameter updates that destabilize learning.Understanding $V(s)$ in Value-Based RL$V(s)$ is Implicitly Computed, Not Separately StoredIn DQN, we only learn the Q-function $Q_\\theta(s,a)$ using a neural network. We don‚Äôt maintain a separate value function. However, $V(s)$ isn‚Äôt missing. It is simply computed implicitly whenever we need it through the relationship:\\[V(s) = \\max_a Q_\\theta(s, a)\\]This means $V(s)$ automatically stays synchronized with our Q-values. Every time the parameters $\\theta$change and our Q-function improves, the value function $V(s)$ implicitly changes along with it because it‚Äôs derived from the Q-values. We‚Äôre not storing two separate functions that could fall out of sync; we‚Äôre storing one function (Q) and computing the other (V) from it on-demand (if at all needed).Why Advantages and Q-Values Lead to the Same PolicyThe advantage function is defined as $A(s,a) = Q(s,a) - V(s)$. You might wonder whether we should be taking the argmax over advantages or over Q-values. The answer is that it doesn‚Äôt matter, since they give the same result.When we‚Äôre selecting an action, we compute $\\argmax_a A(s,a) = \\argmax_a [Q(s,a) - V(s)]$. The critical observation is that $V(s)$ is the same value for all actions in state $s$ at any given moment in time. Since $V(s)$ doesn‚Äôt depend on which action we‚Äôre considering, it‚Äôs just a constant that gets subtracted from all Q-values equally. Adding or subtracting the same constant from every option doesn‚Äôt change which option is largest, so:\\[\\argmax_a [Q(s,a) - V(s)] = \\argmax_a Q(s,a)\\]This is why DQN can work directly with Q-values for action selection without ever explicitly computing advantages. The policy is simply:\\[\\pi(s) = \\argmax_a Q_\\theta(s,a)\\]Advanced Policy Gradients: Bridging Policy-Based and Value-Based MethodsMotivationUp to this point, we‚Äôve studied two distinct families of reinforcement learning algorithms. Policy gradient methods learn an explicit policy (perhaps with an on-policy value function for variance reduction), and they‚Äôre fundamentally on-policy ‚Äî they must collect data using the current policy being optimized. Value-based methods like Q-learning don‚Äôt maintain an explicit policy at all; instead, they learn a value function and derive the policy implicitly through argmax operations, which allows them to be off-policy and leverage replay buffers for sample efficiency.A natural question arises: can we build a hybrid that combines the best of both worlds? Such an algorithm should be off-policy to benefit from replay buffers and reuse old data, yet it should maintain an explicit policy network that we can optimize using actor-critic style updates. This would give us the expressiveness and continuous action handling of policy gradients with the sample efficiency of value-based methods. The answer is yes, and this leads us to a family of algorithms called off-policy policy gradient methods. The challenge is: how do we derive a principled way to make this work?The Performance Difference LemmaThe key mathematical tool that enables off-policy policy gradients is the Performance Difference Lemma. This lemma provides an exact relationship between the performance of any two policies $\\pi$ and $\\mu$.Recall that the advantage function for a policy $\\mu$ is defined as:üí°$$A^{\\mu}(s, a) = Q^{\\mu}(s, a) - V^{\\mu}(s)$$This measures how much better action $a$ is compared to the average value of being in state $s$ under policy $\\mu$. Importantly, we have the property that the expected advantage under the policy‚Äôs own action distribution is zero:üí°$$\\mathbb{E}_{a \\sim \\mu(\\cdot|s)} [A^{\\mu}(s, a)] = 0$$This is because when we average the advantage over all actions weighted by how often the policy takes them, the positive and negative deviations from the mean cancel out.Now, the Performance Difference Lemma states that for any two policies $\\pi$ (the policy we‚Äôre evaluating) and $\\mu$ (some other policy), the difference in their expected returns can be expressed as:üí°$$J(\\pi) - J(\\mu) = \\mathbb{E}_{\\tau \\sim P_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right]$$What the above equation means: The trajectories $\\tau$ are generated by following policy $\\pi$ (so states are distributed according to $\\pi$‚Äôs state visitation), but the advantages being summed are computed with respect to policy $\\mu$. It connects the performance of $\\pi$ to advantages measured under a different policy $\\mu$.Why is this useful? This lemma tells us that to improve $\\pi$ relative to $\\mu$, we want to maximize the expected sum of $\\mu$‚Äôs advantages along trajectories generated by $\\pi$. If we can accurately estimate $A^\\mu$ (using a critic), we can optimize $\\pi$ to take actions that have high advantage according to $\\mu$.DerivationWe start with the definition of the return:\\[J(\\pi) = \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right]\\]Now, notice that the entire sum of rewards is just $V^\\pi(s_0)$, the value of the initial state under policy $\\pi$. So we can write:\\[J(\\pi) = \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\pi}(s_0)]\\]The key insight here is that the trajectory distribution $p_\\pi(\\tau)$ can be decomposed: first we sample $s_0$ from some initial state distribution $p(s_0)$, and then the rest of the trajectory unfolds according to $\\pi$. Since $s_0$ is sampled independently of the policy, the expectation over trajectories becomes an expectation over just the initial state.Similarly, for policy $\\mu$:\\[J(\\mu) = \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\mu}(s_0)]\\]Now let‚Äôs compute the difference:\\[J(\\pi) - J(\\mu) = \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\pi}(s_0)] - \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\mu}(s_0)]\\]Since both expectations are over the same distribution, we can combine them:\\[J(\\pi) - J(\\mu) = \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\pi}(s_0) - V^{\\mu}(s_0)]\\]We want to express this difference in terms of advantages. To do this, we need to ‚Äúunroll‚Äù the value function recursively using the Bellman equation.Recall the Bellman equation for $V^\\pi$:\\[V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s), s' \\sim p(\\cdot|s,a)} [r(s,a) + \\gamma V^{\\pi}(s')]\\]Let‚Äôs apply this to $s_0$:\\[V^{\\pi}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi(\\cdot|s_0), s_1 \\sim p(¬∑|s_0, a_0)} [r(s_0, a_0) + \\gamma V^{\\pi}(s_1)]\\]Now we want to introduce $V^\\mu$ into this expression. Notice that we can write:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [r(s_0, a_0) + \\gamma V^{\\pi}(s_1)] - V^{\\mu}(s_0)\\]\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [r(s_0, a_0) + \\gamma V^{\\pi}(s_1) - \\gamma V^{\\mu}(s_1) + \\gamma V^{\\mu}(s_1)] - V^{\\mu}(s_0)\\]Rearranging:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [r(s_0, a_0) + \\gamma V^{\\mu}(s_1) - V^{\\mu}(s_0) + \\gamma(V^{\\pi}(s_1) - V^{\\mu}(s_1))]\\]Recall that:\\[A^{\\mu}(s_0, a_0) = Q^{\\mu}(s_0, a_0) - V^{\\mu}(s_0) = r(s_0, a_0) + \\gamma V^{\\mu}(s_1) - V^{\\mu}(s_0)\\]\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [A^{\\mu}(s_0, a_0) + \\gamma(V^{\\pi}(s_1) - V^{\\mu}(s_1))]\\]Now we have a recursive structure. The last term $\\gamma (V^\\pi(s_1) - V^\\mu(s_1))$ has the same form as what we started with, just at time step 1 instead of time step 0. We can apply the same trick again:\\[V^{\\pi}(s_1) - V^{\\mu}(s_1) = \\mathbb{E}_{a_1 \\sim \\pi, s_2} [A^{\\mu}(s_1, a_1) + \\gamma(V^{\\pi}(s_2) - V^{\\mu}(s_2))]\\]Substituting this back:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [A^{\\mu}(s_0, a_0) + \\gamma \\mathbb{E}_{a_1 \\sim \\pi, s_2} [A^{\\mu}(s_1, a_1) + \\gamma(V^{\\pi}(s_2) - V^{\\mu}(s_2))]]\\]Distributing the outer expectation and the $\\gamma$:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{a_0 \\sim \\pi, s_1} [A^{\\mu}(s_0, a_0)] + \\gamma \\mathbb{E}_{a_0 \\sim \\pi, s_1, a_1 \\sim \\pi, s_2} [A^{\\mu}(s_1, a_1)] + \\gamma^2 \\mathbb{E}_{...} [V^{\\pi}(s_2) - V^{\\mu}(s_2)]\\]If we keep expanding this pattern infinitely (which is valid as long as $\\gamma &lt;1$ ensures convergence), we get:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{s_0, a_0, s_1, a_1, ..., s_t, a_t} [A^{\\mu}(s_t, a_t)]\\] where the expectation is taken over trajectories generated by policy $\\pi$ (so $s_0 \\sim p(s_0)$, then $a_0 \\sim \\pi(\\cdot s_0)$, then $s_1 \\sim p(\\cdot s_0,a_0)$, and so on). We can write this more compactly as:\\[V^{\\pi}(s_0) - V^{\\mu}(s_0) = \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right]\\]Finally, we take the expectation over the initial state distribution:\\[J(\\pi) - J(\\mu) = \\mathbb{E}_{s_0 \\sim p(s_0)} [V^{\\pi}(s_0) - V^{\\mu}(s_0)]\\]\\[= \\mathbb{E}_{s_0 \\sim p(s_0)} \\left[ \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right] \\right]\\]\\[J(\\pi) - J(\\mu) = \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right]\\]The Optimization ProblemThe Performance Difference Lemma gives us an exact expression for $J(\\pi) - J(\\mu)$, but how do we use this to actually optimize $\\pi$? We can frame this as a constrained optimization problem:üí°$$\\max_{\\pi} \\left( J(\\pi) - J(\\mu) \\right) = \\max_{\\pi} \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right]$$But there‚Äôs a catch: the expectation is over trajectories sampled from $\\pi$. This means to evaluate this expression, we need to actually run policy $\\pi$ in the environment, collect full trajectories, and compute advantages along those trajectories. Every time we update $\\pi$, we‚Äôd need fresh data from the new $\\pi$. This is on-policy learning, and it is sample-inefficient because we can‚Äôt reuse old data.When we write $\\tau \\sim p_\\pi(\\tau)$, we‚Äôre sampling an entire trajectory according to $\\pi$‚Äôs distribution. But what does this really mean? A trajectory is a sequence $(s_0, a_0, s_1, a_1, s_2, a_2, ‚Ä¶)$, and the distribution over trajectories can be factored: First, we sample $s_0$ from the initial state distribution $p(s_0)$ Then we sample $a_0$ from $\\pi(\\cdot s_0)$ Then the environment gives us $s_1$ from $p(\\cdot s_0, a_0)$ Then we sample $a_1$ from $\\pi(\\cdot s_1)$ And so on‚Ä¶The key is that which states we visit is determined by the policy. Policy $\\pi$ creates a specific pattern of state visitation over time. We call this the ‚Äústate visitation distribution‚Äù or ‚Äústate marginal distribution‚Äù and denote it $p_\\pi(s_t)$ for time step $t$, or more generally $d^\\pi(s)$ for the discounted state visitation.Here‚Äôs where we make a crucial approximation that enables off-policy learning. Instead of collecting data under $\\pi$ (the policy we‚Äôre optimizing), we can collect data under $\\mu$ (some behavioral policy, perhaps an older version of $\\pi$) and correct for the distribution mismatch. We rewrite the expectation over $\\pi$‚Äôs trajectories as:üí°$$\\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right] \\approx \\sum_{t=0}^{\\infty} \\mathbb{E}_{s_t \\sim p_{\\mu}(s_t)} \\left[ \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)} [\\gamma^t A^{\\mu}(s_t, a_t)] \\right]$$This approximation says: instead of following $\\pi$‚Äôs entire trajectory distribution (which determines which states we visit), we use states visited by $\\mu$, but we still choose actions according to $\\pi$ at each state. The key insight is that when $\\pi$ and $\\mu$ are close (which we‚Äôll enforce with a constraint), the states visited by the two policies are similar, so this approximation is reasonable.Now the inner expectation only involves sampling actions from $\\pi$ given states from $\\mu$‚Äôs distribution:\\[\\mathbb{E}_{s \\sim d^{\\mu}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [A^{\\mu}(s, a)] \\right]\\]where $d^\\mu(s)$ represents the discounted state visitation distribution under policy $\\mu$. This is something we can estimate from a replay buffer filled with $\\mu$‚Äôs experiences.We add a constraint to keep $\\pi$ close to $\\mu$, typically measured by a divergence $D(\\pi,\\mu)$. This gives us the constrained optimization problem:üí°$$\\max_{\\pi} \\mathbb{E}_{s \\sim d^{\\mu}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [A^{\\mu}(s, a)] - \\alpha D(\\pi, \\mu)\\right]$$where $\\alpha$ controls how much we penalize deviation from $\\mu$. The divergence $D$ can be chosen in different ways, such as KL divergence, total variation distance, etc., and each choice leads to a different algorithm. If $D(\\pi(\\cdot s), \\mu(\\cdot s))$ is small, then it implies that $p_\\pi(s) \\sim p_\\mu(s)$. Some Additional Mathüí°$$\\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t f(s_t, a_t) \\right] = \\frac{1}{1-\\gamma} \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [f(s,a)] \\right]$$The left side sums over time steps in a trajectory. The right side removes the explicit time dependence by using the discounted state visitation distribution $d^\\pi(s)$.The discounted state visitation distribution $d^\\pi(s)$ is defined as:üí°$$d^{\\pi}(s) = (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t p_{\\pi}(s_t = s)$$This is a distribution (not just a sum) because of the normalization factor $(1-\\gamma)$. It represents ‚Äúhow often do we visit state $s$, weighted by how far in the future it is.‚Äù The factor $(1-\\gamma)$ ensures that when we sum over all states $s$, we get $1$ (a valid probability distribution).Let‚Äôs start with the left side and manipulate it:\\[\\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t f(s_t, a_t) \\right]\\]\\[= \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} [f(s_t, a_t)]\\]\\[= \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{s_t \\sim p_{\\pi}(s_t)} \\left[ \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)} [f(s_t, a_t)] \\right]\\]Here $p_\\pi(s_t)$ is the marginal distribution over states at time $t$ when following policy $\\pi$.Now multiply and divide by $(1-\\gamma)$:\\[= \\frac{1}{1-\\gamma} \\sum_{t=0}^{\\infty} (1-\\gamma) \\gamma^t \\mathbb{E}_{s_t \\sim p_{\\pi}(s_t)} \\left[ \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)} [f(s_t, a_t)] \\right]\\]By the definition of $d^\\pi(s)$, we can write:\\[\\sum_{t=0}^{\\infty} (1-\\gamma) \\gamma^t \\mathbb{E}_{s_t \\sim p_{\\pi}(s_t)} [g(s_t)] = \\mathbb{E}_{s \\sim d^{\\pi}(s)} [g(s)]\\]This is because:\\[\\mathbb{E}_{s \\sim d^{\\pi}(s)} [g(s)] = \\sum_s d^{\\pi}(s) g(s) = \\sum_s \\left[ (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t p_{\\pi}(s_t=s) \\right] g(s)\\]Rearranging the sums:\\[= \\sum_{t=0}^{\\infty} (1-\\gamma) \\gamma^t \\sum_s p_{\\pi}(s_t=s) g(s) = \\sum_{t=0}^{\\infty} (1-\\gamma) \\gamma^t \\mathbb{E}_{s_t \\sim p_{\\pi}(s_t)} [g(s_t)]\\] Applying this to our case where $g(s) = E_{a \\sim \\pi(¬∑ s)}[f(s,a)]$: \\[= \\frac{1}{1-\\gamma} \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [f(s,a)] \\right]\\]This identity is useful because it converts a time-dependent sum (iterating through a trajectory) into a time-independent expectation over states. Instead of thinking about ‚Äúwhat happens at time 0, time 1, time 2, etc.‚Äù, we think about ‚Äúwhat happens in state $s$, weighted by how often we visit that state.‚Äù This is often more convenient for analysis and for designing algorithms.So we have:üí°$$J(\\pi) - J(\\mu) = \\mathbb{E}_{\\tau \\sim p_{\\pi}(\\tau)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A^{\\mu}(s_t, a_t) \\right] = \\frac{1}{1-\\gamma} \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [A^\\mu(s,a)] \\right]$$Different Divergence Choices Lead to Different AlgorithmsThe natural choice of divergence here is the Total Variation (TV) distance between the two policy distributions.\\[D(\\pi,\\mu) = D_{TV}(\\pi(\\cdot|s), \\mu(\\cdot|s))\\]The total variation distance between two probability distributions $p$ and $q$ over a space $\\mathcal{X}$ is defined as:\\[D_{TV}(p(\\cdot), q(\\cdot)) = \\sum_{x \\in \\mathcal{X}} |p(x) - q(x)|\\]\\[D_{TV}(p(\\cdot), q(\\cdot)) = \\int_x |p(x) - q(x)| \\, dx\\] Intuition: At every single point x in your space, you measure how much $p$ and $q$ disagree at that point, and that‚Äôs just $ p(x) - q(x) $. Then you sum up all these local disagreements across the entire space. The result is the total amount by which the two distributions differ from each other. But since it is very hard to compute the total variation distance in a very large action space using sampling, we look at other methods for estimating this.The choice of divergence measure $D(\\pi,\\mu)$ significantly impacts the resulting algorithm and its properties: **Choice 1: $D_{KL}(\\pi ¬† \\mu)$ :** This is the KL divergence from $\\pi$ to $\\mu$, measuring how much information is lost when using $\\mu$ to approximate $\\pi$. Using this divergence leads to the AWR (Advantage Weighted Regression) algorithm. **Choice 2: $D_{KL}(\\mu ¬† \\pi)$ :** This is the reverse KL divergence, from $\\mu$ to $\\pi$. Using this divergence leads to PPO (Proximal Policy Optimization) and TRPO (Trust Region Policy Optimization) algorithms. \\[D_{KL}(p||q) = \\sum_\\mathcal{X} p(x) \\log{\\frac{p(x)}{q(x)}}\\]\\[D_{KL}(p||q) = \\int_x p(x) \\log{\\frac{p(x)}{q(x)}}\\]The mathematical properties of these divergences are different. The KL divergence satisfies several useful inequalities, particularly Pinsker‚Äôs inequality, which relates KL divergence to total variation distance:\\[D_{TV}(p, q) \\leq \\sqrt{\\frac{1}{2} D_{KL}(p || q)}\\]\\[D_{TV}(p, q) \\leq \\sqrt{\\frac{1}{2} D_{KL}(q || p)}\\]These relationships help bound how different the state distributions under $\\pi$ and $\\mu$ can be, which justifies the approximation we made earlier.Advantage Weighted Regression (AWR) AWR is the algorithm that arises when we choose $D_KL(\\pi ¬† \\mu)$ as our divergence measure ‚Äî that is, we penalize $\\pi$ for being too different from $\\mu$ in the ‚Äúforward‚Äù KL direction. The key insight of AWR is that this constrained optimization problem ‚Äî maximizing the expected advantage while staying close to $\\mu$ in KL divergence ‚Äî has an analytical solution. We don‚Äôt need to run gradient descent on this objective; we can directly write down what the optimal policy looks like.To see why, let‚Äôs think about a simpler version of the problem. Suppose we want to find a distribution $p(x)$ that maximizes the expected value of some function $f(x)$, while staying close to some reference distribution $q(x)$ in KL divergence:\\[\\max_{p(x)} \\mathbb{E}_{x \\sim p(x)} [f(x)] - \\alpha D_{KL}(p \\| q)\\]This is a standard problem in variational inference, and its solution is:\\[p^*(x) \\propto q(x) \\exp\\left(\\frac{f(x)}{\\alpha}\\right)\\]The optimal distribution is the reference distribution $q$, reweighted by the exponential of the function $f$, scaled by $1/\\alpha$. Intuitively, this makes sense: we start from $q$ and upweight regions where $f$ is large (good actions) and downweight regions where $f$ is small (bad actions), with $\\alpha$ controlling how aggressively we do this reweighting.Applying this general solution to our policy optimization problem, where $p$ is $\\pi$, $q$ is $\\mu$, and $f(x)$ is $A^\\mu(s,a)$, we get the optimal policy:\\[\\pi^*(a|s) \\propto \\mu(a|s) \\exp\\left(\\frac{A^{\\mu}(s, a)}{\\alpha}\\right)\\]This is the AWR oracle solution. The optimal policy takes the behavior policy $\\mu$ and reweights each action by the exponential of its advantage under $\\mu$, scaled by the temperature parameter $\\alpha$. Actions with high positive advantage (much better than average) get upweighted exponentially, while actions with negative advantage (worse than average) get downweighted exponentially.The temperature $\\alpha$ controls the sharpness of this reweighting. A very small $\\alpha$ makes the exponential very peaked, concentrating all the probability mass on the single best action (approaching a deterministic greedy policy). A large $\\alpha$ makes the exponential flat, keeping the policy close to $\\mu$ regardless of the advantages (safe but slow improvement).How Do We Actually Use This Oracle Solution? The optimal policy $\\pi^*(a s) \\propto \\mu(a s) \\exp(A^\\mu(s,a)/\\alpha)$ is not directly a neural network we can deploy, it‚Äôs defined implicitly through this proportionality relationship. We need to find a way to fit a practical parameterized policy $\\pi_\\theta$ to this oracle. We use regression: we find the parameters $\\theta$ that make $\\pi_\\theta$ as close as possible to $\\pi^*$ by minimizing the KL divergence between them. This is equivalent to maximum likelihood estimation, where the ‚Äúlabels‚Äù are the exponential weights:\\[\\max_{\\theta} \\mathbb{E}_{s \\sim d^{\\mu}(s)} \\left[ \\mathbb{E}_{a \\sim \\mu(\\cdot|s)} \\left[ \\exp\\left(\\frac{A^{\\mu}(s,a)}{\\alpha}\\right) \\log \\pi_{\\theta}(a|s) \\right] \\right]\\] This is where the name ‚ÄúAdvantage Weighted Regression‚Äù comes from. We‚Äôre doing a weighted regression of $\\log \\pi_\\theta(a s)$, where the weights are the exponentiated advantages $\\exp(A^\\mu(s,a)/\\alpha)$. Actions that were particularly good (high advantage) contribute more to the regression loss, effectively pushing the policy to assign higher probability to those actions. The Practical AWR AlgorithmPutting this all together, the AWR algorithm works as follows. First we collect data using the current behavior policy $\\mu$ and store transitions in a replay buffer. We then train a critic to estimate the advantage function $A^\\mu(s,a)$ from this data. Finally we update the policy by solving the weighted regression problem:\\[\\max_{\\theta} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\left[ \\exp\\left(\\frac{A^{\\mu}(s,a)}{\\alpha}\\right) \\log \\pi_{\\theta}(a|s) \\right]\\]where the expectation is taken over state-action pairs sampled from the replay buffer $\\mathcal{D}$. This is just a weighted maximum likelihood problem ‚Äî we‚Äôre fitting the policy to the data, but upweighting good actions and downweighting bad ones according to their exponentiated advantages.Why AWR is Off-PolicyNotice that nowhere in the final objective do we need to sample new actions from $\\pi$. We use actions that were already stored in the replay buffer (sampled from $\\mu$). This makes AWR genuinely off-policy: we can collect a large dataset using any behavior policy, train a critic on that dataset, and then improve the policy using weighted regression on the same dataset. We can even reuse old data from earlier versions of the policy, just like DQN‚Äôs replay buffer.This is the major practical advantage of AWR over standard policy gradient methods. Standard REINFORCE or actor-critic methods must discard old data after each policy update (since the gradient estimates are only valid for the current policy). AWR can reuse all historical data, making it much more sample efficient.The Role of Temperature $\\alpha$The temperature parameter $\\alpha$ plays a crucial role that‚Äôs worth understanding deeply. When $\\alpha$ is very small, the exponential weights become very large for positive advantage actions and very small for negative advantage ones. The policy update becomes aggressive ‚Äî it strongly concentrates probability on the best observed actions. This can lead to fast improvement but risks overfitting to the finite sample of observed actions in the replay buffer.When $\\alpha$ is large, the exponential weights are all close to 1, and the regression becomes nearly unifor, which means that the policy doesn‚Äôt change much. This is safe but slow.In practice, $\\alpha$ needs to be tuned carefully. Too small and the policy collapses to imitating a few high-advantage actions (losing diversity and exploration). Too large and the policy barely improves. The right value of $\\alpha$ depends on the scale of the advantage function, which itself depends on the reward scale of the specific problem.Proximal Policy Optimization (PPO)PPO is one of the most widely used RL algorithms today, powering many real-world systems including early versions of ChatGPT‚Äôs RLHF training. It arises from a specific design choice in how we handle the off-policy optimization problem.Recall that our general off-policy optimization problem (after approximating $d^\\pi$ with $d^\\mu$) is:\\[\\max_{\\pi} \\mathbb{E}_{s \\sim d^{\\mu}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [A^{\\mu}(s, a)] \\right] - \\alpha D(\\pi, \\mu)\\] AWR used $D_{KL}(\\pi ¬† \\mu)$ as the divergence, which gave a nice closed-form solution. PPO instead uses $D_{KL}(\\mu ¬† \\pi)$, the reverse KL, leading to a very different (but equally principled) algorithm. The full PPO objective is: üí°$$\\max_{\\theta} \\mathbb{E}_{s \\sim d^{\\mu}(s)} \\left[ \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot|s)} [A^{\\mu}(s, a)] \\right] - \\alpha D_{KL}(\\mu(\\cdot|s) \\| \\pi_{\\theta}(\\cdot|s))$$The inner expectation is over actions sampled from $\\pi_\\theta$, not from $\\mu$. This means we need to sample new actions from our current policy at each state. But what if we want to learn from the actions that $\\mu$ already took? This is where importance sampling comes in.Importance SamplingImportance sampling is a technique for computing an expectation under one distribution using samples from a different distribution:\\[\\mathbb{E}_{x \\sim p(x)}[f(x)] = \\mathbb{E}_{x \\sim q(x)}\\left[\\frac{p(x)}{q(x)} f(x)\\right]\\]The ratio $p(x)/q(x)$ is called the importance weight. It corrects for the fact that we‚Äôre sampling from the wrong distribution. Applied to our policy gradient, if we want to compute an expectation under $\\pi_\\theta$ but we only have actions sampled from $\\mu$, we can write:üí°$$\\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot|s)} [A^{\\mu}(s,a)] = \\mathbb{E}_{a \\sim \\mu(\\cdot|s)} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\mu(a|s)} A^{\\mu}(s,a) \\right]$$This is the ‚Äúimportance-weighted policy gradient.‚Äù Now we can use actions already stored in our replay buffer (sampled from $\\mu$) and just reweight them by the probability ratio. Let‚Äôs define this ratio for convenience:\\[r(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\mu(a|s)}\\]The full PPO objective (before clipping) becomes:\\[\\nabla_{\\theta} \\left[ \\mathbb{E}_{s \\sim d^{\\mu}(s), a \\sim \\mu(\\cdot|s)} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\mu(a|s)} A^{\\mu}(s,a) \\right] - \\alpha D_{KL}(\\mu(\\cdot|s) \\| \\pi_{\\theta}(\\cdot|s)) \\right]\\]\\[D_{KL}(\\mu(\\cdot|s) \\| \\pi_{\\theta}(\\cdot|s)) = \\mathbb{E}_{a \\sim \\mu(\\cdot|s)} \\left[ \\log \\mu(a|s) - \\log \\pi_{\\theta}(a|s) \\right]\\]\\[\\nabla_{\\theta} D_{KL}(\\mu \\| \\pi_{\\theta}) = -\\mathbb{E}_{a \\sim \\mu(\\cdot|s)} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\right]\\]Do We Actually Need the KL Term?At this point, we might think: the KL divergence term is there for theoretical reasons (to keep the approximation valid), but is it strictly necessary in practice? The answer is nuanced.Strictly speaking, no the KL term is not a fundamental part of the optimization; it‚Äôs a regularizer to prevent $\\pi_\\theta$ from drifting too far from $\\mu$. But if we simply remove it and maximize the importance-weighted objective without any constraint, our policy updates can become very large, completely invalidating the $d^\\pi \\approx d^\\mu$ approximation and causing catastrophic policy degradation.How does $D_{KL}$ prevent this?The KL term penalizes $\\theta$ whenever $\\pi_\\theta$ deviates from $\\mu$. When you take the gradient of the full objective and do gradient ascent, the gradient of the first term (importance-weighted advantage) pushes $\\theta$ toward higher-advantage actions, potentially making large changes. The gradient of the KL term simultaneously pushes $\\theta$ back toward $\\mu$, resisting those large changes.The parameter $\\alpha$ controls the strength of this resistance. A large $\\alpha$ means the KL penalty dominates, keeping $\\pi_\\theta$ very close to $\\mu$. A small $\\alpha$ lets the advantage term dominate, allowing larger updates. At any given gradient step, these two forces balance each other, and the equilibrium point is a policy that improves on the advantage objective but doesn‚Äôt stray too far from $\\mu$. Key Insight: notice that $D_{KL}(\\mu ¬† \\pi_\\theta)$ being small implies that $\\pi_\\theta$ and $\\mu$ assign similar probabilities to all actions, which means the importance ratio $r(\\theta) = \\pi_\\theta(a s)/\\mu(a s)$ stays close to 1. So instead of explicitly computing and penalizing the KL divergence (which requires knowing $\\mu$‚Äôs probabilities for all actions), we can equivalently constrain the importance ratio $r(\\theta)$ to stay close to 1. This leads to the PPO clipping trick. The PPO Clip ObjectiveThe practical PPO implementation replaces the KL penalty with a hard clip on the importance ratio:\\[\\mathcal{L}^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{s \\sim d^{\\mu}(s), a \\sim \\mu(\\cdot|s)} \\left[ \\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\mu(a|s)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\cdot A^{\\mu} \\right]\\]where $\\epsilon$ is a hyperparameter (typically 0.1 or 0.2) that controls how much the policy is allowed to change. The clip function simply caps the importance ratio: if $r(\\theta) &lt; 1-\\epsilon$, we use $1-\\epsilon$; if $r(\\theta) &lt; 1+\\epsilon$, we use $1+\\epsilon$; otherwise we use $r(\\theta)$ as-is.Intuition: We‚Äôre saying ‚Äúwe‚Äôll only trust our importance-weighted estimates when $\\pi_\\theta$ and $\\mu$ are reasonably close (within $\\epsilon$ of each other). Beyond that, we don‚Äôt let the gradient push us further.‚Äù This prevents the runaway updates that would occur without the KL penalty, but in a computationally simpler and more stable way.In practice, the actual PPO objective uses a min to ensure we take a conservative update:\\[\\mathcal{L}^{\\text{CLIP}}(\\theta) = \\mathbb{E} \\left[ \\min\\left( r(\\theta) \\cdot A^{\\mu}, \\; \\text{clip}(r(\\theta), 1-\\varepsilon, 1+\\varepsilon) \\cdot A^{\\mu} \\right) \\right]\\]The min ensures that we never benefit from going outside the trust region, even if the clipped value happens to be larger than the unclipped value in certain cases.Problems with the Clip ObjectiveThe clipping approach works well in practice but has some subtle issues worth understanding. Let‚Äôs think through what happens in different cases by considering the sign of the advantage and the size of the ratio. Case 1: $r(\\theta) &lt; 1-\\epsilon$ and $A^\\mu(s,a) &gt;0$. The action has low probability under $\\pi_\\theta$ relative to $\\mu$, but it‚Äôs a good action (positive advantage). The clip fires and we use $r(\\theta)\\cdot A^\\mu$ instead of $(1-\\epsilon)¬∑A^\\mu$. We still receive a gradient signal pushing us to increase $\\pi_\\theta(a s)$, and this is correct behavior. Case 2: $r(\\theta) &lt; 1-\\epsilon$ and $A^\\mu(s,a) &lt; 0$. The action has low probability under $\\pi_\\theta$, and it‚Äôs a bad action (negative advantage). We get no gradient signal here because of the min objective, since $\\pi_\\theta$ is already avoiding this and we use $(1-\\epsilon)¬∑A^\\mu$ instead of $r(\\theta)\\cdot A^\\mu$. This is a good behavior.Case 3: $r(\\theta) &gt; 1+\\epsilon$ and $A^\\mu(s,a) &lt; 0$. The action has high probability under $\\pi_\\theta$ but it‚Äôs a bad action. The clip fires and we use $(1+\\epsilon)¬∑A^\\mu$. We receive a gradient signal ‚Äúunlearning‚Äù this bad action, and hence this is correct behavior.Case 4: $r(\\theta) &gt; 1+\\epsilon$ and $A^\\mu(s,a) &gt;0$. The action has high probability under $\\pi_\\theta$ and it‚Äôs a good action. The clip fires and we receive no gradient signal to further increase this action‚Äôs probability. This is a conservative choice ‚Äî we‚Äôre saying ‚Äúwe‚Äôve already increased this action‚Äôs probability enough; let‚Äôs not push further without fresh data.‚Äù This is what makes PPO safe, but it also hurts exploration: the policy won‚Äôt keep pushing toward good actions that it‚Äôs already committing to, which can slow learning.The last case is particularly interesting because it represents a deliberate conservative choice: we‚Äôre not giving gradient signal to reinforce good actions that are already highly likely. This is justified theoretically (we don‚Äôt want to stray too far from the trust region), but can slow exploration in practice.The Asymmetric Clip: DAPOThe clipping issues above motivate a variant called the asymmetric clip, used in the DAPO algorithm (particularly relevant for LLM training). The idea is to use different $\\epsilon$ values for the upper and lower clipping thresholds:\\[\\text{clip}\\left(r(\\theta), 1-\\varepsilon_{\\text{low}}, 1+\\varepsilon_{\\text{high}}\\right) \\quad \\text{where} \\quad \\varepsilon_{\\text{high}} &gt; \\varepsilon_{\\text{low}}\\]By using a larger $\\varepsilon_{\\text{high}}$ than $\\varepsilon_{\\text{low}}$, we allow the policy to more aggressively increase probability on good actions (larger trust region in the ‚Äúincrease‚Äù direction) while being more conservative about decreasing probability on bad actions (smaller trust region in the ‚Äúdecrease‚Äù direction). This asymmetry encourages more exploration by reducing the cases where we get no gradient signal for good actions.Option 2: Off-Policy Actor-Critic with a Learned Q-FunctionInstead of using $A^\\mu$ in our objective, what if we used $A^\\pi$, the advantage of the current policy $\\pi$? This requires learning $Q^\\pi$ off-policy. The objective becomes:\\[\\max_{\\theta} \\mathbb{E}_{s, a \\sim \\mu} [Q^{\\pi}_{\\phi}(s, a)]\\]The crucial insight here is that $Q^\\pi$ can be learned from any data in the replay buffer because the Bellman equation for $Q^\\pi$ doesn‚Äôt require on-policy data. We train $Q^\\pi_\\phi$ using the standard TD loss:\\[\\mathcal{L}(\\phi) = \\mathbb{E}_{s, a, r, s' \\sim \\text{RB}} \\left[ \\left( Q_{\\phi}(s,a) - y(s,a) \\right)^2 \\right]\\]where the target $y(s,a)$ uses the current policy $\\pi_\\theta$ to select the next action:\\[y(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(\\cdot|s,a), a' \\sim \\pi_{\\theta}(\\cdot|s')} [Q^{\\text{target}}_{\\bar{\\phi}}(s', a')]\\] In practice, we compute this by sampling $a‚Äô \\sim \\pi_\\theta(\\cdot s‚Äô)$ from the current policy and querying the target network $Q^{\\text{target}}_{\\bar{\\phi}}(s‚Äô, a‚Äô)$. All the machinery we learned from value-based methods applies directly: target networks, soft/hard target updates, replay buffers, and so on. This is the foundation of DDPG and TD3, which we‚Äôll cover next. What are $\\pi$ and $\\mu$? $\\mu$ is the behavior policy ‚Äî the policy that actually interacts with the environment and collects data. It‚Äôs the policy whose transitions (s, a, r, s') get stored in the replay buffer. $\\pi_\\theta$ is the target policy ‚Äî the policy we‚Äôre trying to optimize. It‚Äôs a neural network parameterized by $\\theta$ that we‚Äôre improving through gradient ascent on the Q-function objective. Crucially, $\\pi_\\theta$ does NOT directly collect data. Instead, it participates in computing the Bellman target by selecting actions at the next state $s‚Äô$.Why Is This Off-Policy?This looks like standard Q-learning because the critic training is essentially the same TD loss. But the crucial difference is that the data in the replay buffer was collected by $\\mu$, not by $\\pi_\\theta$. In on-policy methods, every gradient update must use fresh data from the current policy ‚Äî you cannot reuse old data because the gradient estimates would be biased. Here, we‚Äôre explicitly reusing old data collected by $\\mu$, which makes it off-policy.The reason this works is that the Bellman equation for $Q^\\pi$ holds for any state-action pair, regardless of which policy generated the data. When we train the critic using transitions from the replay buffer, we‚Äôre just supervising $Q_\\phi(s,a)$ toward the TD target, and this supervision is valid regardless of how $(s,a)$ was generated. The states and actions just need to be real experiences from the environment, and don‚Äôt need to come from $\\pi_\\theta$ itself.The policy update is also off-policy for the same reason. When we maximize $\\mathbb{E}\\left[Q_\\phi(s, \\pi_\\theta(s))\\right]$, the states $s$ come from $\\mu$‚Äôs replay buffer, not from $\\pi_\\theta$‚Äôs own trajectories. We‚Äôre asking ‚Äúgiven states that $\\mu$ visited, what action would $\\pi_\\theta$ take, and how good does the critic think that action is?‚Äù This is an off-policy evaluation of $\\pi_\\theta$ using $\\mu$‚Äôs state distribution.The Subtle Difference from Pure Q-LearningIn pure Q-learning (like DQN), there‚Äôs no separate actor network ‚Äî the policy is just the implicit argmax of the Q-function. Here, we have an explicit policy network $\\pi_\\theta$ that we‚Äôre optimizing separately. This matters because in continuous action spaces, you can‚Äôt take an argmax over infinitely many actions. Instead, you maintain $\\pi_\\theta$ as a differentiable function and backpropagate through $Q_\\phi$ into $\\pi_\\theta$ to push the policy toward higher-value actions. This is the actor-critic structure: the critic ($Q_\\phi$) evaluates actions and the actor ($\\pi_\\theta$) is optimized to maximize those evaluations, all using off-policy data from $\\mu$ stored in the replay buffer.Why do we maximize $Q^\\pi$ and not $A^\\pi$?Recall the definition of advantage:\\[A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)\\]Now when we write the policy optimization objective, we‚Äôre taking an expectation over states $s$ and actions $a \\sim \\pi_\\theta$:\\[\\max_{\\theta} \\mathbb{E}_{s, a \\sim \\pi_{\\theta}} \\left[ A^{\\pi}(s, a) \\right] = \\max_{\\theta} \\mathbb{E}_{s, a \\sim \\pi_{\\theta}} \\left[ Q^{\\pi}(s, a) - V^{\\pi}(s) \\right]\\]Now look at the $V^\\pi(s)$ term. It depends only on $s$, not on the action $a$. Since we‚Äôre optimizing over $\\theta$, which only affects how actions are chosen, not which states we visit (we‚Äôre using states from $\\mu$‚Äôs replay buffer), $V^\\pi(s)$ is just a constant with respect to the optimization over $\\theta$. Subtracting a constant from an objective doesn‚Äôt change where the maximum is. So:\\[\\max_{\\theta} \\mathbb{E}_{s, a \\sim \\pi_{\\theta}} \\left[ Q^{\\pi}(s, a) - V^{\\pi}(s) \\right] = \\max_{\\theta} \\mathbb{E}_{s, a \\sim \\pi_{\\theta}} \\left[ Q^{\\pi}(s, a) \\right]\\]They have the same maximizer. The policy that maximizes expected Q-value is identical to the policy that maximizes expected advantage.DDPG and TD3: Off-Policy Actor-Critic for Continuous ActionsThese algorithms are specifically designed for continuous action spaces, where taking the argmax over actions (as in DQN) is intractable.The key idea is to maintain both an explicit policy network $\\pi_\\theta$ (the actor) and a Q-function $Q_\\phi$ (the critic), and alternate between updating them. The critic is trained using the TD loss above, and the actor is updated to maximize the Q-values:\\[\\max_{\\theta} \\mathbb{E}_{s \\sim \\text{RB}} [Q_{\\phi}(s, \\pi_{\\theta}(s))]\\]Since $\\pi_\\theta(s)$ is differentiable with respect to $\\theta$, we can backpropagate through the Q-function all the way into the policy. This is called the deterministic policy gradient. The policy just shifts its output to wherever the Q-function says is best, guided by gradient ascent through the critic.Both DDPG and TD3 inherit all the stability mechanisms we‚Äôve discussed: replay buffers to decorrelate data, target networks to stabilize the Q-function training, and soft target updates to prevent sudden shifts. TD3 additionally incorporates ideas like Double DQN (using two Q-networks to reduce overestimation bias) and delayed policy updates (updating the actor less frequently than the critic, giving the critic time to settle before the policy chases it).The Deterministic Policy Gradient: How Backprop Works Through the CriticNow, the key question is: how exactly do we compute $\\nabla_\\theta \\mathbb{E}[Q^\\pi_\\phi(s, a)]$ when $a$ comes from $\\pi_\\theta$? If the policy is deterministic, meaning it outputs a single action $a_\\theta(s)$ rather than a distribution, then we can apply the chain rule directly:\\[\\nabla_{\\theta} Q^{\\pi}_{\\phi}(s, a_{\\theta}(s)) = \\nabla_{a} Q^{\\pi}_{\\phi}(s, a) \\big|_{a = a_{\\theta}(s)} \\cdot \\nabla_{\\theta} a_{\\theta}(s)\\]This is the deterministic policy gradient.Interpretation: First, ask the critic ‚Äúwhich direction should I push the action to increase $Q$?‚Äù (that‚Äôs $\\nabla_a Q$), and then ask the policy ‚Äúhow does changing $\\theta$ change the action?‚Äù (that‚Äôs $\\nabla_\\theta a_\\theta(s)$). Multiply these together via the chain rule and you have a gradient that flows all the way from the Q-function back into the policy parameters. This is exactly what DDPG uses ‚Äî the gradient signal travels from the Q-network, through the action, into the policy network.What is the policy is stochastic? If the policy is stochastic instead, outputting a Gaussian distribution $\\pi_\\theta(\\cdot s) = \\mathcal{N}(\\mu_\\theta(s), \\sum_\\theta(s))$, then we can‚Äôt directly differentiate through sampling. Instead, we use the reparameterization trick: sampling $a \\sim \\pi_\\theta(\\cdot s)$ is equivalent to sampling $\\epsilon \\sim \\mathcal{N}(0, I)$ and computing $a = \\pi_\\theta(s) + \\epsilon \\cdot \\sigma_\\theta(s)$. Now $a$ is a deterministic function of $\\epsilon$ and $\\theta$, so we can backpropagate through it. This is what SAC uses. DDPG and TD3: The Full AlgorithmWith the deterministic policy gradient in hand, we can now describe DDPG and its improved variant TD3. The core structure has two interacting components: a deterministic policy network $\\pi_\\theta(s)$ (the actor) and a Q-function network $Q_\\phi(s,a)$ (the critic).Training the Critic: We sample transitions (s, a, r, s') from the replay buffer and minimize the TD loss:\\[\\mathcal{L}(\\phi) = \\mathbb{E}_{s,a,r,s' \\sim \\text{RB}} \\left[ \\left( Q_{\\phi}(s,a) - y(s,a) \\right)^2 \\right]\\]where the target is computed using the target networks and the current policy:\\[y(s,a) = r(s,a) + \\gamma \\min\\left( Q^{\\text{target}}_{\\phi_1}(s', a'), Q^{\\text{target}}_{\\phi_2}(s', a') \\right), \\quad a' \\leftarrow \\pi_{\\theta}(s') + \\varepsilon\\]Notice two things here. First, we use two Q-networks $Q_{\\phi1}$ and $Q_{\\phi2}$ and take the minimum of their predictions when computing the target. This is the Double Q-learning trick borrowed from DQN ‚Äî it directly addresses overestimation bias, because if either network overestimates, the $\\min$ operator selects the more conservative (lower) estimate. Second, we add a small noise $\\epsilon$ to the next action $a‚Äô$ when computing the target. This is called target policy smoothing and prevents the policy from exploiting sharp Q-function peaks that might be artifacts of function approximation error.Data Collection: Data is collected by running the deterministic policy with added exploration noise:\\[a \\sim \\pi_{\\theta}(s) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]Since a deterministic policy always outputs the same action for a given state, it would never explore without this noise. The noise provides the necessary stochasticity during data collection.Training the Actor: The policy is updated to maximize the Q-function using the deterministic policy gradient:\\[\\theta' \\leftarrow \\theta + \\eta \\nabla_{\\theta} \\mathbb{E}_{s \\sim \\text{RB}} \\left[ Q_{\\phi_1}(s, \\pi_{\\theta}(s)) \\right]\\]One important detail in TD3: the actor is updated less frequently than the critic, typically once every $d$ critic updates ($d=2$ is common). The motivation is to let the critic stabilize before the policy chases after it. If the policy updates as fast as the critic, it might end up chasing noisy or inaccurate Q-value estimates, destabilizing training.Is there a separate policy $\\mu$?Yes and no. There is no separate policy $\\mu$ that‚Äôs distinct from $\\pi_\\theta$. Instead, $\\mu$ is just an older or noisier version of $\\pi_\\theta$ itself.In DDPG/TD3, the ‚Äúbehavior policy‚Äù $\\mu$ that collects data is simply:\\[\\mu(a|s) = \\pi_{\\theta_{\\text{old}}}(s) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]where $\\pi_{\\theta_{\\text{old}}}$ is the policy from some previous iteration (or even the current iteration), and $\\epsilon$ is exploration noise. So $\\mu$ is not a separately maintained network ‚Äî it‚Äôs the actor network $\\pi_\\theta$, possibly from a few steps ago, with Gaussian noise added for exploration.The Replay Buffer PerspectiveThe replay buffer in DDPG/TD3 contains transitions (s, a, r, s') that were collected by running $\\pi_\\theta + \\epsilon$ at various points during training. So the $\\mu$ we keep referring to is really a mixture of past versions of $\\pi_\\theta$, all with exploration noise added.When we say ‚Äústates come from $d^\\mu$,‚Äù we mean: the states in the replay buffer were visited by various historical versions of $\\pi_\\theta$ (with noise), and we‚Äôre reusing those states to train the current $\\pi_\\theta$.Why This Works (Off-Policy Nature)The key insight is that DDPG/TD3 don‚Äôt care which policy generated the data in the replay buffer, as long as: The Q-function can be learned from that data (which it can, because Bellman equations hold for any state-action pairs) The states cover regions where the current policy $\\pi_\\theta$ actually operates (which they do if $\\pi_\\theta$ hasn‚Äôt changed too drastically)So unlike PPO where $\\mu$ is explicitly the ‚Äúold policy‚Äù from the previous iteration and we carefully track the distribution mismatch with importance ratios, in DDPG/TD3 we just say ‚Äú$\\mu$ is whatever policy (or mixture of policies) generated the data in the replay buffer‚Äù and we don‚Äôt worry about correcting for distribution shift ‚Äî we just trust that the off-policy learning will work as long as the data is reasonably diverse and recent.Comparison to AWR/PPOTo clarify the distinction: PPO/AWR: $\\mu$ is explicitly defined as $\\pi_{\\theta_{\\text{old}}}$ from the previous iteration. We track it carefully and either use importance sampling (PPO) or solve a constrained optimization (AWR) to account for the distribution shift. DDPG/TD3: $\\mu$ is implicitly ‚Äúwhatever generated the replay buffer data.‚Äù We don‚Äôt explicitly track it or correct for it ‚Äî we just assume the off-policy Q-learning will handle the distribution shift automatically.This is one reason DDPG/TD3 can be less stable than SAC. There‚Äôs no explicit mechanism to keep $\\pi_\\theta$ close to the data distribution, so if the policy changes too quickly, it can end up in regions of the state-action space where the Q-function is poorly trained, leading to bad updates. SAC‚Äôs entropy regularization helps with this by keeping the policy ‚Äúspread out‚Äù and thus naturally staying closer to the data distribution.Soft Actor-Critic (SAC)SAC represents a conceptually deeper departure from DDPG/TD3 than just an engineering improvement. It is built on a fundamentally different objective called Maximum Entropy RL.What is Maximum Entropy RL?Standard RL seeks a policy that maximizes expected cumulative reward:\\[\\pi^*_{\\text{RL}} = \\arg\\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_t \\gamma^t r(s_t, a_t) \\right]\\]Maximum Entropy RL adds a bonus for the policy being uncertain (i.e., having high entropy):\\[\\pi^*_{\\text{maxent}} = \\arg\\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t \\left( r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t)) \\right) \\right]\\] where $\\mathcal{H}(\\pi(\\cdot s_t))$ is the entropy of the policy‚Äôs action distribution at state $s_t$: \\[\\mathcal{H}(\\pi(\\cdot|s_t)) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} \\left[ -\\log \\pi(\\cdot|s_t) \\right]\\]So at every time step, the policy is rewarded not just for taking high-reward actions, but also for being as random as possible ‚Äî for spreading probability across many actions rather than collapsing onto a single one. The temperature $\\alpha$ controls this trade-off: large $\\alpha$ means exploration is heavily rewarded, small $\\alpha$ recovers standard RL.Intuition: Think of standard RL as a student who memorizes only the answer to a specific exam problem. MaxEnt RL is like a student who tries to understand all plausible solutions to a problem ‚Äî even if one solution is clearly best, it maintains some understanding of other approaches. This breadth of knowledge turns out to be crucial for generalization and robustness.Simplifying the MaxEnt Objective The entropy term can be pulled inside the sum and absorbed into a modified reward function. Since $\\mathcal{H}(\\pi(\\cdot s_t)) = \\mathbb{E}_{a \\sim \\pi(\\cdot s_t)} \\left[ -\\log \\pi(\\cdot s_t) \\right]$, we can write the whole objective as: \\[\\pi^*_{\\text{maxent}} = \\arg\\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t \\left( r(s_t, a_t) - \\alpha \\log \\pi(a_t|s_t) \\right) \\right]\\]This gives us a modified reward:\\[\\tilde{r}_{\\alpha}(s, a) = r(s, a) - \\alpha \\log \\pi(a|s)\\] This is an elegant simplification: we‚Äôve turned MaxEnt RL into standard RL, just with a reward function that includes a penalty for being too deterministic. The $-\\alpha \\log \\pi(a s)$ term acts as an intrinsic reward for exploration ‚Äî actions that the policy assigns low probability to (meaning they‚Äôre surprising/exploratory) have a large $-\\log \\pi$ value and thus contribute more intrinsic reward. Backup the entropy: the entropy bonus at each time step gets backed up through the Bellman equation, just like rewards do, so the Q-function automatically accounts for future entropy bonuses throughout the trajectory.The SAC Q-FunctionSubstituting the modified reward into the Bellman equation gives SAC‚Äôs Q-function update:\\[Q(s, a) \\leftarrow r(s,a) + \\gamma \\mathbb{E}_{a' \\sim \\pi} \\left[ Q(s', a') - \\alpha \\log \\pi(a'|s') \\right]\\] Compare this to TD3‚Äôs Bellman update, which just has $r(s,a) + \\gamma \\mathbb{E}[Q(s‚Äô,a‚Äô)]$. SAC‚Äôs update additionally subtracts $\\alpha \\log \\pi(a‚Äô s‚Äô)$ from the target, which means Q-values are lower for actions that the policy takes with high probability. This elegantly discourages the policy from collapsing onto a single action even when it‚Äôs the best-reward action. SAC‚Äôs Policy UpdateFor the policy update, SAC also benefits from the MaxEnt objective. The gradient update for the policy becomes:\\[\\theta' \\leftarrow \\theta + \\eta \\nabla_{\\theta} \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot|s)} \\left[ Q_{\\phi}(s, a) - \\alpha \\log \\pi_{\\theta}(a|s) \\right]\\]The policy is now simultaneously trying to maximize Q-values (exploit good actions) and maximize entropy (stay diverse and exploratory). These are competing objectives kept in balance by $\\alpha$. When $\\alpha$ is large, the policy spreads out; when $\\alpha$ is small, it concentrates on high-Q actions. This balance is what gives SAC its remarkable stability. Unlike DDPG which can suddenly collapse its policy onto a single action, SAC always maintains a floor of exploration.Practical Design Choices in SACSAC makes three key practical design decisions that distinguish it from DDPG/TD3. Like TD3, SAC uses two critic networks and takes the minimum of their Q-value estimates. This is critical in SAC because the policy is stochastic and is being trained to maximize Q-values. If either Q-network overestimates, the policy will exploit that overestimation and degrade. The $\\min$ operator provides a conservative lower bound:\\[Q_{\\text{target}} = r + \\gamma \\mathbb{E}_{a'} \\left[ \\min(Q_{\\phi_1}(s', a'), Q_{\\phi_2}(s', a')) - \\alpha \\log \\pi(a'|s') \\right]\\] SAC requires no explicit exploration noise during data collection, unlike DDPG/TD3 which must add Gaussian noise to a deterministic policy. Because $\\pi_\\theta$ is inherently stochastic (a Gaussian whose variance is also learned), every action sampled from it naturally has uncertainty. The entropy term in the objective actively encourages this uncertainty to remain non-negligible throughout training, so exploration emerges automatically from the policy itself. The temperature $\\alpha$ is dynamically tuned during training. Rather than treating $\\alpha$ as a fixed hyperparameter, SAC frames it as a constrained optimization: we want the policy to maintain at least a target level of entropy $\\mathcal{H}_{\\text{target}}$ (which is typically set to $- \\mathcal{A} $, the negative dimensionality of the action space). The value of $\\alpha$ is then automatically adjusted to ensure this constraint is met. If the policy‚Äôs entropy falls below the target, $\\alpha$ increases to encourage more exploration; if it‚Äôs above the target, $\\alpha$ decreases to let the policy specialize. Why Does Adding Entropy Help?The entropy bonus has two deeply important practical benefits. First, a stochastic policy is simply easier to optimize than a deterministic one. The optimization landscape is smoother because the policy‚Äôs output is a distribution, not a point ‚Äî gradients flow more reliably and the policy doesn‚Äôt get stuck in sharp local optima as easily. Second, and more fundamentally, the entropy bonus creates ‚Äúmore ways to succeed.‚Äù Rather than converging to a single behavior that achieves high reward, MaxEnt RL finds a distribution over behaviors that are all approximately equally good. If the environment has multiple valid strategies, SAC discovers all of them. This makes SAC much more robust: if one strategy stops working (due to environment changes or unseen situations), the policy still knows about other strategies and can switch.Think of it like this: a deterministic policy is like a chess player who has memorized one perfect opening. A MaxEnt policy is like a chess player who has mastered many different openings. Against a novel opponent, the second player is far more adaptable.Summary: DDPG vs TD3 vs SACAll three algorithms share the same fundamental structure, an off-policy actor-critic with a replay buffer and target networks, but differ in key design choices. DDPG uses a deterministic policy with a single Q-network, making it simple but prone to overestimation and instability. TD3 fixes DDPG‚Äôs instability with two Q-networks (clipped double Q), delayed policy updates, and target policy smoothing. SAC goes further by switching to a stochastic policy with a MaxEnt objective, which provides natural exploration, better optimization landscapes, and automatic entropy tuning, but at the cost of slightly more complexity.In practice, SAC tends to be the most reliable of the three, which is why it remains a dominant algorithm for continuous control tasks today." }, { "title": "Robot Learning", "url": "/posts/robot-learning/", "categories": "CMU MRSD, Robotics", "tags": "rl, learning, nnets", "date": "2026-01-19 11:00:00 -0500", "snippet": "" }, { "title": "Reinforcement Learning", "url": "/posts/reinforcement-learning/", "categories": "CMU MRSD, Robotics", "tags": "rl, learning, nnets", "date": "2026-01-19 11:00:00 -0500", "snippet": "Deep LearningThis series of blogs are my notes from the class 10-703 Deep Reinforcement Learning and Control, taught by Katerina Fragkiadaki and Aviral Kumar at CMU, as well as learnings from the book ‚ÄúReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto‚Äù. For my own sake of understanding and simplicity, the blog has been divided into 2 categories: Introduction to Reinforcement Learning Deep Reinforcement Learning Advanced Deep Reinforcement Learning" }, { "title": "Multi-Modal Machine Learning", "url": "/posts/mmml/", "categories": "CMU MRSD, Robotics", "tags": "ml, learning, nnets", "date": "2026-01-19 11:00:00 -0500", "snippet": "" }, { "title": "Introduction to Reinforcement Learning", "url": "/posts/intro-to-rl/", "categories": "Blog, Robotics", "tags": "learning, rl", "date": "2026-01-19 11:00:00 -0500", "snippet": "In ProgressThis blog is a collection of my notes based on the book ‚ÄúReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto‚Äù.Finite Markov Decision ProcessesContextThe agent-environment interactionReinforcement learning is built around the interaction between an agent and an environment over time. At each step: The agent observes the current situation (state). It chooses an action based on a policy. The environment responds by providing a reward and transitioning to a new state.This back-and-forth loop defines the learning problem.Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized.üí°$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots¬†= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$where $0 \\le \\gamma \\le 1,$ is called the discount rate.The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. If $\\gamma = 0,$ the agent is ‚Äúmyopic‚Äù in being concerned only with maximizing immediate rewards.In case of episodic tasks that have a ‚Äútermination state‚Äù, the discounted rewards can be written asüí°$$G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$$Markov Decision ProcessIn the most general case, the environment‚Äôs response at time $t+1$ can depend on everything that has happened earlier. Thus, the dynamics can be defined by the complete probability distribution:\\[\\Pr\\{ R_{t+1} = r , S_{t+1} = s' \\mid S_0, A_0, R_1, \\ldots, S_{t-1}, A_{t-1}, R_t, S_t, A_t \\}\\]If the state signal has the Markov property, then the environment‚Äôs response at time $t+1$ depends only on the current state and action. In this case, the environment‚Äôs dynamics are simplified to:\\[p(s', r \\mid s, a) = \\Pr\\{ R_{t+1} = r, S_{t+1} = s' \\mid S_t = s, A_t = a \\}\\]Given these dynamics, we can compute everything else we might want to know about the environment.Expected reward for a state‚Äìaction pair:üí°$$r(s,a)= \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a] = \\sum_{r\\in\\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a).$$From definition of conditional expectation\\[\\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a] = \\sum_{r\\in\\mathcal{R}} r \\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a\\}\\]Using law of total probability\\[\\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a\\}= \\sum_{s'\\in\\mathcal{S}} \\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\} \\\\= \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a).\\]Plugging back in, we get\\[r(s,a)= \\sum_{r\\in\\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a)\\]State transition probability:üí°$$p(s'\\mid s,a)= \\sum_{r\\in\\mathcal{R}} \\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\}= \\sum_{r\\in\\mathcal{R}} p(s',r\\mid s,a).$$Expected rewards for state‚Äìaction‚Äìnext-state triples:üí°$$r(s,a,s') = \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a, S_{t+1}=s']= \\frac{\\sum_{r\\in\\mathcal{R}} r p(s',r\\mid s,a)}{p(s'\\mid s,a)}$$Start from conditional expectation\\[r(s,a,s') = \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a, S_{t+1}=s'] \\\\ =\\sum_{r\\in\\mathcal{R}} r \\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a, S_{t+1}=s'\\}\\]Then using Bayes‚Äô rule\\[\\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a, S_{t+1}=s'\\}=\\frac{\\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\}}{\\Pr\\{S_{t+1}=s' \\mid S_t=s, A_t=a\\}}=\\frac{p(s',r\\mid s,a)}{p(s'\\mid s,a)}.\\]Plugging this in above, we get\\[r(s,a,s') = \\frac{\\sum_{r\\in\\mathcal{R}} r p(s',r\\mid s,a)}{p(s'\\mid s,a)}\\]Value FunctionsState-Value Functionüí°$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]= \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s \\right]$$This is the expected long-term return (sum of discounted rewards) if you start in state $s$ and follow policy $\\pi$.Intuition: ‚ÄúIf I‚Äôm standing in this state, and I keep behaving according to my current policy, how good is this situation in the long run?‚ÄùAction-Value Functionüí°$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]= \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right]$$This is the expected long-term return if you start in state $s,$ take action $a$ immediately, and then afterwards follow policy $\\pi$.Intuition: ‚ÄúIf I‚Äôm in this state and I try this particular move right now, and then keep following my usual policy, how good will things turn out?‚ÄùRelationship between state-value and action-value functionsüí°$$v_\\pi(s) = \\sum_a\\pi(a|s)\\space q_\\pi(s,a)$$The value functions $v_\\pi$ and $q_\\pi$ can be estimated from experience. For example, if an agent follows policy $\\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state‚Äôs value, $v_\\pi(s),$ as the number of times that state is encountered approaches infinity. If separate averages are kept for each action taken in a state, then these averages will similarly converge to the action values, $q_\\pi(s,a).$ These estimation methods are called Monte Carlo methods, and are discussed in the sections below.Bellman Equation\\[v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\\]Consider random variables $X,Y,Z$\\[\\mathbb{E}[X \\mid Z] = \\sum_y \\mathbb{E}[X \\mid Y=y, Z] \\space \\Pr(Y=y \\mid Z)\\]Let\\[X = R_{t+1} + \\gamma G_{t+1}, \\quad Y = A_t, \\quad Z = \\{ S_t=s \\}\\]Then\\[\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s] = \\sum_a \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a] \\Pr(A_t=a \\mid S_t=s) \\\\ v_\\pi(s) = \\sum_a \\pi(a \\mid s) \\space \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a]\\]Applying the same law again\\[\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a]\\\\= \\sum_{s'} \\sum_r \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] \\space p(s',r \\mid s,a)\\]Given $R_{t+1} = r$ we know the first term is just $r.$For the second term, by Markov property, the future return $G_{t+1}$ depends on the future only through $S_{t+1},$ so\\[\\mathbb{E}_{\\pi}[G_{t+1} \\mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] = \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s']\\]Therefore the inner expectation becomes\\[r + \\gamma \\space \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s']\\]Plugging this back in, we get\\[v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) \\Big[ r + \\gamma \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s'] \\Big]\\]Finally, recognise that $\\mathbb{E}{\\pi}[ G{t+1} \\mid S_{t+1}=s‚Äô] = v_\\pi(s‚Äô)$. So we haveüí°$$v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) \\Big[ r + \\gamma v_{\\pi}(s') \\Big]$$Intuition:In a nutshell, the Bellman equation says that: The value of a state = immediate reward + future value. But instead of computing everything into the infinite future directly, it breaks the problem down recursively.The value of a state $v_\\pi(s)$ is obtained by: Looking at all possible actions $a$ that policy $\\pi$ might choose For each action, look at all possible next states $s‚Äô$ and rewards $r$ that the environment could produce Weight each outcome by how likely it is Add up the immediate reward $r$ plus the discounted value of the next state $\\gamma v_\\pi(s‚Äô)$Optimal Value FunctionsThere exists at least one policy that is better than or equal to all other policies. This is called the optimal policy.Optimal state-value function:üí°$$v_*(s) = \\max_{\\pi} v_\\pi(s)$$for all $s \\in S$.Optimal action-value function:üí°$$q_*(s,a) = \\max_\\pi q_\\pi(s,a)$$for all $s \\in S, \\space a \\in \\mathcal{A}(s).$For the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_$ in terms of $v_$ as:üí°$$q_*(s,a) = \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_{t}=s, A_t=a] $$Bellman Optimality EquationBellman Optimality for state-value function:It expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\\[{v_{*}(s)} = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_{}}(s,a) \\newline = \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[G_t \\middle| S_t = s, A_t = a \\right] \\newline = \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right] \\newline= \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[ R_{t+1} + \\gamma \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+2} \\middle| S_t = s, A_t = a \\right] \\newline= \\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1}) \\mid S_t = s, A_t = a] \\newline= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s',r} p(s',r \\mid s,a) \\big[ r + \\gamma v_{*}(s') \\big]\\]So,üí°$$v_*(s)= \\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1}) \\mid S_t = s, A_t = a]\\\\ = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s',r} p(s',r \\mid s,a) \\big[ r + \\gamma v_{*}(s') \\big]$$Bellman Optimality for action-value function:üí°$$q_*(s,a) = \\mathbb{E}\\big[R_{t+1} + \\gamma \\max_{a'}q_*(S_{t+1},a') \\mid S_t=s, A_t=a\\big] \\\\= \\sum_{s',r}p(s',r \\mid s,a) \\big[r + \\gamma \\max_{a'}q_*(s',a') \\big]$$Backup diagrams for (a) v‚àó and (b) q‚àóOptimal Value Functions to Optimal PoliciesUsing the Optimal State-Value Function $v_*$:Once we know $v_*,$ we can derive an optimal policy by: Looking at the actions that achieve the maximum in the Bellman optimality equation Any policy that assigns probability only to these maximizing actions is optimalThis process is like a one-step lookahead search ‚Äî $v_*$ already accounts for all long-term returns, so just looking at the immediate action + expected next-state values is enough.Simply put, a policy that is greedy w.r.t $v_{*}$ (chooses the best action based only on short-term consequences evaluated via $v_{*}$) is optimal in the long run, because $v_{*}$ already encodes all future returns.Using the Optimal Action-Value Function $q_*$:With $q_*$ the process is even simpler. For any state $s$, just pick the action $a$ that maximizes $q_*(s,a)$.This is easier because: $q_*$ already represents the cached results of the one-step lookahead No need to know transition probabilities $p(s‚Äô,r \\mid s,a)$ or successor states Directly gives the optimal expected return for each state‚Äìaction pairKey insights: $v_* \\implies$need one-step lookahead to find best actions $q_* \\implies$no lookahead needed, just take the $\\max_a q_*(s,a)$Practical Limits True optimality is rarely achievable Computing the exact optimal policy is usually too expensive (requires solving the Bellman optimality equation exactly). Even with a complete model of the environment, tasks like chess are too complex to solve optimally. Constraints in practice Computation per step is limited (agent can‚Äôt spend forever planning) Memory is limited (can‚Äôt store values for every possible state) Tabular vs Function Approximation Tabular methods: possible when state/action space is small (store values in arrays/tables) Function approximation: required when state space is huge or continuous (use compact parameterized functions, e.g. neural networks) Approximating optimal behavior Not all states matter equally Agents can focus on frequent states and ignore rare states with little effect on overall performance Dynamic ProgrammingPolicy EvaluationPolicy ImprovementPolicy IterationValue IterationAsynchronous Dynamic ProgrammingGeneralized Policy IterationEfficiencyMonte Carlo MethodsTemporal Difference Learning" }, { "title": "Diffusion and Flow Matching", "url": "/posts/diffusion-flow-matching/", "categories": "Blog, Robotics", "tags": "diffusion, flow, nnnets, genai", "date": "2026-01-01 11:00:00 -0500", "snippet": "Diffusion and Flow Matching" }, { "title": "Memory Management in C++", "url": "/posts/memory-management-cpp/", "categories": "Blog, C++", "tags": "cpp, c++, memory, notebooklm", "date": "2026-01-01 11:00:00 -0500", "snippet": "A Complete Guide to Memory Management in C++Introduction: The Power and Responsibility of C++As C++ creator Bjarne Stroustrup once said, ‚ÄúC makes it easy to shoot yourself in the foot; C++ makes it harder, but when you do, you blow your whole leg off.‚Äù This quote perfectly captures the essence of memory management in C++.Unlike languages with automatic garbage collection like Java or C#, which have an internal process to release memory, C++ grants the programmer direct control over memory. This control is a primary source of its power and performance, allowing for the creation of extremely fast and efficient code.However, this control comes with the responsibility of managing resources manually. A failure to release an unused resource is called a memory leak. A leaked resource becomes unavailable for reuse by the program itself, gradually consuming memory until the process exits. Memory leaks are a common cause of bugs and instability.This guide will equip you with the principles and modern techniques to manage memory safely and effectively in C++. The goal is to transform what can be a daunting task into a manageable and systematic one, enabling you to write C++ code that is not only powerful but also robust and safe.1. The Two Worlds of Memory: The Stack and The HeapA C++ program primarily uses two distinct areas of memory for its data: the Stack and the Heap (also known as the free store). The Stack The stack is a highly organized and fast region of memory where data is allocated and deallocated automatically. It operates on a ‚ÄúLast-In, First-Out‚Äù (LIFO) principle. The stack is where local variables and function call information are stored. When a function is called, its variables are ‚Äúpushed‚Äù onto the stack; when the function exits, they are automatically ‚Äúpopped‚Äù off. Its primary limitation is its fixed and relatively small size. The Heap The heap is a large, less organized pool of memory available for data that needs to have a long lifetime or is too large to fit on the stack. Unlike the stack, memory on the heap must be allocated and deallocated manually by the programmer using the new and delete operators. While flexible, this manual control is where most memory management errors occur.The following table synthesizes the key differences between these two memory regions. Feature The Stack The Heap (Free Store) Allocation/Deallocation Automatic (managed by the compiler) Manual (managed by the programmer via new/delete) Speed Fast allocation and deallocation Slower due to more complex management Size Fixed and limited in size (~2MB) Large and flexible Management Managed by the compiler/runtime (via function calls) Handled by the programmer and memory manager Risk Low risk of errors Prone to errors like fragmentation and leaks It is important to note that the actual physical location of these two areas of memory is ultimately the same, the RAM.Understanding the difference between the stack and the heap is fundamental, as the most common and dangerous errors arise from mismanaging the heap.2. The Perils of Manual Memory ManagementC-style or ‚Äúnaive‚Äù C++ approaches to memory management are a frequent source of bugs. A common but unreliable approach is the ‚ÄúSandwich Pattern,‚Äù where a call to new is followed by some working code and then a corresponding call to delete. This pattern is insidious because it offers no guarantee that the delete statement will ever be reached ‚Äî an exception or a premature loop exit can easily cause it to be skipped, leading to a memory leak.2.1. Memory Leaks: The Silent Resource DrainA memory leak occurs when heap-allocated memory is no longer needed by the program but is not released back to the operating system with delete. This memory becomes unusable for the remainder of the program‚Äôs execution, slowly draining available resources.void cause_a_leak() { // Memory is allocated for an integer on the heap. int* leaky_ptr = new int(42); // The function returns, but `delete leaky_ptr;` was never called. // The pointer `leaky_ptr` is gone, but the memory it pointed to is now lost.} // Memory leak occurs here.2.2. Dangling Pointers: Touching Freed MemoryA dangling pointer is a pointer that continues to point to a memory location that has already been deallocated (freed via delete). Attempting to access or use a dangling pointer leads to undefined behavior, which can manifest as corrupted data, unexpected crashes, or security vulnerabilities.void create_dangling_pointer() { int* ptr = new int(10); int* dangling_ptr = ptr; // The memory is freed. delete ptr; // `dangling_ptr` now points to deallocated memory. // Using it here is dangerous and results in undefined behavior. // *dangling_ptr = 100; // CRASH! Or worse...}2.3. Double Free: Deleting TwiceA double free error occurs when the program attempts to delete the same memory location more than once. This action also leads to undefined behavior and can corrupt the internal data structures that the heap manager uses to track memory, often leading to a crash.void cause_double_free() { int* ptr = new int(20); delete ptr; // Attempting to delete the same memory again is a serious error. // delete ptr; // Undefined behavior!}2.4. Buffer Overruns &amp; Underruns: Out of BoundsC++ does not have built-in range checking for raw arrays. This means it is possible to write or read past the allocated boundaries of an array, an error known as an ‚Äúinvalid write‚Äù or ‚Äúinvalid read.‚Äù This is a major source of security vulnerabilities, as it can be exploited to overwrite critical program data or execute arbitrary code.void buffer_overrun() { // Allocate a heap array for 10 integers (indices 0 through 9). int* numbers = new int[10]; // This loop writes one element past the end of the array. for (int i = 0; i &lt;= 10; ++i) { // Off-by-one error: should be i &lt; 10 // On the last iteration (i=10), we write out of bounds. numbers[i] = i; // Invalid write! } delete[] numbers;}2.5. Heap Fragmentation: The Checkerboard of MemoryWhen a program performs many frequent allocations and deallocations of small objects on the heap, the memory can become fragmented. Imagine a solid block of memory that, over time, turns into a checkerboard of used and free chunks.The primary consequence is that even if the total amount of free memory is sufficient, the program may be unable to find a single contiguous block large enough to satisfy a large allocation request. This can slow down an application or, in extreme cases, cause it to fail.// Heap Fragmentation Simulation#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;new&gt;int main() { // We will simulate frequent allocations of small amounts of memory const int NUM_SMALL_CHUNKS = 10000; const size_t SMALL_SIZE = 64; // Small objects like station codes std::vector&lt;char*&gt; pointers; std::cout &lt;&lt; \"Step 1: Filling the heap with small chunks...\\n\"; for (int i = 0; i &lt; NUM_SMALL_CHUNKS; ++i) { pointers.push_back(new char[SMALL_SIZE]); } // Step 2: Create the \"Checkerboard\" // We deallocate every second chunk. This leaves holes of free memory // separated by small \"islands\" of still-allocated memory. std::cout &lt;&lt; \"Step 2: Creating a checkerboard pattern by deallocating every other block...\\n\"; for (int i = 0; i &lt; NUM_SMALL_CHUNKS; i += 2) { delete[] pointers[i]; pointers[i] = nullptr; } // Step 3: Attempt a large contiguous allocation // Total free memory is (NUM_SMALL_CHUNKS / 2) * SMALL_SIZE. // However, the largest *contiguous* block is only SMALL_SIZE. size_t large_request = SMALL_SIZE * 5; std::cout &lt;&lt; \"Step 3: Attempting to allocate a large block: \" &lt;&lt; large_request &lt;&lt; \" bytes...\\n\"; try { char* large_block = new char[large_request]; std::cout &lt;&lt; \"Success: Contiguous block found!\\n\"; delete[] large_block; } catch (const std::bad_alloc&amp; e) { // The heap manager cannot find a single block large enough std::cerr &lt;&lt; \"Failure: \" &lt;&lt; e.what() &lt;&lt; \" - The heap is too fragmented! [2]\\n\"; } // Cleanup remaining pointers for (auto p : pointers) if (p) delete[] p; return 0;}These classic pitfalls demonstrate that manual memory management is fragile because it relies on the programmer to perform cleanup. Modern C++ provides a robust philosophy that solves these problems by making cleanup and .3. The Guiding Principle: RAII (Resource Acquisition Is Initialization)RAII is the central pillar of modern C++ resource management. The principle is simple yet powerful: resource ownership should be tied to an object‚Äôs lifetime. This means that an object should acquire a resource in its constructor and release it in its destructor.The RAII lifecycle works in three deterministic steps: A resource (such as heap memory, a file handle, or a network socket) is acquired in an object‚Äôs constructor. The owning object is declared on the stack. When the object goes out of scope (for example, at the end of a function), its destructor is automatically and guaranteed to be called. The destructor then releases the resource.This pattern makes resource release deterministic and automatic, ensuring that resources are properly cleaned up even if errors occur or exceptions are thrown.Let‚Äôs look at a practical example.Before RAII (Manual Management) Here, the widget class manually allocates and deallocates memory, requiring an explicit destructor.class widget {private: int* data;public: widget(const int size) { data = new int[size]; // 1. Acquire resource in constructor } ~widget() { delete[] data; // 3. Release resource in destructor } void do_something() {}};void functionUsingWidget() { widget w(1000000); // 2. Object is created on the stack w.do_something();} // w goes out of scope, its destructor is automatically called.Modern C++ with RAII (Using a Smart Pointer) By using a smart pointer (std::unique_ptr), we delegate memory ownership to a dedicated RAII object. This eliminates the need for an explicit destructor in our class, making the code simpler and safer.#include &lt;memory&gt;class widget {private: std::unique_ptr&lt;int[]&gt; data; // The smart pointer now owns the memorypublic: widget(const int size) { data = std::make_unique&lt;int[]&gt;(size); // 1. Resource is acquired } // 3. No explicit destructor needed! The unique_ptr handles it. void do_something() {}};void functionUsingWidget() { widget w(1000000); // 2. Object created on the stack w.do_something();} // w goes out of scope, its member `data` is automatically destroyed, releasing the memory.RAII is the philosophy, and smart pointers are the primary tool you will use to implement it for dynamically allocated memory.4. The Modern Toolkit: Smart PointersSmart pointers are class templates provided by the C++ Standard Library. They act as wrappers around a raw pointer, automatically managing its lifetime and ensuring that the memory it points to is correctly deallocated. They are the C++ way of enforcing RAII for heap memory.4.1. std::unique_ptr: The Default Choice for Exclusive OwnershipA std::unique_ptr maintains exclusive ownership of a heap-allocated object. This means: It cannot be copied. You can only move ownership from one unique_ptr to another. This rule is enforced at compile time, guaranteeing that only one unique_ptr can own the resource at any given time. It is a ‚Äúzero-cost abstraction‚Äù, meaning it has no performance or memory overhead compared to a raw pointer.For these reasons, std::unique_ptr should be your default choice for managing dynamic memory.#include &lt;iostream&gt;#include &lt;memory&gt;void use_unique_pointer() { // Create a unique_ptr using std::make_unique (preferred way since C++14). auto my_ptr = std::make_unique&lt;int&gt;(42); std::cout &lt;&lt; \"Value: \" &lt;&lt; *my_ptr &lt;&lt; '\\n'; // No need to call delete. The memory is automatically freed when my_ptr goes out of scope.} // my_ptr is destroyed here.4.2. std::shared_ptr: For Shared Ownership ScenariosA std::shared_ptr is used when a resource needs to be owned by multiple pointers simultaneously. It uses a technique called reference counting. It keeps a count of how many shared_ptr instances are pointing to the resource. The resource is only deleted when the last shared_ptr pointing to it is destroyed, causing the reference count to drop to zero. Trade-off: This flexibility comes at a cost. shared_ptr incurs a small performance overhead for storing and atomically updating the reference count.#include &lt;iostream&gt;#include &lt;memory&gt;void use_shared_pointer() { std::shared_ptr&lt;int&gt; ptr1; { // Create a shared_ptr. Reference count is 1. auto ptr2 = std::make_shared&lt;int&gt;(100); // Copy the shared_ptr. Both now point to the same memory. // The reference count becomes 2. ptr1 = ptr2; std::cout &lt;&lt; \"Value: \" &lt;&lt; *ptr1 &lt;&lt; '\\n'; } // ptr2 goes out of scope. Reference count drops to 1. The memory is NOT deleted. std::cout &lt;&lt; \"ptr1 is still valid.\" &lt;&lt; '\\n';} // ptr1 goes out of scope. Reference count drops to 0. The memory is now deleted.4.3. Choosing the Right Smart PointerThis table provides a clear guide for when to use each type of pointer. Pointer Type Ownership Semantics Performance Overhead Safety Primary Use Case Raw Pointer (new/delete) Manual / Unclear None Prone to leaks and errors Legacy code or low-level interaction with C APIs. std::unique_ptr Exclusive / Unique None (Zero-cost) High (Compile-time checks) The default choice for all owning pointers. std::shared_ptr Shared / Reference-counted Yes (Reference count) High (Runtime checks) For shared ownership, such as in graph data structures or implementing an Observer pattern where the lifetime of the subject and observers are not strictly nested. Managing memory for a single thread is one challenge; ensuring safety when multiple threads are involved adds another layer of complexity.5. Memory in a Multi-Threaded WorldMemory management becomes significantly more complex in concurrent applications because all threads in a process typically share the same heap. Without proper safeguards, this shared access can lead to chaos.5.1. The Ultimate Danger: Data RacesA data race is the most dangerous type of concurrency bug. It occurs when: Two or more threads access the same memory location concurrently. At least one of the accesses is a write. There is no synchronization mechanism to protect the access.A data race results in undefined behavior. This is not just a theoretical problem; it can lead to silent data corruption, unpredictable crashes, and other hard-to-diagnose issues.5.2. Preventing Data Races with SynchronizationTo prevent data races, C++ provides two primary tools that ensure operations on shared memory are orderly and safe. std::mutex : A mutex (short for ‚Äúmutual exclusion‚Äù) acts as a lock. It ensures that only one thread can access a ‚Äúcritical section‚Äù of code at a time. A thread must lock() the mutex to enter the critical section and unlock() it upon exit, allowing other threads to proceed. While you can manually call .lock() and .unlock(), using a Resource Acquisition Is Initialization (RAII) helper like std::lock_guard ensures the mutex is always released, even if an exception occurs. #include &lt;iostream&gt; #include &lt;thread&gt; #include &lt;mutex&gt; #include &lt;vector&gt; std::mutex mtx; // The \"lock\" int shared_counter = 0; // Shared resource void increment(int iterations) { for (int i = 0; i &lt; iterations; ++i) { // The lock_guard constructor locks the mutex. // Its destructor (at the end of the loop scope) automatically unlocks it. [6] std::lock_guard&lt;std::mutex&gt; lock(mtx); // --- CRITICAL SECTION START --- shared_counter++; // --- CRITICAL SECTION END --- } } int main() { std::thread t1(increment, 1000); std::thread t2(increment, 1000); t1.join(); t2.join(); std::cout &lt;&lt; \"Final Counter: \" &lt;&lt; shared_counter &lt;&lt; std::endl; } std::atomic : The std::atomic template provides types that guarantee that operations (like reads, writes, and increments) are ‚Äúatomic.‚Äù Unlike a mutex, which stops other threads, atomic operations are indivisible at the hardware level; they cannot be interrupted mid-operation [source text in prompt]. By default, atomic operations provide inter-thread synchronization, meaning a store in one thread synchronizes with a load in another. #include &lt;iostream&gt; #include &lt;thread&gt; #include &lt;atomic&gt; // std::atomic provides data-race-free access without a full lock. std::atomic&lt;int&gt; atomic_counter(0); void atomic_increment(int iterations) { for (int i = 0; i &lt; iterations; ++i) { // This operation is indivisible and safe from data races. // It is shorthand for atomic_counter.fetch_add(1); atomic_counter++; } } int main() { std::thread t1(atomic_increment, 1000); std::thread t2(atomic_increment, 1000); t1.join(); t2.join(); // Default load() is sequentially consistent (memory_order_seq_cst). std::cout &lt;&lt; \"Final Atomic Counter: \" &lt;&lt; atomic_counter.load() &lt;&lt; std::endl; } By eliminating data races from your program, the C++ memory model guarantees sequentially consistent execution, meaning the result of your multi-threaded program will be predictable and reliable, as if the operations of all threads were executed in some single sequential order.While smart pointers handle most dynamic memory needs, sometimes you need finer control over how containers like std::vector get their memory. This is the job of allocators.6. Under the Hood: AllocatorsAn allocator is a component of the C++ Standard Library responsible for handling all memory allocation and deallocation requests for containers like std::vector, std::map, and std::list.By default, all standard containers use std::allocator, which is a general-purpose allocator that simply calls the global operator new and operator delete functions. However, there are scenarios where you might want to provide a custom allocator.There are two primary reasons for writing a custom allocator: Performance: For applications that perform many frequent allocations of small amounts of memory (like in a std::list or std::map), the default allocator can be slow and lead to heap fragmentation. A custom allocator that uses a memory pool ‚Äî a pre-allocated large block of memory‚Äîcan serve these small requests much faster by simply handing out chunks from the pool. Specialized Memory: Custom allocators can encapsulate access to different types of memory, such as shared memory that needs to be accessible by multiple processes, or memory managed by a third-party garbage collector.Knowing the theory is essential, but a skilled C++ programmer also needs practical tools and habits to diagnose problems and write robust code.7. In the Trenches: Debugging and Best Practices7.1. Finding Leaks and Errors with ValgrindValgrind is a powerful instrumentation framework for dynamically analyzing programs. Its Memcheck tool is invaluable for finding memory leaks and memory errors (like invalid reads and writes) in C++ programs.Here is a simple step-by-step guide to using it: Step 1: Compile with Debug Symbols To get meaningful output with file names and line numbers, compile your program with debug symbols using the ggdb3 or Og flag. Step 2: Run Your Program via Valgrind Execute your program through Valgrind with flags that provide detailed leak information. Step 3: Interpret the Output A Clean Run: A successful run with no leaks will show a ‚ÄúHEAP SUMMARY‚Äù. A Leak Report: If a leak is detected, Valgrind will show you where the leaked memory was allocated. The backtrace points directly to the new or malloc call that is the source of the leak. Resource: https://stackoverflow.com/questions/5134891/how-do-i-use-valgrind-to-find-memory-leaks7.2. A Defensive Coder‚Äôs ChecklistAdopting good habits is the best way to prevent memory errors before they happen. Prefer Stack Allocation: If data doesn‚Äôt need to outlive the function it‚Äôs created in and isn‚Äôt excessively large, always prefer creating it on the stack. It‚Äôs faster and automatically managed. Embrace RAII and Smart Pointers: For all heap allocations, use std::unique_ptr by default. Only use std::shared_ptr when you are certain that shared ownership is a necessary part of your design. Trust Nothing: Never assume function arguments are valid, especially raw pointers. Always check for nullptr and validate inputs to prevent crashes and security exploits. Initialize Everything: Always initialize variables when you declare them to avoid using indeterminate, garbage values. Crucially, initialize pointers to nullptr so that delete can be safely called on them even if they never end up owning a resource. This prevents a common class of errors when cleaning up objects in complex states. Write ‚ÄúBoring‚Äù Code: Avoid being overly ‚Äúclever‚Äù or tricky. Simple, readable, and direct code is easier to maintain, reason about, and is far less prone to subtle bugs. Make your solution fit the problem. Use Safe Wrappers: When interfacing with legacy C-style APIs that return raw pointers (e.g., from malloc), immediately wrap the returned pointer in an appropriate smart pointer or a custom RAII class to ensure its lifetime is managed safely.ConclusionModern C++ has transformed memory management from a manual, error-prone chore into a safe and systematic process. By understanding and applying a few core principles, you can write code that is both highly performant and exceptionally robust.Here are the three most important takeaways: Scope is Your Garbage Collector: The RAII principle is the foundation of C++ memory safety. By tying the lifetime of a resource to a stack-based object, cleanup becomes automatic, predictable, and exception-safe. Smart Pointers are Your Primary Tool: std::unique_ptr should be your default choice for all dynamically allocated memory. It is safe, has no overhead, and clearly communicates the exclusive ownership of a resource. Ownership is the Central Concept: Always be clear about which part of your code owns a resource and is therefore responsible for its cleanup. Modern C++ features like smart pointers are designed to make this ownership explicit and verifiable by the compiler.By internalizing these principles, you can confidently wield the power of C++ to build applications that are not only fast but also safe, maintainable, and correct." }, { "title": "Case Studies of Planning and Decision-Making in Robotics", "url": "/posts/planning-case-studies/", "categories": "Blog, Robotics", "tags": "cmu, planning, manipulators, cbs", "date": "2025-11-30 11:00:00 -0500", "snippet": "In progressAutonomous DrivingMobile ManipulatorsLegged RobotsCoverage, Mapping, and SurveyalFrontier-based PlanningA frontier is any region that lies at the boundary between explored space and unexplored space.Moving toward a frontier guarantees that the robot will gather new information.This idea applies equally well to both mapping and coverage, even though the environments differ in how much initial knowledge the robot has.In coverage, the map is already known, but the robot must visit every area to ‚Äúcover‚Äù it with its sensors. In mapping, the environment is unknown beforehand, and the robot‚Äôs sensors reveal the map gradually. Yet in both scenarios, the frontier represents the next most informative or unobserved area for the robot to visit.CoverageIn the coverage scenario, the robot has access to a known map but must physically move to all parts of the environment to observe or ‚Äúcover‚Äù it. Coverage tasks include vacuum cleaning, lawn mowing, painting a surface with a sprayer, or security patrolling.Initially, simple patterns like lawn-mower (boustrophedon) patterns are commonly used. These are efficient in open areas, but they degrade quickly in environments with irregular shapes, tight corridors, or obstacles. The robot may skip regions, require manual stitching of patterns, or need a complex global plan.Frontier-based planning offers a more adaptive and principled alternative. The robot maintains a record of which cells or regions it has already covered, and at every step, it identifies frontier cells‚Äîthose cells that have not yet been visited but are adjacent to visited ones. The robot then chooses the nearest such frontier and computes a path to it. Once it reaches that frontier, new cells become observed, the set of frontiers changes, and the robot repeats the process. Eventually, all free cells will have been visited, guaranteeing complete coverage.Why it works?Frontier-based coverage works well because it always moves the robot toward ‚Äúnew‚Äù portions of the environment. Since frontier cells are, by definition, the closest areas that remain unobserved, the robot makes steady progress toward the coverage goal without repeatedly visiting the same areas. This reduces unnecessary overlap and ensures that the robot moves in a purposeful manner.Moreover, frontier-based coverage naturally adapts to obstacles and environmental geometry. Unlike lawn-mowing patterns, frontiers reorganize themselves around walls, corners, and narrow passages. This allows the robot to navigate complex spaces efficiently, making the method widely used in real-world applications such as vacuum robots or cleaning drones.MappingMapping differs from coverage in one crucial way: the robot begins with no prior knowledge of the environment. As the robot moves and senses its surroundings, it gradually constructs an occupancy grid or map of free and occupied space. However, at any moment, most of the environment remains unknown.Frontier-based planning for mapping uses the same principle ‚Äî Always move to the closest frontier: the boundary between what has already been scanned and what is still unknown.In mapping, every frontier visit provides maximum information gain, because the robot‚Äôs sensors reveal new cells beyond the frontier. As the robot moves and senses, the set of frontiers changes. Some frontiers disappear as the robot explores them, and others appear as new unknown spaces are exposed.The process continues until no frontiers remain, meaning the entire reachable environment has been mapped.Using Multi-Goal A* for Frontier NavigationOne subtle but important detail is how to choose the closest frontier. Since the robot may have many frontier cells at once, finding the nearest one requires solving a multi-goal shortest-path problem.Rather than running A* separately for each frontier (which would be inefficient), we perform a graph transformation that allows A* to check all frontiers in a single search. This is based on the multi-goal A* technique.The idea is: Treat all frontier cells as goal nodes in one A* search. Expand the search until the first frontier is reached. This frontier is guaranteed to be the closest in terms of path cost.SurveyalSurveyal refers to a class of robotics tasks in which the robot must visit a set of predefined points of interest in the environment and perform sensing actions at each one, such as taking photos, scanning objects, or collecting data. Unlike simple navigation problems, surveyal does not ask the robot to reach a single goal state. Instead, it requires the robot to construct a least-cost route that visits all required waypoints, in any order, while respecting motion constraints and avoiding obstacles. This makes surveyal a combination of combinatorial reasoning (deciding the order in which to visit waypoints) and motion planning (finding feasible paths between them).The core difficulty in surveyal lies in the fact that the robot cannot teleport between waypoints ‚Äî it must travel along feasible trajectories. This means that the true ‚Äúcost‚Äù of going from waypoint A to waypoint B is not simply Euclidean distance, but the cost of a real motion-planning solution that accounts for obstacles, robot dynamics, vehicle orientation, and constraints such as minimum turning radius. Therefore, surveyal cannot be solved by simply treating it as a Traveling Salesman Problem (TSP) over Euclidean distances. Instead, we must combine low-level continuous motion planning and high-level discrete graph search into one unified planning pipeline.Constructing the Surveyal Search SpaceTo solve the surveyal problem using graph search, we must construct a discrete search space that captures two key aspects of the robot‚Äôs progress: Which waypoints have already been visited, and Where the robot currently is (and possibly its orientation)To encode this, we define the search state as:\\[v = \\{ \\alpha, \\Omega \\}\\]Here, $\\alpha$ is a vector of $M$ bits, where each bit corresponds to one waypoint. A bit is set to 1 if the corresponding waypoint has been visited, and 0 otherwise. This bitmask representation allows the planner to concisely describe all possible ‚Äúprogress states‚Äù of the mission. For example, if the robot has three waypoints to visit, then $\\alpha=$ 101 means the robot has visited waypoints 0 and 2 but not 1.$\\Omega$ encodes the robot‚Äôs current position in the waypoint graph. For holonomic robots (which can move in any direction), $\\Omega$ is simply the index of the current waypoint. For non-holonomic robots (cars, drones with yaw constraints, differential-drive robots), $\\Omega$ may also include the robot‚Äôs orientation at that waypoint. Orientation is important because the robot‚Äôs ability to travel to the next waypoint may depend strongly on how it is oriented when it leaves its current waypoint.This representation guarantees that each search state contains all relevant information that affects future motion, which means that the state space satisfies the Markov property. From any state {Œ±, Œ©}, the planner can determine exactly which next waypoints are allowed and what costs are involved in reaching them.Goal States in SurveyalThe goal of the surveyal problem is to visit all required waypoints. Therefore, the goal condition is simply:\\[\\alpha = [1, 1, \\dots, 1]\\]Once all bits in $\\alpha$ are set to one, the robot has successfully surveyed every waypoint, and the search can terminate. Importantly, the robot‚Äôs orientation at the final waypoint does not affect goal satisfaction ‚Äî only the fact that every waypoint has been visited.Role of Low-Level Motion PlanningBefore graph search can be performed, the system must know the cost of traveling between each pair of waypoints. These costs cannot be assumed ‚Äî they must be computed by invoking a continuous motion planner (e.g., A* on a grid, Hybrid A, RRT, lattice planning).For every ordered pair of waypoints (A, B), a low-level planner is used to determine: Whether a feasible path exists between A and B The cost of this feasible path The resulting orientation of the robot at waypoint BThus, edges in the high-level surveyal graph correspond to actual robot trajectories, not simple straight lines. This is what makes surveyal fundamentally richer than classical TSP.High-Level Graph Search: Where A* Comes InOnce feasible motion edges are computed, surveyal becomes a graph search problem over the state space {Œ±, Œ©}. Each node in this graph represents a partially completed mission. Each outgoing edge from that node corresponds to: Selecting an unvisited waypoint $i$ Following a feasible low-level path from the current waypoint to $i$ Updating the visitation bitmask $\\alpha$ Updating orientation (if applicable)Graph search algorithms such as A* or Dijkstra explore this state space to find the minimum-cost path from:\\[{ \\alpha = 0\\ldots0, \\space \\Omega = \\text{start waypoint} }\\quad \\rightarrow \\quad{ \\alpha = 1\\ldots1, \\space \\Omega = \\text{any waypoint} }\\]This is where graph search actually happens. The search determines the optimal sequence of waypoints to visit, accounting for true motion constraints and path costs.This formulation elegantly integrates: Combinatorial reasoning about waypoint order Continuous geometric motion planning Robot kinematic/dynamic constraints Obstacle avoidance Optimality of the final surveyal tourBy encoding both ‚Äúwhat has been visited‚Äù and ‚Äúwhere the robot is now,‚Äù the search graph becomes fully Markovian. The planner can reason about the mission using a well-structured state space while relying on low-level planners to guarantee physical feasibility.Because of this, surveyal planning becomes a clean example of how discrete search and continuous planning must work together to solve real robotic tasks." }, { "title": "All About Search Algorithms", "url": "/posts/all-about-search-algorithms/", "categories": "Blog, Robotics", "tags": "cmu, planning, manipulators, cbs", "date": "2025-11-30 11:00:00 -0500", "snippet": "In progressTO-DO: Refine RRT, RRT-Connect, RRT* POMDP Multi-robot planningGraph Search ProblemOnce a robot converts the environment into a discrete representation, whether by grid decomposition, Voronoi skeletons, adaptive cell decompositions, or lattices, the planning problem becomes a classical least-cost path search on a graph. Each state represents a robot configuration, and each edge carries a transition cost, typically the motion cost between configurations.The goal of search is to compute a path from a designated start state $s_{start}$ to a goal state $s_{goal}$ that minimizes cumulative edge cost. Many search algorithms operate by estimating or computing the optimal cost-to-come, expressed as:\\[g(s) = \\min_{s'' \\in \\text{pred}(s)} \\left[ g(s'') + c(s'', s) \\right]\\]Here, $g(s)$ is the minimum cost from start to $s$, $c(s‚Äô‚Äò,s)$ is the cost of transitioning from predecessor $s‚Äô‚Äô$ to $s$, and the recursion states that the optimal cost-to-come is obtained by choosing the predecessor that yields the cheapest total.Because this recurrence defines a dynamic programming relation, the overall optimal path can be reconstructed greedily by backtracking from the goal using the minimizing predecessor:\\[s' = \\arg\\min_{s'' \\in \\text{pred}(s)} \\left[ g(s'') + c(s'', s) \\right]\\]This backtracking forms the optimal path even though search expands states in the forward direction.A*A* expands states in increasing order of:\\[f(s) = g(s) + h(s)\\]where $g(s) =$ best-known cost from start to $s$ $h(s) =$ admissible heuristic estimating $s$ to goalA consistent heuristic ensures that every state g-value is optimal exactly at the moment the state is expanded. That is the heart of A‚Äôs correctness proof: A never has to revisit or ‚Äúfix‚Äù a g-value later, because consistency ensures no cheaper path will ever emerge after expansion.The algorithm expands only states whose f-values are competitive with the optimal path to the goal. This is why A* is so efficient: it prunes everything that cannot possibly lead to the optimal solution.Pseudocode:main()g(s_start) = 0; all other g-values = INF;OPEN = {s_start};computePath();computePath()while(s_goal is not expanded and OPEN != 0)\tremove s with the smallest [f(s) = g(s)+h(s)] from OPEN;\tinsert s into CLOSED;\tfor every successor s‚Äô of s such that s‚Äô not in CLOSED:\t\tif g(s‚Äô) &gt; g(s) + c(s,s‚Äô):\t\t\tg(s‚Äô) = g(s) + c(s,s‚Äô);\t\t\tinsert s‚Äô into OPEN;For every expanded state $g(s)$ is optimal. For every other state, $g(s)$ is an upper-bound.A* performs provably minimal number of state expansions required to guarantee optimality ‚Äì optimal in terms of the computations.Multi-Goal A*Many robotic tasks require a path to any one of multiple possible goals. Examples include: picking the best parking spot out of many, intercepting a moving target at any predicted point along its trajectory, exploration where several frontier cells represent different potential ‚Äúcompletion‚Äù points.The key insight is that A* does not need special logic for multiple goals. Instead, introduce an imaginary super-goal connected to each real goal with zero-cost edges.Once we add the super-goal node, the problem becomes identical to standard A*: now we just search from the start to this single super-goal. The f-values handle all the rest.This equivalence is mathematically correct because:\\[c^*(s_{\\text{start}}, g_{\\text{imag}})= \\min_i c^*(s_{\\text{start}}, g_i)\\quad \\text{(zero-cost jump at the end)}\\]This works even with unequal goal quality ‚Äî e.g., one parking spot may be worse than another. In that case each real goal is connected to the imaginary super-goal with an edge equal to the additional cost (e.g., ‚Äúgoal cost‚Äù), so A* automatically prefers the better goal.The transformed graph remains admissible for the original problem, and A* still returns the optimal plan among all candidate goals.To incorporate this, assign each goal a terminal cost $w_i$, and connect to the imaginary goal with edge cost $w_i:$\\[c(g_i, g_{\\text{imag}}) = w_i\\]\\[c^*(s_{\\text{start}}, g_{\\text{imag}})= \\min_i \\left[ c^*(s_{\\text{start}}, g_i) + w_i \\right]\\]Weighted A*Weighted A* modifies the f-value:\\[f(s) = g(s) + \\epsilon h(s)\\]with $\\epsilon&gt;1$.By inflating the heuristic weight, the search biases more strongly toward the goal, reducing exploration of irrelevant detours. This yields an $\\epsilon-$suboptimal algorithm, meaning:\\[\\text{cost(solution)} \\le \\epsilon \\cdot \\text{cost(optimal solution)}\\]Why it works?The quality of A*‚Äôs heuristic is captured by the difference:\\[e(s) = h(s) - h^*(s)\\] $h(s):$ the heuristic estimate $h^*(s):$ the true cost-to-go $e(s):$ indicates how much the heuristic underestimates the real costSince heuristics must be admissible, $e(s)$ is non-positive (never &gt; 0).Regions where the heuristic underestimates a lot (very negative $e(s)$) form local minima in the error landscape. A* expands many states inside these ‚Äúdeep valleys‚Äù because the f-values look artificially good.A deep valley $\\rightarrow$ many states look promising $\\rightarrow$ A* wastes expansions.Weighted A* modifies f-values:\\[f(s) = g(s) + \\epsilon h(s), \\quad \\epsilon &gt; 1\\]Multiplying the heuristic: raises the $h(s)$ makes the heuristic less conservative makes error $e(s) = \\epsilon h(s) - h^*(s)$ turns deep valleys into shallow onesThis reduces the ‚Äúfunnel effect‚Äù and helps the search avoid unnecessary exploration.Because local minima become shallow: A* commits sooner toward the goal explores fewer misleading states avoids wasting time in heuristic dipsThus the search becomes directed, not exploratory.No one currently knows precisely how the set of expanded states depends on the heuristic landscape. This remains an active research topic in heuristic search.Backward A*Backward A* simply reverses the direction of search. Instead of searching forward from the start, it starts from the goal and expands backwards:Pseudocode:main()g(s_goal) = 0; all other g-values = INF;OPEN = {s_goal};computePath();computePath()while(s_start is not expanded and OPEN != 0)\tremove s with the smallest [f(s) = g(s)+h(s)] from OPEN;\tinsert s into CLOSED;\tfor every predecessor s‚Äô of s such that s‚Äô not in CLOSED:\t\tif g(s‚Äô) &gt; c(s',s) + g(s):\t\t\tg(s‚Äô) = c(s',s) + g(s);\t\t\tinsert s‚Äô into OPEN;HeuristicsThe efficiency of A* is driven entirely by how good your heuristic is. A heuristic represents an ‚Äúoptimistic guess‚Äù of how far we still are from the goal. When the heuristic is informative, A* expands only a small portion of the graph; when the heuristic is uninformative, A* degenerates toward Dijkstra, expanding almost everything.The formal requirement for heuristics in A* is admissibility: the heuristic must never overestimate the true cost-to-go. In symbolic terms:\\[0 \\le h(s) \\le c^*(s,s_{goal})\\]where $c^*(s,s_{goal})$ is the minimal cost from $s$ to $s_{goal}$.This ensures that A* never prunes or skips any potentially optimal path. But admissibility alone does not guarantee efficiency. For A* to run fast, the heuristic must not only be admissible, but also consistent (monotone). Consistency means:\\[h(s_{goal}, s_{goal})=0\\]\\[h(s) \\le c(s,s') + h(s')\\]for every $s\\ne s_{goal}$ and its successors $s‚Äô.$Admissibility provably follows from consistency and often (not always) consistency follows from admissibility.FunctionsFor grid-based navigation: Euclidean distance Admissable for 4-connected and 8-connected grid Manhattan distance: $h(x,y) = abs(x-x_{goal}) + abs(y-y_{goal})$ Admissable for 4-connected grid (perfect heuristic) Inadmissable for 8-connected grid (Octile function is the perfect heuristic) Diagonal distance: $h(x,y) = max( abs(x-x_{goal}), abs(y-y_{goal}))$Octile Heuristic:In more complex spaces like a 3D lattice $(x,y,\\theta)$ or high-DoF manipulators, a na√Øve heuristic becomes almost useless because it collapses the structure of the real problem into something too simple to guide search well.A common and powerful strategy is to compute a lower-dimensional search to define a heuristic for a higher-dimensional state. For example, running a 2D Dijkstra search on the grid (starting from the goal) produces distances that account for real obstacles; these distances can then be used as heuristic values for every $(x,y,\\theta)$ configuration. This improves guidance but can still fail in problems where orientation or arm constraints dominate the difficulty.In many robotic tasks, even a sophisticated lower-dimensional heuristic is not enough because the true difficulty comes from complex geometric or kinematic constraints. In these cases, we often turn to inadmissible heuristics that violate the admissibility rule but encode meaningful structure‚Äîsuch as end-effector distance through obstacles or orientation differences of manipulated objects. These heuristics are informative but unsafe to use alone because they can lead A* into local minima or cause it to miss valid solutions.Key Properties of Combining HeuristicsIf two heuristics $h_1(s)$ and $h_2(s)$ are consistent, then using their maximum,\\[h(s) = \\max(h_1(s), h_2(s))\\]remains consistent and therefore admissible. This is useful but extremely limited: the max operation destroys information, produces new local minima, and requires every heuristic to be admissible, which is unrealistic in high-dimensional robotics.More generally, if heuristics are combined additively,\\[h(s) = h_1(s) + h_2(s)\\]the result is typically $\\epsilon-$consistent, meaning the heuristic may overestimate by at most a bounded factor. Weighted A* is equivalent to using such $\\epsilon-$consistent heuristics and guarantees\\[\\text{cost(solution)} \\le \\epsilon \\cdot \\text{cost(optimal)}\\]Need for Multiple HeuristicsReal-world manipulation or navigation tasks often have many distinct modes of difficulty, where each heuristic is strong in some region but weak (or utterly misleading) in others. For instance: One heuristic might capture distance in the workspace. Another might encode object orientation costs. A third might describe arm retraction or clearance constraints.No single heuristic, admissible or inadmissible, gives uniformly good guidance across the entire state space.This motivates algorithms that can leverage many heuristics simultaneously without losing theoretical guarantees.Multi-Heuristic A*Version 1 Given $N$ inadmissible heuristics Run $N$ independent searches Hope one of them reaches goalProblems: Each search has its own local minima $N$ times more work No completeness guarantees or bounds on solution qualityWithin the while loop of the ComputePath function:\tfor i=1‚Ä¶N:\t\tremove s with the smallest [f(s) = g(s)+w1*h(s)] from OPENi ;\t\texpand s;Version 2 Given $N$ inadmissible heuristics Run $N$ independent searches Hope one of them reaches goal Share information (g-values) between searchesWithin the while loop of the ComputePath function (CLOSED is shared):\tfor i=1‚Ä¶N:\t\tremove s with the smallest [f(s) = g(s)+w1*h(s)] from OPENi ;\t\texpand s and also insert/update its successors into all other OPEN lists;Benefits: Searches help each other to circumvent local minima States are expanded at most once across ALL searchesRemaining Problem: No completeness guarantees or bounds on solution qualityVersion 3 Given $N$ inadmissible heuristics Run $N$ independent searches Hope one of them reaches goal Share information (g-values) between searches Search with admissible heuristics controls expansionsWithin the while loop of the ComputePath function (CLOSED is shared among searches 1‚Ä¶N. Search 0 has its own CLOSED):\tfor i=1‚Ä¶N:\t\tif(min. f-value in OPENi ‚â§ w2* min. f-value in OPEN0 )\t\t\tremove s with the smallest [f(s) = g(s)+w1*hi(s)] from OPENi ;\t\t\texpand s and also insert/update its successors into all other OPEN lists;\t\telse\t\t\tremove s with the smallest [f(s) = g(s)+w1*h0(s)] from OPEN0 ;\t\t\texpand s and also insert/update its successors into all other OPEN lists;The algorithm runs $N+1$ parallel A*-like searches: Search 0: the anchor search using admissible $h_0$ Search 1‚Ä¶N: searches using inadmissible heuristics $h_1, h_2,‚Ä¶,h_N$Each search has its own OPEN list (priority queue).But: searches 1‚Ä¶N share the same CLOSED list search 0 has its own CLOSED list all searches share g-valuesIf the inadmissible search is ‚Äúgood enough‚Äù: Search i‚Äôs best node looks promising enough compared to the anchor search.\\[\\text{min}f_i \\le w_2.\\text{min}f_0\\]If the inadmissible search is not good enough: Search i‚Äôs best node is not competitive. Do not trust its heuristic right now.\\[\\text{min}f_i &gt; w_2.\\text{min}f_0\\]Need for two weights: $w_1$ inflates heuristics inside each individual search (making $f=g+w_1h$) $w_2$ compares searches against the anchor and decides if inadmissible searches are allowed to expandTogether, they ensure:\\[\\text{cost(solution)} \\le w_1 w_2 \\cdot \\text{optimal cost}\\] Searches 1‚Ä¶N (inadmissible): share a single CLOSED list so no state is expanded more than once among all inadmissible searches Anchor search has a separate CLOSED list so anchor may expand a state once, even if inadmissible searches already expanded it So each state is expanded at most twice.Interleaving Planning and ExecutionIn real-world robotics, planning is not a one-time activity. The robot must continuously update its plan while moving, because the world is not fully known beforehand, and conditions may change during execution.Typical reasons why planning must be repeated are: The environment is only partially known (unknown obstacles revealed during motion). The environment might be dynamic (people, vehicles, or other robots moving). The robot may not follow its plan perfectly due to actuation or drift errors. Its state estimation may be imprecise, causing localization errors.In such cases, the robot must be able to re-plan quickly instead of starting A* from scratch.When a robot must plan repeatedly, there are three main approaches: Anytime heuristic search ‚Äî returns the best available solution within a time limit and keeps improving it. Incremental heuristic search ‚Äî speeds up repeated searches by reusing past search efforts, like g-values, OPEN lists, or search trees. Real-time heuristic search ‚Äî only plans a few steps ahead, executes them, then re-plans later.Anytime Heuristic SearchA basic idea of Anytime search is to start with a fast, suboptimal solution and improve it as more time becomes available.Weighted A* uses:\\[f(s) = g(s) + \\epsilon \\cdot h(s), \\quad \\epsilon &gt; 1\\] With large $\\epsilon$, the solution is found quickly but may not be optimal. As $\\epsilon$ approaches 1, the solution becomes optimal but search becomes slower.A simple but inefficient approach: run Weighted A* multiple times with decreasing $\\epsilon$ ( $2.5 \\rightarrow 1.5 \\rightarrow 1.0$ ), restarting every time. But this is wasteful, because many values do not change between searches.ARA: Anytime Repairing AARA* improves on this by reusing the results of previous searches, instead of restarting from scratch.ARA* solves this by: Quickly returning a first solution using a high weight Reducing the weight step-by-step to improve solution quality Reusing previous g-values, OPEN lists, and search tree structure Avoiding needless re-expansionsPseudocode:main()g(s_start) = 0; all other g-values = INF;OPEN = {s_start};computePathWithReuse();all v-values are initially INF;initialize OPEN with all overconsistent states;computePathWithReuse()while(f(s_goal) &gt; minimum f-value in OPEN)\tremove s with the smallest [f(s) = g(s)+h(s)] from OPEN;\tinsert s into CLOSED;\tv(s) = g(s);\tfor every successor s‚Äô of s such that s‚Äô not in CLOSED:\t\tif g(s‚Äô) &gt; g(s) + c(s,s‚Äô):\t\t\tg(s‚Äô) = g(s) + c(s,s‚Äô);\t\t\tinsert s‚Äô into OPEN;\\[g(s') = \\text{min}_{s'' \\in pred(s')}v(s'')+c(s'',s')\\]$OPEN:$ set of all states with $v(s) &gt; g(s)$. All other states have $v(s)=g(s).$ARA* keeps two cost values for each state: The current best-known cost $g(s)$ The cost when the state was last expanded $v(s)$. If the state was never expanded, then its value is $INF$.The difference between these two tells ARA* whether a state needs to be re-expanded under the new (lower) weight. Consistent: $v(s) = g(s)$ The state‚Äôs value matches the last expansion. So no rework is needed. Overconsistent: $v(s) &gt; g(s)$ The state‚Äôs cost has improved since it was expanded. So it might need re-expansion. Underconsistent: $v(s) &lt; g(s)$ State‚Äôs cost has worsened (rare unless environment changes). So also needs to be fixed. ARA* re-expands only the inconsistent states rather than the whole search space.How ARA* Improves the Solution Without Restarting?When the weight $\\epsilon$ decreases: ARA* inserts all inconsistent states into OPEN It runs a Weighted-A*-like search again, but: g-values remain the old search tree remains states that were consistent stay untouched Expansion continues until: A new solution with the smaller $\\epsilon$ is found No g-value can be further improved This allows ARA* to give multiple solutions, each guaranteed to be within $\\epsilon.$optimal.At every iteration, ARA* guarantees:\\[\\text{cost(solution)} \\le \\epsilon \\cdot \\text{cost(optimal)}\\]Pros:ARA* saves time due to: Reusing previous g(s) values Reusing the previous OPEN list Only expanding inconsistent states Avoiding recomputation of the whole search tree Avoiding redundant expansionsPseudocode using weighted A*:main()g(s_start) = 0; all other g-values = INF;OPEN = {s_start};while e ‚â• 1\tCLOSED = {}; INCONS = {};\tcomputePathwithReuse();\tpublish current e suboptimal solution;\tdecrease e;\tinitialize OPEN = OPEN U INCONS;all v-values are initially INF;initialize OPEN with all overconsistent states;computePathWithReuse()while(f(s_goal) &gt; minimum f-value in OPEN)\tremove s with the smallest [f(s) = g(s)+eh(s)] from OPEN;\tinsert s into CLOSED;\tv(s) = g(s);\tfor every successor s‚Äô of s:\t\tif g(s‚Äô) &gt; g(s) + c(s,s‚Äô):\t\t\tg(s‚Äô) = g(s) + c(s,s‚Äô);\t\t\tif s' not in CLOSED then insert s‚Äô into OPEN;\t\t\totherwise insert s' into INCONSif s' not in CLOSED then insert s‚Äô into OPEN exists because in weighted A*, the g-values are not guaranteed to be optimal. So even if a successor has been expanded before, there is a chance that its g-value can be made better. Hence it is updated but it is not put in the OPEN list again because it has been expanded before.In regular A*, the g-values are already optimal if the state is CLOSED, so they don‚Äôt need to be updated.Example:Real-Time Heuristic SearchTraditional A* and its variants assume that planning is completed before execution begins. This assumption breaks down in realistic robotics settings: The environment is often partially known, and important obstacles become visible only during execution. The world may be dynamic, forcing the robot to react immediately to avoid collisions. Robots experience imprecise motion and noisy localization, which invalidates long pre-computed plans. Some tasks require millisecond-level reactivity, making full-path planning infeasible.Real-time search treats planning and acting as a continuous loop: gather sensor data ‚Üí plan a few steps ‚Üí execute one step ‚Üí repeat.Real-time search limits the amount of planning: At each iteration, the agent may expand only N nodes (where N is small and fixed). After that limited search, the agent chooses the next action that looks best. The agent then moves exactly one step, updates the map, and begins the next lookahead.This approach ensures that the robot always responds in bounded time, even in unknown or rapidly changing environments. It never aims to produce a global path immediately; instead, it incrementally constructs a path through repeated short-range planning.When the map is unknown, real-time search relies on the Freespace Assumption: All unexplored cells are assumed to be free until the robot‚Äôs sensors reveal otherwise. This lets the planner behave optimistically and aggressively move toward the goal. When the robot detects new obstacles, they are incorporated into the map and the next real-time search cycle adapts accordingly. The Freespace Assumption enables motion in unknown worlds but also creates the need for heuristic correction because the robot can fall into local minima where the heuristic dramatically underestimates real cost-to-go.LRTA*: Learning Real-Time A*At each iteration: The agent evaluates all immediate successors $s‚Äô$ of the current state $s$ and moves to the minimum, as follows:\\[s = \\text{argmin}_{s' \\in succ(s)}c(s,s')+h(s')\\] Before leaving state $s$, LRTA* updates the heuristic:\\[h(s) \\leftarrow \\min_{s' \\in succ(s)} \\left( c(s, s') + h(s') \\right)\\] This update increases the heuristic at states where the robot got stuck, reducing the attractiveness of that region. Over many steps, LRTA* guarantees that the heuristic monotonically increases toward the true cost-to-go.LRTA* thus learns a better heuristic online and ensures eventual escape from every local minimum.Convergence Properties of LRTA*LRTA* guarantees that the robot will reach the goal in a finite number of steps: If the graph is finite, action costs are positive, and a solution exists, LRTA* will reach the goal in finite time. It will never oscillate infinitely because heuristic updates eliminate the cause of repeated cycles. The learned heuristic converges to a consistent, admissible function that reflects encountered terrain. All actions are reversible. h-values remain admissible and consistent.This makes LRTA* suitable for unknown environments where the robot must ‚Äúfeel its way‚Äù toward the goal.Multi-Step Lookahead in LRTA*Basic LRTA* examines only immediate neighbors, which can be shortsighted.A more powerful variant performs an N-step lookahead: Expand up to N nodes using a limited search (A*, BFS, or Dijkstra-like expansion). Compute locally optimal g-values for these nodes. Use local dynamic programming updates:\\[h(s) = \\min_{s' \\in succ(s)} \\left( c(s, s') + h(s') \\right)\\] Move on path to the state $s$\\[s = \\text{argmin}_{s' \\in OPEN}g(s')+h(s')\\] This deeper lookahead reduces the number of heuristic updates needed and helps the robot anticipate dead ends earlier.RTAA*: Real-Time Adaptive A*RTAA* is a more efficient form of heuristic learning suitable for large-scale real-time robotics. Instead of performing full DP updates like LRTA, RTAA computes a single batch update per cycle.After expanding a bounded search frontier during lookahead, RTAA* identifies the most promising frontier node $s^*$ (the one minimizing $g+h$), and updates the heuristics of all expanded states using:\\[h(u) \\leftarrow f(s^*) - g(u)\\]PRMFor manipulation tasks, the robot‚Äôs configuration is a vector of joint angles $Q={q_1,\\ldots,q_n}$, which defines a point in a continuous, high-dimensional space called configuration space (C-space). Planning requires computing a continuous path from $Q_{start}$ to $Q_{goal}$ that satisfies all constraints: Joint limits No collisions with obstacles No self-collisionsA direct grid discretization of this space becomes infeasible because the number of grid cells grows exponentially with dimension. Although resolution-complete planners guarantee completeness and quality bounds, they become computationally intractable in 6- to 12-dimensional spaces typically encountered in manipulation.Sampling-based methods exploit the insight that although the configuration space is high-dimensional, the free space is continuous and ‚Äúbenign‚Äù, meaning a sparse set of samples can capture enough structure of the environment to allow feasible paths to be found.OverviewPRMs operate in two phases:Preprocessing Phase (Learning Phase)A roadmap graph $G$ is built using random samples in $C_{free}$. Each sample is connected to nearby samples through short, locally planned paths, typically straight-line interpolation in joint space.Query PhaseGiven arbitrary start and goal configurations: Connect both to the roadmap using a local planner. Run a graph search (A, Dijkstra, Weighted A) on the augmented roadmap.If the roadmap is sufficiently dense, start and goal will connect to the same component of the graph, making a valid continuous path available.A single roadmap can answer many queries efficiently, which is why PRMs are well suited for multi-query environments such as manipulation in structured spaces.Preprocessing PhaseSampling: We repeatedly generate random configurations $\\alpha(i)$ in $C_{free}$. Each sample must satisfy all constraints (no collisions, valid joint limits). Samples can be uniform or follow more sophisticated biasing strategies.Adding Vertices: Whenever a sample lies in free space, it is added as a vertex in the roadmap graph.Neighborhood Selection: For each new sample, we identify a set of nearby vertices in the existing graph. Some commonly used neighborhood definitions: K nearest neighbors (distance in configuration space) Nearest neighbor in each connected component of the roadmap All vertices within a radius $r$The goal is to avoid creating too many edges while guaranteeing connectivity.Connecting Neighbors: For each neighbor vertex $q$, we call a local planner to determine whether a smooth, collision-free path exists between $\\alpha(i)$ and $q$. Even a simple straight-line interpolation in joint space is often sufficient. If this local plan is collision-free, an edge is added to the graph.Connected Components: The roadmap grows organically, gradually connecting isolated components until the free space is covered well enough to support queries.Pseudocode:BUILD_ROADMAP G.init(); i ‚Üê 0; while i &lt; N if Œ±(i) ‚àà C_free then G.add_vertex(Œ±(i)); i ‚Üê i + 1; for each q ‚àà NEIGHBORHOOD(Œ±(i), G) if ( (not G.same_component(Œ±(i), q)) and CONNECT(Œ±(i), q) ) then G.add_edge(Œ±(i), q);Query Phase: Using the RoadmapOnce the roadmap is built, answering queries becomes fast: Given $q_I$ and $q_G$, connect each to nearby vertices in the roadmap using the same local planner Insert both into the graph as temporary nodes Use a shortest-path algorithm such as Dijkstra‚Äôs or A* If both nodes lie in the same connected component, the resulting path is a valid collision-free motionSampling StrategiesUniform sampling is the simplest approach but can be highly inefficient. Certain regions of configuration space ‚Äî such as narrow passages between obstacles ‚Äî have extremely small measure and are rarely sampled. PRMs address this with multiple sampling strategies: Uniform sampling: Samples drawn uniformly from free space. Fast and easy, but may fail to represent narrow corridors adequately. Connectivity-Based Sampling: Sample existing vertices with probability inversely proportional to how well-connected they are. Poorly connected vertices (likely in narrow regions) get sampled more often, increasing coverage of difficult areas. Obstacle-Biased Sampling: Sampling is biased toward the boundary of obstacles. This helps generate samples near constraints where the roadmap usually needs more detail. Gaussian Sampling: Gaussian sampling generates small clusters of samples around a chosen point. If one sample lies in collision and another lies in free space, the free one is kept. This produces samples near obstacle boundaries without explicitly detecting the boundary. Bridge Sampling: Sampling $q_1, q_2, q_3$ from a Gaussian and keeping the middle configuration if it is in free space but neighbors are in collision. This helps find narrow passages effectively. Sampling Away from Obstacles: Useful in cluttered environments where the robot needs large free areas well represented.Each sampling heuristic targets a failure mode of uniform sampling, making the resulting roadmap more robust.Pros and ConsPRMs excel in multi-query, high-dimensional spaces:Strengths Extremely efficient once the roadmap is built Scales well to high-dimensional C-spaces Easy to integrate with collision checkers and kinematic constraint solvers Probabilistically completeWeaknesses Roadmap building can be expensive initially Difficult to handle dynamic environments (PRM is primarily offline) Narrow passages require careful sampling strategies Connectivity depends heavily on sample qualityRRTRRTs are particularly effective for single-query planning problems where a robot must find a feasible path between a specific start and goal configuration but does not expect to reuse planning work later. They scale well to high-dimensional configuration spaces and do not require a preprocessing phase. Compared to PRMs, RRTs sacrifice optimality but gain significant speed and robustness, especially when collision checking is expensive or the environment is cluttered.Basic StructureAn RRT begins with a tree whose root is the start configuration. At each iteration, the planner draws a random configuration from the configuration space and finds the closest node in the tree. Instead of jumping directly to the sample, the algorithm takes only a small step toward it. This step is usually a straight-line interpolation in configuration space. If this short motion is collision-free, the new configuration is added to the tree.This fundamental operation is the EXTEND operator. Conceptually, EXTEND tries to walk from a known configuration toward a random sample by a limited distance. This simple mechanism produces a tree that organically spreads into every region of the free space, with more growth where space is wide open and less growth in constrained areas.The algorithm terminates when the tree grows sufficiently close to the goal, or when a direct local connection to the goal becomes feasible.Psuedocode:BUILD_RRT T.init(q_init); for k = 1 to K q_rand ‚Üê RANDOM_STATE(); EXTEND(T, q_rand); EXTEND(T, q): q_near ‚Üê NEAREST_NEIGHBOR(q, T); q_new ‚Üê EXTEND(q_near, q); if q_new ‚â† NULL then T.add_vertex(q_new); T.add_edge(q_near, q_new); if q_new = q_goal then return PATH(T, q_goal); return PATH_NOT_FOUND;LimitationsAlthough RRTs are powerful explorers, they do not compute optimal or even high-quality paths. Because the algorithm grows outward from the start without explicitly optimizing anything, the resulting path is usually long, jagged, and geometrically irregular. Once a branch is created, the tree structure restricts future updates; the planner cannot easily reorganize previous decisions.As a result, RRTs tend to lock themselves into a single homotopy class early on. They cannot ‚Äúundo‚Äù their earlier tree structure, so even with more computation, the path does not improve. This makes standard RRT a great planner for fast feasibility but a poor choice for optimal path quality.RRT-Connect: A Faster Bi-Directional VariantRRT-Connect significantly accelerates planning by growing two trees simultaneously: one from the start configuration and one from the goal. At each iteration, a random sample is drawn, and one tree grows toward it using the standard EXTEND step. Once that extension succeeds, the algorithm immediately attempts to ‚Äúpull‚Äù the other tree toward the newly created node.This ‚Äúaggressive connection‚Äù uses a CONNECT operation, which repeatedly performs EXTEND toward the same sample until either a collision is detected or the sample is reached. The result is a strong contraction motion: the two trees attempt to meet in the middle as fast as possible.RRT-Connect tends to find feasible paths much faster than basic RRT. It inherits the same exploration bias and probabilistic completeness, but because of bi-directional growth and aggressive connection, it usually solves hard planning problems in far fewer expansions. However, like RRT, it does not produce optimal solutions.The mechanics of EXTEND can be written as:\\[q_{\\text{new}} = q_{\\text{near}} + \\varepsilon \\cdot \\frac{q_{\\text{rand}} - q_{\\text{near}}}{|q_{\\text{rand}} - q_{\\text{near}}|}\\]The algorithm adds $q_{new}$ to the tree only if the local connection segment:\\[\\tau(s) = q_{\\text{near}} + s(q_{\\text{new}} - q_{\\text{near}}), \\quad s \\in [0,1]\\]lies entirely inside $C_{free}.$This geometric structure ensures the tree grows smoothly rather than in erratic jumps.Pseudocode:RRT_CONNECT(q_start, q_goal, N, Œµ): T_start.init(q_start) T_goal.init(q_goal) for k = 1 to N: q_rand ‚Üê SAMPLE_CONFIG() # 1) Grow T_start toward the random sample if EXTEND(T_start, q_rand, Œµ) ‚â† TRAPPED: q_new ‚Üê LAST_ADDED_VERTEX(T_start) # 2) Try to CONNECT T_goal toward q_new aggressively if CONNECT(T_goal, q_new, Œµ) = REACHED: return EXTRACT_PATH_BIDIRECTIONAL(T_start, T_goal, q_new) SWAP(T_start, T_goal) # alternate which tree is \"start\" vs \"goal\" return FAILUREEXTEND(T, q_target, Œµ): q_near ‚Üê NEAREST_NEIGHBOR(T, q_target) q_new ‚Üê STEER(q_near, q_target, Œµ) if COLLISION_FREE(q_near, q_new): T.ADD_VERTEX(q_new) T.ADD_EDGE(q_near, q_new) if q_new = q_target: return REACHED else: return ADVANCED else: return TRAPPEDCONNECT(T, q_target, Œµ): status ‚Üê ADVANCED while status = ADVANCED: status ‚Üê EXTEND(T, q_target, Œµ) return statusWhy RRT Paths Need SmoothingBecause RRTs grow by random exploration, the final paths are rarely smooth. They often zig-zag through space inefficiently. A common remedy is the short-cutting technique, which repeatedly attempts to bypass intermediate vertices on the path by checking whether the straight-line segment between two non-adjacent configurations is collision-free.If the shortcut succeeds, the interior of the path is replaced with the direct link:\\[\\text{if } \\texttt{CONNECT}(q_i, q_j) = \\text{free} \\quad\\Rightarrow\\quad \\text{replace segment }( q_i \\to q_{i+1} \\to \\dots \\to q_j ) \\text{ with } (q_i \\to q_j)\\]This tends to produce significantly shorter and smoother paths while keeping computation lightweight.RRT*RRT* was introduced to address the largest limitation of standard RRT: its inability to improve path quality over time. RRT* modifies the tree structure so that every new sample can potentially improve not just its own branch but also the structure of the nearby tree.When a new node $x_{new}$ is added: Parent Selection:RRT* considers all tree nodes within a radius $r(n)$ around $x_{new}$. Among these candidates, it chooses as parent the neighbor that minimizes the total path cost:\\[x_{\\text{parent}} = \\arg\\min_{x \\in X_{\\text{near}}} \\big( \\text{Cost}(x) + c(x, x_{\\text{new}}) \\big)\\] Rewiring: After adding $x_{new}$, the planner checks whether using $x_{new}$ as a parent would reduce the cost of any neighboring nodes. If so, those nodes are ‚Äúrewired‚Äù to point to $x_{new}$, improving the overall tree. The rewiring step is the critical innovation: it continuously reorganizes the tree as new samples arrive, allowing the planner to refine earlier decisions instead of being trapped by them.Asymptotic Optimality of RRT*RRT* is asymptotically optimal, unlike RRT or RRT-Connect.As the number of samples $n$ approaches infinity:\\[\\lim_{n \\to \\infty} \\text{Cost}( \\text{best path from RRT}^* )= \\text{Cost}( \\text{optimal path} )\\]This result is provable and depends on choosing the neighbor radius:\\[r(n) = \\gamma \\left( \\frac{\\log n}{n} \\right)^{1/d}\\]where $d$ is the dimension of the configuration space.Under this radius schedule, the tree becomes dense enough to guarantee optimal connections while still maintaining computational efficiency.Trade-Offs Between RRT, RRT-Connect, and RRT*There is a clear computational-optimality trade-off between these methods: RRT is extremely fast and good at finding any feasible path, but provides very low path quality. RRT-Connect is usually the fastest at finding a feasible path, especially in cluttered spaces. RRT* is slower per iteration but continuously improves the solution, eventually reaching the optimum.In practice, the choice depends on the task: For real-time feasibility (e.g., robot moving through unknown space), RRT or RRT-Connect is preferred. For motion tasks requiring precise, smooth, or cost-minimal paths (e.g., manipulation, surgical robots), RRT* provides the needed optimality.Markov PropertyIn search-based planning, the Markov property means: When you stand in a state $s$, everything you need to decide ‚Äì what successors you can go to and what they will cost ‚Äì is fully determined by the current state only, not by how you got there.So:\\[\\text{succ}(s) = \\text{function of } s\\]\\[c(s, s') = \\text{function of } (s, s'), \\quad s' \\in \\text{succ}(s)\\]No ‚Äúmemory‚Äù of the previous path is allowed in $succ$ or $c$ (i.e., no dependence on the history of the path leading up to it).Algorithms like Dijkstra, A*, dynamic programming, etc., assume that if you reach the same state again with higher cost, that path is always worse for the future. This is only true if the future behavior (successors and costs) depends only on the state, not on the path you took.If the Markov property is broken: You might incorrectly prune states The algorithm can become incomplete (miss a solution) or sub-optimalIndependent vs Dependent VariablesEach search state $s$ contains a set of variables:\\[X(s) = \\{ X_{\\text{ind}}(s), X_{\\text{dep}}(s) \\}\\]Independent variables $X_{ind}(s)$: These define the state identity Two states are the same if and only if their independent variables are equal\\[s = s' \\quad \\text{iff} \\quad X_{\\text{ind}}(s) = X_{\\text{ind}}(s')\\]Dependent variables $X_{dep}(s)$: Variables derived from the independent variables Used for successor generation, cost computation, heuristics, collision checksThe Markov property holds if dependent variables depend only on the independent ones:\\[X_{\\text{dep}}(s) = f\\big(X_{\\text{ind}}(s)\\big)\\]This means the planner can fully determine successors, transition costs, valid or invalid states, just from the current state itself.Markov property is violated when you compute dependent variables using history:\\[X_{\\text{dep}}(s) = f\\big(X_{\\text{ind}}(s), \\text{history}\\big)\\]IncompletenessIncompleteness arises when your state representation or successor function violates the assumptions needed for graph search. It can happen for three major reasons: Incorrect state definition (missing independent variables) Dependent variables rely on history Successor generation fails to enumerate all possible transitionsTo guarantee completeness: Include every future-relevant variable in $X_{ind}$ Ensure dependent variables are pure functions of $X_{ind}$ Ensure successor generation is fully determined Ensure edge costs depend only on $(s,s‚Äô)$DominanceDominance is a pruning rule used in optimal search to reduce the number of states explored. The basic idea is that if two states represent the exact same situation, but one reached that situation with a lower g-value, and both share the same future possibilities, then the more expensive state can be discarded. This makes the search faster without losing optimality.Two states can only be compared if they are identical in all independent variables:\\[X_{\\text{ind}}(s_1) = X_{\\text{ind}}(s_2)\\]If they differ in any independent variable, then they cannot dominate each other.Dominance only works if independent variables fully determine all future successors and costs. That is the Markov property.\\[\\text{succ}(s) = f(X_{\\text{ind}}(s)), \\quadc(s,s') = g(X_{\\text{ind}}(s), X_{\\text{ind}}(s'))\\]Task PlanningTask planning is a branch of robotics that deals with reasoning about high-level actions ‚Äî actions that manipulate objects, change symbolic states of the world, and achieve complex goals. Unlike motion planning, which concerns itself with geometric paths and kinematic feasibility, task planning is fundamentally symbolic. It focuses on how the world can be transformed step-by-step through discrete, logically defined actions.Task planning involves deciding what sequence of actions a robot must perform to accomplish a goal. These actions may involve picking objects up, stacking them, assembling parts, or performing multi-step procedures such as constructing a birdcage. The key challenge is not continuous robot motion but rather logical ordering: which actions must precede others, what dependencies exist, and how actions change the state of the world.Blocksworld ExampleOne of the most famous examples in symbolic AI is Blocksworld. In this environment, various blocks can be stacked on one another or placed on a table. The robot‚Äôs task is to transform an initial arrangement of blocks into some desired configuration.For instance, consider a start state where block A sits on block B, and block C sits on the table. The goal state may require placing C on A, and A on the table. The core challenge is deciding in what order the robot must move blocks to reach this arrangement. Movement is constrained ‚Äî for example, the robot can only move blocks that have no other block on top of them.The key insight is that Blocksworld is purely symbolic. The robot does not care about precise coordinates or continuous motion but instead about discrete relationships like: ‚ÄúOn(A, B)‚Äù ‚ÄúClear(A)‚Äù (nothing is stacked on top of A) ‚ÄúBlock(C)‚Äù (C is a movable block)These predicates define the symbolic state of the world.Representing Blocksworld as a State-Space GraphTo use search algorithms, we must represent Blocksworld as a graph where each node is a symbolic state and edges correspond to actions. A state is simply the collection of all true statements (predicates) about the world at that moment. For example:\\[\\text{State} ={ \\text{On(A, B)}, \\text{On(B, Table)},\\ \\text{On(C, Table)},\\ \\text{Clear(A)},\\ \\text{Clear(C)},\\ \\text{Block(A)},\\ \\text{Block(B)},\\ \\text{Block(C)} }\\]Actions such as Move(b, x, y) transform one state into another by modifying these predicates. Each action has preconditions that must be true before it can be executed (e.g., the block must be clear, and the target must be clear) and effects that describe how the state changes. If action preconditions are satisfied, the action is applicable, and it creates a successor state in the search graph.This transforms Blocksworld into a state-space search problem, where A* or breadth-first search can discover the shortest action sequence from the start state to any state matching the goal description.STRIPSSTRIPS (Stanford Research Institute Problem Solver) provides a structured way to define symbolic planning problems. It describes a planning problem using three components: state representation, goal representation, and action representation.State Representation in STRIPSA state is represented as a conjunction of positive literals, meaning it is a set of statements about the world that are true. For example:\\[\\text{On(A,B)} \\wedge \\text{On(B,Table)} \\wedge \\text{Clear(A)} \\wedge \\text{Block(A)} \\wedge \\text{Block(B)}\\]Crucially, STRIPS uses the closed-world assumption: any fact not listed in the state is assumed to be false. This means we only list what is true; everything else is false by default.Goal Representation in STRIPSGoals in STRIPS are also defined as a conjunction of positive literals. The goal may be fully specified (e.g., On(B,C) ‚àß On(C,A) ‚àß On(A,Table)) or partially specified. A partial goal describes only what must be true, allowing any additional conditions.For example, ‚ÄúBlock A must be on the table‚Äù would be represented as:\\[\\text{On(A,Table)}\\]Any state that satisfies this literal is a valid goal.Action Representation in STRIPSActions have two components: Preconditions: These are conditions that must be true for the action to be applicable.They are expressed as a conjunction of positive literals. Effects: Effects describe how the state changes after the action is executed. They consist of: Add list (facts that become true) Delete list (facts that become false) For example, the action MoveToTable(b,x) has preconditions:\\[\\text{On(b,x)} \\wedge \\text{Clear(b)} \\wedge \\text{Block(b)} \\wedge \\text{Block(x)}\\]Its effects include adding:\\[\\text{On(b,Table)}, \\text{Clear(x)}\\]and deleting:\\[\\text{On(b,x)}\\]STRIPS therefore provides a fully formal way to describe how actions transform the world.From STRIPS Descriptions to Graph SearchOnce a domain is represented in STRIPS, we can automatically construct the successor function used in search algorithms. A program reads the preconditions of each action and checks whether they are satisfied in the current state. If so, it applies the effects, generating a successor state.This converts symbolic descriptions into a search graph implicitly, without manually enumerating states. A*, breadth-first search, or any other search algorithm can operate on this graph to find a valid plan.This process is known as domain-independent planning because the planner itself is universal ‚Äî only the domain description changes.Example: Blocksworld in the Symbolic GraphStart state:\\[\\text{On(A,B)} \\wedge \\text{On(B,Table)} \\wedge \\text{On(C,Table)} \\wedge \\\\text{Block(A)} \\wedge \\text{Block(B)} \\wedge \\text{Block(C)} \\wedge \\\\text{Clear(A)} \\wedge \\text{Clear(C)}\\]Goal state:\\[\\text{On(B,C)} \\wedge \\text{On(C,A)} \\wedge \\text{On(A,Table)}\\]Given actions like Move(b,x,y) and MoveToTable(b,x), the planner examines whether their preconditions hold in the current symbolic state. If yes, applying the action produces a successor symbolic state.This creates a branching graph of symbolic states, each representing a different configuration of blocks. We can apply A* or Weighted A* to search this symbolic state space, treating each action as an edge of cost 1 (or any defined cost).Need for HeuristicsSymbolic planning expands into enormous state spaces very quickly. With just a few objects and predicates, the number of possible states becomes huge. To make A* practical, we need a heuristic function that estimates how close a symbolic state is to the goal. A heuristic in symbolic planning tries to answer:\\[h(s) = \\text{How many steps remain before } s \\text{ satisfies all goal predicates?}\\]But symbolic states are not geometric; they are sets of logical predicates. So devising heuristics requires reasoning about the structure of the symbolic problem.Domain-Independent Heuristic 1: Counting Unsatisfied Goal LiteralsThe simplest heuristic counts how many goal predicates are missing from the current state. For example, if the goal has three predicates and the current state satisfies only one, then:\\[h(s) = \\#{ \\text{goal literals not yet true in } s }\\]This heuristic is not admissible, because satisfying one goal literal might require a long chain of actions ‚Äî more than one step. However, even though it breaks admissibility, it is still often useful, especially with Weighted A*, where we intentionally accept suboptimality for speed.This heuristic is incredibly easy to compute and already restricts the search greatly.Domain-Independent Heuristic 2: Relaxing Delete Effects (Empty-Delete-List Heuristic)The more powerful approach is to compute a heuristic by solving a relaxed version of the planning problem. The common relaxation is to assume that: No action deletes any predicate. In other words, actions only add facts; they never remove them.This is the delete-relaxation or empty-delete-list heuristic, and it turns the planning problem into a monotone logical problem where the state only grows over time. This version is dramatically easier to solve because we never have to undo actions.The heuristic value is the cost of the optimal solution to this relaxed problem, making it admissible (but expensive to compute). Despite its computational overhead, this relaxation forms the basis of many state-of-the-art planners (e.g., FF, HSP), because it provides extremely informative heuristics that dramatically cut down the search.Total vs Partial Ordering of PlansSymbolic planning domains have extremely high branching factors. For example, in Blocksworld with four blocks, you may have dozens of possible actions in each state ‚Äî every movable block can be moved onto every other clear block or the table.This leads to large branching factors, which greatly slow down state-space search. Therefore, even with STRIPS, storing the full state graph explicitly is impossible. The graph is constructed implicitly, via successor generation rules. But despite this, raw branching can still overwhelm A* unless heuristics are strong.Traditional STRIPS planning (and A* search on symbolic states) implicitly produces total-order plans, meaning: All actions are placed in a fixed sequence, even if their ordering does not matter.For example, if the goal requires stacking block C on A and stacking D on B, these two actions are independent: doing C‚ÜíA first or D‚ÜíB first are both permissible.However, classical planners produce one fixed total ordering. This is unnecessarily rigid. Many tasks permit partial orders ‚Äî some actions can be done in any order or even in parallel. This motivates the introduction of Partial-Order Planning (POP).Partial-Order Planning (POP)Partial-Order Planning represents plans not as strict sequences but as collections of actions plus constraints describing which actions must precede which.A POP ‚Äúplan state‚Äù contains three things: The currently selected set of actions A set of ordering constraints A &lt; B meaning ‚ÄúA must happen before B‚Äù. No cycles allowed (i.e., A &lt; B and B &lt; A is a cycle and makes such state invalid). A set of causal links A =p&gt; B meaning ‚ÄúA achieves precondition p required by B‚Äù.required by action B)The key idea is to add constraints only when necessary. The planner builds the plan ‚Äúfrom the top down‚Äù: Start with a fake Start action and a fake Finish action. The Start action asserts all predicates true in the initial state. The Finish action requires all predicates in the goal.The planner then gradually adds actions needed to satisfy Finish‚Äôs preconditions, then actions needed to satisfy those actions‚Äô preconditions, and so on.At each step, it only imposes ordering constraints needed to avoid contradictions or causal violations. This produces a partial-order plan, where any linearization of the graph is a valid execution sequence.When adding a new action A to satisfy a precondition p for some action B, the planner must ensure that no other action deletes p between A and B. If such an action C exists, the planner resolves the threat by adding either: C &lt; A (C happens earlier), or B &lt; C (C happens later)If both constraints would violate existing ordering constraints (by creating a cycle), the partial plan becomes invalid and is discarded. This careful handling of ‚Äúthreats‚Äù ensures the consistency of causal links and keeps the plan logically sound.POP does not search over symbolic world states. Instead, it searches directly in the space of possible partial plans. Each state in the POP search is a partially constructed plan with: actions included ordering constraints causal linksThe search terminates when all preconditions of all actions are satisfied ‚Äî meaning the partial plan encodes a complete, feasible ordering. The search is typically done using depth-first search, because POP states are often deep but narrow, and we are usually satisfied with any working plan, not necessarily the shortest.Planning Under UncertaintyRobots rarely operate in perfectly predictable environments. They slip, their actuators have unmodeled dynamics, their sensors may be noisy, and sometimes the world itself behaves unpredictably. Up to this point, much of planning assumed a deterministic world: if the planner tells the robot to move from state $s$ to state $s‚Äô$, it will reach $s‚Äô$ exactly. In these settings, planning reduces neatly to graph search ‚Äî find a sequence of actions that leads from the start to the goal. But the real world violates this assumption constantly. The robot may attempt to go ‚Äúeast‚Äù but drift slightly north; it may attempt to grasp but fail; it may plan along a cliff edge but slip into a dangerous region. Planning while ignoring all these uncertainties leads to solutions that are often suboptimal and sometimes catastrophic.Deterministic graph search assumes the robot transitions perfectly from one state to the next. But when actions have unpredictable outcomes, executing a fixed plan no longer guarantees arriving at the intended states. The robot might replan on-the-fly when deviations occur, but this reactive strategy is inefficient and risky.Thus arises the need to model execution uncertainty at planning time, not after failures occur. This is achieved by converting the search graph into an MDP, where at least one action has multiple possible successor states, each with an associated probability and cost.Markov Decision Processes (MDPs)An MDP extends the deterministic planning framework by allowing each action to have multiple possible outcomes. Formally, an MDP is defined by: A set of states $S$ A set of actions $A(s)$ available in each state For each action $a$ in state $s$: a set of possible successor states a probability distribution over these successors an associated cost for each transition PolicyThe crucial shift when moving from deterministic planning to MDPs is that you no longer compute a single sequence of actions, since the robot may not follow the intended path. Instead, you compute a policy:\\[\\pi : S \\rightarrow A\\]A policy maps every state to an action ‚Äî telling the robot what to do regardless of which branch of the uncertainty tree reality follows. This is why planning in MDPs is fundamentally more complex. Instead of a single path, the solution is often a directed acyclic graph (DAG) of possible transitions, representing the branching structure of all possible outcomes and the prescribed actions at each.Objectives for Planning Under UncertaintyIn MDPs, there are multiple ways to define optimality. The lecture focuses on two: Expected-cost planning: Minimize the expected total cost of reaching the goal. Minimax planning: Minimize the worst-case cost, assuming the world behaves in the most adversarial possible way.Minimax PlanningMinimax planning asks: ‚ÄúGiven all possible uncertain outcomes, what policy minimizes the worst-case cost-to-goal?‚ÄùFormally:\\[\\pi^* = \\arg\\min_{\\pi} \\max_{\\text{outcomes of } \\pi}{\\text{cost-to-goal}}\\]In other words: The environment is treated as adversarial. The planner assumes that every action may yield the worst possible successor. The robot must be robust against the worst instance of randomness.This is extremely conservative but extremely safe.A policy maps each possible state to an action, forming a directed acyclic graph (DAG) of possible future evolutions. The graph branches wherever uncertainty exists: one action in the current state leads to multiple possible next states, each requiring its own follow-up action.The graph contains no cycles, because in minimax reasoning a cycle would imply the robot might get stuck forever, producing infinite worst-case cost. Thus, the optimal minimax strategy is always a branching, acyclic decision structure that covers every possible outcome of uncertainty.Computing Minimax Plans: Backward A*Algorithm: Initialize $g(s_{goal}) = 0$ and $g(s) = \\infty,\\ \\forall s \\neq s_{goal}$; $OPEN = {s_{goal}}$ while $s_{start}$ not expanded: remove $s$ with smallest $f(s) = g(s) + h(s)$ from $OPEN$ insert $s$ into $CLOSED$ for every $s‚Äô$ such that $s \\in succ(s‚Äô,a)$ for some $a$ and $s‚Äô$ not in $CLOSED$ if $g(s‚Äô) &gt; \\max_{u \\in succ(s‚Äô,a)} \\big(c(s‚Äô,u) + g(u) \\big)$ $g(s‚Äô) = \\max_{u \\in succ(s‚Äô,a)} \\big(c(s‚Äô,u) + g(u) \\big)$ insert $s‚Äô$ into $OPEN$ After computing the minimax g-values of the states, the optimal policy is obtained by selecting an action that minimizes (think of this to be in the forward pass):\\[\\text{max}_{s' \\in succ(s,a)}\\{c(s,a,s')+g(s')\\}\\]This recursion directly encodes minimax reasoning: a state‚Äôs cost is determined by its worst possible successor, not its best or expected one.Backward minimax A* reduces to classical backward A* when uncertainty disappears. It maintains all the properties of A* (optimality, no re-expansion) if heuristics are consistent.Resulting Policy Structure:The resulting minimax policy is often: A deterministic DAG, not a single path. Branching according to possible uncertain outcomes. Acyclic, because cycles would imply infinite worst-case cost.The branching reflects the policy telling the robot: ‚ÄúIf you end up in this state, take action $a$. If the uncertainty pushes you into another state, take action $b$.‚ÄùWhy not Forward A*?In deterministic planning, forward A* works beautifully because the cost of a state is determined solely by the cost accumulated so far. When you expand a state, you know exactly its future cost-to-goal because the path is deterministic. However, in minimax planning under uncertainty, every action can lead to multiple possible successor states, and the cost of a state is the maximum over the costs of all these possible outcomes.This means that the true cost of a state cannot be known until all of its successors have been evaluated. In other words, the value of a state depends on the value of its children, not the other way around. Forward A* requires knowing or estimating the cost-to-go (the h-value) from the current state before expanding it. But in minimax, that cost is not something you can estimate from heuristics alone ‚Äî it must be computed from the actual successor states. As long as those successor states are not evaluated yet, the minimax backup cannot be computed correctly.Backward A* solves this dependency. By starting from the goal and working backward, the planner always expands states after all their successors‚Äô values have been determined. Because the minimax backup is:\\[g(s) = \\min_{a \\in A(s)} \\max_{u \\in succ(s,a)} \\big( c(s,u) + g(u) \\big)\\]the value of $s$ requires knowing the values of all its successors $u$. Backward search ensures that when we compute $g(s)$, all the $g(s)$ values are already finalized or improved optimally. This maintains the same correctness and optimality guarantees that A* has in deterministic planning.Pros and Cons of Minimax PlanningMinimax planning is powerful but expensive.Pros: Extremely robust to worst-case disturbances Guarantees finite cost regardless of execution randomness Provides strong safety guaranteesCons: Overly pessimistic: assumes worst-case outcome happens always More expensive than deterministic A* Heuristics are less informative because minimax backup uses max, not sum Backward minimax A* may not apply to all MDPs Even when it applies, the search is often slower because uncertainty creates branchingThus minimax is most appropriate for safety-critical or adversarial domains where robustness outweighs efficiency.Expected Cost FormulationMinimax assumes that every time an action is taken, the world will produce the worst possible stochastic outcome. In many domains ‚Äî navigation on slippery surfaces, noisy actuation, imperfect sensing ‚Äî this assumption dramatically overestimates risk. For example, let‚Äôs say in a hill-climbing scenario, the robot has a 10% chance of slipping when moving uphill. If we treat this as an adversarial worst-case outcome, minimax will conclude that climbing is effectively impossible, because in the worst case the robot slips on every attempt. In reality, slipping occasionally is acceptable if the expected time or cost remains low.Expected-cost planning instead models uncertainty probabilistically, so each action outcome contributes to future cost proportionally to its probability. This produces policies that are safer and more realistic than minimax, yet not overly pessimistic.Defining the Expected-Cost ObjectiveThe policy that is optimal under the expected-cost model minimizes:\\[\\pi^* = \\arg\\min_{\\pi} \\mathbb{E}\\{\\text{cost-to-goal} \\mid \\pi\\}\\]Expectation is taken over the stochastic outcomes of each action. To illustrate, consider the following example MDP: From $s_1$, action $a_1$ transitions to $s_{goal}$ with probability $0.9$ and cost $2$ to $s_{2}$ with probability $0.1$ and cost $2$ Two competing strategies exist: $\\pi_1:$ Go through $s_4$ deterministically Expected cost $= 1 + 1+3+1=6$ $\\pi_2:$ Keep trying to reach the goal through the stochastic $s_1$ edge The expected cost sums an infinite geometric series of attempts:\\[0.9 \\cdot (1+2+2)+ 0.9\\cdot0.1 \\cdot (1+2+2+2+2)+ 0.9\\cdot0.1^2 \\cdot (1+2+2+2+2+2+2) \\cdots = 5.444\\dots\\] Thus, under expected-cost reasoning, $\\pi_2$ is better than $\\pi_1$ ‚Äî even though minimax preferred $\\pi_1$. This highlights the fundamental philosophical difference: minimax protects against worst-case scenarios, while expected-cost planning optimizes performance on average.Value FunctionLet $v^(s)$ denote the optimal expected cost-to-goal from state $s.$ $v^(s)$ satisfies the core equation of expected-cost MDPs:\\[v^*(s) =\\min_{a \\in A(s)} \\mathbb{E}\\left[c(s,a,s') + v^*(s')\\right]\\quad \\text{for } s \\neq s_{goal}\\]\\[v^*(s_{goal}) = 0\\]This is the Bellman optimality equation for stochastic shortest-path problems.Given the optimal value function, the optimal policy is simply the greedy policy:\\[\\pi^*(s)= \\arg\\min_{a \\in A(s)} \\mathbb{E}\\left[c(s,a,s') + v^*(s')\\right]\\]This equation formalizes the idea that an optimal action is one whose immediate cost plus expected future cost is minimal.Value IterationValue Iteration solves the Bellman optimality equations through fixed-point iteration. It starts with an initial guess for all $v(s)$, then repeatedly applies the Bellman backup:\\[v(s)\\leftarrow\\min_{a}\\mathbb{E}_{s'}\\left[c(s,a,s') + v(s')\\right]\\]Important details: $v(s_{goal})$ is always fixed at $0$. It is best to initialize to admissible values (underestimates of the actual costs-to-goal). All other values gradually improve (monotonically increasing) until convergence. VI converges in a finite number of iterations. In the worst case we take $N$ sweeps, and in each sweep we take $N$ backups. So the worst case will have atmost $N^2$ backups, compared to only $N$ in A* because the order there is optimal. Convergence means:\\[|v(s) - \\min_{a} \\mathbb{E}[c(s,a,s') + v(s')]| &lt; \\Delta\\] The order in which states are backed up affects speed but not correctness. For stochastic shortest-path problems in which every state can eventually reach the goal, VI always converges to the optimal value function.The expected cost of executing greedy policy is at most:\\[v^*(s_{start})c_{min}/(c_{min}-\\Delta)\\]VI has a drawback: it updates every state in the MDP on every iteration, even states that will never be reached by the optimal policy. This creates inefficiency in large MDPs.Real-Time Dynamic Programming (RTDP)While Value Iteration updates every state repeatedly, RTDP focuses computation only on states that actually matter for the optimal policy. Instead of sweeping through the entire state space, RTDP simulates execution and updates only the states encountered along greedy trajectories.RTDP proceeds as follows: Initialize all values to admissible (optimistic) underestimates. Starting from the real initial state, follow a greedy policy (choosing the action that minimizes expected cost) picking outcomes at random. At each visited state, apply a Bellman backup.\\[v(s)\\leftarrow \\min_{a} \\mathbb{E}_{s'}\\left[ c(s,a,s') + v(s') \\right]\\] If the goal is reached, restart from the start and repeat. Stop when Bellman errors along the greedy path fall below $\\Delta$For the first step: For any state $s$, pick an action $a$ that minimizes\\[E\\{c(s,a,s')+v(s')\\}\\] Pick a successor $s‚Äô$ at random according to probability\\[P(s'|a,s)\\] RTDP is guaranteed to converge in a finite number of iterations (given admissible initial values and a reachable goal), and like VI, it leads to an optimal greedy policy.The expected cost of executing greedy policy is at most:\\[v^*(s_{start})c_{min}/(c_{min}-\\Delta)\\]RewardsThe reward formulation is central whenever tasks do not terminate after reaching a goal, or when the robot continually interacts with the environment ‚Äî patrolling, surveillance, balancing, manipulation, or any behavior that runs indefinitely. Instead of ending at a terminal state (like reaching a goal cell), the system keeps accruing rewards over time. Planning, therefore, must evaluate the long-term desirability of each state.In a reward-based MDP, each transition gives the agent a reward $r(s,a,s‚Äô).$The task becomes: Choose actions to maximize the expected total reward received over time.Unlike the cost formulation, where lower values are better, here higher values are better.The optimal value function satisfies:\\[v^*(s) = \\max_{a \\in A(s)} \\mathbb{E}_{s'} \\big[r(s,a,s') + \\gamma v^*(s') \\big]\\]This is the Bellman optimality equation for reward-MDPs. The appearance of the discount factor $\\gamma$ is crucial ‚Äî without it, the sum of rewards could diverge, and the value function would be undefined.Why discounting?Discounting adjusts how the robot values immediate rewards relative to future ones:\\[0 &lt; \\gamma \\le 1\\]If $\\gamma \\le 1:$ Future rewards count less than immediate rewards Values remain finite, even if the system never terminates VI and PI converge rapidly, because the Bellman operator becomes a contractionIf $\\gamma=1:$ Rewards are not discounted Value iteration may not converge Total reward can become infinite if the robot loops through positive-reward cyclesThus discounting is not merely a mathematical trick ‚Äî it enforces stability and convergence in infinite-horizon tasks.Difference in Problem StructureIn cost-based MDPs: There is always a terminal goal with $v^*(goal)=0$ All states eventually must reach the goal for the formulation to be valid Value iteration converges because costs accumulate positivelyIn reward-based MDPs: There may be no terminal goal The robot may remain in the system forever (e.g., balancing a pole) Discounting controls infinite reward accumulation Policies are evaluated by their stationary, long-term behaviorThe reward formulation is therefore strictly more general.Bellman Equations Now Maximize, Not MinimizeThe Bellman backup in the reward formulation becomes:\\[v(s) \\leftarrow \\max_{a} \\mathbb{E}_{s'} \\big[ r(s,a,s') + \\gamma v(s') \\big]\\]Compare this to the cost version:\\[v(s) \\leftarrow \\min_{a} \\mathbb{E}_{s'} \\big[ c(s,a,s') + v(s') \\big]\\]The structure is the same ‚Äî min becomes max, cost becomes reward, and discounting appears in the infinite-horizon setting. But the planning intuition changes dramatically: instead of trying to reach a goal cheaply, the robot tries to accumulate as much positive reward as possible.Given the optimal value function, the optimal policy is simply the greedy policy:\\[\\pi^*(s)= \\arg\\max_{a \\in A(s)} \\mathbb{E}\\left[r(s,a,s') + \\gamma v^*(s')\\right]\\]Value Iteration for Reward MDPsValue iteration still works ‚Äî but with slightly modified behavior: Initialize all values (optimistic or arbitrary). Apply Bellman backups using maximization and discounting. Values converge to the unique fixed point of the Bellman operator:\\[v(s) = \\max_a \\mathbb{E}[ r + \\gamma v(s') ]\\] Because $\\gamma &lt;1,$ the operator is a contraction ‚Äî guaranteed to converge.Partially Observable Markov Decision Processes (POMDP)A simple path-planning example highlights the differences. In a classical graph search problem, the robot knows its exact state and every action deterministically leads to a single successor. A graph is essentially the tuple {S, A, C}: states, actions, and deterministic costs.Once we allow action uncertainty (e.g., a drone drifting left or right with probability 0.5), the problem becomes an MDP because the effect of an action is no longer deterministic. The transition model must specify a probability over successor states:\\[T(s,a,s') = P(s' \\mid s,a)\\]However, even in an MDP, the robot still assumes perfect localization: it always knows which state it lands in after acting.In a POMDP, we remove this assumption. The robot now has uncertain state and uncertain action outcomes. It only receives limited observations that do not uniquely identify the true state. This situation arises in many robotics settings, such as UAVs navigating without GPS, mobile robots in ambiguous hallways, underwater robots with drifting odometry, or manipulation tasks where object identity is uncertain.Thus, Graphs model perfect knowledge + deterministic actions. MDPs model full observability + stochastic actions. POMDPs generalize both by allowing stochastic actions and partial state observability.What is POMDP?A POMDP is defined as:\\[{S,\\ A,\\ T,\\ R,\\ \\Omega,\\ O}\\]where: $S:$ set of all possible world states $A:$ set of all actions $T(s,a,s‚Äô):$ transition model $R(s,a):$ reward (or cost) function $\\Omega:$ set of possible observations $O(s‚Äô,a,o) = P(o|s‚Äô,a):$ probability of seeing observation $o$ after taking action $a$ and landing in $s‚Äô$The critical new component is the observation model. The robot only perceives partial information about the true state.A POMDP has the following causality chain:\\[s \\xrightarrow{a} s' \\xrightarrow{o} \\text{observation}\\]The observation does not reveal $s‚Äô$ exactly, but it narrows down possible states." }, { "title": "Wheeled Biped", "url": "/posts/wheeled-biped/", "categories": "Projects, Robotics", "tags": "rl, biped, wheeled", "date": "2025-09-03 12:00:00 -0400", "snippet": "In progressWheeled Biped" }, { "title": "Milwaukee Tool", "url": "/posts/milwaukee-tool/", "categories": "", "tags": "", "date": "2025-08-08 00:00:00 -0400", "snippet": "" }, { "title": "Language-Conditioned BEV Perception for Autonomous Driving", "url": "/posts/vlr-bev/", "categories": "Projects, Robotics", "tags": "vlr, vlm, bev, av, perception", "date": "2025-07-02 12:00:00 -0400", "snippet": "Language-Conditioned BEV Perception for Autonomous Driving Your browser does not support PDFs. Please download the report here." }, { "title": "Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences", "url": "/posts/cbs-multi-arm/", "categories": "Projects, Robotics", "tags": "vlr, vlm, bev, av, perception", "date": "2025-07-02 12:00:00 -0400", "snippet": "Techincal Report Your browser does not support PDFs. Please download the report here.ResultsxECBSScene 1Scene 2Scene 3Scene 4ECBSScene 1Scene 2Scene 3Scene 4CBSScene 1Scene 2Scene 3Scene 4RRT-ConnectScene 1Scene 2Scene 3Scene 4" }, { "title": "Deep Learning - Attention & Transformers", "url": "/posts/deep-learning-advanced/", "categories": "Blog, Robotics", "tags": "learning, nnets, transformer, attention, llm", "date": "2025-06-09 12:00:00 -0400", "snippet": "Attention ModelsProblem with vanilla Seq2Seq ModelsIn the vanilla sequence-to-sequence (Seq2Seq) model with an encoder‚Äìdecoder setup: The encoder reads the entire input sequence (e.g., I ate an apple) and compresses all information into a single fixed-length vector (the final hidden state). The decoder then uses this single vector to generate the entire output sequence (e.g., Ich habe einen Apfel gegessen).Why is this a problem? Bottleneck at the final hidden state All the input information is forced into the last encoder hidden state (red box in the image above). This creates an information bottleneck, especially for long sentences. Decoder only gets the encoder hidden state once At time step 0, the decoder receives the encoder‚Äôs final hidden state. After that, the decoder only relies on its own recurrent updates, carrying forward information from previous steps. Information dilution As decoding progresses, the hidden state is increasingly dominated by the decoder‚Äôs own recurrence. The contribution from the encoder‚Äôs final hidden state becomes weaker and ‚Äúdiluted.‚Äù At later steps, the decoder may lose track of crucial information from the input sequence. Because of this, for long and complex sentences, the translations or predictions degrade in quality. Moreover, important words from the beginning of the sentence may be forgotten when generating later outputs. Fundamentally, the model is assuming that a single compressed summary of the entire input is sufficient ‚Äî which is not true for natural language, where each output word often depends on a specific part of the input.Potential Solution 1A simple solution is to have the same encoder representation feed directly into all decoder states as input. Now the decoder‚Äôs recurrence is local, and the encoder signal isn‚Äôt washed out as we move forward.Why a single encoder vector is still not enoughEven with that change, we are still squeezing the entire input into one vector. That single vector becomes overloaded, especially for long inputs. In reality, all encoder hidden states carry information, with each one tied to its corresponding input token plus surrounding context. By relying on a single compressed summary, we inevitably lose detail, and some information from earlier tokens has already been attenuated by the time the encoder reaches its final state.What the decoder really needsEvery output token is related to the input directly. Giving the decoder only one fixed encoder vector (even at all steps) misses the more granular relationship between individual outputs and specific parts of the input. The decoder should be able to draw, at each step, on the set of encoder states rather than only on a single summary, so it can use the portions of the input that are most relevant to the current output token.Potential Solution 2A simple approach is to compute the average of all encoder hidden states:\\[\\text{Average} = \\frac{1}{N} \\sum_{i=1}^{N} h_i\\]This average vector is then given to the decoder at every step. This way, the decoder receives a representation that reflects the entire sequence, not just the final hidden state.Limitations of simple averagingWhile better than relying on one state, averaging has a clear drawback: It applies the same weight to every encoder state. Every decoder step receives the same averaged vector, regardless of which output word is being generated. In reality, different outputs depend on different parts of the input. For example: ‚ÄúIch‚Äù is most closely related to ‚ÄúI‚Äù, while ‚Äúhabe‚Äù and ‚Äúgegessen‚Äù are more related to ‚Äúate‚Äù.Towards Weighted AveragesThe solution is to let each decoder step compute its own weighted average of the encoder hidden states.\\[c_t = \\frac{1}{N} \\sum_{i=1}^{N} w_i(t) h_i\\]$c_t$ is the context vector at decoding step $t$.$w_i(t)$ are the attention weights and are dynamically computed as functions of the decoder state. The expectation is that if this model is well-trained, this will automatically highlight the correct inputs.Attention WeightsRequirementsThe attention weights depend on the decoder‚Äôs current needs. At time step $t$, they are computed using the decoder‚Äôs state from the previous step $s_{t-1}$:\\[w_i(t) = a(h_i, s_{t-1})\\]where $a(.)$ is a function (called the alignment model or score function) that measures how well encoder state $h_i$ matches the decoder state $s_{t-1}$.For the system to work properly, the attention weights must satisfy two conditions: Each weight must be positive All weights for step $t$ must sum to $1$, forming a probability distribution\\[\\sum_{i=1}^{N} w_i(t) = 1\\] To meet these requirements, we compute weights in two stages: Compute a set of raw scores that can be positive or negative\\[e_i(t) = g(h_i, s_{t-1})\\] where $g$ is the scoring function. Convert these scores into valid weights using the softmax function\\[w_i(t) = \\frac{\\exp(e_i(t))}{\\sum_j \\exp(e_j(t))}\\] This guarantees that $w_i(t) \\ge 0, \\space \\sum_i w_i(t) = 1.$Options for scoring function: Dot Product Attention\\[g(h_i, s_{t-1}) = h_i^T s_{t-1}\\] General Attention\\[g(h_i, s_{t-1}) = h_i^T W_g s_{t-1}\\] $W_g$ needs to be learned. Concat (Additive) Attention\\[g(h_i, s_{t-1}) = v_g^T \\tanh ( W_g \\begin{bmatrix} h_i \\\\ s_{t-1}\\end{bmatrix} \\big)\\] $v_g, W_g$ need to be learned. MLP-based Attention\\[g(h_i, s_{t-1}) = \\text{MLP}([h_i, s_{t-1}])\\] $\\text{MLP}$ needs to be learned. We will be using the general attention scoring function.Query-Key-ValueQuery‚ÄìKey‚ÄìValue (QKV) introduces a more flexible and general framework: Each input is represented by a key and a value. Keys represent how an input is searched or matched. Values represent the actual content/information to be passed. Each decoder step generates a query. Queries represent what the decoder is currently looking for. Attention weights are computed as a function of query and keys, and the final context is a weighted sum of values.Keys and values are derived from encoder hidden states:\\[k_i = W_kh_i\\]\\[v_i = W_vh_i\\]Query is derived from the decoder state:\\[q_t = W_qs_{t-1}\\]Attention weights are computed as a function of query and keys:\\[e_i(t) = g(k_i,q_t)\\]\\[w_i(t) = softmax(e_i(t))\\]Context is a weighted sum of values:\\[c_i(t) = \\sum_iw_i(t)v_i\\]Pseudocode:# Assuming encoded input is available# (K,V) = [k_enc[0], k_enc[1], ..., k_enc[T]], [v_enc[0], v_enc[1], ..., v_enc[T]]t = -1h_out[-1] = 0 # Initial Decoder hidden stateq[0] = 0 # Initial query Y_out[0] = &lt;sos&gt;do \tt = t+1\tC = compute_context_with_attention(q[t], K, V)\ty[t],h_out[t],q[t+1] = RNN_decode_step(h_out[t-1], y_out[t-1], C)\ty_out[t] = generate(y[t]) # Random, or greedyuntil y_out[t] == &lt;eos&gt;function compute_context_with_attention(q, K, V)\t# First compute attention\te = []\tfor t = 1:T # Length of input\te[t] = raw_attention(q, K[t])\tend\tmaxe = max(e) # subtract max(e) from everything to prevent underflow\ta[1..T] = exp(e[1..T] - maxe) # Component-wise exponentiation\tsuma = sum(a) # Add all elements of a\ta[1..T] = a[1..T]/suma\t\tC = 0\tfor t = 1..T\t\tC += a[t] * V[t]\tend\t\treturn CBeam SearchOur general goal is to generate the most likely output sequence that ends with &lt;eos&gt;. If we simply pick the most likely word at each step (greedy decoding), it can lead to suboptimal sequences. Example: Choosing ‚ÄúIch ‚Üí habe ‚Üí einen ‚Üí apfel‚Äù step-by-step may fail if another slightly less probable choice early on leads to a better overall translation.So, instead of committing to just one choice at each step, we retain multiple candidate paths. Each word choice forks the network, creating parallel hypotheses. This allows the model to explore different continuations of the sequence.However, keeping all possible continuations would be computationally infeasible. So we prune by keeping only the top K most likely candidate sequences at each time step,Process: At each step, compute the probability of partial sequences:\\[P(O_1, O_2, \\ldots, O_t \\mid I_1, I_2, \\ldots, I_N)\\] Retain the Top-K scoring sequences (based on the product of probabilities) Continue extending them until &lt;eos&gt; is reachedDifferent paths may reach &lt;eos&gt; at different lengths. The final output will be the likely complete sequence ending in &lt;eos&gt;.Pseudocode:# Assuming encoder output H = hin[1]‚Ä¶ hin[T] is availablepath = &lt;sos&gt;beam = {path}pathscore = [path] = 1state[path] = h[0] # computed using your favorite methodcontext[path] = compute_context_with_attention(h[0], H)do # Step forward\tnextbeam = {}\tnextpathscore = []\tnextstate = {}\tnextcontext = {}\tfor path in beam:\t\tcfin = path[end] \t\thpath = state[path]\t\tC = context[path]\t\ty,h = RNN_decode_step(hpath, cfin, C)\t\tnextC = compute_context_with_attention(h, H)\t\tfor c in Symbolset\t\t\tnewpath = path + c\t\t\tnextstate[newpath] = h\t\t\tnextcontext[newpath] = nextC\t\t\tnextpathscore[newpath] = pathscore[path]*y[c]\t\t\tnextbeam += newpath # Set addition\t\tend\tend\tbeam, pathscore, state, context, bestpath = prune (nextstate, nextpathscore, nextbeam, nextcontext)until bestpath[end] = &lt;eos&gt;Training In the forward pass, the encoder reads the input sentence and produces a sequence of hidden states that capture contextual meaning of each word. At each step, the decoder looks at its own previous hidden state, the last generated word, and a context vector formed by attention over encoder states. This gives a probability distribution over possible next words in the target language. Once we have the predicted distributions, we compare them with the actual ground truth sequence. We compute the divergence, and this loss is then backpropagated through the decoder, attention module, and encoder, so that every parameter learns to adjust in a way that improves translation accuracy next time.However, if we always feed the predicted word into the next decoder hidden state, the early training predictions will be poor because these is no one-to-one correspondence between the input and the ouput. So instead, since we already know the target or the true next word, we feed that in directly. This is called teacher forcing, and it makes training much easier because the decoder always has the right context, and the model converges faster.The problem with pure teacher forcing is that at inference time, the decoder won‚Äôt have access to ground truth, and its own mistakes can snowball (a mismatch called exposure bias). So instead, we find the middle ground where sometimes we feed the ground truth, and sometimes feed the predicted word. We could even gradually shift from more teacher forcing early on to less later in training.Another trick is sampling from the model‚Äôs predicted distribution instead of always taking the most likely word. This makes training closer to inference, because at inference we don‚Äôt have ground truth and the model must rely on its own probabilities. But sampling is not naturally differentiable (you can‚Äôt backpropagate through the act of choosing one word). To solve this, we use the Gumbel-Softmax trick.Gumbel Noise Trick:The Gumbel trick allows differentiable sampling. Given decoder logits $l_i$, we add Gumbel noise $g_i$:\\[z_i = l_i + g_i, \\quad g_i \\sim \\text{Gumbel(0,1)}\\] This transforms logits into a set of noisy scores. The max of these corresponds to a true categorical sample. Instead of taking a hard max, apply a softmax with temperature $\\tau$:\\[y_i = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}\\] $y$ is now a continuous vector, approximating a one-hot sample. As $\\tau \\rightarrow 0:$ output becomes nearly one-hot As $\\tau \\rightarrow \\inf:$ output becomes uniform High $\\tau$ implies soft &amp; exploratory, low $\\tau$ implies sharp &amp; close to true sampling Multi-Head AttentionInstead of a single attention function, we use multiple sets of projections of keys, queries, and values.\\[k_i^l = W_k^l h_i, \\quad v_i^l = W_v^l h_i, \\quad q_j^l = W_q^l s_{j-1}\\] $l$ is the head index Each head has its own learnable $W_k^l, W_v^l, W_q^l$So for each head: It computes its own attention scores $e_i^l(t)$ Produces its own context vector $c_t^l$After all heads compute their contexts, we concatenate and project them:\\[\\text{MultiHead}(Q,K,V) = W_o \\big[ c_t^1 ; c_t^2 ; \\dots ; c_t^h \\big]\\]Purpose of Multi-Head: Each head learns to focus on different aspects of the input. One head might align with word position. Another might capture semantic meaning. Another might handle long-range dependencies. Improves representation power by letting the model look at the same sequence from different subspaces.Self-AttentionProblem with Recurrent Encoders In standard encoder-decoder models with RNNs, each hidden state $h_i$ is computed sequentially and contains information from all previous words. When the decoder attends to these $h_i$ vectors, it is implicitly attending to the entire sequence again, not just the specific word. This raises a question: If the decoder is already selecting relevant words through attention, do we really need recurrence in the encoder at all?Instead of using recurrence, we can directly compute embeddings for all words in parallel:\\[h_i = f(x_i)\\]where $f$ could be a simple MLP applied to the word embedding $x_i$.But this removes the context-specificity: Example: the word ‚Äúan‚Äù may map to ‚Äúein‚Äù, ‚Äúeiner‚Äù, or ‚Äúeines‚Äù in German depending on context. Purely local embeddings $h_i$ lose this adaptability.So the solution is to use the attention framework itself to introduce contextspecificity in embeddings.Self-Attention Use the attention mechanism itself to inject context into embeddings. Each word‚Äôs representation is updated by attending to all other words in the sequence (including itself). This creates context-sensitive embeddings without recurrence.Computing Self-Attention Initial hidden representations Each word gets an initial representation:\\[h_i = f(x_i)\\] Compute Queries, Keys, and Values For each word embedding $h_i$:\\[q_i = W_qh_i, \\quad k_i=W_kh_i, \\quad v_i = W_vh_i\\] where $W_q, W_k,W_v$ are learnable matrices. Attention scores The attention score between word $i$ and word $j$ is a function of query $q_i$ and key $k_j$:\\[e_{ij} = q_i^Tk_j\\] Normalize via Softmax\\[w_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{j=1}^N\\exp(e_{ij})}\\] This gives the attention weight of word $i$ on word $j$.\\[w_{i0}, \\ldots, w_{iN} = softmax(e_{i0}, \\ldots e_{iN})\\]\\[w_{ij}=attn(q_i, k_{0:N})\\] Compute New Representation The updated embedding for word $i$ is:\\[o_i = \\sum_{j=1}^Nw_{ij}v_j\\] This means each word now carries information from all other words in proportion to attention weights. Multi-Head Self-AttentionTo capture different aspects of relationships simultaneously, we can also have multiple attention heads.For each head $a$, we project hidden state $h_i$ into queries, keys, and values using learnable matrices:\\[q_i^a = W_q^a h_i¬†\\]\\[k_i^a = W_k^a h_i\\]\\[v_i^a = W_v^a h_i\\]Each head has its own $W_q^a, W_k^a, W_v^a.$For each head $a$, compute attention weights and outputs:\\[w_{ij}^a = \\text{attn}(q_i^a, k_{0:N}^a)\\]\\[o_i^a = \\sum_j w_{ij}^a v_j^a\\]Each head produces its own output representation $o_i^a$, and different heads focus on different aspects of the input. Once all heads compute their outputs, we concatenate them:\\[o_i = [o_i^1;o_i^2;o_i^3;\\ldots;o_i^H]\\]where $H$ is the number of heads.The concatenated vector is passed through an MLP (feedforward block):\\[y_i = \\text{MLP}(o_i)\\]Typically, this is a linear transformation + nonlinearity (ReLU/GeLU), and helps mix the information across heads.The encoder can include many layers of suck multi-head self-attention blocks, and there is no need for recurrence.Positional EncodingNeed to know relative positionAttention mechanisms treat the input sequence as a set of tokens without inherent order. Unlike RNNs or CNNs, there is no built-in sense of sequence or distance between words. But in language, relative position matters: ‚Äúdog bites man‚Äù ‚â† ‚Äúman bites dog‚Äù Words closer together often influence each other more strongly than words far apart.Without positional information, self-attention compares every token with every other token equally ‚Äî the attention score between tokens does not depend on their relative distance. For example, the influence of ‚Äúan‚Äù on ‚Äúapple‚Äù should be much stronger when they are adjacent, and weaker when separated by many words.Thus, we add positional encodings to word embeddings to inject sequence order into the model, giving the network awareness of word positions and distances.Properties:We define a sequence of vectors $P_t$ for each position $t$. These vectors have the following properties: Each position has a unique encoding:\\[P_{t_1} \\neq P_{t_2}, \\quad \\text{for } \\space t_1 \\neq t_2\\]\\[|P_t|^2 = C\\] The correlation between two position encodings falls off with distance:\\[|P_t \\cdot P_{t+\\tau_1}| &gt; |P_t \\cdot P_{t+\\tau_2}|, \\quad \\text{if } |\\tau_1| &lt; |\\tau_2|\\] The correlation is stationary, meaning it depends only on the distance, not the absolute positions:\\[P_{t_1} \\cdot P_{t_1+\\tau} = P_{t_2} \\cdot P_{t_2+\\tau}\\] Constructing Positional Encodings:The most widely used approach (from the original Transformer paper) is to use sinusoids of different frequencies. For a token at position $t$ and embedding dimension $d$:\\[P_t = \\begin{bmatrix}\\sin(\\omega_1 t) \\\\\\cos(\\omega_1 t) \\\\\\sin(\\omega_2 t) \\\\\\cos(\\omega_2 t) \\\\\\vdots \\\\\\sin(\\omega_{d/2} t) \\\\\\cos(\\omega_{d/2} t)\\end{bmatrix}\\]\\[\\omega_l = \\frac{1}{10000^{2l/d}}, \\quad l = 1, \\dots, d/2\\]Thus, even and odd dimensions alternate between sine and cosine functions at exponentially decreasing frequencies.\\[P_t[2i] = \\sin \\left( \\omega_i t \\right), \\quadP_t[2i+1] = \\cos \\left( \\omega_i t \\right)\\]The encoding preserves relative distances. For any position $t$ and offset $\\tau$:\\[P_{t+\\tau} = M_\\tau P_t\\]where\\[M_\\tau = \\text{diag} \\left(\\begin{bmatrix}\\cos(\\omega_l \\tau) &amp; \\sin(\\omega_l \\tau) \\\\-\\sin(\\omega_l \\tau) &amp; \\cos(\\omega_l \\tau)\\end{bmatrix}, \\quad l = 1 \\dots d/2\\right)\\]This means the positional encoding shifts in a predictable way when moving from one position to another, allowing the model to learn shift-invariant (translation-invariant) relationships.So now, with positional encoding, we have a mechanism where the attention that we pay to a word also depends on how far away it is.Masked Self-AttentionThe idea of removing recurrence using self-attention can be extended to the decoder as well. However, in the decoder, generation is sequential. At step $t$, we only know outputs up to $t$ (previous words). So we must prevent looking ahead at future tokens.This is done by masking: when computing attention weights, we set $e_{ij} = -\\inf$ if $j&gt;i.$ This forces the softmax to ignore future words.\\[w_{ij} =\\begin{cases}\\frac{\\exp(e_{ij})}{\\sum_{m=1}^i \\exp(e_{im})}, &amp; j \\leq i \\\\0, &amp; j &gt; i\\end{cases}\\]And the output is still:\\[o_i = \\sum_{j=1}^i w_{ij} v_j\\]This is called masked self-attention, ensuring that each word prediction depends only on past outputs.Just like in the encoder, we stack multiple masked self-attention layers followed by feedforward layers (MLPs).A block looks like: Masked self-attention Feedforward MLP with nonlinearity (e.g., ReLU) Residual connections + normalizationMasked Mullti-Head Self-AttentionInstead of a single set of $Q,K,V,$ we use multiple heads, just like we did before:\\[q_i^a = W_q^a h_i, \\quad k_i^a = W_k^a h_i, \\quad v_i^a = W_v^a h_i\\]Each head computes its own weighted sum:\\[o_i^a = \\sum_{j=1}^i w_{ij}^a v_j^a\\]The outputs of all heads are concatenated:\\[o_i = [o_i^1; o_i^2; \\dots; o_i^H]\\]This allows different heads to capture different aspects of dependencies (e.g., short-range vs. long-range).We can also add positional encoding, like we did for the encoders.TransformersArchitectureComponents: Tokenization ‚Äì Convert text into discrete tokens Embedding Layer ‚Äì Map tokens into continuous vectors Self-Attention (Multi-Headed) ‚Äì Compute contextual representations Position Encoding ‚Äì Add word order information Feed-Forward Network (FFN) ‚Äì Apply non-linear transformations Residual Connections + Layer Normalization ‚Äì Stabilize and improve training Output Projection Layer ‚Äì Map to vocabulary logitsTokenization Splits text into smaller units (words, subwords, characters). Each token is represented by an index in the vocabulary.Example: The sentence ‚ÄúCMU‚Äôs 11785 is the best deep learning course‚Äù becomes token IDs [3, 5, 100, 57, ..., 1].EmbeddingsConverts discrete token IDs into dense vectors using an embedding matrix. In Pytorch, it is the function nn.Embedding, and is essentially a linear layer. If vocabulary size $ V $, embedding dimension $D$, and sequence length $L$, then: \\[X ‚àà ‚Ñù^{L √ó |V|}, \\quad W ‚àà ‚Ñù^{|V| √ó D}\\]\\[Y = XW, \\quad Y ‚àà ‚Ñù^{L √ó D}\\]$X$ is the one-hot vector, $W$ is the weight matrix, and $Y$ is the token embedding.So, each token index is mapped to a continuous embedding vector.Self-AttentionFor input $X \\in ‚Ñù^{L√óD}$:\\[Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\\]where $W_Q, W_K, W_V \\in ‚Ñù^{D√ód_k}$.Compute similarity between queries and keys:\\[e_{ij} = \\frac{Q_i \\cdot K_j^T}{\\sqrt{d_k}}\\]\\[w_{ij} = softmax(e_{ij})\\]\\[O_i = \\sum_j w_{ij} V_j\\]Multi-Head Attention Instead of one attention map, use multiple heads. Each head projects Q, K, V differently, capturing diverse relationships. Outputs are concatenated:\\[MHA(Q,K,V) = Concat(head_1, ..., head_h)W_O\\]Attention Masking Bidirectional attention (encoder): Each token attends to all tokens. Causal attention (decoder): Masks future tokens to prevent cheating during training.Cross-Attention Used in encoder-decoder models. Decoder queries come from previous outputs, keys and values come from encoder outputs.We do not need to mask in cross-attention because the encoder sequence is fully known from the start. The input sentence (source) is fixed and complete. There‚Äôs no notion of ‚Äúfuture‚Äù tokens in the source ‚Äî every encoder hidden state can be used freely. So when the decoder attends to the encoder, it‚Äôs fine to use all encoder tokens.Masking is only needed where there‚Äôs autoregressive generation (decoder self-attention), not when attending to a fully known sequence (encoder outputs).Positional EncodingSince attention is permutation-invariant, we add positional encodings to capture order.\\[p_t^{(i)} = f(t)^{(i)} :=\\begin{cases}\\sin(\\omega_k \\cdot t), &amp; \\text{if } i = 2k \\\\\\cos(\\omega_k \\cdot t), &amp; \\text{if } i = 2k+1\\end{cases}\\quad \\text{where} \\quad\\omega_k = \\frac{1}{10000^{2k/d}}\\]Feed-Forward BlockA two-layer MLP applied to each position independently.\\[FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\\]Residual Connections &amp; Layer NormalizationResidual connections, also called skip connections, allow each block to learn a refinement over its input instead of learning a transformation from scratch.\\[x_{t+1} = x_t + F(x_t)\\]where $F(x_t)$ is the function of the block (like Multi-Head Attention or Feed-Forward Network).This prevents vanishing gradients and allows information to flow directly across layers.Normalization stabilizes training by scaling inputs to have mean 0 and variance 1. In Layer Norm, the normalization is applied across the embedding dimension of each token and each token vector is normalized independently:\\[\\text{LN}(h) = \\frac{h - \\mu}{\\sigma} \\cdot \\gamma + \\beta\\]The position of LayerNorm relative to the residual connection changes training dynamics. Post-Norm (original Transformer) Apply residual connection first, then normalize:\\[x_{t+1} = \\text{LN}(x_t + F(x_t))\\] Pros: empirically achieves slightly higher performance when tuned well. Cons: harder to train, gradients may explode/vanish for deep models. Pre-Norm (modern variants, e.g., GPT-2, BERT) Normalize input before applying the block function:\\[x_{t+1} = x_t + F(\\text{LN}(x_t))\\] Pros: much more stable and easier to train, especially for very deep networks. Cons: may cap performance a bit if not tuned, but usually negligible. Improvements on TransformersTypes of ArchitecturesEncoder-Decoder: Same design as in the original Transformer paper. Input (prompt or source text) goes into the encoder. The decoder generates the output sequence step by step, attending both to previous outputs (masked self-attention) and encoder outputs (cross-attention). Used for tasks that map one text to another (translation, summarization, etc.).Encoder-Only (BERT): Uses only the encoder stack. Training objectives: Masked Language Modeling (MLM): randomly mask tokens and predict them. Next Sentence Prediction (NSP): predict if two sentences follow each other. Produces contextualized representations of tokens that can be fine-tuned for downstream tasks (classification, QA, etc.).Decoder-Only (GPT): Uses only the decoder stack with causal masking. Trained with next token prediction (standard language modeling). Naturally suited for text generation tasks.Positional EncodingAbsolute Position Encoding:\\[PE_{(t, 2i)} = \\sin\\left(\\frac{t}{10000^{2i/d}}\\right) \\PE_{(t, 2i+1)} = \\cos\\left(\\frac{t}{10000^{2i/d}}\\right)\\]Relative Position Encoding:Instead of encoding absolute index, encode relative distance between tokens.Attention score modification:\\[e_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}} + b_{i-j}\\]where $b_{i-j}$ is a learnable bias depending on relative distance.The advantage of this is that it has better generalization to longer sequences than trained on.Rotary Position Encoding (RoPE): Standard sinusoidal positional encodings (used in original Transformers) add position information to token embeddings. RoPE instead represents position by rotating embeddings in a 2D plane. This technique is used in modern LLMs such as LLaMA. Advantage: Encodes relative positions naturally and improves generalization to longer contexts.Each query and key vector is multiplied by a complex exponential to encode its position: Rotation in 2D space corresponds to multiplying by $e^{i\\theta}$ Thus, embeddings are rotated depending on their sequence position.For a query token at position $m$ and a key at position $n$:\\[f_q(x_m, m) = (W_q x_m) e^{i m \\theta}\\]\\[f_k(x_n, n) = (W_k x_n) e^{i n \\theta}\\]The dot product (attention score) between them is:\\[g(x_m, x_n, m-n) = \\text{Re}\\Big[ f_q(x_m, m) \\cdot f_k(x_n, n)^* \\Big]\\]\\[g(x_m, x_n, m-n) = \\text{Re} \\left[ (W_q x_m)(W_k x_n)^{*} e^{i (m-n)\\theta} \\right]\\]The exponential factor $e^{i(m-n)\\theta}$ encodes the relative distance between positions $m,n$. This allows the model to directly incorporate relative positional information into attention scores.Instead of using complex numbers, RoPE can be expressed as a rotation matrix applied to even‚Äìodd index pairs of the embedding vector:\\[f_{q,k}(x_m, m) = R^d_{\\Theta, m} W_{q,k} x_m\\]\\[R^d_{\\Theta, m} =\\begin{bmatrix}\\cos(m\\theta_1) &amp; -\\sin(m\\theta_1) &amp; &amp; &amp; &amp; \\\\\\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; &amp; &amp; &amp; \\\\&amp; &amp; \\cos(m\\theta_2) &amp; -\\sin(m\\theta_2) &amp; &amp; \\\\&amp; &amp; \\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; &amp; \\\\&amp; &amp; &amp; &amp; \\ddots &amp; \\\\&amp; &amp; &amp; &amp; &amp; \\cos(m\\theta_{d/2}) &amp; -\\sin(m\\theta_{d/2}) \\\\&amp; &amp; &amp; &amp; &amp; \\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\end{bmatrix}\\]Each pair of dimensions undergoes a rotation by an angle proportional to $m\\theta_j$.Efficient Attention MechanismThe vanilla self-attention mechanism has quadratic time and memory complexity with respect to sequence length $L.$\\[Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\]This requires: Time complexity: $O(L^2d)$ FLOPs Memory complexity: $O(L^2)$For long sequences (e.g., 4k‚Äì32k tokens), this quadratic cost becomes a major bottleneck.Linear Attention:Idea: Replace the softmax with a kernel function that allows factorization.Standard softmax attention:\\[\\text{Softmax}(QK^T)V = \\frac{\\exp(QK^T)}{\\sum_{i=1}^L \\exp(QK_i^T)}V\\]Instead, approximate similarity using a kernel function $\\phi(.):$\\[\\text{sim}(Q,K) = \\phi(Q)\\phi(K)^T\\]Thus attention becomes linearized:\\[\\frac{\\phi(Q)\\phi(K)^T}{\\sum_{i=1}^L \\phi(Q)\\phi(K_i)^T}V = \\frac{\\phi(Q) (\\phi(K)^TV)}{\\phi(Q)\\sum_{i=1}^L\\phi(K_i)^T}\\]Complexity drops from $O(L^2)$ to $O(Ld‚Äô^2)$.Flash Attention:Standard attention stores the huge $L \\times L$ matrix in memory. Flash Attention avoids materializing it. Instead it computes attention block by block, directly in GPU high-bandwidth memory (HBM).The trick is to: Fuse operations into a single GPU kernel (matrix multiplication + softmax + scaling). Use tiling: split matrices into small blocks that fit into SRAM. Reduces memory reads/writes by orders of magnitude.Issues with softmax: Naive Softmax requires 2 loops: Compute normalizer $\\sum_je^{x_j}$ Divide each exponential Safe Softmax adds a max-subtraction step for numerical stability. Online Softmax combines steps to reduce passes.Flash Attention fuses this into one loop:This keeps the normalization stable without ever storing the full matrix.IO complexity: Standard attention: $\\Omega(Nd+N^2)$ Flash attention: $O(\\frac{N^2d^2}{M})$, where $M$ is SRAM sizeThis results in e-3x speedup and ~10x less memory usage.KV Caching:When decoding autoregressively (LLMs), recomputing all past $K,V$ at each step is wasteful.So the trick is to: Cache previously computed Key and Value vectors. For the next token, only compute new $K_t,V_t$ and append to cache.This avoids recomputation, reducing complexity per token from $O(L^2)$ to $O(L)$.Multi and Grouped Query Attention:Standard Multi-head Attention (MHA): In normal multi-head attention, we have H independent sets of queries, keys, and values. This means memory + compute cost grows with H.Multi-query Attention (MQA): Shares the same keys and values across all query heads. Only queries are separate per head. This reduces memory cost, since we only store one set of K and V, instead of H.Grouped-query Attention (GQA): A middle ground between MHA and MQA. Queries are divided into groups, where each group shares a single key and value set. Reduces KV memory footprint more than MHA but not as aggressively as MQA. Better accuracy than MQA since it doesn‚Äôt collapse all queries into one KV set.Multi-Head Latent Attention:MLA improves on MQA/GQA by: Low-rank compression of K and V: Instead of directly caching K and V, MLA projects them into a latent space. Stores compressed latent representations (much smaller). During inference, only compressed KV pairs are cached, reducing memory usage while maintaining accuracy.Parameter Efficient TuningTraditional fine-tuning involves updating all parameters of a large pre-trained transformer for each downstream task. However, this is expensive in memory and compute.Parameter Efficient Tuning methods update only a small proportion of parameters, while keeping most of the pre-trained model frozen.Prefix Tuning: Idea: Learn a small set of continuous prefix vectors (virtual tokens) that are prepended to the input at each layer. The transformer parameters remain frozen; only these prefix vectors are trained. Works well for sequence generation tasks (translation, summarization, table-to-text).Prompt Tuning: Similar to prefix tuning but even lighter: Learns a small set of soft prompt embeddings (tokens) specific to each task. Instead of modifying all layers, only the input embedding space is modified. Allows a single pre-trained model to adapt to many tasks by swapping task-specific prompts.Adapter: Introduce small MLP ‚Äúadapter‚Äù layers inside each transformer layer (usually after feedforward or attention). During training, pre-trained weights stay frozen, and only adapter parameters are updated. Advantage: Easy to plug in; allows good transfer across tasks.LoRA (Low-Rank Adaptation): Replace large weight update matrices with a low-rank decomposition. Instead of updating $W,$ represent update as:\\[h = W_0 x + \\Delta W x = W_0 x + B A x\\] where: $W_0:$ frozen pre-trained weight $A,B:$ small trainable low-rank matrices Much fewer trainable parameters" }, { "title": "Visual Learning and Recognition", "url": "/posts/visual-learning-recognition/", "categories": "CMU MRSD, Robotics", "tags": "learning, nnets", "date": "2025-06-09 12:00:00 -0400", "snippet": "Language-Conditioned BEV Perception for Autonomous Driving Your browser does not support PDFs. Please download the report here." }, { "title": "Planning and Decision Making", "url": "/posts/planning-decision-making/", "categories": "CMU MRSD, Robotics", "tags": "planning, manipulators, cbs, ecbs, xecbs, multi-robot", "date": "2025-06-09 12:00:00 -0400", "snippet": "In progressBlogThese are my notes as part of the 16-782 Planning and Decision-making in Robotics class taken by Dr. Maxim Likhachev. Basics and Sampling-Based Motion Planning All About Search Algorithms Case Studies of Planning and Decision-Making in RoboticsProject: Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences Your browser does not support PDFs. Please download the report here." }, { "title": "Deep Learning", "url": "/posts/deep-learning/", "categories": "CMU MRSD, Robotics", "tags": "learning, nnets", "date": "2025-06-09 12:00:00 -0400", "snippet": "Deep LearningThis series of blogs are my notes from the class 11-785 Introduction to Deep Learning, taught by Bhiksha Raj at CMU. For my own sake of understanding and simplicity, the blog has been divided into 3 broad categories: Perceptrons CNNs, RNNs, and Language Models Attention and Transformers - Talks about why attention is all we need" }, { "title": "Deep Learning - CNNs, RNNs, & Language Models", "url": "/posts/deep-learning-cnn-rnn-lang/", "categories": "Blog, Robotics", "tags": "learning, nnets, cnn, rnn, sq2sq, attention", "date": "2025-06-09 12:00:00 -0400", "snippet": "In ProgressTo-Do CNNs ‚Äî all LSTM, GRU Connectionist Temporal ClassificationConvolutional Neural Networkshttps://youtu.be/kYeeB3CNcx8?si=otHbiKRITjJZk9S8Forward Pass# Input:# Y(0, :, :, :) = Image # The input image (or feature map) is stored at layer 0# # Dimensions: (channels, width, height)# Iterate over each convolutional layerfor l = 1:L # L = total number of layers # Iterate over the spatial positions (x, y) of the output feature map for x = 1:(W_{l-1} - K_l + 1) # Slide window horizontally for y = 1:(H_{l-1} - K_l + 1) # Slide window vertically # Iterate over the output channels (filters) for j = 1:D_l z(l, j, x, y) = 0 # Initialize pre-activation (accumulator) # Accumulate contributions from all input channels for i = 1:D_{l-1} # D_{l-1} = # input channels to layer l for x' = 1:K_l # K_l = kernel width for y' = 1:K_l # K_l = kernel height # Perform convolution at this location: # weight * input value z(l, j, x, y) += w(l, j, i, x', y') * Y(l-1, i, x + x' - 1, y + y' - 1) # Note: (x + x' - 1) and (y + y' - 1) are for correct indexing # Apply activation function (e.g. ReLU) after convolution Y(l, j, x, y) = activation(z(l, j, x, y))# Final prediction Y is obtained by flattening the last layer's output and applying softmaxY = softmax(Y(L, :, 1, 1) .. Y(L, :, W - K + 1, H - K + 1))BackpropagationI don‚Äôt think I can do a better job in explaining CNN backpropagation than this video. Just watch it. But here‚Äôs the pseudo code for quick reference.# Start with gradient of the loss w.r.t. the final layer outputdY(L) = dDiv / dY(L) # From loss function # Loop backward through layersfor l = L : downto : 1 # Backprop layer-by-layer dw(l) = zeros(D_l √ó D_{l-1} √ó K_l √ó K_l) # Initialize weight gradient dY(l - 1) = zeros(D_{l-1} √ó W_{l-1} √ó H_{l-1}) # Initialize input gradient # Loop over spatial positions in the output feature map for x = W_{l-1} - K_l + 1 : downto : 1 for y = H_{l-1} - K_l + 1 : downto : 1 for j = D_l : downto : 1 # For each output channel # Backprop through activation dz(l, j, x, y) = dY(l, j, x, y) * f‚Ä≤( z(l, j, x, y) ) # Backpropagate error and compute weight gradients for i = D_{l-1} : downto : 1 # Input channels for x‚Ä≤ = K_l : downto : 1 # Kernel width for y‚Ä≤ = K_l : downto : 1 # Kernel height # Chain rule: propagate error to input dY(l - 1, i, x + x‚Ä≤ - 1, y + y‚Ä≤ - 1) += \\ w(l, j, i, x‚Ä≤, y‚Ä≤) * dz(l, j, x, y) # Compute gradient of weight dw(l, j, i, x‚Ä≤, y‚Ä≤) += \\ dz(l, j, x, y) * Y(l - 1, i, x + x‚Ä≤ - 1, y + y‚Ä≤ - 1)Recurrent Neural NetworksState-Space ModelState-space model is a model for infinite response systems.\\[h_t = f(x_t,h_{t-1}) \\\\\\]\\[y_t = g(h_t)\\]There are all different types of RNNs, having varying number of layers.Considering an RNN with just one hidden layer,\\[h_i^{(1)}(t) = f_1 \\left( \\sum_j w_{ji}^{(1)} x_j(t) + \\sum_j w_{ji}^{(11)} h_j^{(1)}(t-1) + b_i^{(1)} \\right) \\\\\\]\\[Y_k(t) = f_2 \\left( \\sum_j w_{jk}^{(2)} h_j^{(1)}(t) + b_k^{(2)} \\right), \\quad k = 1..M\\]In the vector form:\\[\\textbf{h}^{(1)}(t) = f_1(\\textbf{W}^{(1)}\\textbf{X}(t)+\\textbf{W}^{(11)}\\textbf{h}^{(1)}(t-1)+\\textbf{b}^{(1)}) \\\\\\]\\[\\textbf{Y}(t) = f_2(\\textbf{W}^{(2)}\\textbf{h}^{(1)}(t)+\\textbf{b}^{(2)})\\]$f_1()$ is usually the $\\tanh$ activation function. The hidden state captures both new input and temporal memory via the recurrent connection. Output depends only on the current hidden state, but that hidden state has a history encoded in it through $h(t-1), h(t-2),$ etc. The initial values of hidden states are generally learnable paramteres as well.Variants on RNNs One to one: Conventional MLP One to many: Sequence generation, e.g. image to caption Many to one: Sequence based prediction or classification, e.g. Speech recognition,text classification Many to many: Delayed sequence to sequence, e.g. machine translation Many to many: Sequence to sequence, e.g. stock problem, label predictionTrainingForward Passfor t = 0:T-1:\t# Initialize input vectors\th(t,0) = x(t)\tfor l = 1:L:\t\tz(t,l) = Wc(l) * h(t,l-1) + Wr(l) * h(t-1,l) + b(l) # c for current, r for recurrent\t\th(t,l) = tanh(z(t,l))\tzo(t,L) = Wo(L) * h(t,L) + bo\tY(t) = softmax(zo(t,L)) Assuming $h(-1,^*)$ is known Assuming $L$ hidden-state layers and an output layer $W_c, W_r$ are matrices, $b$ are vectors $W_c$ are weights for inputs from current time $W_r$ are recurrent weights applied from previous time $W_o$ are output layer weightsBack Propagation Through Time (BPTT)The divergence computed is between the sequence of outputs by the network and the desired sequence of outputs. So $DIV$ is a scalar function of a series of vectors.The $DIV$ is not necessarily the sum of divergences at individual times, unless we explicitly define it to be that way.Local Gradients from Output:We want:\\[\\frac{\\partial DIV}{\\partial Y_i(t)} \\space \\space \\text{for all i,t}\\]If $DIV$ is the sum of all divergences, then\\[\\frac{\\partial DIV}{\\partial Y_i(t)} = \\frac{\\partial Div(t)}{\\partial Y_i(t)}\\]In any case, let‚Äôs assume we know how to compute this gradient. We will tackle what exact divergence helps us do this later.Backprop for the last time sequence: Final layer\\[\\nabla_{Z^{(2)}(T)} DIV = \\nabla_{Y(T)} DIV \\cdot \\nabla_{Z^{(2)}(T)} Y(T)\\] For element-wise output activation:\\[\\frac{dDIV}{dZ_i^{(2)}(T)} = \\frac{dDIV}{dY_i(T)} \\cdot \\frac{dY_i(T)}{dZ_i^{(2)}(T)}\\] For vector output activation:\\[\\frac{dDIV}{dZ_i^{(2)}(T)} = \\sum_j \\frac{dDIV}{dY_j(T)} \\cdot \\frac{dY_j(T)}{dZ_i^{(2)}(T)}\\] For final layer weights\\[\\nabla_{W^{(2)}} DIV = h(T) \\nabla_{Z^{(2)}(T)} DIV\\]\\[\\frac{dDIV}{dw_{ij}^{(2)}} = \\frac{dDIV}{dZ_j^{(2)}(T)} \\cdot h_i(T)\\] To hidden state\\[\\nabla_{h(T)} DIV = \\nabla_{Z^{(2)}(T)} DIV \\cdot W^{(2)}\\]\\[\\frac{dDIV}{d h_i(T)} = \\sum_j \\frac{dDIV}{dZ_j^{(2)}(T)} \\cdot \\frac{dZ_j^{(2)}(T)}{d h_i(T)} = \\sum_j w_{ij}^{(2)} \\cdot \\frac{dDIV}{dZ_j^{(2)}(T)}\\] \\[\\nabla_{Z^{(1)}(T)} DIV = \\nabla_{h(T)} DIV \\cdot \\nabla_{Z^{(1)}(T)} h(T)\\]\\[\\frac{dDIV}{dZ_i^{(1)}(T)} = \\frac{dDIV}{dh_i(T)} \\cdot \\frac{dh_i(T)}{dZ_i^{(1)}(T)}\\] Weights to hidden state\\[\\nabla_{W^{(1)}} DIV = X(T) \\cdot \\nabla_{Z^{(1)}(T)} DIV\\]\\[\\frac{dDIV}{dw_{ij}^{(1)}} = \\frac{dDIV}{dZ_j^{(1)}(T)} \\cdot X_i(T)\\]\\[\\nabla_{W^{(11)}} DIV = h(T - 1) \\cdot \\nabla_{Z^{(1)}(T)} DIV\\]\\[\\frac{dDIV}{dw_{ij}^{(11)}} = \\frac{dDIV}{dZ_j^{(1)}(T)} \\cdot h_i(T - 1)\\] Progressing backward in time: Final layer\\[\\nabla_{Z^{(2)}(T-1)} DIV = \\nabla_{Y(T-1)} DIV \\cdot \\nabla_{Z^{(2)}(T-1)} Y(T-1)\\]\\[\\frac{dDIV}{dZ_i^{(2)}(T-1)} = \\frac{dDIV}{dY_i(T-1)} \\cdot \\frac{dY_i(T-1)}{dZ_i^{(2)}(T-1)}\\]\\[\\frac{dDIV}{dZ_i^{(2)}(T-1)} = \\sum_j \\frac{dDIV}{dY_j(T-1)} \\cdot \\frac{dY_j(T-1)}{dZ_i^{(2)}(T-1)}\\] For final layer weights\\[\\frac{dDIV}{dw_{ij}^{(2)}} += \\frac{dDIV}{dZ_j^{(2)}(T-1)} \\cdot h_i(T-1)\\]\\[\\nabla_{W^{(2)}} DIV += h(T-1) \\cdot \\nabla_{Z^{(2)}(T-1)} DIV\\] Since the RNN uses the same weights at each time step (weight sharing), the total gradient of the loss with respect to $\\textbf{W}^{(2)}$ is the sum of the gradients over all time steps. Hence we have the increment To hidden state\\[\\nabla_{h(T-1)} DIV = \\nabla_{Z^{(2)}(T-1)} DIV \\cdot W^{(2)} + \\nabla_{Z^{(1)}(T)} DIV \\cdot W^{(11)}\\]\\[\\frac{dDIV}{d h_i(T-1)} = \\sum_j w_{ij}^{(2)} \\frac{dDIV}{d Z_j^{(2)}(T-1)} + \\sum_j w_{ij}^{(11)} \\frac{dDIV}{d Z_j^{(1)}(T)}\\]\\[\\nabla_{Z^{(1)}(T-1)} DIV = \\nabla_{h(T-1)} DIV \\cdot \\nabla_{Z^{(1)}(T-1)} h(T-1)\\]\\[\\frac{dDIV}{d Z_i^{(1)}(T-1)} = \\frac{dDIV}{d h_i(T-1)} \\cdot \\frac{d h_i(T-1)}{d Z_i^{(1)}(T-1)}\\] Weights to hidden state\\[\\nabla_{W^{(1)}} DIV \\mathrel{+}= X(T-1) \\cdot \\nabla_{Z^{(1)}(T-1)} DIV\\]\\[\\frac{dDIV}{dw_{ij}^{(1)}} \\mathrel{+}= \\frac{dDIV}{dZ_j^{(1)}(T-1)} \\cdot X_i(T-1)\\]\\[\\nabla_{W^{(11)}} DIV \\mathrel{+}= h(T-2) \\cdot \\nabla_{Z^{(1)}(T-1)} DIV\\]\\[\\frac{dDIV}{dw_{ij}^{(11)}} \\mathrel{+}= \\frac{dDIV}{dZ_j^{(1)}(T-1)} \\cdot h_i(T-2)\\] We continue this until\\[\\nabla_{h_{-1}} DIV = \\nabla_{Z^{(1)}(0)} DIV \\cdot W^{(11)}\\]\\[\\frac{dDIV}{dh_i(-1)} = \\sum_j w_{ij}^{(11)} \\cdot \\frac{dDIV}{dZ_j^{(1)}(0)}\\]# Assuming forward pass has been completed# Jacobian(x, y) is the Jacobian of x w.r.t. y# Assuming dY(t) = ‚àá_{Y(t)}DIV is available for all t# Assuming all dZ, dH, dW and db are initialized to 0for t = T-1 : downto : 0 # ‚àá_{Z^{(2)}(t)}DIV = ‚àá_{Y(t)}DIV √ó ‚àá_{Z^{(2)}(t)}Y(t) dZ_o(t) = dY(t) √ó Jacobian(Y(t), Z_o(t)) # ‚àá_{W^{(2)}}DIV += h(t, L)^T √ó ‚àá_{Z^{(2)}(t)}DIV dW_o += h(t, L)·µÄ √ó dZ_o(t) # db_o += ‚àá_{Z^{(2)}(t)}DIV db_o += dZ_o(t) # ‚àá_{h(t, L)}DIV += ‚àá_{Z^{(2)}(t)}DIV √ó W^{(2)} dH(t, L) += dZ_o(t) √ó W_o·µÄ # --- Reverse through layers --- for l = L : downto : 1 # ‚àá_{Z^{(1)}(t,l)}DIV = ‚àá_{h(t,l)}DIV √ó ‚àá_{Z^{(1)}(t,l)}h(t,l) dZ(t, l) = dH(t, l) √ó Jacobian(h(t, l), Z(t, l)) # ‚àá_{h(t,l-1)}DIV += ‚àá_{Z^{(1)}(t,l)}DIV √ó W_c(l) dH(t, l-1) += dZ(t, l) √ó W_c(l) # ‚àá_{h(t-1,l)}DIV += ‚àá_{Z^{(1)}(t,l)}DIV √ó W_r(l) dH(t-1, l) += dZ(t, l) √ó W_r(l) # ‚àá_{W_c(l)}DIV += h(t,l-1)^T √ó ‚àá_{Z^{(1)}(t,l)}DIV dW_c(l) += h(t, l-1)·µÄ √ó dZ(t, l) # ‚àá_{W_r(l)}DIV += h(t-1,l)^T √ó ‚àá_{Z^{(1)}(t,l)}DIV dW_r(l) += h(t-1, l)·µÄ √ó dZ(t, l) # ‚àá_{b(l)}DIV += ‚àá_{Z^{(1)}(t,l)}DIV db(l) += dZ(t, l)Bidirectional RNNInference# Subscript \"f\" ‚Üí forward RNN, subscript \"b\" ‚Üí backward RNN# x(t) is input sequence at time t (could be output from a lower layer)# Assume h_f(-1,*) and h_b(‚àû,*) are known initializations# --- Forward recurrence (left to right in time) ---for t = 0 : T-1 # Looping forward through time h_f(t, 0) = x(t) # Input to first layer (layer 0) for l = 1 : L_f # L_f is depth of forward RNN # z_f(t,l) = W_fc √ó h_f(t, l-1) + W_fr √ó h_f(t-1, l) + b_f z_f(t, l) = W_fc(l) √ó h_f(t, l-1) + W_fr(l) √ó h_f(t-1, l) + b_f(l) # h_f(t,l) = tanh(z_f(t,l)) # Assuming tanh activation h_f(t, l) = tanh(z_f(t, l))# --- Backward recurrence (right to left in time) ---h_b(T, :, :) = h_b(‚àû, :, :) # Initial hidden state for backward passfor t = T-1 : downto : 0 # Looping backward through time h_b(t, 0) = x(t) # Input to first layer (layer 0) for l = 1 : L_b # L_b is depth of backward RNN # z_b(t,l) = W_bc √ó h_b(t, l-1) + W_br √ó h_b(t+1, l) + b_b z_b(t, l) = W_bc(l) √ó h_b(t, l-1) + W_br(l) √ó h_b(t+1, l) + b_b(l) # h_b(t,l) = tanh(z_b(t,l)) h_b(t, l) = tanh(z_b(t, l))# --- Output from bidirectional layer ---for t = 0 : T-1 # Concatenate forward and backward hidden states at final layer h(t) = [h_f(t, L_f); h_b(t, L_b)]Forward Pass# Inputs:# L : Number of hidden layers# Wc : Current weights of each layer (input-to-hidden)# Wr : Recurrent weights of each layer (hidden-to-hidden)# b : Biases for each layer# hinit : Initial hidden state (e.g., h(-1,*))# x : Input vector sequence [T x input_dim]# T : Length of the input sequence## Output:# h : Hidden activations after tanh# z : Pre-activations (linear sums before tanh)function RNN_forward(L, Wc, Wr, b, hinit, x, T) h(-1,:) = hinit # h(-1) = hinit for all layers for t = 0:T-1 # Forward through time h(t,0) = x(t) # Input goes into \"layer 0\" for l = 1:L # Forward through layers # Linear sum: z·∂´‚Çú = Wc(l) * h(t, l-1) + Wr(l) * h(t-1, l) + b(l) z(t,l) = Wc(l) * h(t,l-1) + Wr(l) * h(t-1,l) + b(l) # Non-linearity: h·∂´‚Çú = tanh(z·∂´‚Çú) h(t,l) = tanh(z(t,l)) end end return h# Subscript \"f\" = forward net# Subscript \"b\" = backward net# Assuming:# h_f(-1, :) is known as initial state# h_b(‚àû, :) is known as terminal state# Forward pass: go left-to-righth_f = RNN_forward(Lf, Wfc, Wfr, b_f, h_f(-1,:), x, T)# Backward pass: go right-to-leftx_rev = fliplr(x) # Flip input in timeh_brev = RNN_forward(Lb, Wbc, Wbr, b_b, h_b(inf,:), x_rev, T)h_b = fliplr(h_brev) # Flip hidden sequence to match forward time# Combine both forward and backward outputs for each time step:for t = 0:T-1 # Final output h(t) is a concat of last layer forward + backward h(t) = [h_f(t, Lf); h_b(t, Lb)] # You can also use another output layer like: y(t) = W_o * h(t)endBack Propagation Through Time# Inputs:# L : Number of hidden layers# W_c, W_r, b : current (feedforward) weights, recurrent weights, and biases# hinit : Initial hidden state (corresponds to h(-1,*))# x : Input sequence (T time steps)# T : Length of input sequence# dh_top : Gradient from the layer above (dDIV/dh(t,L) for all t)# h, z : Stored values from the forward pass# Output:# dx : Gradient w.r.t input x(t)# dW_c : Gradient w.r.t current weights# dW_r : Gradient w.r.t recurrent weights# db : Gradient w.r.t bias# dh(-1) : Gradient w.r.t initial hidden statefunction RNN_bptt(L, W_c, W_r, b, hinit, x, T, dh_top, h, z) dh = zeros # initialize full dh(t,l) for t = T-1:down to:0 # Backward through time dh(t, L) += dh_top(t) # Add top gradient to dh(t, L) h(t, 0) = x(t) # Initialize input to first layer for l = L:1 # Reverse through layers dz(t,l) = dh(t,l) * Jacobian(h(t,l), z(t,l)) # ‚àáh ‚Üí ‚àáz using chain rule dh(t, l-1) += dz(t,l) * W_c(l) # Backprop through current weights dh(t-1, l) = dz(t,l) * W_r(l) # Backprop through recurrent weights dW_c(l) += h(t, l-1) * dz(t,l) # Accumulate ‚àÇL/‚àÇW_c dW_r(l) += h(t-1, l) * dz(t,l) # Accumulate ‚àÇL/‚àÇW_r db(l) += dz(t,l) # Accumulate ‚àÇL/‚àÇb dx(t) = dh(t,0) # Gradient w.r.t. input x(t) return dx, dW_c, dW_r, db, dh(-1) # dh(-1) = dh(-1,1:L,:) ‚Üí for use by previous layer# Assumptions:# dh(t) : Gradient from the upper layer, shape (T, Lf+Lb)# x(t) : Input to the BRNN block# z_f, h_f : Forward net activations (t=0...T-1)# z_b, h_b : Backward net activations (t=0...T-1)# Lf, Lb : Number of forward and backward layersfor t = 0:T-1 dh_f(t) = dh(t, 1:Lf) # Split forward part dh_b(t) = dh(t, Lf+1:Lf+Lb) # Split backward part# === Forward Net Backprop ===[dx_f, dW_fc, dW_fr, db_f, dh_f(-1)] = RNN_bptt(L, W_fc, W_fr, b_f, h_f(-1,:), x, T, dh_f, h_f, z_f)# === Backward Net Backprop ===x_rev = fliplr(x) # Reverse input time orderdh_brev = fliplr(dh_b) # Reverse dh_bh_brev = fliplr(h_b) # Reverse activationsz_brev = fliplr(z_b) # Reverse preactivations[dx_brev, dW_bc, dW_br, db_b, dh_b(inf)] = RNN_bptt(L, W_bc, W_br, b_b, h_b(inf,:), x_rev, T, dh_brev, h_brev, z_brev)dx_b = fliplr(dx_brev) # Flip back the dx# === Combine the gradients from both passes ===for t = 0:T-1 dx(t) = dx_f(t) + dx_b(t) # Total gradient w.r.t input x(t)Behavior of RecurrenceBIBO StabilityTime-delay structures have bounded input if: The activation function $f()$ has a bounded output for a bounded input, and this is generally the case for every activaton function that we use The input $X(t)$ is boundedThis is called the ‚ÄúBounded Input Bounded Output‚Äù stability, and is highly desirable.Is a Recurrent Network BIBO?If a non-linear activation is used, then the output obtained from it is always bounded and hence behaves as a BIBO.But what if we use a linear activation function? To understand this, we can analyze only the behavior of the recurrent hidden layer $h_t$. This is called the Streetlight effect. Consider the following example of an identity activation:\\[z_t = W_hh_{t-1} + W_xx_t \\\\ h_t = z_t\\]Now,\\[h_t = W_hh_{t-1} + W_xx_t \\\\ h_{t-1} = W_hh_{t-2} + W_xx_{t-1}\\]This gives,\\[h_t = W_h^2h_{t-2} + W_hW_xx_{t-1} + W_xx_t\\]Repeating this for the entire time series, we get,\\[h_t = W_h^{t+1}h_{-1} + W_h^tW_xx_0 + W_h^{t-1}W_xx_1 + W_h^{t-2}W_xx_2 + \\ldots + W_xx_t\\]This becomes,\\[h_t = H_t(h_{-1}) + H_t(x_0) + H_t(x_1) + ... H_t(x_t)\\]If $H_t(x_0)$ blows up, the entire system blows up. If this does not blow up, then the responses to later inputs will also not blow up.So $h_k$ is a sum of weighted transformations of all past inputs and the initial state.Suppose we have nothing else except the input $x(0)$.\\[h_0(t) = W^t C x(0)\\]So the response at time $t$ is governed by the matrix power $W^t$. Assume $W$ is diagonalisable. Then the power of $W$ is\\[W^t = U \\Lambda^t U^{-1}\\]with\\[\\Lambda^t = \\begin{bmatrix}\\lambda_1^t &amp; 0 &amp; 0 \\\\0 &amp; \\lambda_2^t &amp; 0 \\\\0 &amp; 0 &amp; \\ddots\\end{bmatrix}\\]Lets say $\\lambda_1$ is the largest eigenvalue of $W.$ Then by bringing it outside the matrix, we have\\[\\Lambda^t = \\lambda_1^t\\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; (\\frac{\\lambda_2}{\\lambda_1})^t &amp; 0 \\\\0 &amp; 0 &amp; \\ddots\\end{bmatrix}\\]So this essentially becomes dominated by the largest eigenvalue $\\lambda_1$.\\[h_0(t) = W^t C x(0) = U \\Lambda^t U^{-1} C x(0)\\]The growth or decay of $h(t)$ as $t \\rightarrow \\inf$ is controlled by the largest eigenvalue of $W$: If $|\\lambda_1| &gt; 1, h(t)$ grows exponentially ‚Äî system blows up If $|\\lambda_1| &lt; 1, h(t)$ shrinks to zero ‚Äî system vanishes If $|\\lambda_1| = 1, h(t)$ is marginally stable Complex eigenvalues will cause oscillatory reponse but with the same overall trendsWith a non-linear activation function: Sigmoid\\[h(t) = sigmoid(wh(t-1) +cx(t)+b)\\] Final value depends only on $b, w$ and not $x$ Tanh\\[h(t) = tanh(wh(t-1) +cx(t)+b)\\] Final value depends only on $b, w$ and not $x$ ReLU\\[h(t) = relu(wh(t-1) +cx(t)+b)\\] Relu blows up if $w&gt;1$ for $x&gt;0$, and dies for $x&lt;0$. So it is pretty unstable and useless For a vector processing:So Tanh activations are slightly more effective at storing memory, but they too not for very long (the inputs are eventually forgotten in an exponential manner).Exploding &amp; Vanishing GradientsWe know about the vanishing gradients problem in MLPs ‚Äî that for most common activation functions which have derivatives always less than 1, multiplication by the Jacobian is always a shrinking operation. Because of this, the derivative of divergence after a few layers at any time is totally forgotten.Effect of weights:Each weight matrix $W_n$ performs a linear transformation of the gradient. Specifically: It scales the gradient in different directions. The amount of scaling is determined by the singular values (or eigenvalues for symmetric matrices) of the matrix.Let‚Äôs say the singular values of a matrix $W_n$ are $\\sigma_1, \\sigma_2, \\ldots, \\sigma_m$ If all $\\sigma_i&lt;1 \\rightarrow$ gradient will shrink in all directions If any $\\sigma_i &gt; 1 \\rightarrow$ gradient may grow in that directionIf many weight matrices have singular values greater than 1, then the gradients will explode.If many weight matrices have singular values lesser than 1, then the gradients will vanish.Important points: The derivatives for most parameters will become vanishingly small as we backpropagate the loss gradient through deep networks The derivatives for a small number of parameters will blow up and become large and unstable as we propagate the los gradient through deep networks The derivatives would be more stable if the recurrent weight matrices had singular values equal to 1 The derivatives would be more stable if the recurrent activations were identity transforms (with identity Jacobian matrices) The memory would be more stable if the recurrent weight matrix were an identity matrix (i.e. a diagonal matrix with diagonal values equal to ‚Äú1‚Äù) The memory would be more stable if the recurrent activations were identity transforms (which are linear and do not scale up or shrink the output)LSTMWe want the RNN to remember for extended periods of time and recall when necessary by tackling the expanding and vanishing gradients problem.We do this by getting rid of the Jacobian and weight matrices, but we retain memories until a switch based on the input flags them as okay to forget, and recall them on demand.This is called the constant error carousel ‚Äî they have something in memory, and at each time $t$, what is in memory is modified based on what is encountered in the input.The Long Short-Term Memory neuron allows us to explicitly latch information to prevent decay and blowing up of gradients.GRUTime-Synchronous RecurrenceThey have one output corresponding to every input. They can be bidirectional as well.Assumption: Sequence divergence is the sum of the divergence at individual instants\\[Div(Y_{target}(1 ... T), Y(1 ... T)) = \\Sigma_t Div(Y_{target}(t), Y(t))\\]\\[‚àá_{Y(t)} Div(Y_{target}(1 ... T), Y(1 ... T)) = ‚àá_{Y(t)} Div(Y_{target}(t), Y(t))\\]Typical divergence for classification:\\[Div(Y_{target}(t), Y(t)) = KL(Y_{target}(t), Y(t))\\]VariantsThese are all sequence to sequence models, and we will soon transition into a separate heading to tackle these problems, specifically on training them.Many to One:Consider a many to one RNN. In such networks: Even though we only read the final output at the end of the sequence, every hidden state at each time step does generate an output internally. These outputs are typically ignored during inference, unless you‚Äôre doing something like attention or intermediate supervision. During training, the divergence (loss) is only computed at the final step.We define the divergence as the KL divergence between the target phoneme and the predicted output at the final time step:\\[DIV(Y_{\\text{target}}, Y) = KL(Y(T), \\text{Phoneme})\\]This approach assumes that the only useful information is at the final step. But intermediate states may have rich information too! So we‚Äôre wasting learning potential by ignoring outputs from intermediate inputs.So the solution to this is to use all outputs generated during the forward pass ‚Äî not just the final one ‚Äî to compute divergence. That is, assume that each hidden state output should try to predict the same target phoneme. So instead of just computing divergence at $t=T$, we compute it for all $t \\in {0,1,2}:$\\[DIV(Y_{\\text{target}}, Y) = \\sum_t w_t \\cdot KL(Y(t), \\text{Phoneme})\\]$w_t$ is a weighting factor for each time step. You can keep them equal, or weigh later steps more if needed.Many to One:Objective: Given a sequence of inputs, asynchronously output a sequence of symbols.This is just a simple concatenation of many copies of the simple ‚Äúoutput at the end of the input sequence‚Äù model discussed above.But during inference, the network is producing an output at every time and we need to figure out when to read the outputs because we are not given this information. This process of obtaining an output from the network is called decoding.Solution to decoding:Option 1:At each time, the network outputs a probability for each output class given all inputs until that time.\\[y_k^D = prob(s_4=D|X_0\\ldots X_k)\\]Just like we do in any neural network with a softmax or logistic output, we select the class with the highest probability results. Using the same principle here, we find the most likely symbol sequence at each time given the inputs. Then we merge adjacent repeated symbols, and place the actual emission of the symbol in the final instant.# Pseudo-code# Assuming y(t,i), t = 1...T, i = 1...N is already computed using the underlying RNN# N is the number of classes in outputn = 1best(1) = argmax_i y(1, i)for t = 1 to T: best(t) = argmax_i y(t, i) if best(t) != best(t-1): out(n) = best(t-1) # stores the previous class (the segment that just ended) time(n) = t - 1 # stores the time it ended n = n + 1But the problem with this is that we cannot distuinguish between an extended symbol and repetitions of symbols.Sequence-to-Sequence ModelsTrainingDuring training, if we have the input sequences and also the output sequence, i.e., at what exact time step we have a label, then training becomes easy. We can then easily convert it into a time-synchronous problem and train the model.But the problem arises when no timing information of the output is provided. Only the sequence of output symbols is provided for the training data, but there is no indication of which one occurs where. This becomes a problem of ‚Äútraining without alignment‚Äù, and can be solved in two ways: Guess the alignment ‚Äî Viterbi algorithm Consider all possible alignments ‚Äî Connectionist Temporal ClassificationFormally, we want to solve:Given an unaligned $K$ length compressed symbol sequence $S = s_0, \\dots, s_{K-1}$, and an $N$ length input $(N\\geq K)$, we want to find the most likely alignment:\\[\\argmax_{s_0, \\dots, s_{N-1}} \\Pr(s_0, \\dots, s_{N-1} \\mid S, X)\\quad \\text{such that} \\quad \\text{compress}(s_0, \\dots, s_{N-1}) = S\\]The expression inside the $\\text{argmax}$ represents the model‚Äôs belief about the alignment. The constraint ensures that the sequence, once compressed, matches the given label sequence.Important points: Alignment tells us which portion of the input aligns with what symbol in the sequence An order-synchronous symbol sequence that is shorter than the input can be ‚Äúaligned‚Äù to the input by repeating symbols until the expanded sequence is exactly as long as the input The ‚Äúalignment‚Äù of an order-synchronous symbol sequence to an input is a time synchronous symbol sequence A symbol sequence that is time-synchronous with an input can be compressed to a shorter order-synchronous input by eliminating repetitions of symbols Order-synchronous symbol sequences that are shorter than the input are compressed symbol sequencesTraining: Guessing the Alignment using Viterbi AlgorithmWhen the alignment is not provided, one approach is to guess it and refine it iteratively. Procedure: Initialize: Assign an initial alignment (can be random, heuristic-based, uniform, etc.) Train: Train the network using the current alignment Re-estimate: Update the alignment based on the model‚Äôs improved predictions Repeat steps 2-3 until convergenceWhen a model outputs a distribution over symbols at each time step, greedy decoding (i.e., taking the highest probability symbol at each step) can produce invalid outputs.To tackle this, we prune the model‚Äôs output grid and retain only the rows corresponding to symbols that occur in the target sequence. This gives us a reduced output table, eliminating unwanted paths. This assures that we only hypothesize symbols that are valid, but it still doesn‚Äôt enforce their correct order.To ensure that the output expands the target sequence exactly, we: Create one row per symbol in the output sequence ‚Äî If a symbol repeats, create multiple rows Arrange the rows in the desired order Build a table such that at each time step, you have probabilities for that symbol This ensures the alignment can only traverse through the correct symbol order. Once the output table is arranged, we model it as a graph: Rows represent the positions of target symbols Columns represent time steps $t=0,1,\\ldots,T-1$ Nodes contain the network‚Äôs output scores for the corresponding symbol at time $t$:\\[y_t^{s_i} = \\text{Pr}(s_i|x_0,\\ldots,x_t)\\] Edges have a score of $1$ and connect a node to: The next time step on the same row (symbol is held) The next row (next symbol) at the next time step We define valid paths to: Start at top-left End at bottom-right Monotonically move downward or forward in timeGraph constraints:To ensure valid decoding, we impose these: The first symbol must begin at the top-left node. The last symbol must end at the bottom-right node. All paths must be monotonic: You can stay on the same row (repeat symbol) Or move downward to the next symbol (progress in output) But you can‚Äôt go backward This guarantees that the path represents a valid expansion of the target sequence.Scoring Paths:Each path through the graph represents a possible alignment. The score of a path is the product of the probabilities of its nodes:\\[\\text{Score(Path)} = \\prod_{(t, i) \\in \\text{Path}} y_t^{s_i}\\]This score represents the model‚Äôs confidence in that specific alignment.To pick the most likely alignment given the input and given this graph, we need to pick the path that has the highest probability.Using this, we can find the most probable path from source to sink using any dynamic programming algorithm, such as breadth-first search, Dijkstra‚Äôs, A*, etc. One such algorithm is the Viterbi algorithm.Viterbi AlgorithmThe basic idea is that the best path to any node must be an extension of the best path to one of its parent nodes. Any other path would necessarily have a lower probability.The best parent is simply the parent with the best scoring best path.Algorithm: Dynamically track the best path (and the score of the best path) from the source node to every node in the graph At each node, keep track of: The best incoming parent edge The score of the best path from the source to the node through this best parent edge Eventually compute the best path from source to sinkPseudocode: Initialization\\[BP(0, i) = \\text{null}, \\quad i = 0, \\ldots, K-1\\]\\[Bscr(0,0) = y_0^{S(0)}, \\quad Bscr(0,i) = -\\infty, \\quad i = 1, \\ldots, K-1\\] For $t = 1, \\ldots, T-1$\\[BP(t,0) = 0, \\quad Bscr(t,0) = Bscr(t-1,0) \\times y_t^{S(0)}\\] For $l = 1, \\ldots, K-1$\\[BP(t,l) = \\begin{cases} l-1, &amp; \\text{if } Bscr(t-1,l-1) &gt; Bscr(t-1,l) \\\\ l, &amp; \\text{otherwise} \\end{cases}\\]\\[Bscr(t,l) = Bscr(BP(t,l)) \\times y_t^{S(l)}\\] $s(T-1) = s(K-1)$ for $t=T-1:1$ $s(t-1) = BP(s(t))$ Loss:\\[DIV = \\sum_t KL(Y_t, symbol_t^{bestpath}) = -\\sum_t \\log Y(t, symbol_t^{bestpath})\\]\\[\\nabla_{Y_t} DIV =\\begin{bmatrix}0 &amp; 0 &amp; \\cdots &amp; \\frac{-1}{Y(t, symbol_t^{bestpath})} &amp; 0 &amp; \\cdots &amp; 0\\end{bmatrix}\\]The gradient is zero everywhere except at the component corresponding to the target in the estimated alignment. This shows that only the probabilities along the best path (red boxes) receive gradient updates.Problem with Iterative Update: Approach is heavily dependent on initial alignment Prone to poor local optima The process works well if we have large amounts of training dataAlternate solution: Do not commit to an alignment during any passTraining: Connectionist Temporal ClassificationInstead of only selecting the most likely alignment, we use the statistical expectation over all possible alignments.\\[DIV = \\mathbb{E} \\left[ -\\sum_t \\log Y(t, s_t) \\right]\\]We want to train a model that outputs, at each frame $t$, a probability distribution $Y_t(.)$ over symbols.We are given an unaligned label sequence $\\mathbf{S} = S_0, S_1, ‚Ä¶, S_{K-1}$ and an input frame sequence $\\mathbf{X} = X_0, X_1, ‚Ä¶, X_{N-1}$.Because alignment is unknown, there are many ways to align $\\mathbf{S}$ to the frames. Instead of picking one alignment (best path), we average over all valid alignments. Concretely, we need the probability that a particular symbol $s$ is aligned to frame $t$, given $\\mathbf{S,X}$. That probability is:\\[P(s_t=S|\\mathbf{S},\\mathbf{X})\\]Another way of explaining this is that this is the posterior (conditional) probability that the model is aligned with symbol $S$ at time $t$, given that we already know the model should produce the overall label sequence $\\mathbf{S}$, and we have the input $\\mathbf{X}$. Or, given that the model correctly produces the target sequence $\\mathbf{S}$, what‚Äôs the chance that, at frame $t$, it‚Äôs currently aligned to symbol $S$.From expectation to a computable loss:Start from the expectation over alignments:\\[DIV = \\mathbb{E} \\left[ -\\sum_t \\log Y(t, s_t) \\right]\\]This expectation can be written using the frame-label posterior $P(s_t=S|\\mathbf{S},\\mathbf{X})$:\\[DIV = -\\sum_{t}\\sum_{s\\in{S_0,\\dots,S_{K-1}}} P(s_t=S\\mid \\mathbf{S},\\mathbf{X})\\log Y(t,s_t={S})\\]So training reduces to two things: compute the posteriors $P(s_t=S|\\mathbf{S},\\mathbf{X})$ for every $t,s$ take a weighted cross-entropy using those posteriors as targetsHow do we compute $P(s_t=S|\\mathbf{S},\\mathbf{X})$?\\[P(s_t=S|\\mathbf{S_r},\\mathbf{X}) \\propto P(s_t=S_r, \\mathbf{S}|\\mathbf{X})\\]$P(s_t=S_r, \\mathbf{S}|\\mathbf{X})$ is the joint probability that: At time $t$, the model is aligned with symbol $S_r$ and over the entire sequence, the model emits (or aligns to) the full target label sequence $\\mathbf{S}$Intuitively, this answers what is the total probability that, while generating the entire target sequence $\\mathbf{S}$, the model goes through symbol $S_r$ at time $t$.To compute this efficiently, we split the trellis into two parts:\\[P(s_t = S_r, S \\mid X)= P(S_0, ..., S_r, s_t = S_r \\mid X) \\timesP(S_{t+1}, ..., S_{K-1} \\mid S_0, ..., S_r, s_t = S_r, X)\\]The first path\\[P(S_0, ..., S_r, s_t = S_r \\mid X)\\]represents the probability of all partial paths that lead up to state $S_r$ at time $t$. This is the forward probability, denoted by $\\alpha_t$.\\[\\alpha(t, r) = P(S_0, S_1, ..., S_r, s_t = S_r \\mid X)\\]This is the total probability of all valid paths that align the prefix of the target sequence up to symbol $S_r$ to the first $t$ input frames, and that end exactly at symbol $S_r$ at time $t$.\\[\\alpha(t, r) = \\sum_{q : S_q \\in pred(S_r)} \\alpha(t-1, q) Y_t^{S(r)}\\] $pred(S_r)$ is the set of symbols that are allowed to transition into $S_r$. It usually includes itself (stay transition) and the previous symbol (advance transition) $q$ is its row index $Y_t^{S(r)}$ is the network‚Äôs predicted probability of symbol $S_r$ at time $t$ $\\alpha(t-1,q)$ is the total probability of reaching predecessor $S_q$ at the previous frameThe second part\\[P(S_{t+1}, ..., S_{K-1} \\mid S_0, ..., S_r, s_t = S_r, X)\\]represents the probability of all possible continuations after time $t$ that follow valid transitions and end in the final symbol. This is the backward probability, denoted by $\\beta_t$.\\[\\beta(t, r) = P(\\text{rest of the path from } (t, r) \\text{ to the end} \\mid X)\\]$\\beta(t,r)$ is the total probability of all valid paths that start after time $t$, given that you are currently at state $S_r$ at time $t$, and that follow legal transitions until the sequence ends.$\\beta(t,r)$ is the probability of the exposed subgraph, not including the orange shaded box. For convenience, let us include the box in the graph, and factor it out later.$\\hat{\\beta}(t,r)$ is the probability of graph including node at $(t,r)$.\\[\\beta(t, r) = \\frac{1}{y_t^{S_r}} \\hat{\\beta}(t, r)\\]\\[\\hat{\\beta}(t, r)= y_t^{S_r} \\sum_{q \\in succ(r)} \\hat{\\beta}(t+1, q)\\] $succ(r)$ is the set of successor states that can follow $S_r$ $y_t^{S_r}$ emission probability of symbol $S_r$ at time $t$\\[\\beta(t, r)= \\frac{\\hat{\\beta}(t, r)}{y_t^{S_r}}\\]To get the posterior probability (the fraction of total path probability passing through this node):\\[P(s_t = S_r \\mid S, X)= \\frac{\\alpha(t,r)\\beta(t,r)}{\\sum_j \\alpha(t,j)\\beta(t,j)}\\]Finally, putting all this back into the expected divergence\\[DIV = -\\sum_{t}\\sum_{s\\in{S_0,\\dots,S_{K-1}}} P(s_t=S\\mid \\mathbf{S},\\mathbf{X})\\log Y(t,s_t={S})\\]Forward Algorithm Initialization\\[\\alpha(0,0) = y_0^{S(0)}, \\quad \\alpha(0,r) = 0, r &gt; 0\\] $\\text{for } t = 1, \\ldots, T-1$:\\[\\alpha(t,0) = \\alpha(t-1,0) y_t^{S(0)}\\] $\\text{for } l = 1, \\ldots, K-1$:\\[\\alpha(t,l) = \\big(\\alpha(t-1,l) + \\alpha(t-1,l-1)\\big) y_t^{S(l)}\\] Backward Algorithm Initialization\\[\\hat{\\beta}(T-1, K-1) = y_{T-1}^{S(K-1)}, \\quad \\hat{\\beta}(T-1, r) = 0, r &lt; K-1\\] $\\text{for } t = T-2 \\text{ downto } 0$ $\\text{for } r = K-1 \\text{ downto } 0$\\[\\hat{\\beta}(t, r) = y_t^{S(r)} \\sum_{q \\in succ(r)} \\hat{\\beta}(t+1, q)\\]\\[\\beta(t, r) = \\frac{1}{y_t^{S(r)}} \\hat{\\beta}(t, r)\\] Tackling RepetitionsInferenceLanguage ModelsLanguage Models (LMs) model the probability distribution of token sequences in a language. For example, word sequences, if words are the tokens.LMs can be used to: Compute the probability of a given token sequence Generate new sequences from the learned distribution of the languageA language model assigns a probability to an entire sequence of tokens (words, subwords, characters). If your sentence is $(w_1,w_2,w_3,‚Ä¶)$, the LM‚Äôs job is to say how likely that whole sequence is. Because directly modeling $P(w_1,w_2,‚Ä¶,w_T)$ is hard, we use the chain rule of probability to factor it into conditional next-token probabilities:\\[P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P\\left(w_t \\mid w_1,\\ldots,w_{t-1}\\right)\\]Representing WordsWords are represented as one-hot vectors. We pre-specify a vocabulary $V$ of $N$ words in a fixed (e.g., lexical) order. Example: V = [ A, AARDVARK, AARON, ABACK, ABACUS, ‚Ä¶, ZZYP ]Each word $w \\in V$ is mapped to a vector $\\mathbf{e}_w \\in \\mathbf{R}^N$ with all zeros except a single 1 at that word‚Äôs index. If ‚ÄúARRON‚Äù is index 3,\\[\\mathbf{e}_{ARRON} = [0, 0, 1, 0, ..., 0]\\]Characters can be encoded the same way with a smaller $N (\\approx 100$ for English if you include case, punctuation, digits, symbols, and space).Why use one-hot representation? It makes no assumptions about the relative importance of words. All word vectors are the same length It makes no assumptions about the relationships between words The distance between every pair of words is the sameWhy one-hot is not enough? Sparsity &amp; dimensionality Vectors are length $N$ and almost entirely zero. Memory &amp; compute scale with $N$. With a 100k-word vocab, each vector is 100k-dimensional. It uses only $N$ corners of the $2^N$ corners of a unit cube. No notion of similarity Any two distinct one-hots have cosine similarity $0$ and Euclidean distance $\\sqrt{2}$. ‚Äúcat‚Äù and ‚Äúdog‚Äù are as dissimilar as ‚Äúcat‚Äù and ‚Äúbanana.‚Äù So the model must learn from scratch that some words are related. UNK problem It is not possible to include every possible word in your vocabulary $V$. You fix $V$(say, 50 000 most frequent tokens). Anything not in $V$, such as a new name, misspelling, slang, or foreign word, is replaced by a special [UNK] token (‚Äúunknown‚Äù). Solution to the dimensonality problem: EmbeddingsThe goal is to reduce the dimensonality of one-hot vectors while preserving semantic relationships among words.Each work $W$ (a one-hot vector) is projected into a smaller, dense vector using a learned projection matrix $P$:\\[W \\rightarrow P W\\]Here, $W$ is the one-hot vector of dimension $N$ $P$ is the projection matrix of size $M \\times N$, where $M ¬´¬†N$ $PW$ is the new dense vector representation (embedding) of dimension $M$Intuition: This is a linear transformation into a lower-dimensional space Each original axis (word) is now represented as a point in this smaller subspace If trained properly, distances between projected points will correspond to semantic similarity between wordsBenefits of projection: Reduces computational complexity drastically Converts sparse high-dimensional one-hots into dense, meaningful embeddings Word similarity (e.g., ‚Äúcat‚Äù and ‚Äúdog‚Äù) becomes measurable via cosine similarity or Euclidean distanceEmbeddings, however, still don‚Äôt enable you to find representations for words that are not part of our training vocabulary.Each word‚Äôs one-hot vector $W_i$ is multiplied by $P$ to get its embedding:\\[PW_i = \\text{embedding of word } W_i\\]Then, a function $f(.)$ predicts the next word based on the embeddings of the previous words:\\[W_n = f(PW_0, PW_1, ..., PW_{n-1})\\] $P$ is shared across all words ‚Äî it‚Äôs a single learned matrix The same transformation is applied to every word‚Äôs one-hot vector $f$ can be a neural network that learns to combine embeddings from context words $P$ is updated during training, so embeddings gradually become semantically meaningfulMathematically, $P$ acts as a linear layer with $M$ outputs and $N$ inputs:\\[\\text{Embedding} = P W = E[w]\\]This can be implemented as: A linear transformation shared across all inputs, or A lookup operation in an embedding table (each row is one projected vector).Beginnning and End of SentencesA sequence of words by itself does not indicate if it is a complete sentence or not. To make it explicit, we will add two additional symbols (in addition to the words) to the base vocabulary. &lt;sos&gt; Indicates start of a sentence &lt;eos&gt; Indicates end of a sentenceIn situations where the start of sequence is obvious, the &lt;sos&gt; may not be needed, but &lt;eos&gt; is required to terminate sequences.So when predicting the next token, we feed the drawn word as the next word in the series by sampling from the output probability distribution. We continue this process until we draw an &lt;eos&gt;, or if we decide to terminate generation based on some other criterion.Delayed Sequence to Sequence ModelsPseudocode:# --- Step 1: Encode the input sequence ---# First run the inputs through the network # Assuming h(-1,1) is available for all layers for t = 0 : T-1 # Including both ends of the index [h(t), ...] = RNN_input_step(x(t), h(t-1), ...) H = h(T-1) # Final hidden state after reading all inputs # --- Step 2: Generate the output sequence ---# Now generate the output y_out(1), y_out(2), ... t = 0 h_out(0) = H do t = t + 1 [y(t), h_out(t)] = RNN_output_step(h_out(t-1)) Y_out(t) = draw_word_from(y(t)) until Y_out(t) == &lt;eos&gt; # --- Explanation ---# x(t) : input at time step t (e.g., word embedding)# h(t) : hidden state carrying memory of previous inputs# H : final hidden state summarizing the input sequence# RNN_input_step() : recurrence for encoding# RNN_output_step() : recurrence for decoding# draw_word_from() : selects the most likely or sampled word# &lt;eos&gt; : end-of-sequence token signaling generation stop Encoder network: The input sequence feeds into a recurrent structure The input sequence is terminated by an explicit &lt;eos&gt; symbol The hidden activation at the &lt;eos&gt; ‚Äústores‚Äù all information about the sentence Decoder network: Subsequently a second RNN uses the hidden activation as initial state, and &lt;sos&gt; as initial symbol, to produce a sequence of outputs The output at each time becomes the input at the next time Output production continues until an &lt;eos&gt; is produced The one-hot word representations can also be compressed via embeddings. These embeddings will be learned along with the rest of the network.At each time $k$ the network produces a probability distribution over the output vocabulary\\[y_k^w = P(O_k=w | O_{k-1}, ..., O_1, I_1, ..., I_N)\\]At each time a word is drawn from the output distribution and is provided as an input to the next time. Over the entire predicted sequence, we compute the total probability as\\[P(O_1, ..., O_L \\mid I_1, ..., I_N)= P(O_1 \\mid I_{1}, ..., I_N) P(O_2 \\mid O_1, I_1, ..., I_N) P(O_3 \\mid O_1, O_2, I_1, ..., I_N) ... P(O_L \\mid O_1, ..., O_{L-1}, I_1, ..., I_N)= y_1^{O_1} , y_2^{O_2} , ... , y_L^{O_L}\\]Picking the next token given the probability distributionThe model gives a distribution $y_t$ at each time step $t$. The ideal decoder would pick the sequence\\[\\arg\\max_{o_{1:L}} \\prod_{t=1}^{L} y_t^{(o_t)}\\]i.e., the most probable sequence as a whole, not just the best token at each step. That‚Äôs hard to search exactly, so we approximate.Greedy decoding:At each step, choose the token with the highest probability in $y_t$, feed it back, and repeat until &lt;eos&gt;.Why this often fails? Myopia: A locally best token can push the model into a worse future. Example: choosing ‚Äúhe‚Äù at $t=0$ might make ‚Äúnose‚Äù at $t=1$ more likely than a better global path (‚Äúknows‚Äù later). Distribution shaping: The token you pick changes the next distribution. Sometimes picking a slightly less likely token now makes the next distribution much ‚Äúpeakier,‚Äù yielding a higher overall product of probabilities. Error compounding: One early poor choice can lock you into a suboptimal continuation.Sampling:At each step, sample a token from the full distribution $y_t$. It introduces diversity and can escape greedy‚Äôs local traps. It also sometimes stochastically finds better (higher-probability) sequences than greedy.Why it also fails? Unreliable: You may sample low-probability ‚Äújunk‚Äù early, which can derail future steps High variance: Two runs can differ wildly; average quality can drop without constraints.What is actually used in practice? Beam search: Keep the top-$B$ partial sequences at each step (by cumulative log-probability), expand each, prune again. Top-k sampling: At each step, restrict to the $k$ most probable tokens (renormalize), then sample. Nucleus (Top-p) Sampling: Choose the smallest set of tokens whose cumulative probability $\\ge p$, renormalize, sample.Beam SearchAt each time step, the model can produce many possible next words. If we try to explore all of them, the search tree explodes exponentially (like ‚ÄúHe knows‚Ä¶‚Äù vs. ‚ÄúThe nose‚Ä¶‚Äù).So, the idea is to prune ‚Äî keep only the top few promising paths instead of exploring everything.We compute the cumulative probability for each partial sequence up to time $t$:\\[P(O_1, ..., O_t \\mid I_1, ..., I_N) = \\prod_{n=1}^{t} P(O_n \\mid O_{1:n-1}, I_1, ..., I_N)\\]At every time step, we: Expand each of the current candidate sequences by all possible next words. Compute their cumulative probabilities (product of conditional probabilities). Retain only the top K candidates (where K is called the beam width).The search continues until one of the active hypotheses produces an &lt;eos&gt; token. We can employ one of two strategies: Early termination: Stop as soon as the most likely overall path ends with &lt;eos&gt;.This is used for fast inference. N-best search: Continue expanding other beams even after some reach &lt;eos&gt; to collect multiple high-probability completions. This is used in translation or summarization tasks.Once a path emits the &lt;eos&gt; token, it cannot be extended further ‚Äî it‚Äôs a completed sequence. Different beams may terminate at different lengths. The final output is the sequence ending with &lt;eos&gt; that has the highest cumulative probability across all terminated beams.Pseudocode:# Assuming encoder output H is availablepath = &lt;sos&gt;beam = {path}pathscore = [path] = 1state[path] = h[0] # Output of encoderdo # Step forward nextbeam = {} nextpathscore = {} nextstate = {} for path in beam: cfin = path[end] hpath = state[path] [y, h] = RNN_output_step(hpath, cfin) for c in Symbolset: newpath = path + c nextstate[newpath] = h nextpathscore[newpath] = pathscore[path] * y[c] nextbeam += newpath # Set addition end\t\tend beam, pathscore, state, bestpath = prune(nextstate, nextpathscore, nextbeam, bw)until bestpath[end] == &lt;eos&gt;function prune(state, score, beam, beamwidth): sortedscore = sort(score) threshold = sortedscore[beamwidth] prunedstate = {} prunedscore = {} prunedbeam = {} bestscore = -inf bestpath = None for path in beam: if score[path] &gt; threshold: prunedbeam += path # Set addition prunedstate[path] = state[path] prunedscore[path] = score[path] if score[path] &gt; bestscore: bestscore = score[path] bestpath = path end end end return prunedbeam, prunedscore, prunedstate, bestpathTrainingForward PassInput the source and target sequences, sequentially. The output will be a probability distributon over target symbol set (vocabulary).Backward PassWe compute the divergence between the output distribution and target word sequence. Then we backpropagate the derivatives of the divergence through the network to learn.In practice, if we apply SGD, we may randomly sample words from the output to actually use for the backprop and update." }, { "title": "Deep Learning - Perceptrons", "url": "/posts/deep-learning-perceptrons/", "categories": "Blog, Robotics", "tags": "learning, nnets, backpropagation", "date": "2025-06-09 12:00:00 -0400", "snippet": "Neural NetworksDepth - length of longest path from source to sinkLayer - Set of all neurons which are all at the same depth with respect to the sourceGradientFor a scalar function $f(x)$ with a multivariate input $x$:\\[df\\left( x\\right) =\\nabla _{x}f\\left( x\\right) dx\\]\\[\\nabla _{x}f\\left( x\\right) =\\left[ \\dfrac{\\partial f}{\\partial x_{1}}\\dfrac{\\partial f}{\\partial x_{2}}\\ldots \\dfrac{\\partial f}{\\partial x_{n}}\\right]\\]Because it is a dot product, for an increment $dX$ of any given length, $df$ is maximum if the increment $dX$ is aligned with the gradient direction $\\nabla _{x}f\\left( x\\right)^T$. So the gradient is the direction of steepest ascent.So if you want a function to decrease, your increment should be in the direction of $-\\nabla _{x}f\\left( x\\right)^T$.To find a maximum move in the direction of gradient:\\[x^{k+1} = x^k + \\eta^k \\nabla_x f(x^k)^T \\\\[1em]\\]To find a minimum move in the direction of gradient:\\[x^{k+1} = x^k - \\eta^k \\nabla_x f(x^k)^T \\\\[1em]\\]There are many solutions to choosing step size $\\eta^k$.Hessian\\[\\nabla^2_{x} f(x_1, \\ldots, x_n) = \\begin{bmatrix}\\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2}\\end{bmatrix}\\]NetworkA continuous activation function applied to an affine function of the inputs\\[y = f\\left( \\sum_i w_i x_i + b \\right) \\\\[1em]y = f(x_1, x_2, \\ldots, x_N; W)\\]Activation Functions and DerivativesSigmoid:\\[f(z) = \\frac{1}{1 + \\exp(-z)} \\\\f'(z) = f(z)(1 - f(z)) \\\\[1.5em]\\]Tanh:\\[f(z) = \\tanh(z) \\\\f'(z) = 1 - f^2(z)\\]ReLU:\\[f(z) =¬†\\begin{cases}z, &amp; z \\geq 0 \\\\0, &amp; z &lt; 0\\end{cases} \\\\f'(z) =¬†\\begin{cases}1, &amp; z \\geq 0 \\\\0, &amp; z &lt; 0\\end{cases}¬†\\]Softplus:\\[f(z) = \\log(1 + \\exp(z)) \\\\f'(z) = \\frac{1}{1 + \\exp(-z)}\\]Softmax:\\[f(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\\\f'(z_i) = \\begin{cases}f(z_i)(1 - f(z_i)), &amp; \\text{if } i = j \\\\- f(z_i) f(z_j), &amp; \\text{if } i \\ne j\\end{cases}\\]Training by Back PropagationForward PassSetting $y_i^{(0)} = x_i$ and $w_{0j}^{(k)} = b_j^{(k)}$. Let the bias equal $1$ for simplicity.For layer 1,\\[z_j^{(1)} = \\sum_i w_{ij}^{(1)}y_i^{(0)} \\\\ y_j^{(1)} = f_1(z_j^{(1)})\\]For layer 2,\\[z_j^{(2)} = \\sum_i w_{ij}^{(2)}y_i^{(1)} \\\\ y_j^{(2)} = f_2(z_j^{(2)})\\]Similarly,\\[z_j^{(N)} = \\sum_i w_{ij}^{(N)}y_i^{(N-1)} \\\\ y_j^{(N)} = f_N(z_j^{(N)})\\]Backward PassStep 1: Initialize Gradient at Output LayerWe start by computing the gradient of the loss w.r.t. the network output:\\[\\frac{\\partial Div}{\\partial y^{(N)}_i} = \\frac{\\partial Div(Y, d)}{\\partial y_i}\\]Then, propagate this to the pre-activation output $z^{(N)}$:\\[\\frac{\\partial Div}{\\partial z^{(N)}_i} = \\frac{\\partial y_i^{(N)}}{\\partial z_i^{(N)}} \\cdot \\frac{\\partial Div}{\\partial y^{(N)}_i} =f_N'\\left(z^{(N)}_i\\right) \\cdot \\frac{\\partial Div}{\\partial y^{(N)}_i}\\]In case of a vector activation function, such as the softmax function, $y_i^{(N)}$ is influenced by every $z_i^{(N)}$:\\[\\frac{\\partial Div}{\\partial z_i^{(N)}} = \\sum_j \\frac{\\partial y_j^{(N)}}{\\partial z_i^{(N)}} \\cdot \\frac{\\partial Div}{\\partial y_j^{(N)}}\\]Step 2: Backpropagation Through LayersLoop from layers $k = (N-1) \\rightarrow 0$:For each layer $k$ and for each neuron $i$ in that layer:Compute gradient of loss w.r.t. activation:\\[\\frac{\\partial Div}{\\partial y^{(k)}_i} = \\sum_j w_{ij}^{(k+1)} \\cdot \\frac{\\partial Div}{\\partial z^{(k+1)}_j}\\]Chain through activation function:\\[\\frac{\\partial Div}{\\partial z^{(k)}_i} = \\frac{\\partial y_i^{(k)}}{\\partial z_i^{(k)}} \\cdot \\frac{\\partial Div}{\\partial y^{(k)}_i} = f_k'\\left(z^{(k)}_i\\right) \\cdot \\frac{\\partial Div}{\\partial y^{(k)}_i}\\]Step 3: Gradient w.r.t. WeightsFor each weight connecting neuron $i$ in layer $k$ to neuron $j$ in layer $k$:\\[\\frac{\\partial Div}{\\partial w_{ij}^{(k)}} = y^{(k-1)}_i \\cdot \\frac{\\partial Div}{\\partial z^{(k)}_j}\\]Step 4: Updating WeightsActual loss is the sum of the divergence over all training instances:\\[Loss = \\frac{1}{|\\{X\\}|}\\sum_X Div(Y(X), d(X))\\]Actual gradient is the average of the derivatives computed for each training instance:\\[\\nabla_W Loss = \\frac{1}{|\\{X\\}|}\\sum_X \\nabla_W Div(Y(X), d(X))\\]\\[W \\leftarrow W - \\eta \\nabla_W Loss^T\\]SummaryVector Formulation\\[\\mathbf{z}_k =¬†\\begin{bmatrix}z^{(k)}_1 \\\\z^{(k)}_2 \\\\\\vdots \\\\z^{(k)}_{D_k}\\end{bmatrix}\\qquad\\mathbf{y}_k =¬†\\begin{bmatrix}y^{(k)}_1 \\\\y^{(k)}_2 \\\\\\vdots \\\\y^{(k)}_{D_k}\\end{bmatrix}\\]\\[\\mathbf{W}_k =\\begin{bmatrix}w^{(k)}_{11} &amp; w^{(k)}_{21} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}1} \\\\w^{(k)}_{12} &amp; w^{(k)}_{22} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}2} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\w^{(k)}_{1D_k} &amp; w^{(k)}_{2D_k} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}D_k}\\end{bmatrix}\\qquad\\mathbf{b}_k =¬†\\begin{bmatrix}b^{(k)}_1 \\\\b^{(k)}_2 \\\\\\vdots \\\\b^{(k)}_{D_k}\\end{bmatrix}\\]\\[\\mathbf{z_k} = \\mathbf{W_k y_{k-1} + b_k} \\\\\\]\\[\\mathbf{y_k} = \\mathbf{f_k (z_k)}\\]Setup: Let $\\mathbf{y_n = Y}$, the network output Let $\\mathbf{y_0 = X}$, the input Initialize:\\[\\nabla_{\\mathbf{y}_N} Div = \\nabla_{\\mathbf{Y}} Div\\] For each layer $k = N \\rightarrow 1$: Compute the Jacobian of activation: \\[J_{\\mathbf{y}_k}(\\mathbf{z}_k) = \\frac{\\partial \\mathbf{y}_k}{\\partial \\mathbf{z}_k}\\] This is a matrix of partial derivatives: \\[J_{\\mathbf{y}}(\\mathbf{z}) =\\begin{bmatrix}\\frac{\\partial y_1}{\\partial z_1} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial z_D} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial y_M}{\\partial z_1} &amp; \\cdots &amp; \\frac{\\partial y_M}{\\partial z_D}\\end{bmatrix}\\] Backward recursion step: \\[\\nabla_{\\mathbf{z}_k} Div = \\nabla_{\\mathbf{y}_k} Div \\cdot J_{\\mathbf{y}_k}(\\mathbf{z}_k)\\]\\[\\nabla_{\\mathbf{y}_{k-1}} Div = \\nabla_{\\mathbf{z}_k} Div \\cdot \\mathbf{W}_k\\] Gradient Computation for all $k$:\\[\\nabla_{\\mathbf{W}_k} Div = \\mathbf{y}_{k-1} \\cdot \\nabla_{\\mathbf{z}_k} Div\\]\\[\\nabla_{\\mathbf{b}_k} Div = \\nabla_{\\mathbf{z}_k} Div\\] SummaryAlgorithm: Batch Gradient Descent with BackpropagationInitialize all weights and biases: W1, b1, W2, b2, ..., WN, bNLoss ‚Üê 0For all k: ‚àáWk Loss ‚Üê 0 ‚àábk Loss ‚Üê 0For t = 1 to T: # Loop through training examples Compute output Y(Xt) Compute divergence Div(Yt, dt) For all k: # Backward pass ‚àáWk Div(Yt, dt), ‚àábk Div(Yt, dt) ‚àáWk Loss ‚Üê ‚àáWk Loss + ‚àáWk Div(Yt, dt) ‚àábk Loss ‚Üê ‚àábk Loss + ‚àábk Div(Yt, dt)For all k: # Gradient Descent Update Wk ‚Üê Wk - (Œ∑ / T) * (‚àáWk Loss)^T bk ‚Üê bk - (Œ∑ / T) * (‚àábk Loss)^TRepeat until Loss has convergedImportant Points Backpropagation will often not find a separating solution even though the solution is within the class of functions learnable by the network, because the separable solution is not a feasible optimum for the loss function. It is minimally changed by new training instances ‚Äî doesn‚Äôt swing wildly in response to small changes to the input It prefers consistency over perfection, due to which it works better even if there are outliers It is a low-variance estimator, at the potential cost of bias Minimizing the differentiable loss function does not imply minimizing the classification error.Convergence of Gradient DescentCovergence RateIt measures how quickly an iterative optimization algorithm approaches the solution.\\[R = \\left| \\frac{f(x^{(k+1)}) - f(x^*)}{f(x^{(k)}) - f(x^*)} \\right|\\]Where: $x^{(k)}$: point at iteration $k$ $x^*$: optimal point (solution) $f(x)$: objective functionIf $R&lt;1$, that means that the function value is getting closer to the optimum and is hence converging. The smaller the $R$, the faster the convergence.If $R$ is a constant or upper-bounded, then the algorithm has linear convergence. This means that the difference between the function value and the optimum shrinks exponentially with iterations.\\[|f(x^{(k)}) - f(x^*)| \\leq R^k \\cdot |f(x^{(0)}) - f(x^*)|\\]Convergence for Quadratic SurfacesWhat is the optimal step size $\\eta$ to reach the minimum value of the error fastest? Consider a general quadratic function:\\[E(w) = \\frac{1}{2}aw^2 + bw + c\\] Gradient descent update:\\[w^{(k+1)} = w^{(k)} - \\eta \\frac{dE(w^{(k)})}{dw}\\] Taylor expansion of $E(w)$ around $w^{(k)}$:\\[E(w) \\approx E(w^{(k)}) + E'(w^{(k)})(w - w^{(k)}) + \\frac{1}{2} E''(w^{(k)})(w - w^{(k)})^2\\] Newton‚Äôs method gives:\\[w_{\\text{min}} = w^{(k)} - \\left(E''(w^{(k)})\\right)^{-1} E'(w^{(k)})\\] Optimal learning rate:\\[\\eta_{\\text{opt}} = \\left(E''(w^{(k)})\\right)^{-1} = a^{-1}\\]Effect of step size $\\eta$: $\\eta &lt; \\eta_{opt} \\rightarrow$ monotonic convergence $\\eta = \\eta_{opt} \\rightarrow$ fast convergence in one step $\\eta_{opt}&lt;\\eta &lt; 2\\eta_{opt} \\rightarrow$ oscillating convergence $\\eta \\geq 2\\eta_{opt} \\rightarrow$ divergenceConvergence for Multivariate Quadratic Functions General form of a quadratic convex function:\\[\\mathbf{w} = [w_1, w_2, ..., w_N] \\\\ E(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T A \\mathbf{w} + \\mathbf{w}^T \\mathbf{b} + c\\] If $A$ is diagonal:\\[E(\\mathbf{w}) = \\frac{1}{2} \\sum_i (a_{ii} w_i^2 + b_i w_i) + c\\]This means the cost function is a sum of independent univariate quadratics. Each direction $w_i$ is uncoupled.Equal-Value Contours: For diagonal $A$, the contours are ellipses aligned with coordinate axes. Each coordinate‚Äôs behavior is independent of the others.Optimal Step Size:For each dimension $i$:\\[\\eta_{i,\\text{opt}} = \\frac{1}{a_{ii}} = \\left( \\frac{\\partial^2 E}{\\partial w_i^2} \\right)^{-1}\\]Each coordinate has a different optimal learning rate.Problem with Vector Update Rule:Conventional update applies the same step size $\\eta$ to all coordinates. An issue with this is that one direction might converge optimally while another direction might diverge due to too large a step.\\[\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\eta \\nabla_{\\mathbf{w}} E\\]Safe Learning Rate Rule to Avoid Divergence:\\[\\eta &lt; 2 \\cdot \\min_i \\eta_{i,\\text{opt}}\\]This guarantees convergence but slows down overall learning.Generic Convex Functions We can apply Taylor expansion to approximate any smooth convex function:\\[E(\\mathbf{w}) \\approx E(\\mathbf{w}^{(k)}) + \\nabla_{\\mathbf{w}} E(\\mathbf{w}^{(k)})(\\mathbf{w} - \\mathbf{w}^{(k)})+ \\frac{1}{2} (\\mathbf{w} - \\mathbf{w}^{(k)})^T H_E(\\mathbf{w}^{(k)})(\\mathbf{w} - \\mathbf{w}^{(k)})\\] $H_E$ is the Hessian of second derivatives and measures the curvature of loss function Optimal step size is inversely proportional to eigenvalues of the Hessian The eigenvalues give curvature in orthogonal directions For the smoothest convergence, the eigenvalues must all be equal Convergence Challenges In high dimensions, convergence becomes harder to control Ideally, the step size $\\eta$ should work for both:\\[\\max_i \\eta_{i,\\text{opt}}, \\quad \\min_i \\eta_{i,\\text{opt}}\\] If the following condition number is large, then convergence is slow and unstable.\\[\\frac{\\max_i \\eta_{i,\\text{opt}}}{\\min_i \\eta_{i,\\text{opt}}}\\]Decaying Learning RateThe loss surface has many saddle points and gradient descent can stagnate on saddle points Start with a large learning rate to explore fast Gradually reduce step size to fine-tune near the minimum Prevents overshooting and bouncing around the optimumCommon Decay Schedules:\\[\\text{Linear:} \\quad \\eta_k = \\frac{\\eta_0}{k + 1}\\]\\[\\text{Quadratic:} \\quad \\eta_k = \\frac{\\eta_0}{(k + 1)^2}\\]\\[\\text{Exponential:} \\quad \\eta_k = \\eta_0 \\cdot e^{-\\beta k}, \\quad \\beta &gt; 0\\]Common Approach for Neural Networks: Train with a fixed learning rate until the validation loss stagnates Reduce learning rate:\\[\\eta \\leftarrow \\alpha \\eta \\quad \\text{(e.g., } \\alpha = 0.1 \\text{)}\\] Resume training from the same weights, repeat as neededRProp Resilient Propagation is a first-order optimization algorithm that adjusts step size independently for each parameter It doesn‚Äôt rely on the gradient magnitude, rather only on the sign of the gradient It is more robust than vanilla gradient descent Does not need a global learning rate Doesn‚Äôt require Hessian or curvature information and doesn‚Äôt assume convexityThe core idea is that at each step: If the gradient sign has not changed ‚Üí increase step size in the same direction If the gradient sign has flipped ‚Üí reduce step size and reverse direction (overshoot detected)Algorithm: For each layer $l$, for each parameter $w_{l,i,j}$:\\[\\Delta w_{l, i, j} &gt; 0\\]\\[\\text{prevD}(l, i, j) = \\frac{d \\, \\text{Loss}(w_{l, i, j})}{d w_{l, i, j}}\\]\\[\\Delta w_{l, i, j} = \\text{sign(prevD}(l, i, j)) \\cdot \\Delta w_{l, i, j}\\] While not converged: Update parameter: \\[w_{l, i, j} = w_{l, i, j} - \\Delta w_{l, i, j}\\] Recompute gradient: \\[D(l, i, j) = \\frac{d \\, \\text{Loss}(w_{l, i, j})}{d w_{l, i, j}}\\] Check sign consistency: If:\\[\\text{sign(prevD}(l, i, j)) == \\text{sign}(D(l, i, j))\\] Then,\\[\\Delta w_{l, i, j} = \\min(\\alpha \\cdot \\Delta w_{l, i, j}, \\Delta_{\\max})\\]\\[\\text{prevD}(l, i, j) = D(l, i, j)\\] Else undo step:\\[w_{l, i, j} = w_{l, i, j} + \\Delta w_{l, i, j} \\quad\\]\\[\\Delta w_{l, i, j} = \\max(\\beta \\cdot \\Delta w_{l, i, j}, \\Delta_{\\min})\\] QuickPropMomentumIncremental Stochastic Gradient Descent Feature Batch Gradient Descent Incremental Gradient Descent (SGD) Update Frequency Once per full dataset After each training point Speed per Update Slow Fast Stability High (smooth updates) Noisy but flexible Memory Usage High (needs all data at once) Low (uses one sample at a time) Suitable for Big Data Not ideal Very efficient Escaping Local Minima Less likely More likely (due to randomness/noise) Batch Gradient Descent computes gradients for all training samples and makes one big update after processing the full batch.Incremental Gradient Descent will update one sample at a time without waiting for the full dataset.Algorithm Given training data $(X_1, d_1), (X_2, d_2), \\ldots, (X_T, d_T)$ Initialize weights $W_1, W_2, \\ldots, W_K \\space; \\space j=0$ Repeat (over multiple epochs) Randomly permute $(X_1, d_1), (X_2, d_2), \\ldots, (X_T, d_T)$ $\\rightarrow$ stochastic For all $t = 1 \\rightarrow T:$ $j = j + 1$ For each layer $k$: Compute Gradient $\\nabla_{W_k} \\text{Div}(Y_t, d_t)$ Update weights \\[W_k = W_k - \\eta_j \\nabla_{W_k} \\text{Div}(Y_t, d_t)^T\\] Until loss convergedImportant Points If we loop through the samples in the same order, we may get a cyclic behavior and can oscillate. We must go through them randomly to get more convergent behavior. Hence the ‚Äústochastic‚Äù nature. Since batch gradient descent operates over $T$ training instances to get a single update, while SGD gets $T$ updates for the same computation, the advantage of SGD is more pronounced when the data points are very similar. But as data gets increasingly diverse, the benefits of incremental update decreases, but do not entirely vanish. Risk of chasing the latest input: Since SGD updates the model after every point, it could swing drastically towards the latest input and never converge. To tackle this, we shrink the learning rate with each iteration. Caveat: If learning rate shrinks too much, model becomes unresponsive to new data. $\\eta_k$ reduces with $k$ and must satisfy the conditions: Ensure full parameter space is explored: \\[\\sum_k \\eta_k = \\infty\\] Ensure steps shrink over time: \\[\\sum_k \\eta_k^2 &lt; \\infty\\] The fastest converging series that satisfies both above requirements is: \\[\\eta_k \\propto \\frac{1}{k}\\] If the loss is convex, SGD converges to the optimal solution. For non-convex losses, SGD converges to a local minimum. SGD converges faster than batch gradient descent, but arrives at a poorer optima.VarianceVariance refers to how much our estimated loss (or error) fluctuates depending on which samples we use. When we approximate the true expected error using a finite number of training samples (empirical risk), the result is an unbiased estimate, meaning it‚Äôs correct on average, but it has variance, which affects how reliable the estimate is.Empirical Risk Variance:We estimate the expected divergence using:\\[\\text{Loss}(W) = \\frac{1}{N} \\sum_{i=1}^{N} \\text{div}(f(X_i; W), d_i) \\\\ \\mathbb{E}[\\text{Loss}(W)] = \\mathbb{E}[\\text{div}(f(X; W), g(X))]\\]The variance of this estimate is:\\[\\text{Var}(\\text{Loss}) = \\frac{1}{N} \\, \\text{Var}(\\text{div})\\] More samples (larger $N$) ‚Üí lower variance ‚Üí more stable learning. Fewer samples ‚Üí higher variance ‚Üí model may learn unstable or misleading patterns.Why Variance Matters: High variance means your model could ‚Äúswing‚Äù wildly depending on the exact training data. If you only use one or two samples, the estimated error could drastically change just by replacing one data point. This affects optimization ‚Äî the model might overcorrect in response to noisy updates.In SGD:When using Stochastic Gradient Descent, each update uses only one sample:\\[\\text{div}(f(X_i; W), d_i)\\]This is still an unbiased estimate of the expected error:\\[\\mathbb{E}[\\text{div}(f(X_i; W), d_i)] = \\mathbb{E}[\\text{div}(f(X; W), g(X))]\\]But the variance is:\\[\\text{Var}_{\\text{SGD}} = \\text{Var}(\\text{div})\\]So SGD has $N$ times more variance per update than full batch training.In the above image, we are trying to fit the red line to the blue line. Having more samples makes the estimate more robust to changes in the position of samples (smaller variance). But having very few samples makes the estimate swing wildly with the sample position (high variance).Mini-Batch Stochastic Gradient DescentSGD uses the gradient from only one sample at a time, and is consequently high variance. But it also provides significantly quicker updates than batch gradient descent. So to get the best of both worlds, we use mini-batch incremental updates.Algorithm Given training data $(X_1, d_1), (X_2, d_2), \\ldots, (X_T, d_T)$ Initialize weights $W_1, W_2, \\ldots, W_K \\space; \\space j=0$ Repeat (over multiple epochs) Randomly permute $(X_1, d_1), (X_2, d_2), \\ldots, (X_T, d_T)$ $\\rightarrow$ stochastic For all $t = 1:b:T$ $j = j + 1$ For each layer $k$: $\\nabla W_k = 0$ For $t‚Äô = t:t+b-1$ For every layer $k:$ Compute Gradient $\\nabla_{W_k} \\text{Div}(Y_t, d_t)$ Cumulative loss for the mini-batch: \\[\\nabla W_k = \\nabla W_k + \\frac{1}{b}\\nabla_{W_k} \\text{Div}(Y_t, d_t)^T\\] Update weights For every layer $k:$ \\[W_k = W_k - \\eta_j \\nabla{W_k}\\] Until loss convergedImportant Points The mini-batch size is generally set to the largest that your hardware will support (in memory) without compromising overall compute time. Larger minibatches implies less variance and fewer updates per epoch. Simple technique for convergence: fix learning rate until the error plateaus, then reduce learning rate by a fixed factor. Estimates have lower variance than SGD. Convergence rate is theoretically worse than SGD But we compensate by being able to perform batch processing Choosing a DivergenceL2 DivergenceUsed in regression\\[\\text{Div} = \\frac{1}{2}(y - d)^2 \\\\ \\text{Div} = \\frac{1}{2} \\sum_i (y_i - d_i)^2\\]\\[\\frac{\\partial \\mathcal{L}}{\\partial z_k} = y_k - d_k\\]KL DivergenceUsed in classification tasks, especially with softmax outputs. It measures how different two probability distributions are. In classification, $Y$ is the predicted distribution and $d$ is the ground truth, usually a one-hot encoding.\\[KL(Y, d) = \\sum_i d_i \\log \\frac{d_i}{y_i} \\\\ KL(Y, d) = \\sum_i d_i \\log(d_i) - \\sum_i d_i \\log(y_i)\\]For one-hot, $d$ simplifies to\\[KL(Y, d) = -\\log y_c\\]This means that it is minimized when correct class is predicted with the highest probability $y_c \\rightarrow 1$.Gradient:\\[\\frac{d}{dy_c} KL(Y, d) = -\\frac{1}{y_c}\\]For scalar binary classification:\\[\\text{Div} = -d \\log(y) - (1 - d)\\log(1 - y)\\]Deriving Gradient of KL Divergence with Softmax Output:KL Divergence Loss:\\[\\mathcal{L} = KL(\\mathbf{d} \\| \\mathbf{y}) = \\sum_{i=1}^C d_i \\log \\left( \\frac{d_i}{y_i} \\right)\\]Softmax function:\\[y_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}}\\]Gradient of loss w.r.t $z_k$:\\[\\frac{\\partial \\mathcal{L}}{\\partial z_k} = - \\sum_{i=1}^C d_i \\cdot \\frac{\\partial}{\\partial z_k} \\log y_i\\]Derivative of log-softmax:\\[\\log y_i = z_i - \\log \\sum_j e^{z_j}\\]\\[\\frac{\\partial \\log y_i}{\\partial z_k} = \\frac{\\partial z_i}{\\partial z_k} - \\frac{\\partial}{\\partial z_k} \\log \\left( \\sum_j e^{z_j} \\right)\\]\\[\\frac{\\partial z_i}{\\partial z_k} =¬†\\begin{cases}1 &amp; \\text{if } i = k \\\\0 &amp; \\text{if } i \\neq k\\end{cases}= \\delta_{ik}\\]\\[\\frac{\\partial}{\\partial z_k} \\log \\left( \\sum_j e^{z_j} \\right) = \\frac{e^{z_k}}{\\sum_j e^{z_j}} = y_k\\]Therefore,\\[\\frac{\\partial \\log y_i}{\\partial z_k} = \\delta_{ik} - y_k\\]\\[\\frac{\\partial \\mathcal{L}}{\\partial z_k} = - \\sum_i d_i \\left( \\delta_{ik} - y_k \\right) \\\\ = - \\left( d_k (1 - y_k) + \\sum_{i \\ne k} d_i (-y_k) \\right) = - \\left( d_k - d_k y_k - y_k \\sum_{i \\ne k} d_i \\right)\\]Note that,\\[\\sum_i d_i = 1 \\Rightarrow \\sum_{i \\ne k} d_i = 1 - d_k\\]So,\\[\\frac{\\partial \\mathcal{L}}{\\partial z_k} = - \\left( d_k - d_k y_k - y_k (1 - d_k) \\right) \\\\ = y_k - d_k\\]Hence,\\[\\frac{\\partial \\mathcal{L}}{\\partial z_k} = y_k - d_k\\]How to choose?If we plot the L2 and KL divergences as a function of $y$ (one-dimensional input), we find that both are convex, and L2 appears more bowl-like and nicer compared to KL which appears to be very steep at the extremes and flatten badly near the minimum. This would show that L2 is a better divergence function for classification applications. But why do we still use KL?Because now when we plot the divergence as a function of $z$, only the KL is nice and convex.Their plots as a function of weights for a two-dimensional input look like this:Batch NormalizationTraining assumes the training data are all similarly distributed, i.e., that minibatches have similar distribution. But in practice, each minibatch may have a different distribution. So there is essentially a covariance shift and sometimes this shift could be large.So the solution to this is to move all the minibatches to a standard location. We do this by: Shifting all batches to the origin by subtracting each data point in the batch with their mean Normalize each batch by their standard deviationThis will move the entire collection to an appropriate location and is called batch normalization.Batch normalization is done during training and can be done independently for each unit.For a minibatch size $B$, we have the mean $\\mu_B$ and standard deviation $\\sigma_B^2$,\\[\\mu_B = \\frac{1}{B}\\sum_{i=1}^B z_i \\\\ \\sigma_B^2 = \\frac{1}{B}\\sum_{i=1}^B(z_i - \\mu_B)^2\\]After batch normalization,\\[u_i = \\frac{z_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\\[\\hat{z_i} = \\gamma u_i + \\beta\\]where $\\gamma, \\beta$ are learnable parameters.In a normal minibatch gradient descent, we compute the loss of gradients as,\\[\\text{Loss(minibatch)} = \\frac{1}{B}\\nabla_{W_k} \\text{Div}(Y_t(X_t), d_t(X_t))\\]But now with batch normalization, the outputs are also functions of $\\mu_B, \\sigma_B^2$\\[\\text{Loss(minibatch)} = \\frac{1}{B}\\nabla_{W_k} \\text{Div}(Y_t(X_t, \\mu_B, \\sigma_B^2), d_t(X_t))\\]Since batch normalization is a vector function applied over all inputs from a minibatch, every $z_i$ affects every $\\hat{z_j}$.Here‚Äôs how we backpropagate,\\[\\frac{dLoss}{d\\hat{z}} = f'(\\hat{z})\\frac{dLoss}{dy}\\]\\[\\frac{dLoss}{d\\gamma} = \\frac{d\\hat{z}}{d\\gamma}\\frac{dLoss}{d\\hat{z}} = u \\frac{dLoss}{d\\hat{z}} \\\\ \\frac{dLoss}{d\\beta} = \\frac{d\\hat{z}}{d\\beta}\\frac{dLoss}{d\\hat{z}} = \\frac{dLoss}{d\\hat{z}}\\]\\[\\frac{dLoss}{du_i} = \\frac{d\\hat{z_i}}{du_i}\\frac{dLoss}{d\\hat{z_i}} = \\gamma \\frac{dLoss}{d\\hat{z_i}}\\]Now we need to compute $\\frac{dLoss}{du_i}$ for every $u_i$.The complete derivative of the mini-batch loss w.r.t. $z_i$\\[\\frac{dLoss}{dz_i} = \\sum_j\\frac{dLoss}{du_j}\\frac{du_j}{dz_i}\\]The first part of this has been computed above. We now need to compute $\\frac{du_i}{dz_i}$ for every pair $i,j$.First when $i=j$,\\[\\frac{du_i}{dz_i} = \\frac{\\partial u_i}{\\partial z_i} + \\frac{\\partial u_i}{\\partial \\mu_B}\\frac{\\partial \\mu_B}{\\partial z_i} + \\frac{\\partial u_i}{\\partial \\sigma_B^2}\\frac{d \\sigma_B^2}{d z_i}\\]First term:\\[\\frac{\\partial u_i}{\\partial z_i} = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]Second term:\\[\\frac{\\partial u_i}{\\partial \\mu_B} = \\frac{-1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\\\ \\frac{\\partial \\mu_B}{\\partial z_i} = \\frac{1}{B}\\]Third term:\\[\\frac{\\partial u_i}{\\partial \\sigma_B^2} = \\frac{-(z_i - \\mu_B)}{2(\\sigma_B^2 + \\epsilon)^{3/2}}\\]\\[\\frac{d \\sigma_B^2}{dz_i} = \\frac{\\partial d\\sigma_B^2}{\\partial z_i} + \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B}\\frac{\\partial \\mu_B}{\\partial z_i}\\]\\[\\frac{\\partial \\sigma_B^2}{\\partial z_i} = \\frac{2(z_i - \\mu_B)}{B} \\\\ \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} = 0\\]\\[\\frac{\\partial u_i}{\\partial \\sigma_B^2} \\cdot \\frac{\\partial \\sigma_B^2}{\\partial z_i} = \\frac{-(z_i - \\mu_B)}{2(\\sigma_B^2 + \\epsilon)^{3/2}} \\cdot \\frac{2(z_i - \\mu_B)}{B} \\\\ = \\frac{-(z_i - \\mu_B)^2}{B(\\sigma_B^2 + \\epsilon)^{3/2}}\\]So, for $i=j$,\\[\\frac{d u_i}{d z_i} = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-1}{B \\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-(z_i - \\mu_B)^2}{B(\\sigma_B^2 + \\epsilon)^{3/2}}\\]Now when $i \\neq j$,\\[\\frac{d u_j}{d z_i} = \\frac{\\partial u_j}{\\partial \\mu_B} \\frac{\\partial \\mu_B}{\\partial z_i} + \\frac{\\partial u_j}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial z_i}\\]This is similar to the case when $i=j$ but without the first term. So,\\[\\frac{d u_j}{d z_i} = \\frac{-1}{B \\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-(z_i - \\mu_B)(z_j - \\mu_B)}{B(\\sigma_B^2 + \\epsilon)^{3/2}}\\]Full derivative summary:\\[\\frac{d u_j}{d z_i} =\\begin{cases}\\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-1}{B \\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-(z_i - \\mu_B)^2}{B(\\sigma_B^2 + \\epsilon)^{3/2}} &amp; \\text{if } i = j \\\\\\frac{-1}{B \\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{-(z_i - \\mu_B)(z_j - \\mu_B)}{B(\\sigma_B^2 + \\epsilon)^{3/2}} &amp; \\text{if } i \\ne j\\end{cases}\\]\\[\\frac{dLoss}{dz_i} = \\sum_j\\frac{dLoss}{du_j}\\frac{du_j}{dz_i}\\]\\[\\frac{d \\mathcal{L}}{d z_i} = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\cdot \\frac{d \\mathcal{L}}{d u_i} + \\frac{-1}{B \\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_j \\frac{d \\mathcal{L}}{d u_j} + \\frac{-(z_i - \\mu_B)}{B(\\sigma_B^2 + \\epsilon)^{3/2}} \\sum_j \\frac{d \\mathcal{L}}{d u_j} (z_j - \\mu_B)\\]The rest of the backpropagation continues normally from $\\frac{d \\mathcal{L}}{d z_i}$InferenceOn test data, we still do batch normalization. But don‚Äôt have minibatches and we perform inference over individual instances. So we will use the average over all training batches, as follows:\\[\\mu_{BN} = \\frac{1} \\sum_{batch} \\mu_B({batch})\\]\\[\\sigma_{BN}^2 = \\frac{B}{(B-1){Nbatches}} \\sum_{batch} \\sigma_B^2({batch})\\]$\\mu_B(batch), \\sigma_B^2(batch)$ are obtained from the final converged network.Important Points Batch normalization may only be applied to some layers or even only selected neurons in a layer Improves both convergence rate and neural network performance Anecdotal evidence suggests that BN eliminates the need for dropout To get maximum benefit from BN, learning rates must be increased and learning rate decay can be faster Also needs better randomization of training data orderRegularizationFor a simple binary classifier, the desired output would be the smooth blue curve. But the perceptron network is also fully capable of learning the purple curve. This is because the perceptrons in the network are individually capable of sharp changes in output.If you consider a sigmoid activation function, the function becomes steeper with increasing magnitude of weights. So the model can fully learn large weights so that the output has steep edges.So the solution to preventing this is contrain the weights $w$ to be low, which will force perceptrons to learn slowly and give a smoother output response.Regularized training loss:\\[L(W_1, W_2, \\ldots, W_K) = \\frac{1}{T} \\sum_t \\text{Div}(Y_t, d_t) + \\frac{1}{2} \\lambda \\sum_k \\| W_k \\|_F^2\\]Batch Mode Gradient:\\[\\nabla_{W_k} L = \\frac{1}{T} \\sum_t \\nabla_{W_k} \\text{Div}(Y_t, d_t) + \\lambda W_k^T\\]Stochastic Gradient Descent:\\[\\nabla_{W_k} L = \\nabla_{W_k} \\text{Div}(Y_t, d_t) + \\lambda W_k^T\\]Mini-batch Gradient:\\[\\nabla_{W_k} L = \\frac{1}{b} \\sum_{\\tau = t}^{t + b - 1} \\nabla_{W_k} \\text{Div}(Y_{\\tau}, d_{\\tau}) + \\lambda W_k^T\\]Weight update:\\[W_k \\leftarrow (1 - \\lambda) W_k - \\eta \\frac{1}{b} \\sum_{\\tau = t}^{t + b - 1} \\nabla_{W_k} \\text{Div}(Y_{\\tau}, d_{\\tau})\\]$\\lambda$ gives a measure of how important it is to prioritize regularization.Important Points For a given number of parameters, deeper networks impose more smoothness than shallow ones. This is because each layer works on the already smooth surface output by the previous layer. Data underspecification can result in overfitted models and must be handled by regularization and more constrained (generally deeper) network architecturesDropoutBagging: This is a method that samples training data and trains several different classifiers Then we classify the test instances with the entire ensemble of classifiers, after which we vote across classifiers for final decision It is empirically shown to improve significantly over training a single classifier from combined dataIn dropout, during training for each input, for every iteration, we turn of each neuron with a probability $1-\\alpha$. In practice, set them to 0 according to the failure of a Bernoulli random number generator with success probability $\\alpha$. Backpropagation is effectively performed only over the remaining network.So each training instance or epoch, essentially sees a different training network.Statistical interpretation: For a network with a total $N$ neurons, there are $2^N$ possible sub-networks obtained by randomly turning on and off the neurons. Since $2^N$ is a very large number, dropout samples over all these possible networks. Then by bagging, we learn a network that averages over all possible networks.Another interpretation: In a rich and dense network, there is nothing stopping the network from learning a pass-through, i.e., just cloning the input to its output. Because of this, we would essentially lose the neuron and maybe a whole layer. Dropout forces the neurons to learn denser patterns.AlgorithmForward Pass: Given inputs $\\mathbf{x} = [x_j, \\quad j = 1 \\ldots D]$ Set: $D_0 = D$, is the width of the $0^{th}$ (input) layer $y_j^{(0)} = x_j, \\quad j = 1 \\ldots D$ $y_0^{(k=1\\ldots N)} = x_0 = 1$ For layers $k = 1 \\ldots N$ Apply dropout mask: $\\text{mask}(k - 1, j) = \\text{Bernoulli}(\\alpha), \\quad j = 1 \\ldots D_{k-1}$ $\\tilde{y}j^{(k-1)} = y_j^{(k-1)} \\cdot \\text{mask}(k - 1, j), \\quad j = 1 \\ldots D{k-1}$ For each neuron $j = 1 \\ldots D_k$:\\[z_j^{(k)} = \\sum_{i=0}^{D_{k-1}} w_{ij}^{(k)} \\cdot \\tilde{y}_i^{(k-1)} + b_j^{(k)}\\]\\[y_j^{(k)} = f_k \\left( z_j^{(k)} \\right)\\] Output: $Y = y_j^{(N)}, \\quad j = 1 \\ldots D_N$Backward Pass: Output layer $(k=N)$:\\[\\frac{\\partial D}{\\partial y_i} = \\frac{\\partial \\text{Div}(Y, d)}{\\partial y_i^{(N)}}\\]\\[\\frac{\\partial \\text{Div}}{\\partial z_i^{(k)}} = f_k' \\left( z_i^{(k)} \\right) \\cdot \\frac{\\partial \\text{Div}}{\\partial y_i^{(k)}}\\] For layers $k = (N-1) \\ldots 0$: For each neuron $i = 1 \\ldots D_k$:\\[\\frac{\\partial \\text{Div}}{\\partial y_i^{(k)}} = \\text{mask}(k, i) \\cdot \\sum_j w_{ij}^{(k+1)} \\cdot \\frac{\\partial \\text{Div}}{\\partial z_j^{(k+1)}}\\]\\[\\frac{\\partial \\text{Div}}{\\partial z_i^{(k)}} = f_k' \\left( z_i^{(k)} \\right) \\cdot \\frac{\\partial \\text{Div}}{\\partial y_i^{(k)}}\\]\\[\\frac{\\partial \\text{Div}}{\\partial w_{ij}^{(k+1)}} = y_i^{(k)} \\cdot \\frac{\\partial \\text{Div}}{\\partial z_j^{(k+1)}}\\] Inference: Given inputs $\\mathbf{x} = [x_j, \\quad j = 1 \\ldots D]$ Set: $D_0 = D$, is the width of the $0^{th}$ (input) layer $y_j^{(0)} = x_j, \\quad j = 1 \\ldots D$ $y_0^{(k=1\\ldots N)} = x_0 = 1$ For layers $k = 1 \\ldots N$ For each neuron $j = 1 \\ldots D_k$:\\[z_j^{(k)} = \\sum_{i=0}^{D_{k-1}} w_{ij}^{(k)} \\cdot {y}_i^{(k-1)} + b_j^{(k)}\\]\\[y_j^{(k)} = \\alpha f_k \\left( z_j^{(k)} \\right)\\] Output: $Y = y_j^{(N)}, \\quad j = 1 \\ldots D_N$Exploding &amp; Vanishing GradientsA Multilayer Perceptron (MLP) with multiple hidden layers is essentially a nested composition of functions:\\[Y = f_N\\left(W_N f_{N-1}\\left(W_{N-1} \\cdots f_1(W_1 X) \\right) \\right)\\]We define the error or divergence on the output prediction as:\\[Div(X) = D\\left(f_N\\left(W_N f_{N-1}\\left(W_{N-1} \\cdots f_1(W_1 X) \\right) \\right)\\right)\\]To perform backpropagation through this nested structure, we compute the gradient of the divergence with respect to the output of each hidden layer:\\[\\nabla_{f_k} Div = \\nabla D \\cdot \\nabla f_N \\cdot W_N \\cdot \\nabla f_{N-1} \\cdot W_{N-1} \\cdots \\nabla f_{k+1} \\cdot W_{k+1}\\]Why Deep Networks are Hard to TrainThe challenge arises because of repeated multiplication of Jacobian matrices during backpropagation: For typical activations like ReLU, sigmoid, tanh: The Jacobian is a diagonal matrix. Each diagonal entry is the derivative of the activation, i.e., $f‚Äô_k(z_i)$\\[\\nabla f_k(z) =\\begin{bmatrix}f_k'(z_1) &amp; 0 &amp; \\cdots &amp; 0 \\\\0 &amp; f_k'(z_2) &amp; \\cdots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; 0 &amp; \\cdots &amp; f_k'(z_n)\\end{bmatrix}\\] These derivatives are typically $&lt;1$ Hence, multiplying by Jacobians shrinks the gradient at each layer So after a few layers, the derivative of the divergence at any time is totally forgottenEffect of weights:Each weight matrix $W_n$ performs a linear transformation of the gradient. Specifically: It scales the gradient in different directions. The amount of scaling is determined by the singular values (or eigenvalues for symmetric matrices) of the matrix.Let‚Äôs say the singular values of a matrix $W_n$ are $\\sigma_1, \\sigma_2, \\ldots, \\sigma_m$ If all $\\sigma_i&lt;1 \\rightarrow$ gradient will shrink in all directions If any $\\sigma_i &gt; 1 \\rightarrow$ gradient may grow in that directionIf many weight matrices have singular values greater than 1, then the gradients will explode.If many weight matrices have singular values lesser than 1, then the gradients will vanish." }, { "title": "Robot Autonomy", "url": "/posts/robot-autonomy-25/", "categories": "CMU MRSD, Robotics", "tags": "manipulators, franka", "date": "2025-03-09 12:00:00 -0400", "snippet": "Adaptive Pixel Art using a Franka ArmDemoMinimum Viable ProductStretch GoalImages Ground Truth Franka Pixel Art Report Your browser does not support PDFs. Please download the report here.Presentation Your browser does not support PDFs. Please download the report here." }, { "title": "Adaptive Pixel Art using a Franka Arm", "url": "/posts/pixel-art-franka/", "categories": "Projects, Robotics", "tags": "manipulators, franka", "date": "2025-03-09 12:00:00 -0400", "snippet": "Adaptive Pixel Art using a Franka ArmDemoMinimum Viable ProductStretch GoalImages Ground Truth Franka Pixel Art Report Your browser does not support PDFs. Please download the report here.Presentation Your browser does not support PDFs. Please download the report here." }, { "title": "Learning for 3D Vision", "url": "/posts/learning-3d-vision-25/", "categories": "CMU MRSD, Robotics", "tags": "computer vision, deep learning, gaussian splatting, nerf, multi-view geometry", "date": "2025-03-09 12:00:00 -0400", "snippet": "Assignment 1: Rendering Basics with PyTorch3DQuestions: Github Assignment 11. Practicing with Cameras1.1 360-degree RendersUsage:python -m submissions.src.render_360 --num_frames 100 --fps 15 --output_file submissions/360-render.gif360-degree gif video that shows many continuous views of the provided cow mesh:1.2 Re-creating the Dolly ZoomUsage:python -m starter.dolly_zoom --num_frames 20 --output_file submissions/dolly.gifMy recreated Dolly Zoom effect:2. Practicing with Meshes2.1 Constructing a TetrahedronUsage:python -m submissions.src.mesh_practice --shape tetrahedron --num_frames 50 --fps 15 --output_file submissions/tetrahedron.gif --image_size 512360-degree gif animation of tetrahedron:Number of vertices = 4 Number of faces = 42.2 Constructing a CubeUsage:python -m submissions.src.mesh_practice --shape cube --num_frames 50 --fps 15 --output_file submissions/cube.gif --image_size 512360-degree gif animation of cube:Number of vertices = 8 Number of faces = 123. Re-texturing a meshChosen colors: color1 = [0, 1, 1] color2 = [1, 1, 0]Usage:python -m submissions.src.retexturing_mesh --num_frames 50 --fps 15 --output_file submissions/retexture_mesh.gif --image_size 512Gif of rendered mesh:4. Camera Transformations$1)$ Rotate about z-axis by -90 degrees:$R_{relative} = [[\\cos(-\\pi/2), -\\sin(-\\pi/2), 0], [\\sin(-\\pi/2), \\cos(-\\pi/2), 0], [0, 0, 1]]$Use original translation matrix: $T_{relative} = [0, 0, 0]$$2)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$Move along z-axis by 2: $T_{relative} = [0, 0, 2]$$3)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$Move along x-axis by 0.5 and along y-axis by -0.5: $T_{relative} = [0.5, -0.5, 0]$$4)$ Rotate along y-axis by 90 degrees: $R_{relative} = [[\\cos(\\pi/2), 0, \\sin(\\pi/2)], [0, 1, 0], [-\\sin(\\pi/2), 0, \\cos(\\pi/2)]]$Move along x-axis by -3 and along z-axis by 3: $T_{relative} = [-3, 0, 3]$5. Rendering Generic 3D Representations5.1 Rendering Point Clouds from RGB-D ImagesUsage:python -m submissions.src.pcl_render --image_size 512Gif of point cloud corresponding to the first image:Gif of point cloud corresponding to the second image:Gif of point cloud formed by the union of the first 2 point clouds:5.2 Parametric FunctionsUsage:python -m submissions.src.torus_render --function parametric --image_size 512 --num_samples 500Parametric equations of Torus:\\[x = (R + r\\cos\\theta)\\cos\\phi \\\\y = (R + r\\cos\\theta)\\sin\\phi \\\\z = r\\sin\\theta\\]where\\(\\theta \\in [0,2\\pi) \\\\\\phi \\in [0,2\\pi)\\)The major radius $R$ is the distance from the center of the tube to the center of the torus and the minor radius $r$ is the radius of the tube360-degree gif of torus, with visible hole:Parametric equations of Superquadric Surface:\\[x = a(\\cos\\theta)^m(\\cos\\phi)^n \\\\y = b(\\cos\\theta)^m(\\sin\\phi)^n \\\\z = c(\\sin\\theta)^m\\]where\\(\\theta \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}] \\\\\\phi \\in [0,2\\pi)\\)360-degree gif of Superquadric Surface:5.3 Implicit SurfacesUsage:python -m submissions.src.torus_render --function implicit --image_size 512Implicit equation of torus:\\[F(X,Y,Z) = (R - \\sqrt{X^2+Y^2})^2 + Z^2 - r^2\\]360-degree gif of torus, with visible hole:Implicit equation of Superquadric Surface:\\[F(X,Y,Z) = \\bigg(\\bigg(\\bigg\\rvert\\frac{X}{a}\\bigg\\rvert \\bigg)^\\frac{2}{n} + \\bigg(\\bigg\\rvert\\frac{Y}{b}\\bigg\\rvert \\bigg)^\\frac{2}{n} \\bigg)^\\frac{n}{m} + \\bigg(\\bigg\\rvert\\frac{Z}{c}\\bigg\\rvert \\bigg)^\\frac{2}{m} - 1\\]360-degree gif of Superquadric Surface:Tradeoffs between rendering as a mesh vs a point cloud: Method of Generation: Point Clouds: Formed by directly sampling a parametric function. Meshes: Built by voxelizing a 3D space, sampling an implicit function, and then extracting surfaces using the Marching Cubes algorithm. Rendering Speed: Point Clouds: Faster to render since they simply use sampled points without extra processing. Meshes: Slower because they need additional steps like voxelization and surface extraction before rendering. Accuracy &amp; Visual Quality: Point Clouds: More accurate at capturing fine details because each point represents a sampled location. However, they don‚Äôt have surfaces, making shading and texturing more difficult. Meshes: Can be less accurate due to voxelization, but increasing the resolution can improve precision. They also provide continuous surfaces, which makes them easier to texture and shade. Computational Efficiency: Point Clouds: Easier to rotate, scale, and modify since they are just a collection of points. Meshes: More computationally expensive to modify because updating a mesh requires adjusting vertex positions and their connections. 6. Do Something FunHere is a 360 degree view of a cottage and also a dolly zoom view:Usage:python -m submissions.src.fun --function full --image_size 512 --output_file submissions/cottage_render_360.gifUsage:python -m submissions.src.fun --function dolly --image_size 512 --output_file submissions/cottage_dolly.gif(Extra Credit) 7. Sampling Points on MeshesAssignment 2: Single View to 3DQuestions: Github Assignment 20. SetupDownloaded the shapenet single-class dataset. Unzipped the dataset and set the appropriate path in dataset_location.py.1. Exploring loss functions1.1. Fitting a voxel gridTo align a predicted voxel grid with a target shape, I used a binary cross-entropy (BCE) loss function. A 3D voxel grid consists of 0 (empty) and 1 (occupied) values, making this a binary classification problem where we predict occupancy probabilities of each voxel.Implementation:def voxel_loss(voxel_src,voxel_tgt):\t# voxel_src: b x h x w x d\t# voxel_tgt: b x h x w x d\tvoxel_src.unsqueeze(1)\tvoxel_tgt.type(dtype=torch.LongTensor)\t\tloss = torch.nn.functional.binary_cross_entropy(voxel_src, voxel_tgt)\treturn lossUsage:python fit_data.py --type 'vox' Ground Truth Optimized Voxel I trained the data for 10000 iterations.1.2. Fitting a point cloudUsage:python fit_data.py --type 'point' Ground Truth Optimized Point Cloud def chamfer_loss(point_cloud_src,point_cloud_tgt):\t# point_cloud_src, point_cloud_src: b x n_points x 3 \tp1 = knn_points(point_cloud_src, point_cloud_tgt)\tp2 = knn_points(point_cloud_tgt, point_cloud_src)\tloss_chamfer = torch.mean(torch.sum(p1.dists + p2.dists))\t\treturn loss_chamfer1.3. Fitting a meshUsage:python fit_data.py --type 'mesh' Ground Truth Optimized Mesh def smoothness_loss(mesh_src):\tloss_laplacian = mesh_laplacian_smoothing(mesh_src)\treturn loss_laplacian2. Reconstructing 3D from single view2.1. Image to voxel grid Input RGB Ground Truth Mesh Ground Truth Voxel Predicted 3D Voxel I implemented the decoder architecture from the paper Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images.For the encoder, I used the pre-trained ResNet-18 model, which computes a set of features for the decoder to recover the 3D shape of the object.The decoder is responsible for transforming information of 2D feature maps into 3D volumes. I specifically implemented a slightly modified version of the Pix2Vox-F architecture from the above paper. The input of the decoder is of size [batch_size x 512] and the output is [batch_size x 32 x 32 x 32]. The decoder contains five 3D transposed convolutional layers. The first four transposed convolutional layers are of kernel size $4^3$, with stride of $2$ and padding of $1$. The last layer has a kernel of size $1^3$. Each transposed convolutional layer is followed by a LeakyReLU activation function, except for the last layer which is followed by a sigmoid activation function. The number of output channels for each layer follows the Pix2Vox-F configuration: 128 -&gt; 64 -&gt; 32 -&gt; 8 -&gt; 1.I trained the model for 10000 iterations, with the default batch size of 32 and learning rate of 4e-4.Usage:python train_model.py --type 'vox' --max_iter 10000 --save_freq 500 python eval_model.py --type 'vox' --load_checkpoint2.2. Image to point cloud Input RGB Ground Truth Mesh Ground Truth Point Cloud Predicted 3D Point Cloud I used an approach similar to the Pix2Vox-F decoder that I implemented above. The ResNet-18 model encodes the input images into feature maps, and a decoder reconstructs the 3D shape of the object from them.The decoder takes in an input of size [batch_size x 512] and gives an output of size [batch_size x n_points x 3]. The decoder architecture comprises of 4 fully connected layers, three of which are followed by a LeakyReLU activation function.I trained the model for 2000 iterations, with the default batch size of 32 and learning rate of 4e-4.Usage:python train_model.py --type 'point' --max_iter 2000 --save_freq 500 --n_points 5000 python eval_model.py --type 'point' --load_checkpoint --n_points 50002.3. Image to mesh Input RGB Ground Truth Mesh Predicted Mesh Instead of encoding an image like I did in case of image to voxel and image to point cloud, the meshes are constructed from an icosphere mesh. The purpose of the decoder is to refine this initial mesh by giving per-vertex displacement vector as an ouput.The decoder architecture that I implemented is very similar to that in case of image to point cloud, as mentioned above. It takes an input of size [batch_size x 512] and gives an output of size [batch_size x num_vertices x 3]. It comprises of 4 fully connected layers, three of which are followed by a ReLU activation function.I trained the model for 2000 iterations, with the default batch size of 32, learning rate of 4e-4.Usage:python train_model.py --type 'mesh' --max_iter 2000 --save_freq 500python eval_model.py --type 'mesh' --load_checkpoint2.4. Quantitative comparisionsF1-score curves at different thresholds: Voxel Grid Point Cloud Mesh From the above plots, we can infer that the point cloud model performed the best, giving the highest F1-score, followed by the mesh model and the voxel model.Intuitively, I think the reason the point cloud outperformed the voxel and mesh models is because it aligns well with the evaluation method, which compares points directly from the network output to the ground truth. Since point clouds don‚Äôt need to define surfaces or connections, they are more flexible and avoid errors caused by surface sampling. This makes them easier to optimize and more accurate in reconstruction.The mesh model performed slightly worse primarily due to the challenges associated with sampling points from a continuous surface. Unlike point clouds, where each output point is directly predicted, meshes require proper face orientation and connectivity. Due to this, the sampled points might not always align perfectly with the ground truth, especially when dealing with complex geometries like thin structures (legs of the chair).The voxel model has the lowest F1-score because representing 3D space as voxels limits a lot of detail and accuracy. Fine details can be lost due to this fixed resolution and sampled points may not always align perfectly with the object‚Äôs actual surface, affecting evaluation results.2.5. Analyse effects of hyperparams variationsI have chosen to vary the w_smooth hyperparameter and analyze the changes in the mesh model prediction. The default value of w_smooth is 0.1, and its results have been shown in Section 2.3. I sampled 4 other values of w_smooth - 0.001, 0.01, 1, 10. Input RGB Ground Truth Mesh w_smooth=0.001 w_smooth=0.01 w_smooth=0.1 (Default) w_smooth=1 w_smooth=10 F1-score curves for different variations in w_smooth for the mesh model: w_smooth=0.001 w_smooth=0.01 w_smooth=0.1 (Default) w_smooth=1 w_smooth=10 Avg F1-score@0.05 = 72.977 Avg F1-score@0.05 = 72.133 Avg F1-score@0.05 = 70.951 (Default) Avg F1-score@0.05 = 71.834 Avg F1-score@0.05 = 72.337 For low values of w_smooth = 0.001, 0.01: They preserve the fine details but introduce noise and distortions It results in rough and fragmented surfaces They show a slightly higher F1-score because they retain geometric detailsFor high values of w_smooth = 1, 10: They produce cleaner and more smooth meshes with reduced artifacts It over-smooths the surface, causing loss of sharp details The F1-score improves slightly likely because they more likely fall within the threshold radius of the ground truthThe default value of w_smooth = 0.1 falls in between the above two categories.2.6. Interpret your modelPer-Point Error Visualization Input RGB Ground Truth Point Cloud Predicted 3D Point Cloud Per-Point Error I used per-point error to gauge how well each predicted 3D point matches its corresponding point in the ground truth. In my approach, I compute the distance between each point in my reconstructed point cloud and its nearest neighbor in the ground truth. Then, I color-code these distances such that points with very small errors appear in cool colors (blue), while those with larger errors show up in warm colors (red).From the above gifs, we can clearly see that some regions are rendered in cool tones, which tells me that my model is accurately capturing those parts of the object, such as the seat surface or the main body of the chair. On the other hand, areas highlighted in warm colors reveal where the model struggles, like along the thin chair legs or at complex curves of the backrest.This visualization pinpoints the exact regions that need improvement.Failure Case AnalysisIn analyzing my 3D voxel model‚Äôs predictions, I noticed that while it reconstructs the backrest of chairs quite well, it struggles significantly with legs, seats, and unusual shapes. These failure cases provide valuable insight into the model‚Äôs learning behavior and what its limitations are. Legs:Chair legs vary widely in shape, thickness, and placement across different samples in the dataset. Some chairs have four standard legs, while others may have a single central support or a complex curved base. Because the model tries to generalize patterns across the dataset, it struggles to reconstruct legs consistently. Additionally, legs are usually thin and small compared to the rest of the chair, and this makes them more prone to voxelization errors. Seats:Many chair designs have gaps in them, which makes it challenging for the model to learn and also more prone to voxelization errors. Since the model tries to reconstruct a smoothed version of objects, it often fails to represent holes correctly, either closing them off entirely or introducing unexpected artifacts. Unusual shapes:Some chairs in the dataset have very unique designs. Since the model is trained on a limited dataset, it may not have seen enough similar examples to generalize well. 3. Exploring other architectures / datasets3.3 Extended dataset for trainingdataset_location.py updated to include the extended dataset containing three classes (chair, car, and plane). category #instances airplane 36400 car 31460 chair 61000 total 128860 I trained and evaluated the point cloud model with n_points = 5000.Quantitative evaluation: Point Cloud trained on one class Point Cloud trained on three classes Qualitative evlautaion by comparing ‚Äútraining on one class‚Äù vs ‚Äútraining on three classes‚Äù: Input RGB Ground Truth Point Cloud Predicted 3D Point Cloud for 1 Class Training Predicted 3D Point Cloud for 3 Classes Training 3D consistency and diversity of output samples:Training the model on a single class, like chairs, results in more consistent and refined reconstructions. Since the model only sees one object type during training, it gets really good at capturing the details and structure unique to that class. However, this also means that the model becomes highly specialized. So when it is faced with a completely new object type (such as an airplane or car), it struggles because it hasn‚Äôt learned to handle the variation.On the other hand, training on multiple classes (airplanes, cars, and chairs) allows the model to adapt better to different object shapes. Instead of focusing on one type, it learns general patterns that apply across different categories. This makes it more versatile when reconstructing new objects.So in conclusion, single-class models tend to produce more uniform outputs because they have learned a very specific structural representation but lack adaptability. Multi-class models generate more diverse outputs because they have seen various object types and have learned to adapt to different shapes but at the cost of some fine-grained details.Assignment 3: Part-1 Neural Volume RenderingQuestions: Github Assignment 30. Transmittance Calculation1. Differentiable Volume Rendering1.3. Ray samplingUsage:python volume_rendering_main.py --config-name=box1.4. Point samplingUsage:python volume_rendering_main.py --config-name=box1.5. Volume renderingUsage:python volume_rendering_main.py --config-name=box2. Optimizing a basic implicit volume2.1. Random ray samplingdef get_random_pixels_from_image(n_pixels, image_size, camera): xy_grid = get_pixels_from_image(image_size, camera) # Random subsampling of pixel coordinaters xy_grid_sub = xy_grid[np.random.choice(xy_grid.shape[0], n_pixels)].to(\"cuda\") return xy_grid_sub.reshape(-1, 2)[:n_pixels]2.2. Loss and trainingloss = torch.nn.functional.mse_loss(rgb_gt, out['feature'])Usage:python volume_rendering_main.py --config-name=train_boxBox center: (0.2502, 0.2506, -0.0005) Box side lengths: (2.0051, 1.5036, 1.5034)2.3. VisualizationUsage:python volume_rendering_main.py --config-name=train_box3. Optimizing a Neural Radiance Field (NeRF)Usage:python volume_rendering_main.py --config-name=nerf_lego4. NeRF Extras4.1 View DependenceUsage:python volume_rendering_main.py --config-name=nerf_materialsTrade-offs between increased view dependence and generalization quality: Adding view dependence allows the model to capture complex lighting effects like reflections, and translucency. But excessive view dependence can create inconsistencies when interpolating between unique views, which will make the rendering look unnatural. If the network heavily relies on viewing direction, it may overfit to the specific camera angles in the training data. This can lead to poor generalization to unseen viewpoints. It can increase the network‚Äôs complexity, requiring more parameters and training time.Assignment 3: Part-2 Neural Surface Rendering5. Sphere TracingMy implementation of the SphereTracingRenderer class uses the sphere tracing algorithm to find intersections between rays and an implicit surface of a torus defined by a signed distance field. The algorithm iteratively updates points along each ray by moving in the direction of the ray by the amount of the SDF value at the current point. This process continues until the maximum number of iterations is reached or the SDF value becomes very close to zero (threshold of 1e-6), indicating a surface intersection.Usage:python -m surface_rendering_main --config-name=torus_surface6. Optimizing a Neural SDFUsage:python -m surface_rendering_main --config-name=points_surface Input lr=0.0001 lr=0.001 lr=0.00001 Loss 0.001279 0.000428 0.001635 eikonal_loss = ((gradients.norm(2, dim=1) - 1.0) ** 2).mean()Implementation:Input (XYZ points) -&gt; Harmonic Embedding -&gt; Layer 1 (Linear + ReLU) -&gt; Layer 2 (Linear + ReLU) -&gt; Layer 3 (Linear + ReLU) -&gt; ... -&gt; Layer N (Linear + ReLU) -&gt; Linear SDF (Output: Signed Distance Function)7. VolSDFUsage:python -m surface_rendering_main --config-name=volsdf_surface Alpha: Scales the overall density. A higher value increases the density, while a lower value reduces it. Beta: Controls how quickly the density changes with distance from the surface. A smaller beta results in a sharper transition, while a larger beta smooths the transition.def sdf_to_density(signed_distance, alpha, beta): return torch.where( signed_distance &gt; 0, 0.5 * torch.exp(-signed_distance / beta), 1 - 0.5 * torch.exp(signed_distance / beta), ) * alpha Geometry Result Loss 0.006958 The above renders are for values alpha=10.0 and beta=0.05.When alpha=10.0 and beta is changed: beta beta=0.05 beta=0.1 beta=0.5 Geometry Render Loss 0.006958 0.010227 0.020789 When beta=0.05 and alpha is changed: alpha alpha=1 alpha=10 alpha=50 Geometry Render Loss 0.022317 0.006958 0.004329 How does high beta bias your learned SDF? What about low beta?High beta makes the transition between occupied and free space more gradual, leading to a smoother SDF. This can cause a bias where surfaces appear more diffused rather than sharp.Low beta results in a sharper transition, meaning the SDF will be more precise in distinguishing surfaces, but it can also lead to unstable gradients and more difficult optimization.Would an SDF be easier to train with volume rendering and low beta or high beta? Why?An SDF is generally easier to train with volume rendering when using a high beta. This is because high beta values cause a larger number of points along each ray to have non-zero density, allowing gradients to be backpropagated through more points simultaneously. This leads to denser gradients and faster convergence during training.Training with a low beta can be more challenging because it forces the network to learn very sharp transitions, which means only points very close to the surface contributes significantly to the rendering. This can lead to sparse gradients and slower convergence.Would you be more likely to learn an accurate surface with high beta or low beta? Why?You are more likely to learn an accurate surface with a low beta. A low beta encourages sharp boundaries and a more precise surface representation, as the density function closely approximates a step function. High beta values, on the other hand, lead to smoother surfaces, which can be less accurate.Implementation:Input (SDF Feature + XYZ Embedding) -&gt; Layer 1 (Linear + ReLU) -&gt; Layer 2 (Linear + ReLU) -&gt; Layer 3 (Linear + ReLU) -&gt; ... -&gt; Layer N (Linear + ReLU) -&gt; Linear RGB (Output: 3D Color Prediction) 8. Neural Surface Extras8.3 Alternate SDF to Density ConversionsLogistic density distribution function:\\[\\phi_s(x) = \\frac{se^{-sx}}{(1+e^{-sx})^2}\\]def neus_sdf_to_density(signed_distance, s): return s * torch.exp(-s * signed_distance) / ((1 + torch.exp(-s * signed_distance))**2) s s=10 s=50 Geometry Render Loss 0.005590 0.006529 Low s results in a more blurry render, while a higher value of s makes it look sharp.Assignment 4Questions: Github Assignment 41. 3D Gaussian Splatting1.1 3D Gaussian RasterizationRun:python render.py --gaussians_per_splat 1024Output GIF:1.2 Training 3D Gaussian RepresentationsRun:python train.py --gaussians_per_splat 2048 --device cudaI modified the run_training function to improve performance and reduce CUDA memory usage. Specifically, I: Reduced the number of Gaussians from 10k to 7k by subsampling the points file, lowering the overall GPU memory footprint. Ensured key Gaussian parameters (opacities, scales, colours, and means) were trainable and set up an optimizer with different learning rates for each parameter group (as mentioned in 1.2.1) Wrapped the forward pass in autocast and used GradScaler to scale the loss, which both reduced memory usage and accelerated computation I called the scene.render method to generate the predicted image from the input camera parameters, using the specified image size, background color, and the number of Gaussians per splat. I then computed the L1 loss between the rendered (predicted) image and the ground truth image.Learning rates that I used for each parameter: pre_act_opacities = 0.001 pre_act_scales = 0.001 colours = 0.02 means = 0.0002Number of iterations that I trained the model for = 1000Mean PSNR: 27.356Mean SSIM: 0.915Training final render GIF:Training progress GIF:1.3 Extensions1.3.1 Rendering Using Spherical HarmonicsRun:python render.py --gaussians_per_splat 1024GIF: Original Spherical Harmonics RGB image comparisons of the renderings obtained from both the cases: Frame Original Spherical Harmonics 000 015 021 030 Differences that can be observed: Frame 000 and 015: The spherical harmonics rendering looks more photorealistic because the angular variations in color better capture how the material responds to illumination from different directions. Frames 021 and 030: The spherical harmonics rendering looks more glossy and reflective because the added directional sensitivity leads to more dynamic and detailed shading.2. Diffusion-guided Optimization2.1 SDS Loss + Image OptimizationRun:python Q21_image_optimization.py --sds_guidance 1 Prompt Without Guidance With Guidance Iterations 400 2000 ‚Äúa hamburger‚Äù ‚Äúa standing corgi dog‚Äù ‚Äúa fish in a pan‚Äù ‚Äúa mansion‚Äù 2.2 Texture Map Optimization for MeshRun:python python Q22_mesh_optimization.pyIn order to reduce the CUDA memory footprint, I reduced the image size to 256x256. Prompt Initial Mesh GIF Final Mesh GIF ‚Äúa tiger‚Äù ‚Äúa zebra‚Äù 2.3 NeRF OptimizationI perfomed no CUDA memory optimization here. All values were the same as default as pulled from GitHub.Prompt: ‚Äúa standing corgi dog‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a standing corgi dog\" --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa hamburger‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a hamburger\" --iters 2500 --lambda_entropy 1e-3 --lambda_orient 1e-2 --latent_iter_ratio 0.2 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa mansion‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a mansion\" --iters 2500 --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.2.4 Extensions2.4.1 View-dependent text embeddingPrompt: ‚Äúa standing corgi dog‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a standing corgi dog\" --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 --view_dep_text 1Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa hamburger‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a hamburger\" --iters 2500 --lambda_entropy 1e-3 --lambda_orient 1e-2 --latent_iter_ratio 0.2 --view_dep_text 1Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Comparing with 2.3:In 2.3, the NeRF model used one fixed text embedding for all views. This led to consistent but somewhat flat results and sometimes produced artifacts like multiple front faces (standing corgi dog example) because the model didn‚Äôt know how to adjust for different angles.With view-dependent text conditioning, different embeddings (like front, side, and back) are used based on the camera angle. This helps the model adjust lighting, highlights, and reflections for each view, resulting in more realistic and 3D-consistent images.The ‚Äúhamburger‚Äù looks really good with view-dependent conditioning, while the ‚Äústanding corgi dog‚Äù did not turn out as well, likely because it needs more training to capture all its details.Assignment 5Questions: Github Assignment 51. Classification ModelRun:python train.py --task clspython eval_cls.pyThe model was trained for 250 epochs, and the best model was saved at epoch 115.Train loss = 1.1288Test accuracy of best model = 0.9696Visualization of a few random test point clouds and their predicted classes: Correct Prediction Chairs Vases Lamps Point Cloud Point Cloud Visualization of a failure prediction for each class: Correct Label Vase Lamp Vase Point Cloud Incorrect Prediction Lamp Vase Lamp The successful results show that the model is able to pick out important features. For example, it correctly identifies the distinct structure of chairs, vases, and lamps. But in some cases, the shapes can be ambiguous or similar, which causes the model to misclassify objects. This can happen when the point sampling misses some important details or when the features are too similar between classes.2. Segmentation ModelRun:python train.py --task segpython eval_seg.pyThe model was trained for 250 epochs, and the best model was saved at epoch 225.Train loss = 13.0442Test accuracy of best model = 0.8966Visualization of correct segmentation predictions along with their corresponding ground truth: Accuracy 0.9317 0.9147 0.9308 Ground Truth Good Segmentation Predictions Visualization of incorrect segmentation predictions along with their corresponding ground truth: Accuracy 0.5604 0.5572 0.4702 Ground Truth Bad Segmentation Predictions The good segmentation predictions show that the model can distinguish the chair‚Äôs seat, back, and legs fairly accurately. In the failure cases, we can see that certain parts like the seat or back merges with the legs or other regions. A possible reason could be that if the training data includes ambiguous or poorly differentiated boundaries, the model may struggle to learn to differentiate segments in these areas.3. Robustness Analysis3.1 Rotate Input Point Clouds3.1.1 Classification ModelVisualization of a few random test point clouds and their predicted classes: Angle Accuracy Chairs Vases Lamps 10 degrees 0.958 Successful Successful Successful 10 degrees 0.958 Successful Successful Successful 20 degrees 0.894 Successful Successful Successful 20 degrees 0.894 Wrong predicted as lamp Successful Successful 30 degrees 0.679 Successful Successful Successful 30 degrees 0.679 Wrong predicted as lamp Wrongly predicted as lamp Successful 60 degrees 0.299 Wrongly predicted as vase Successful Wrongly predicted as chair 60 degrees 0.299 Wrong predicted as lamp Wrongly predicted as lamp Successful From the observations above, we see that the model classifies most rotated objects correctly when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), the accuracy drops and the model starts confusing objects. This happens because the model wasn‚Äôt trained with any rotational variations and so the learned features become orientation-dependent.3.1.2 Segmentation Model Angle Model X X X X 10 degrees Ground Truth 10 degrees Prediction Success: 0.9185 Success: 0.9134 Success: 0.9055 Failure: 0.4798 20 degrees Ground Truth 20 degrees Prediction Success: 0.9396 Success: 0.9191 Success: 0.9307 Failure: 0.5906 30 degrees Ground Truth ¬† 30 degrees Prediction Success: 0.9093 Success: 0.9429 Failure: 0.4707 ¬† 60 degrees Ground Truth ¬† 60 degrees Prediction Failure: 0.4707 Failure: 0.5283 Failure: 0.5478 ¬† From the observations above, we see that the model segments most portions of the rotated objects well when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), we see a massive accuracy drop. This happens because the model wasn‚Äôt trained with any rotational variations and so the learned features become orientation-dependent. The rotations make it difficult for the network to correctly differentiate between segments.3.2 Different Number of Point Points per Object3.2.1 Classification Model Number of Points Accuracy Chairs Vases Lamps 5000 0.968 Successful Successful Successful 1000 0.9695 Successful Successful Successful 500 0.9622 Successful Successful Successful 100 0.882 Successful Successful Successful We see that the classification model performs well even with fewer points. The drop in accuracy when the number of points is reduced to 100 or below is expected because fewer points reduce geometric detail, which makes it harder to distinguish specific features. Still, even at 100 points, the outline structure is preserved, allowing the model to classify successfully in many cases.3.2.2 Segmentation Model Number of Points Model X X X X 5000 Ground Truth 5000 Prediction Success: 0.9448 Success: 0.992 Success: 0.9658 Failure: 0.5366 1000 Ground Truth 1000 Prediction Success: 0.952 Success: 0.988 Success: 0.904 Failure: 0.515 500 Ground Truth 500 Prediction Success: 0.934 Success: 0.998 Success: 0.926 Failure: 0.484 100 Ground Truth 100 Prediction Success: 0.92 Success: 0.96 Success: 0.91 Failure: 0.58 We see that the segmentation model performs well even with fewer points. While the model performs well even with 500 points, the performance becomes more unstable at 100 points, especially in complex or ambiguous regions. This is because fewer points reduce the structural detail, making it harder for the model to distinguish fine boundaries and specific object features like thin legs and armrests.4. Bonus Question - LocalityI implemented a general Transformer framework for both the classification model and the segmentation model.The classification model predicts a single label for an entire point cloud. Each point‚Äôs 3D coordinates are passed through two linear layers: one to embed the input features, and another to learn positional information. These two embeddings are added together and fed into a standard Transformer Encoder. After processing, I apply max pooling across all points to extract a global feature vector. This is then passed through fully connected layers with batch normalization, ReLU activation, and dropout. The log softmax over the class scores is then returned.The segmentation model /assets/images/L3D/a5/outputs a class label for each point. I embed the 3D points and add positional information before passing them through a Transformer Encoder. Instead of pooling the features, I use 1D convolution layers to generate per-point predictions. These /assets/images/L3D/a5/outputs are also passed through a log softmax to get per-point class probabilities.4.1 Classification Model using TransformersRun:python train.py --task clspython eval_cls.py --load_checkpoint best_model --/assets/images/L3D/a5/output_dir \".//assets/images/L3D/a5/output/Q4/cls\"The model was trained for 100 epochs, and the best model was saved at epoch 18.Train loss = 30.6177Test accuracy of best model = 0.9454Visualization of a few random test point clouds and their predicted classes using Transformers: Correct Prediction Chairs Vases Lamps Point Cloud Point Cloud Visualization of a failure prediction for each class using Transformers: Correct Label Vase Lamp Lamp Point Cloud Incorrect Prediction Lamp Vase Chair The model was run only for 100 epochs because it was taking a lot of time. I also did not have the time to run the segmentation Transformer model. However, there were no errors in starting the training and it only had to be left running for ~8 hours on my laptop.Moreover, the Transformer model gave a very high accuracy of 94% for just training for 100 epochs. The classification model without Transformer gave an accuracy of 97% for training for 250 epochs." }, { "title": "Introduction to Robotics Business", "url": "/posts/intro-robotics-business-25/", "categories": "CMU MRSD, Robotics", "tags": "business, construction, startup", "date": "2025-03-09 12:00:00 -0400", "snippet": "Company Analysis: Advanced Construction Robotics Your browser does not support PDFs. Please download the report here.Patent Analysis Your browser does not support PDFs. Please download the report here." }, { "title": "Coursework at CMU", "url": "/posts/cmu-blog/", "categories": "Blog, Robotics", "tags": "cmu, robotics, masters", "date": "2025-02-06 11:00:00 -0500", "snippet": "This blog serves as a comprehensive collection of notes, assignments and insights from my coursework in the Master‚Äôs in Robotic Systems Development program at Carnegie Mellon University.Links will redirect to CMU MRSD.Spring 2026 Introduction to Reinforcement Learning Deep Reinforcement Learning Robot Learning Multi-Modal Machine LearningFall 2025 Introduction to Deep Learning Visual Learning and Recognition Planning and Decision Making in Robotics: Dividing this into 3 sections Basics and Sampling-Based Motion Planning All About Search Algorithms Case Studies of Planning and Decision-Making in Robotics Advanced Robotics Business - Would love to share more on this, but my team and I would like to gatekeep our startup idea :)Spring 2025 Learning for 3D Vision Robot Autonomy Introduction to Robotics BusinessFall 2024 Manipulation, Estimation, and Control Robot Mobility on Air, Land, and Sea Introduction to Computer Vision Systems Engineering and Project Management for Robotics" }, { "title": "Micro-ROS, RCL, & RCLC", "url": "/posts/micro-ros/", "categories": "Blog, Robotics", "tags": "mujoco, manipulation, ik, fk", "date": "2025-02-01 11:00:00 -0500", "snippet": "Micro-ROS Publisher#include &lt;micro_ros_arduino.h&gt; #include &lt;stdio.h&gt;#include &lt;rcl/rcl.h&gt;#include &lt;rcl/error_handling.h&gt;#include &lt;rclc/rclc.h&gt;#include &lt;rclc/executor.h&gt;#include &lt;std_msgs/msg/int32.h&gt;rcl_publisher_t publisher;std_msgs__msg__Int32 msg;rclc_executor_t executor;rclc_support_t support;rcl_allocator_t allocator;rcl_node_t node;rcl_timer_t timer;#define LED_PIN 13#define RCCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){error_loop();}}#define RCSOFTCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){}}void error_loop(){ while(1){ digitalWrite(LED_PIN, !digitalRead(LED_PIN)); delay(100); }}void timer_callback(rcl_timer_t * timer, int64_t last_call_time){ RCLC_UNUSED(last_call_time); if (timer != NULL) { RCSOFTCHECK(rcl_publish(&amp;publisher, &amp;msg, NULL)); msg.data++; }}void setup() { set_microros_transports(); pinMode(LED_PIN, OUTPUT); digitalWrite(LED_PIN, HIGH); delay(2000); allocator = rcl_get_default_allocator(); //create init_options RCCHECK(rclc_support_init(&amp;support, 0, NULL, &amp;allocator)); // create node RCCHECK(rclc_node_init_default(&amp;node, \"micro_ros_arduino_node\", \"\", &amp;support)); // create publisher RCCHECK(rclc_publisher_init_default( &amp;publisher, &amp;node, ROSIDL_GET_MSG_TYPE_SUPPORT(std_msgs, msg, Int32), \"micro_ros_arduino_node_publisher\")); // create timer, const unsigned int timer_timeout = 1000; RCCHECK(rclc_timer_init_default( &amp;timer, &amp;support, RCL_MS_TO_NS(timer_timeout), timer_callback)); // create executor RCCHECK(rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator)); RCCHECK(rclc_executor_add_timer(&amp;executor, &amp;timer)); msg.data = 0;}void loop() { delay(100); RCSOFTCHECK(rclc_executor_spin_some(&amp;executor, RCL_MS_TO_NS(100)));}#include &lt;micro_ros_arduino.h&gt;#include &lt;rcl/rcl.h&gt;#include &lt;rcl/error_handling.h&gt;#include &lt;rclc/rclc.h&gt;#include &lt;rclc/executor.h&gt;#include &lt;std_msgs/msg/int32.h&gt; micro_ros_arduino.h: Includes Micro-ROS Arduino-specific functions. rcl/rcl.h and rclc: Provides core ROS client library (RCL) and RCLC utilities. std_msgs/msg/int32.h: Includes the ROS std_msgs::msg::Int32 message type for publishing integers.rcl_publisher_t publisher;std_msgs__msg__Int32 msg;rclc_executor_t executor;rclc_support_t support;rcl_allocator_t allocator;rcl_node_t node;rcl_timer_t timer; executor: Manages callbacks (like timers). support: Manages initialization options for Micro-ROS. allocator: Allocates memory for the Micro-ROS framework. node: A Micro-ROS node (basic ROS entity that performs tasks). timer: Periodically triggers the timer_callback.#define RCCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){error_loop();}}#define RCSOFTCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){}} RCCHECK: Ensures critical operations succeed. If an error occurs, it enters the error_loop. RCSOFTCHECK: Similar, but doesn‚Äôt halt execution on failure.void error_loop(){ while(1){ digitalWrite(LED_PIN, !digitalRead(LED_PIN)); delay(100); }} Blinks the onboard LED continuously to indicate an error.void timer_callback(rcl_timer_t * timer, int64_t last_call_time){ RCLC_UNUSED(last_call_time); if (timer != NULL) { RCSOFTCHECK(rcl_publish(&amp;publisher, &amp;msg, NULL)); msg.data++; }} This function is called each time the timer triggers. Increments msg.data and publishes it to the topic.void setup() { set_microros_transports(); // Initialize communication (e.g., Serial, WiFi, etc.) pinMode(LED_PIN, OUTPUT); // Set LED_PIN as output. digitalWrite(LED_PIN, HIGH); delay(2000); // Wait for 2 seconds for initialization. allocator = rcl_get_default_allocator(); // Set up the memory allocator. // Initialize Micro-ROS support and components RCCHECK(rclc_support_init(&amp;support, 0, NULL, &amp;allocator)); RCCHECK(rclc_node_init_default(&amp;node, \"micro_ros_arduino_node\", \"\", &amp;support)); RCCHECK(rclc_publisher_init_default( &amp;publisher, &amp;node, ROSIDL_GET_MSG_TYPE_SUPPORT(std_msgs, msg, Int32), \"micro_ros_arduino_node_publisher\")); const unsigned int timer_timeout = 1000; // Timer interval in milliseconds. RCCHECK(rclc_timer_init_default( &amp;timer, &amp;support, RCL_MS_TO_NS(timer_timeout), timer_callback)); RCCHECK(rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator)); RCCHECK(rclc_executor_add_timer(&amp;executor, &amp;timer)); msg.data = 0; // Initialize the message value.}Key setup steps: Transport Setup: Configures the communication method (e.g., serial or WiFi). Micro-ROS Initialization: Initializes the support structure. Creates a node (\"micro_ros_arduino_node\"). Sets up the publisher for the topic \"micro_ros_arduino_node_publisher\". Timer Setup: Calls timer_callback every second (1000 ms). Executor Setup: Manages the timer callback execution. Message Initialization: Starts with msg.data = 0.void loop() { delay(100); // Short delay. RCSOFTCHECK(rclc_executor_spin_some(&amp;executor, RCL_MS_TO_NS(100)));} The executor handles the timer callback, ensuring timer_callback is executed as needed.RCLROS Client LibraryCommon Types and Return Codes rcl_ret_t : The type that holds an rcl return code Codes: RCL_RET_OK: Success return code RCL_RET_ERROR: Unspecified error return code RCL_RET_NOT_INIT: rcl_init()¬†not yet called return code RCL_RET_ Allocatorallocator.h rcl_allocator_t : Encapsulation of an allocator RCL_CHECK_ALLOCATOR : Check that the given allocator is initialized. If the allocator is not initialized, run the fail_statement RCL_CHECK_ALLOCATOR_WITH_MSG(allocator,¬†msg,¬†fail_statement) : Check that the given allocator is initialized, or fail with a message. If the allocator is not initialized, set the error to msg, and run the fail_statement rcl_get_default_allocator : Return a properly initialized rcl_allocator_t with default valuesrcl_allocator_t allocator = rcl_get_default_allocator();if (!allocator.allocate) { printf(\"Allocator not initialized\\n\");}Timertimer.h rcl_timer_t: Structure which encapsulates a ROS Timer Macros: RCL_MS_TO_NS : Convenience macro to convert milliseconds to nanoseconds rcl_timer_init: Initializes a timer with a callback rcl_ret_t rcl_timer_init( rcl_timer_t *timer, rcl_clock_t *clock, rcl_context_t *context, int64_t period, const rcl_timer_callback_t callback, rcl_allocator_t allocator ) Publisherpublisher.h rcl_publisher_t : Structure which encapsulates a ROS Publisher Initialization: rcl_ret_t rcl_publisher_init( rcl_publisher_t *publisher, const rcl_node_t *node, const rosidl_message_type_support_t *type_support, const char *topic_name, const rcl_publisher_options_t *options ) Publish messages: rcl_ret_t rcl_publish( const rcl_publisher_t *publisher, const void *ros_message, rmw_publisher_allocation_t *allocation ) Stop publisher: After calling, the node will no longer be advertising that it is publishing on this topic (assuming this is the only publisher on this topic) rcl_ret_t rcl_publisher_fini( \t\tconst rcl_publisher_t *publisher, \t\tconst rcl_node_t *node ) Nodenode.h rcl_node_t : Structure which encapsulates a ROS Node rcl_node_impl_t rcl_service_t Initialization: rcl_ret_t rcl_node_init( \t\trcl_node_t *node, \t\tconst char *name, \t\tconst char *namespace_, \t\trcl_context_t *context, \t\tconst rcl_node_options_t *options ) rcl_ret_t rcl_node_fini(rcl_node_t¬†*node): Destroys any automatically created infrastructure and deallocates memory. After calling, the rcl_node_t can be safely deallocatedRCLCRCLCSupport RCLC_UNUSED rclc_support_t : Initializes RCL and creates foundational data structures rclc_support_init: Initializes rcl and creates some support data structures. Initializes clock as RCL_STEADY_TIME rcl_ret_t rclc_support_init( \t\trclc_support_t *support, \t\tint argc, \t\tchar const *const *argv, \t\trcl_allocator_t *allocator ) Node rcl_node_t : Structure which encapsulates a ROS Node rclc_node_init_default: Creates a default RCL node rcl_ret_t rclc_node_init_default( \t\trcl_node_t *node, \t\tconst char *name, \t\tconst char *namespace_, \t\trclc_support_t *support ) Timer rcl_timer_t : Structure which encapsulates a ROS Timer rclc_timer_init_default: Creates an rcl timer rcl_ret_t rclc_timer_init_default( \t\trcl_timer_t *timer, \t\trclc_support_t *support, \t\tconst uint64_t timeout_ns, \t\tconst rcl_timer_callback_t callback ) Executor rclc_executor_t: Handles callback execution for nodes, timers, and other handles Initialization: rcl_ret_t rclc_executor_init( \t\trclc_executor_t *executor, \t\trcl_context_t *context, \t\tconst size_t number_of_handles, \t\tconst rcl_allocator_t *allocator ) Add Timer to Executor: Adds time to an executor rcl_ret_t rclc_executor_add_timer( \t\trclc_executor_t *executor, \t rcl_timer_t *timer ) Publisher rclc_publisher_init_default: Creates an rcl publisher rcl_ret_t rclc_publisher_init_default( \t\trcl_publisher_t *publisher, \t\tconst rcl_node_t *node, \t\tconst rosidl_message_type_support_t *type_support, \t\tconst char *topic_name ) Quick Initialization Flow Support Initialization: rclc_support_t support; rclc_support_init(&amp;support, argc, argv, &amp;allocator); Node Initialization: rcl_node_t node; rclc_node_init_default(&amp;node, \"node_name\", \"namespace\", &amp;support); Publisher Creation: rcl_publisher_t publisher; rclc_publisher_init_default(&amp;publisher, &amp;node, type_support, \"topic_name\"); Timer and Executor: rcl_timer_t timer; rclc_timer_init_default(&amp;timer, &amp;support, RCL_MS_TO_NS(1000), my_callback); rclc_executor_t executor; rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator); rclc_executor_add_timer(&amp;executor, &amp;timer); " }, { "title": "MuJoCo Basics", "url": "/posts/mujoco-basics/", "categories": "Blog, Robotics", "tags": "mujoco, manipulation, ik, fk", "date": "2025-02-01 11:00:00 -0500", "snippet": "Overview of mjModel and mjData mjModel: Holds static information about the simulation model (e.g., geometry, joint limits, sensor definitions). Think of it as a blueprint for the robot/environment. mjData: Holds dynamic state information for the simulation (e.g., joint positions, velocities, applied forces). It tracks how the simulation evolves over time.mjModel Function Description Example Usage mj_name2id Converts a name (e.g., of a joint, body, or sensor) to its corresponding ID. joint_id = mj_name2id(model, mjOBJ_JOINT, \"joint1\") mj_id2name Converts an ID to its corresponding name. joint_name = mj_id2name(model, mjOBJ_JOINT, joint_id) mj_getGeomId Retrieves the ID of a geometry by name. geom_id = mj_getGeomId(model, \"geom1\") mj_getBodyId Retrieves the ID of a body by name. body_id = mj_getBodyId(model, \"robot_body\") mj_getJointId Retrieves the ID of a joint by name. joint_id = mj_getJointId(model, \"joint1\") mj_getSiteId Retrieves the ID of a site by name. site_id = mj_getSiteId(model, \"end_effector_site\") mj_printModel Dumps the entire model structure to a file. mj_printModel(model, \"model.txt\") mj_setGeomMass Dynamically changes the mass of a geometry. mj_setGeomMass(model, geom_id, 1.5) mj_setBodyMass Dynamically changes the mass of a body. mj_setBodyMass(model, body_id, 10.0) mj_setLengthRange Sets the length range of a tendon. mj_setLengthRange(model, tendon_id, 0.1, 0.5) Use mj_name2id and mj_id2name to access objects in your model dynamically, when you don‚Äôt hardcode object IDs. Functions like mj_setGeomMass and mj_setBodyMass allow you to tweak physical properties during runtime.Attributes Function Description Example Usage qpos0 Default joint positions (initial state). print(model.qpos0) qpos_spring Rest positions of joints with springs. print(model.qpos_spring) jnt_range Joint position limits, given as [min, max] for each joint. joint_limits = model.jnt_range jnt_type Type of each joint (hinge, slide, etc.). print(model.jnt_type) body_pos Positions of all bodies in the model‚Äôs local frame. body_positions = model.body_pos body_quat Orientations of bodies in quaternion format. body_orientations = model.body_quat geom_pos Positions of geometries relative to their parent body. geom_positions = model.geom_pos geom_size Dimensions of geometries (e.g., radius for spheres, size for boxes). geom_sizes = model.geom_size geom_type Type of each geometry (sphere, box, capsule, etc.). print(model.geom_type) dof_damping Damping coefficients for degrees of freedom (DOF). damping_values = model.dof_damping dof_armature Armature values for rotational DOF. armature_values = model.dof_armature actuator_ctrlrange Control range ([min, max]) for each actuator. ctrl_ranges = model.actuator_ctrlrange actuator_forcerange Force range ([min, max]) for each actuator. force_ranges = model.actuator_forcerange actuator_trntype Type of actuator transmission (joint, tendon, etc.). print(model.actuator_trntype) sensor_type Type of each sensor (force, torque, etc.). sensor_types = model.sensor_type sensor_dim Dimension of each sensor output (e.g., 3 for a 3D force sensor). sensor_dimensions = model.sensor_dim sensor_addr Index in the sensordata array where each sensor‚Äôs data starts. print(model.sensor_addr) The mjOBJ_JOINT is a constant identifier in MuJoCo that represents the object type ‚Äújoint‚Äù.Some common object types and their constants include: mjOBJ_BODY: Refers to a body. mjOBJ_JOINT: Refers to a joint. mjOBJ_GEOM: Refers to a geometry. mjOBJ_SITE: Refers to a site. mjOBJ_SENSOR: Refers to a sensor. mjOBJ_ACTUATOR: Refers to an actuator. mjOBJ_TENDON: Refers to a tendon.mjData Function Description Example Usage mj_resetData Resets the simulation to the initial state. mj_resetData(model, data) mj_step Advances the simulation by one timestep. mj_step(model, data) mj_forward Computes forward dynamics without advancing time. mj_forward(model, data) mj_inverse Computes inverse dynamics (forces needed for desired accelerations). mj_inverse(model, data) mj_applyFT Applies a force/torque to a body at a specific point. mj_applyFT(model, data, force, torque, body_id, point) mj_getControl Retrieves the current control inputs. controls = mj_getControl(data) mj_setControl Sets the control inputs for actuators. mj_setControl(data, control_values) mj_contactForce Returns the forces at a specific contact point. force = mj_contactForce(model, data, contact_id) mj_time Returns the current simulation time. current_time = mj_time(data) mj_energyPos Computes the system‚Äôs potential energy. potential_energy = mj_energyPos(model, data) mj_energyVel Computes the system‚Äôs kinetic energy. kinetic_energy = mj_energyVel(model, data) Use mj_step to progress the simulation. You can modify data (e.g., joint positions or control inputs) before calling mj_step. mj_contactForce is useful for analyzing interaction forces in contact-rich environments.Attributes Function Description Example Usage qpos Joint positions, representing the current state of the system. print(data.qpos) qvel Joint velocities. print(data.qvel) qacc Joint accelerations. print(data.qacc) ctrl Control inputs applied to actuators. data.ctrl[:] = [0.1, 0.2, 0.3] qfrc_applied Forces applied directly to joints. data.qfrc_applied[joint_id] = 1.0 xfrc_applied External forces/torques applied to bodies. data.xfrc_applied[body_id][:3] = [1.0, 0.0, 0.0] xpos Global positions of bodies in the simulation. print(data.xpos[body_id]) xquat Global orientations of bodies in quaternion format. print(data.xquat[body_id]) xmat Global orientations of bodies in rotation matrix format. print(data.xmat[body_id]) cvel Combined spatial velocity (angular + linear) for all bodies in the local frame. spatial_velocity = data.cvel[body_id] \\angular_velocity = data.cvel[body_id][:3]\\ linear_velocity = data.cvel[body_id][3:] geom_xpos Global positions of geometries. print(data.geom_xpos[geom_id]) geom_xmat Global orientations of geometries. print(data.geom_xmat[geom_id]) sensordata Sensor data output. print(data.sensordata) subtree_com Center of mass of each kinematic subtree. print(data.subtree_com[body_id]) subtree_mass Mass of each kinematic subtree. print(data.subtree_mass[body_id]) cfrc_ext Contact forces/torques acting on bodies. print(data.cfrc_ext[body_id]) cfrc_int Internal forces/torques acting on bodies. print(data.cfrc_int[body_id]) ten_length Current lengths of tendons. print(data.ten_length[tendon_id]) ten_velocity Velocities of tendons. print(data.ten_velocity[tendon_id]) Other MuJoCo Functions Function Description Example Usage mj_loadXML Loads an XML model into MuJoCo. model = mj_loadXML(\"model.xml\") mj_makeData Creates a new mjData object for the loaded model. data = mj_makeData(model) mj_deleteModel Frees memory allocated for mjModel. mj_deleteModel(model) mj_deleteData Frees memory allocated for mjData. mj_deleteData(data) mjv_updateScene Updates the visual representation of the model. mjv_updateScene(model, data, options, scene, camera) mjr_render Renders the scene to a viewport. mjr_render(viewport, scene) " }, { "title": "Introduction to Computer Vision", "url": "/posts/computer-vision-24/", "categories": "CMU MRSD, Robotics", "tags": "computer vision", "date": "2024-12-05 11:00:00 -0500", "snippet": "Image Processing and Representation Your browser does not support PDFs. Please download the report here.Cameras and Image Formation Your browser does not support PDFs. Please download the report here.Multi-view Geometry and 3D Reconstruction Your browser does not support PDFs. Please download the report here.Deep Learning and Segmentation Your browser does not support PDFs. Please download the report here.Motion Your browser does not support PDFs. Please download the report here.Physics Based Vision Your browser does not support PDFs. Please download the report here." }, { "title": "Robot Mobility on Air, Land, and Sea", "url": "/posts/mobility-24/", "categories": "CMU MRSD, Robotics", "tags": "autonomous driving, wheeled mobility, air mobility, legged mobility, marine mobility", "date": "2024-12-04 11:00:00 -0500", "snippet": "Wheeled Mobility Your browser does not support PDFs. Please download the report here.Aerial Mobility Your browser does not support PDFs. Please download the report here." }, { "title": "Manipulation, Estimation, and Control", "url": "/posts/mec-24/", "categories": "CMU MRSD, Robotics", "tags": "estimation, manipulation, control, matlab, kalman filter, efk, particle filter, kinematics, inverse kinematics, dynamics", "date": "2024-12-04 11:00:00 -0500", "snippet": "ControlsEstimation Your browser does not support PDFs. Please download the report here.Manipulation" }, { "title": "Lunar ROADSTER", "url": "/posts/lunar-roadster-cmu/", "categories": "Robotics", "tags": "space, navigation, localization, perception, control, planning, electronics, design", "date": "2024-12-04 11:00:00 -0500", "snippet": "Lunar Robotic Operator for Autonomous Development of Surface Trails and Exploration Routes (ROADSTER)Supervisor: Dr. William ‚ÄúRed‚Äù WhittakerTeam: Bhaswanth Ayapilla, Ankit Aggarwal, Deepam Ameria, Simson D‚ÄôSouza, Boxiang (William) FuVisit the Lunar ROADSTER Website Lunar-ROADSTER GitHub Fall Validation DemoDemoFVD PresentationDate: $17^{th}$ November, $2025$ Your browser does not support PDFs. Please download the report here.EncoreEncore PresentationDate: $24^{th}$ November, $2025$ Your browser does not support PDFs. Please download the report here.Spring Validation DemoDemoSVD PresentationDate: $17^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.EncoreDate: $24^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Final Project Report Your browser does not support PDFs. Please download the report here.Design ReviewSystem Development Review (SDR)Date: $19^{th}$ Oct, $2025$ Your browser does not support PDFs. Please download the report here.Critical Design Review Report (CDRR)Date: $3^{rd}$ May, $2025$ Your browser does not support PDFs. Please download the report here.Critical Design Review Presentation (CDRP)Date: $29^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Preliminary Design Review (PDR)Date: $11^{th}$ March, $2025$ Your browser does not support PDFs. Please download the report here.Conceptual Design Review Report (CoDRR)Date: $12^{th}$ December, $2024$ Your browser does not support PDFs. Please download the report here.Conceptual Design Review Presentation (CoDRP) Your browser does not support PDFs. Please download the slides here.Standards and Regulations Your browser does not support PDFs. Please download the slides here.Test PlansFall Test Plan Your browser does not support PDFs. Please download the report here.Fall Validation Demo Test Plan Your browser does not support PDFs. Please download the report here.Spring Test Plan Your browser does not support PDFs. Please download the report here.Spring Validation Demo Test Plan Your browser does not support PDFs. Please download the report here.Progress Review PresentationsProgress Review 1Date: $13^{th}$ February, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 2Date: $27^{th}$ February, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 3Date: $20^{th}$ March, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 4Date: $8^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 5Spring Validation DemoProgress Review 6Spring Validation Demo EncoreProgress Review 7Date: $10^{th}$ September, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 8Date: $24^{th}$ September, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 9Date: $8^{th}$ October, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 10Date: $29^{th}$ October, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 11Date: $12^{th}$ November, $2025$ Your browser does not support PDFs. Please download the report here.Individual Lab ReportsILR 01Date: $7^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 02Date: $14^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 03Date: $28^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 04Date: $21^{th}$ March, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 05Date: $9^{th}$ April, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 06Date: $11^{th}$ September, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 07Date: $25^{th}$ September, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 08Date: $9^{th}$ October, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 09Date: $30^{th}$ October, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 10Date: $13^{th}$ November, $2025$ Your browser does not support PDFs. Please download the slides here." }, { "title": "Docker for Robotics", "url": "/posts/docker-for-robotics/", "categories": "Blog, Robotics", "tags": "docker, software, programming, development, robotics", "date": "2024-12-01 11:00:00 -0500", "snippet": "Docker OverviewDocker is a tool that helps you create, share, and run applications in containers. Containers are small, lightweight packages that include everything your application needs, like the code, libraries, and settings. They are faster and use fewer resources compared to virtual machines (VMs).Difference Between Containers and Virtual Machines: Virtual Machines (VMs): Full operating systems running on top of a hypervisor. They are resource-heavy and slow to start. Docker Containers: Share the host OS kernel, making them faster, more lightweight, and more resource-efficient.Key components: Images: Think of an image like a recipe. It tells Docker what to include in the container (software, libraries, etc.). Containers: These are the actual ‚Äúlive‚Äù versions of the image. It‚Äôs like cooking from the recipe ‚Äî your container is the meal ready to eat. Dockerfile: A script that defines how an image is built, including base images, commands, and configurations. Volumes: Persistent storage that containers can use to save data. Docker Hub: A cloud-based repository where pre-built Docker images are shared.Why Use Docker in Robotics?Working on robots means dealing with lots of software and tools, which can sometimes conflict or break things. Docker helps solve these problems: Easily Switch Between Setups: Robots often use specific software versions, like different ROS distributions (Robot Operating System) or Ubuntu versions. For example, if you‚Äôre using an Nvidia Jetson board with JetPack (their software toolkit), Docker lets you use the version you want, even if Nvidia hasn‚Äôt updated JetPack in a while. Consistent Building and Testing: Docker ensures you build and test your software in the same environment every time, no matter where you run it. Same Environment for Everyone: All developers on a team can work with the exact same setup using Docker. This avoids problems where something works on one computer but not on another. Easy to Update Robots: When you need to update a robot‚Äôs software, Docker lets you make small, controlled changes without messing up the entire system. Code Defines the Environment: Docker lets you describe your software environment in code, so it‚Äôs easy to share, version, and reproduce. Cloud Development: Docker makes it easy to set up powerful computers in the cloud to develop and test your robotics software. You can even connect to these remote setups with tools like VS Code. Docker CommandsImages Build an image from a Dockerfile: $ docker build -t &lt;image_name&gt; Pull an image from a Docker Hub: $ docker image pull &lt;image_name&gt;:&lt;tag&gt; Search Hub for an image: $ docker image search &lt;image_name&gt; List local images: $ docker image ls $ docker images Delete an image: $ docker image rm &lt;image_name&gt; $ docker rmi &lt;image_name&gt; Remove all unused images: $ docker image prune Containers Create and run a container from an image, with a custom name: $ docker run --name &lt;container_name&gt; &lt;image_name&gt; Run a container with terminal: $ docker run -it &lt;image_name&gt; Start or stop an existing container: $ docker start|stop &lt;container_name&gt; (or &lt;container-id&gt;) Start an existing container with terminal: $ docker start -i &lt;container_name&gt; List running containers: $ docker container ls $ docker ps List all containers (even the stopped ones): $ docker container ls -a $ docker ps -a Remove a stopped container: $ docker rm &lt;container_name&gt; Remove all available containers: $ docker container prune Open terminal inside a running container: $ docker container exec -it &lt;container_name&gt; /bin/bash For any commands within a running container: $ docker container exec -it &lt;container_name&gt; &lt;command&gt; Working with Volumes Mount Host Directory to Container: This is how we can make a directory on host available inside the container$ docker run -it -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; &lt;image_name&gt;$ docker run -it --network=host --ipc=host -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; &lt;image_name&gt;Any files created in a container in a shared volume will be locked ‚Äî can be accessed only by the root.Note: docker run always creates a new container. We lose any changes we make to the environment every time we run the container.Setting up a DockerfileThe Dockerfile contains the steps for creating an image. It typically starts with a base image and includes commands for installing software, setting environment variables, and defining the container‚Äôs entrypoint.# FROM &lt;base_image_name&gt;FROM osrf/ros:humble-desktop-full# Commands to perform on base imageRUN apt-get -y update \\ &amp;&amp; apt-get -y install some_package \\ &amp;&amp; git clone https://github.com/some_user/some_repository some_repo \\ &amp;&amp; cd some_repo \\ &amp;&amp; mkdir build \\ &amp;&amp; cd build \\ &amp;&amp; cmake .. \\ &amp;&amp; make -j$(nproc) \\ &amp;&amp; make install \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# Install additional packagesRUN apt-get update &amp;&amp; apt-get install -y \\ python3-pip \\ ros-humble-turtlebot3-simulations# Set up workspaceENV WS_DIR=\"/root/ros2_ws\"WORKDIR ${WS_DIR}RUN /bin/bash -c \"source /opt/ros/humble/setup.bash &amp;&amp; colcon build\"# Default commandCMD [\"/bin/bash\"]# COPY &lt;configuration_file to copy&gt; &lt;direction in image to be copied into&gt;COPY config/ site_config/# Define the script that should be launched upon start of the containerENTRYPOINT [\"/root/ros2_ws/src/my_script.sh\"] All commands run in the docker container will run as root. The COPY command assumes paths are relative to the build context specified in the docker-compose.yml or the docker build command. To build and run the Docker image, go into the directory which will be the new image$ docker image build -t &lt;new_image_name&gt; &lt;directory&gt;$ docker run -it &lt;new_image_name&gt;Entrypoint ScriptsEntrypoint scripts automate container setup at runtime. It runs every time the container is brought up. Create a new file called entrypoint.sh inside the directory.#!/bin/bashsource /opt/ros/humble/setup.bashexec \"$@\" Add it to the DockerfileCOPY entrypoint.sh /entrypoint.shENTRYPOINT [\"/entrypoint.sh\"]GUI in Docker$ docker run -it --network=host --ipc=host -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; -v /tmp/.X11-unix:/tmp/.X11-unix:rw --env=DISPLAY &lt;image_name&gt;Docker ComposeDocker Compose simplifies multi-container applications by allowing you to define and manage them through a YAML file (docker-compose.yml).For newer versions (Docker v2.0 and later), use docker compose instead of docker-compose.Basic Commands Start and run all services defined in the docker-compose.yml file:$ docker compose up Start services in detached mode (background):$ docker compose up -d Stop all running services:$ docker compose stop Stop and remove all services, networks, and volumes:$ docker compose down Restart all services:$ docker compose restartConfiguration Management Validate the docker-compose.yml file:$ docker compose config View the service logs (real-time streaming):$ docker compose logs View logs of a specific service:$ docker compose logs &lt;service_name&gt; Build or rebuild services$ docker compose build Build a specific service:$ docker compose build &lt;service_name&gt; Pull service images defined in the docker-compose.yml file:$ docker compose pullService ManagementA service represents a single containerized application or component in a multi-container setup. Each service corresponds to a container, and the docker-compose.yml file is used to define the configuration for these services. Start a specific service$ docker compose up &lt;service_name&gt; Stop a specific service:$ docker compose stop &lt;service_name&gt; Remove stopped service containers:$ docker compose rm Remove a specific service container:$ docker compose rm &lt;service_name&gt;Network and Volume Management View networks created by Docker Compose:$ docker network ls View volumes created by Docker Compose:$ docker volume ls Remove unused networks:$ docker network prune Remove unused volumes:$ docker volume pruneWriting and launching a Docker-Compose fileExample docker-compose.yml for a ROS 2 project:version: '3.8'services: ros-master: image: osrf/ros:humble-ros-core container_name: ros-master networks: - ros-network turtlebot-sim: image: osrf/ros:humble-desktop container_name: turtlebot-sim depends_on: - ros-master networks: - ros-networknetworks: ros-network: driver: bridgeAfter having created both a¬†Dockerfile¬†as well as a¬†docker-compose.yml¬†you can launch them with:$ docker compose -f docker-compose.yml build$ docker compose -f docker-compose.yml upwhere with the option¬†-f¬†a Docker-Compose file with a different filename can be provided. If not given it will default to¬†docker-compose.yml.More general docker-compose.yml:version: \"3.9\"services: some_service: # Name of the particular service (Equivalent to the Docker --name option) build: # Use Dockerfile to build image context: . # The folder that should be used as a reference for the Dockerfile and mounting volumes dockerfile: Dockerfile # The name of the Dockerfile container_name: some_container stdin_open: true # Equivalent to the Docker -i option tty: true # Equivalent to the Docker docker run -t option volumes: - /a_folder_on_the_host:/a_folder_inside_the_container # Source folder on host : Destination folder inside the container another_service: image: ubuntu/20.04 # Use a Docker image from Dockerhub container_name: another_container volumes: - /another_folder_on_the_host:/another_folder_inside_the_containervolumes: - ../yet_another_folder_on_host:/a_folder_inside_both_containers # Another folder to be accessed by both imagesIf instead you wanted only to run a particular service you could do so with:$ docker compose -f docker-compose.yml run my_serviceThen similar to the previous section, we can connect to the container from another console with$ docker compose exec &lt;docker_name&gt; shwhere¬†&lt;docker_name&gt;¬†is given by the name specified in the¬†docker-compose.yml¬†file and¬†sh¬†stands for the type of comand to be execute, in this case we open a¬†shell.Docker Registry Build image locally:$ docker compose build &lt;servie_name&gt; Tag the resulting image for Docker Hub:$ docker tag &lt;service_name&gt; &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt; Push the image to Docker Hub:$ docker push &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt;In case you‚Äôre not logged in, use$ docker login -u &lt;username&gt;And then enter the password.It is necessary to include your Docker Hub username in the tag.Building Docker Images for Multiple Architectures Ensure qemu emulation is enabled: You need to have qemu-user-static installed and properly configured for cross-platform builds.$ sudo apt-get install -y qemu-user-static$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yesThis ensures that the qemu emulator is registered for the required architectures.When building or running Docker images for a different architecture: Build Process: QEMU emulates the target architecture (e.g., arm64) on the host (e.g., amd64), enabling you to compile binaries and packages for the target system. Run Process: QEMU interprets arm64 instructions so that the container can run on an amd64 host without errors. Setup buildx$ docker buildx create --name multiarch --use$ docker buildx inspect --bootstrap Tag the image that you want to push$ docker tag &lt;service_name&gt; &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt; Build the multi-arch image$ docker buildx build --platform linux/amd64,linux/arm64/v8 \\ -t your-dockerhub-username/your-image-name:tag \\ --push \\ -f /path/to/Dockerfile /path/to/context The image will be pushed to Docker HubAdditional Resources Docker for Development Docker for Robotics YouTube" }, { "title": "Systems Engineering and Project Management for Robotics", "url": "/posts/systems-engineering-24/", "categories": "CMU MRSD, Robotics", "tags": "systems engineering, project management, v-model", "date": "2024-12-01 11:00:00 -0500", "snippet": "The content below are my notes for the class \"16-650 Systems Engineering and Management for Robotics‚Äù Fall 2024, by Dr. Dimi Apostolopoulos. The credit for the full content goes to Professor Dimi along with several internet sources. You can see a clear relation of the below concepts with my MRSD Capstone Project, the Lunar ROADSTER ‚Äì check Conceptual Design Review Report (CoDRR).SystemDefinitionA system is an arrangement of parts or elements that work together toexhibit behavior or meaning that is not obtainable by the individualelements alone. It can be physical, conceptual, or a combination ofboth.Key aspects of a system include: Interconnected elements: A system comprises multiple components thatinteract with each other. Purposeful arrangement: The elements are organized in a specific wayto achieve one or more stated purposes. Emergent properties: The system exhibits behaviors orcharacteristics that cannot be attributed to any single component. Boundaries: Systems have defined boundaries that separate them fromtheir surroundings. Inputs and outputs: Systems typically receive inputs, process them,and produce outputs. System vs Product: A product is a system that has been specificallydesigned, developed, and packaged to meet user needs and be offered inthe market. It is typically a refined and polished version of a system.Common Characteristics of Man-Made Systems: Complexity: Man-made systems are typically complex, consisting ofmultiple interconnected components that work together to achieve aspecific purpose. Interdisciplinary Nature: These systems often require expertise fromvarious fields of engineering and other disciplines to design,develop, and maintain. Evolutionary Nature: Man-made systems tend to evolve over time, withupdates, improvements, and adaptations to meet changing requirementsor technological advancements. Team-Based Development: They are usually created and managed byteams of professionals with diverse specialties, reflecting theinterdisciplinary nature of the systems. Formal Systems Engineering Approach: The development and managementof these systems typically follow structured systems engineeringprocesses and methodologies. Meta System: Multiple systems can work together to form a \"Systemof Systems\", whose results cannot be obtained by a single system.What do Systems Entail? Hierarchy: Systems often have layered structures with subsystems andcomponents. Attributes: Characteristics of system elements, including functionaland non-functional properties. Relationships: Interconnections and interactions between systemelements. State: Condition of an element or the entire system at a specificpoint in time. Behaviors: Describes how the sequences of element states and systemresponses change over time. Processes: Sequence and timing of behaviors. Significance of Top-Down Representation of a System:Top-down representation involves starting with a high-level view of thesystem and progressively breaking it down into its constituent parts.Its importance: Comprehensive Understanding: It provides a holistic view of how allcomponents interact within the system, facilitating bettercomprehension of the overall functionality. Effective Problem-Solving: By identifying the system‚Äôs structure,potential issues can be localized more efficiently, allowing fortargeted interventions. Enhanced Planning: A top-down approach aids in organizing tasks andallocating resources effectively by establishing clear prioritiesand dependencies among components. Improved Communication: This method simplifies the explanation ofcomplex systems to stakeholders by presenting information in alogical and structured manner. Design Flexibility: Changes can be implemented at lower levelswithout necessitating a complete redesign of the entire system, thuspromoting adaptability. Functional vs Non-Functional AttributesFunctional Attributes: Define what the system or software shall do - specificfunctionalities and features. Specified by users/customers. Describe system behavior and functions that it should perform duringoperation. Captured in use cases. Mandatory to implement. It refers to the system as a whole. Non-Functional Attributes: Define how the system should perform or behave - quality attributes. Specified by technical teams (architects, developers). Describe system properties and constraints. Captured as quality attributes. Not always mandatory, but impact overall quality. Some non-functional attributes can include: Physical parameters - Size, weight, speed, range, accuracy, flowrate, throughput, etc. Deployment and distribution - Geographic deployment locations;Quantity of personnel, equipment, facilities, etc. at each location Life cycle horizon - Who will operate the system? For how long? Whatinventory of parts do I need to maintain? Utilization - How many hours/day, days/week, weeks/year will be thesystem be used? How many operational cycles/period? How will usersinteract with the system? Effectiveness - Availability, mean time between failures,maintenance downtime, operator skill level Environmental - Temperature, humidity, altitude, Airborne, ground,underwater Interoperability with other systems Key differences: Functional requirements specify \"what\" the system does, whilenon-functional requirements specify \"how\" it does it. Functional requirements are usually visible to users, whilenon-functional requirements are often behind-the-scenes qualities.Systems EngineeringIt focuses on defining customer needs and required functionality earlyin the development cycle, documenting requirements, and then proceedingwith design synthesis and system validation while considering thecomplete problem: operations, cost and schedule, performance, trainingand support, test, manufacturing, and disposal.Distinctions of systems engineering: Top-down, holistic approach: Views the entire system beforeexamining individual parts, considers system-wide goals andrequirements first, and ensures cohesive integration of allcomponents. Focused on the what, not the how: Defines system requirements andcapabilities, and avoids prescribing specific implementationmethods. Life-cycle oriented: Considers all stages from conception toretirement, addresses long-term sustainability and adaptability,plans for maintenance, upgrades, and eventual disposal. Interdisciplinary: Integrates knowledge from multiple fields,facilitates collaboration between different specialists, ensurescomprehensive consideration of all system aspects. Waterfall Model All major phases of design and engineering happen sequentially Each phase is distinct There is no iteration between phases Requires careful planning and execution Each phase must be carried out in great detail and address potentialrisks Effective if requirements are well defined, budget allows forcomplete development cycles, and prior similar developments exist Not suitable for large projects and brand new systems subject toevolving requirements REQUIREMENTS CAPTURE $‚Äì&gt;$ DESIGN SYSTEM $‚Äì&gt;$ BUILD SYSTEM $‚Äì&gt;$DEVELOPMENTAL TESTING $‚Äì&gt;$ ACCEPTANCE TESTING $‚Äì&gt;$ SYSTEM FIELDED ANDIN USESpiral Model Four major phases Iterative process Multiple waterfalls with feedback System matures with each iteration Various prototypes Prototyping follows risk identification More suitable for large projects and large budgets, handles changingrequirements, customers can use interim versions of system Complex to plan and execute, high cost, risks must be identifiedcorrectly V Model Left side of V: Requirements, architectures, design; fromhigher-level to detailed Bottom of V: Build, fabrication, assembly, prototyping, initialintegration Right of V: Full integration, testing, validation, acceptance; fromsmall unit testing to full-system testing and acceptance Testing matches level of system hierarchy, e.g. element designrequirements are validated at the unit testing level, subsystemdesign requirements are validated at the subsystem level, etc. Encourages careful definition of requirements and architecturesupfront. Detailed execution of the early stages is critical to thesuccess of the system and project Facilitates deliberate testing, debugging, and validation of smallersystem elements before escalating to larger elements Any change in requirements must be handled by a thorough review ofarchitecture and design and, if necessary, re-architecting andre-designing Since no early system prototypes are built, one must follow processin completing the testing and validation steps thoroughly to ensurecompliance to requirements For larger, more complex systems and projects we apply the V method inapplied R$\\&amp;$D.Agile Model Incorporates both incremental and iterative methods Emphasizes flexibility and adapting to changing priorities Uses short iterative cycles with frequent demonstrations and userfeedback Focuses on continuous delivery of value and customer satisfaction Better suited for projects with high uncertainty or changingrequirements For retrofits-for-automation and subsystem R$\\&amp;$D we apply agilemethods.\\What is the difference between Spiral model and Agile model? The spiral model is more structured, relies heavily on riskanalysis, and is best suited for larger, more complex projects. Itbalances planning and prototyping with iterative development. The agile model is more flexible, emphasizes on frequent delivery ofworking software, and continuously involves customer interaction. Itis best suited for projects that need to quickly adapt to changingrequirements. Key Considerations when Choosing a Systems Engineering Model Treatment of Requirements: Decide between defining all requirementsupfront or using an iterative approach. Attributes of System: Assess if the system is new, modified, orretrofitted, and its size (small, medium, large). Budget and Time Constraints: Evaluate available funding and projecttimelines for system delivery. Technical Expertise and Organizational Capabilities: Consider theskills of the team and the maturity of organizational processes. Treatment of Risk: Determine if the design will focus on riskmitigation and how to handle uncertainties. Concept Development PhaseIt is the first phase of the system life cycle. Its key commitments are: Function: Defined in the Functional Architecture What the system needs to do Capabilities and behaviors Form: Captured in the Cyberphysical Architecture How the system will be physically realized Hardware and software components Its main purposes are to: Develop detailed conceptual designs Conduct trade studies to evaluate alternatives Make important subsystem-level decisions Formulate approach for system development Outline project management strategy It sets the foundation for the entire project and bridges the gapbetween high-level needs and concrete system concepts.Needs AnalysisStudy, analyze, and assess: Study Market and Technology Data gathering: Collect relevant information Market analysis: Understand current market conditions and trends Techno-economic feasibility: Assess technical and economicviability Analyze Need Socio-economic objectives: Identify broader societal andeconomic goals Functional/operational objectives: Define specific systemfunctions and operations Key performance measures: Establish metrics to evaluate systemsuccess Assess Technology New or improvement: Determine if developing new technology orimproving existing State-of-art: Evaluate current technological landscape, identifyareas for improvement or innovation, set standards forcomparison Patents: Review existing patents to avoid infringement andidentify opportunities Concept Exploration - Requirements AnalysisRequirements translate stakeholder needs into specific, actionablecriteria for system design and development. Systems must comply tospecific requirements. They should describe the \"what\", not the\"how\", and must be unambiguous.Types and classification of Requirements: User requirements (needs, musts, wishes, wants...) System requirements (some user, some derived) -Functional/Performance requirements, Non-functional requirements Mandatory (threshold) and desirable (stretch, objective) Hardware and software subsystem requirements Interface requirements The life cycle of Requirements are: Elicitation (seek, extract) Development (formulate, write, edit) system,functional/non-functional Analysis (allocate, prioritize) subsystem, threshold/objective Tracking Validation Revision Performance Requirements: They define how well the system shouldperform its functional requirements and meet its capabilities. Keyfeatures include: Quantitative nature: They are measurable and expressed in specific,numerical terms. Minimal quantitative baselines are set, and can also include\"stretch\" goals for exceptional performance. They specify the level of performance for each functionalrequirement. They are reviewed and refined throughout concept development andpossibly engineering development phases. Functional requirements start with ShallPerformance requirements start with WillNon-functional requirements start with WillElicit RequirementsWhere do the requirements come from? These are some of the ways by whichrequirements can be elicited: Learn from users, customer, sponsor Learn from the application Study operational documents Study design documents Examine state-of-art and relevant designs Review market studies Review technology studies etc.While eliciting requirements, it is important to keep in mind the typeof system, such as new, upgrade or retrofit, and whether the system isbuilt for a user or stakeholder.User vs Stakeholder:Users directly interact with the system on a daily basis and havefirsthand experience with the system functionality and usability. Theyare responsible for operating, maintaining, and managing the system.Stakeholders are a broader group with interest or influence in thesystem. They can include users, but can also be non-users with someinterests in the system. They may not directly use the system but areaffected by its outcomes.Key difference between User and Stakeholder: All Users are Stakeholders, but not all Stakeholders are Users. Stakeholders typically impose design constraints: Cost and schedulelimitations; Standards, policies, and procedures to follow; Safetyand security requirements; Performance specifications; Systemqualities (e.g., reliability, maintainability). While Users provide valuable insights into day-to-day systemoperation, Stakeholders shape the broader context and constraintswithin which the system must function. Elicitation for a Novel SystemWhen we‚Äôre trying to figure out what a brand new system should do, weface both good things and tricky parts:Good Things: Big Picture Thinking: It‚Äôs easier to talk about what the systemshould do in general terms. We‚Äôre not stuck with old ideas. Blank Canvas: We can design the system however we want. There‚Äôs noold system holding us back. New Ideas Welcome: We can use the latest tech and come up with freshsolutions. Tricky Parts: No Users Yet: We might not have actual users to ask about what theyneed. Unfamiliar Territory: People aren‚Äôt used to this kind of system, sothey might struggle to tell us what they want. Seeing the Whole Picture: It‚Äôs hard to think of everything thesystem needs to do when it‚Äôs all new. Unclear Connections: We might not know how different parts of thesystem should work together at first. No Instructions: There are no old manuals or documents to look atfor guidance. What to Keep in Mind: Focus on what the system needs to achieve, not specific featuresyet. Use models or demos to help people imagine the system. Look at what similar products do and what people like or dislikeabout them. Brainstorm a lot and think about different situations where thesystem might be used. Be ready to change your ideas as you learn more. Note: When working on a completely new system, it‚Äôs important to becreative and flexible. Keep talking to people who might use or beaffected by the system, and don‚Äôt be afraid to change your plans as yougo along.Elicitation for Retrofitting/Upgrading a SystemGood Things: Experienced Users: People have used the system and know it well.They can tell us what‚Äôs good and what needs fixing. Real System to Look At: We can see and work with the actual system.This helps us understand how it works and what needs changing. Existing Documentation: There are usually manuals or guides aboutthe system. Tricky Parts: Limited by Old Design: The current system might restrict what we canchange. We might have to work around existing parts or features. Outdated Tech: The system might use old technology. This can make ithard to add new, modern features. Too Many Rules: The existing system might have lots of requirementsthat we have to follow. This can limit how much we can changethings. User Bias: People used to the old system might resist big changes.They might prefer familiar features, even if they‚Äôre not the best. What to Keep in Mind: Listen to users, but also think about new possibilities they mightnot have considered. Look for ways to improve the system while keeping what works well. Check if old documents are still accurate before relying on them. Balance keeping familiar features with introducing helpful new ones. Consider if some parts of the system need a complete overhaul ratherthan just an upgrade. Note: Upgrading an existing system means working with what‚Äôs alreadythere. It‚Äôs about finding the right balance between keeping what worksand making important improvements.Methods and Techniques to Elicit Requirements: Objectives Tree Use Cases Surveys/Questionnaires Focus Groups Interviews Prototypingetc.Objectives TreeAn Objectives Tree is like a family tree, but for our project goals. Ithelps us organize what we want to achieve, from big ideas down tospecific tasks. It‚Äôs a great way to make sure everyone understands thebig picture and their part in it. Key features of objectives trees are: Top-Down Structure: Starts with our main goal at the top, thenbranches out into smaller objectives. The objectives become morespecific as we go down the tree. Breaks Down Big Goals: Takes our big idea and splits it intosmaller, manageable pieces. Shows Relationships: We can see how different objectives connect toeach other. This helps us understand which tasks support whichgoals. Use CasesUse Cases are like stories that explain how people will use a system toget things done. They‚Äôre really helpful when designing new systems orimproving old ones. They‚Äôre a great way to make sure everyoneunderstands what the system needs to do, without getting lost intechnical details too early. Key features of use cases are: Collection of Scenarios: They‚Äôre a bunch of examples showing how thesystem will be used. Think of them as \"day in the life\" stories ofsystem users. Goal-Focused: Each Use Case shows how the system helps achieve aspecific goal or task. Multiple Cases Needed: Complex systems need several Use Cases tocover everything. Not About How, But What: Use Cases describe what the system does,not how it does it. Communication Tool: They help designers and customers understandeach other better. How should the use cases be written? We should focus entirely on the user and what they want to doinstead of the system doing things. The title should be what the goal is or what the user wants toachieve. Clearly define the \"actor\" that desires the goal. This could be aperson or an interfacing system. Write about what should happen when everything goes right. Don‚Äôtworry about the failure cases. Narrate the story in such a way that both technical andnon-technical people can readily understand it. It doesn‚Äôt matter if a step is done by a person, software, ormachine. Concept Definition - System ArchitecturesSystem Architecture is like creating a blueprint for a complex machineor process. It‚Äôs important because it helps us understand how everythingworks together. It shows where you‚Äôre going, how all the parts connect,and helps everyone work together more smoothly. It‚Äôs a tool that makescomplex systems easier to understand, build, and manage.It captures the flow of functions and relationships and capturesbreak-down of systems from top to down. There are two mains types ofsystem architectures: Functional Architecture Cyberphysical Architecture - Cyber (software) + physical (hardware) The cyberphysical architecture is derived from the functionalarchitecture.Difference between Functional Architecture and CyberphysicalArchitecture:Functional architecture describes what the system shall do, i.e., thefunctions and attributes of the components of the system. It focuses onthe logical components and their interactions. They are typicallyrepresented using functional block diagrams.Cyberphysical architecture describes how the system will be physicallyrealised. It focuses on the physical, hardware and software components.They are typically represented by system block diagrams.Functional Decomposition MethodIt comprises of two main steps: Identify the functions carried out by the system Apply different ways to logically connect functions Key points to keep in mind while developing a functional architecture: The functions are identified by needs analysis and elicitingrequirements using objective trees, use cases, etc. It is essential to have formulated the functional requirementsbefore developing the functional architecture. Apply different ways to logically connect the functions. Capture the \"what\", not the \"how\". Think of transformation of material, information and energy. Start synthesizing architectures assuming little or no sharing offunctions among subsystems. Deciding on connectivity and flow for our architectures is criticalto what our system will be. Include only what is needed and makes sense. It can be reiteratedduring the concept development phase. Morphological ChartsMorphological charts aid in concept synthesis by: Breaking down the system into key functional attributes Explore different possible solutions for each function Allowing systematic exploration of different combinations ofsolutions Their significance: Encouraging creative thinking and innovation Providing a structured approach to generating diverse systemconcepts Facilitating the identification of novel combinations that might notbe immediately obvious Supporting a comprehensive exploration of the solution space Trade StudiesIt is a systematic and structured process for comparing and selectingamong different alternatives or options for a system design or solution.It is used to inform decisions by methodically framing the trade spaceand systematically evaluating alternatives. They help balance multiple,often conflicting criteria to find the best overall solution.Weighted Objectives Method:It is a specific technique used within trade studies and involvesassigning numerical values to features or alternatives based onpredefined criteria. Each criterion is given a different weight based onits importance.How to perform? Identify and list all possible options Define relevant criteria Assign numeric weighting values to each criterion (ensure weights total to 100) Score each option against the criteria (e.g., on a scale of 1-5 or1-10) Calculate weighted scores by multiplying the score by the weightingvalue Sum up the total score for each option Compare the results to identify the best alternative What are the evaluation criteria and weighting factors?Evaluation criteria: Performance requirements Non-functional requirements Life-cycle parameters (cost, reliability, sustainability, etc.) Technical specifications (manuals, product reports, etc.) Weighting factors: Customer priorities, market pull, technology push, etc. Team and project specific prioritization Requires a lot of communication with the stakeholders Significance: Provides a structured, objective approach to decision-making Allows for consideration of multiple factors with varying importance Helps justify decisions with quantitative data Useful for complex decisions with multiple stakeholders Can be applied to various fields including systems engineering,product management, and project planning Project ManagementIntroductionProject management is the application of knowledge, skills, andtechniques to execute projects effectively and efficiently. It‚Äôs astrategic competency for organizations, enabling them to tie projectresults to business goals ‚Äî and thus, better compete in their markets.Perspective ModelThe Perspective Model for Project Management, as depicted in the image,outlines a systematic, structured approach to managing projects frominitiation to closure.How does this translate to the V-model of systems engineering? The perspective model starts by defining and initiating the project,and formulating its goals, objectives and scopes. This is translatedto V-model‚Äôs focus on developing requirements. In the Perspective Model, detailed work plans, schedules, budgets,and risk management are created. This aligns with the V-Model‚Äôsdecomposition and detailing phase, where system architecture andsubsystem designs are detailed. The Perspective Model implements the plan, while the V-Modeltransitions to fabrication and assembling elements. The Perspective Model monitors progress, comparable to the V-Model‚Äôsintegration and validation phase, which tests elements, subsystems,and the entire system. This stage involves proper tracking,evaluating, iterating, and replanning if necessary. Both models conclude by ensuring the final output meets the initialobjectives‚ÄîPerspective Model through project evaluation andclosure, and V-Model through acceptance testing and systemvalidation. Systems engineering is concerned with a top-down system-oriented lifecycle. Project management is concerned with a bottom-up project-focusedlife cycle.Initiating a ProjectIn order to obtain formal authorization to start the project, thefollowing major initiation/formulation documents are required: Statement of Work - Discusses the work to be accomplished, inputrequirements, work not to be accomplished, and specific results anddeliverables. Technical Proposal - Details the technical approach andmethodologies Management Proposal - Outlines team structure, schedule, and riskmanagement Cost Proposal - Breaks down project costs and budget allocation Develop a Work PlanA top-down breakdown of the work needs to be performed. This should bedone in levels as: Level 1: Total scope, system, product Level 2: Sub-projects, subsystems, project activities; Technical,management, facilities, etc. Level 3: Functions, activities, major tasks, assemblies/components Work Breakdown StructureThe WBS is a tool to break down the total scope of a project intosmaller, manageable components. It provides a structured view of whatneeds to be done, facilitating planning, assigning tasks, trackingprogress, and managing resources. The WBS helps ensure that no importanttasks are overlooked, and it provides clarity on how each task fits intothe overall project.Product-Oriented WBS: Focus - The breakdown is based on the product or service beingcreated or delivered. This approach organizes work around theproduct‚Äôs features, components, and deliverables. Structure - Focuses on specific product components or sub-systemsthat need to be built or created. Advantages - It makes the product features clear and shows howindividual components fit together. It focuses on product outputs,so progress can be tracked by evaluating the completion of productcomponents. Process-Oriented WBS: Focus - Focuses on the processes or phases required to complete theproject. It breaks down the work based on the sequence of activitiesor workflows involved in delivering the product or service. Structure - Organizes the project into processes, procedures, orphases, detailing the steps or activities required to move throughthe project lifecycle. Advantages - It provides insight into the steps and processesinvolved in delivering the project, helping with resource allocationand scheduling. It works well when the project requires specificworkflows or well-defined phases to be followed in sequence. DictionaryWhen analyzing each work package in a Work Breakdown Structure, it isessential to capture detailed information to ensure proper management,execution, and tracking of the project. A WBS Dictionary is typicallyused to document this information, providing an organized descriptionfor each work package, task, or deliverable. This ensures clarity andconsistency across the project.Key components: WBS Work Package Task: Specific task/activity Estimated Level of Effort: By first order estimation and refinethrough iterations (in hours) Owner: Specify primary owner and secondary owner Resources Needed: Number and specialty of engineers, manager,subcontractors, Design, build, test facilities, hardware/softwaretools, budget Work Products: Quantified output of work, Cyberphysical product,Documents, Deliverables, Recipient of delvierables Description: Precise description of task and breakdown of what eachsubtasks are involved Input: Define connection to other work packages Dependencies Risks: First pass on identifying risks, The lower the level of thework package, the more technical risks dominate, Risks related toscope of tasks, resources, budget, effort allocation Develop a ScheduleThe following factors need to be considered for effective estimation ofa project schedule: Work Effort vs. Duration: Work effort refers to labor hours (e.g.,40 hours); duration accounts for calendar time, factoring inavailability and efficiency. Account for non-project activities such as administrative tasks,approvals, and external dependencies such as delays from vendors. Estimation at the Lowest Level: Break tasks into smaller workpackages or activities to ensure precision, and roll up estimatesfor an overall view. Skill Set and Expertise: Identify and assign specific skills neededfor the task; Leverage experienced resources for faster, moreaccurate task completion; Use lessons learned from similar projectsto improve accuracy. Prior Estimations and Performance: Analyze past estimates to refinecurrent ones; Identify recurring inaccuracies to enhance futureplanning. Contingency Planning: Add time and cost buffers to account for risksand unforeseen challenges; Link contingency to specific risksidentified in the WBS. By addressing these considerations, estimates become more realistic,reducing risks and improving project outcomes.MilestonesTypes of milestones: External: These are mandated by the sponsor Required Internal: Based on project flow such as integration, firstroll-out, etc. Non-mandatory Internal: Used to facilitate management ofactivities/tasks, costs, schedule, and risks Comments on milestones: Milestones are significant zero-duration events in the life-cycle ofa project Changing a milestone impacts all facets of the project If an external milestone is changed, a partial or full projectre-planning is required External milestones are rigidly defined and cannot be changed unlessthere is formal contract modification Internal milestones should be treated as if they were external Risk ManagementIssue vs RiskIssue: An issue is a current problem or challenge that is activelyimpacting the project. It requires immediate attention to resolve ormitigate its impact.Risk: A risk is a potential problem or uncertainty that could occurin the future and might affect the project. It requires planning toprevent or minimize impact if the risk materializes.Components of a Risk Root cause: The underlying reason or source of the risk. Identifyingthe root cause helps in understanding why the risk might occur andprovides insights for mitigation strategies Probability: The likelihood or chance of the risk occurring. This isoften assessed on a scale (e.g., low, medium, high) or as apercentage. Understanding probability helps prioritize risks andfocus on those more likely to happen The potential impact or effect the risk would have if itmaterializes. Consequences can affect scope, schedule, cost, orquality and are often quantified in terms of severity (e.g.,minimal, moderate, critical) ApproachThe following approach should be followed to manage risks in a project:Identify risks (what could go wrong?): Review your WBS work packages Examine what could go wrong Compile a list of potential risk root causes Analyze risks (how big is the risk?): Use likelihood and consequences matrix to rank Be conservative for the initial pass Discuss with stakeholders Formulate action plan: Identify simple, realistic actions you will take - eliminate rootcause if possible, or transfer risk if possible, or assume risk andcontinue State what will be done, when, and by whom Set success metrics to mitigate risk (specific quantitative targets) " }, { "title": "Controller for Autonomous Surface Vehicle", "url": "/posts/asv/", "categories": "Projects, Robotics", "tags": "matlab, underwater, dynamics, simulation, pid, control", "date": "2023-12-04 11:00:00 -0500", "snippet": "The following project was completed as part of a competition at the Institute for Systems and Robotics, Lisbon under the supervision of Dr. David Cabecinhas and Dr. Pedro Batista" }, { "title": "Development of Python-Based Simulator for Analyzing Autonomous Underwater Glider Motions and Performance", "url": "/posts/aug-simulator/", "categories": "Projects, Robotics", "tags": "python, underwater, dynamics, simulation, pid, control", "date": "2023-12-04 11:00:00 -0500", "snippet": "The following project was completed as part of my undergraduate thesis at the Institute for Systems and Robotics, Lisbon under the supervision of Dr. David Cabecinhas and Dr. Pedro BatistaAbstractThe objective of this thesis is to design, develop, and implement a comprehensive Python-based simulator for Autonomous Underwater Gliders (AUGs), enabling the precise simulation and in-depth analysis of various motions exhibited by them. Underwater Gliders are a unique kind of vehicle that fall under the class of Autonomous Underwater Vehicles (AUVs), which maneuver through a water body by varying their buoyancy and attitude using internal mass actuators and a buoyancy engine.The simulator will be used in analyzing the dynamics and behaviors of various glider models, such as the Slocum, Spray, and Rogue, both in a simplified two dimensional setting, as well as in full three dimensional gliding. Marine researchers can configure the simulator to specific mission scenarios and for specific glider designs.The development of such a robust platform will serve as a valuable resource for researchers and practitioners in the field of marine robotics by facilitating the examination and evaluation of different motion patterns. This enhances our understanding of AUG behavior and performance, thereby furthering the development of such vehicles.IntroductionAutonomous Underwater GlidersAutonomous Underwater Gliders (AUGs) are a class of underwater robots that fall under the category of Autonomous Underwater Vehicles (AUVs). Unlike traditional propeller-driven vehicles that use external thrusters to move, AUGs are set apart because of their reliance on buoyancy-driven propulsion systems. This unique feature enables these gliders to achieve sustained vertical movement through the water column by harnessing changes in buoyancy and hydrodynamic forces.DesignThe design of AUGs can vary slightly depending on the models and the applications that they serve. However, most of them have a cylindrical hull, and are roughly shaped like a torpedo. They are typically small and reusable vehicles, with a weight of around 50 kg and 2m length, making them suitable for small missions and easily operated by fewer people. They are capable of operating at low speeds of 20-30 cm/s and can traverse thousands of kilometers over several months.Several AUG models have been developed over the past years, such as the Slocum, Spray, and Seaglider. Each of these AUG models have slightly different design approaches, including battery and thermal powered propulsion, speed and depth capabilities, strategies to reduce drag, wing spans, and communication methods. Despite these differences, all of them have some standard dynamic characteristics, which will be discussed in Chapter 3.ApplicationsSome of the applications of AUGs include: Study of ocean currents - Equipped with various sensors, AUGs can collect data on water temperature, salanity, and other parameters to better understand the patterns of ocean currents. They can also measure the levels of dissolved oxygen, pH, and other nutrients to assess the health of marine ecosystems. Climate change - They can help in predicting the rise in sea level due to the melting of polar ice caps, and can also monitor the levels of carbon dioxide in the water. Defense - AUGs can be used by the military/Navy for monitoring underwater activities, locate sea mines, and carry out secret missions. Disaster management - They can be used to detect changes in the oceans, such as temperature, vibrations in the sea floor that may indicate the occurrence of a tsunami or a hurricane.MotivationThe development of a simulator for AUGs would help researchers to assess glider behavior under various conditions, validate algorithms, and conduct virtual missions.Need for a SimulatorThe use of AUGs has become increasingly important in the field of oceanography due to their exceptional efficiency and adaptability in collecting valuable data. These underwater vehicles are designed to independently navigate the ocean, carrying out various missions ranging from oceanographic data collection to environmental monitoring. This makes them essential assets for understanding and addressing critical challenges in marine science and technology.Despite their growing importance in marine research and exploration, a noticeable shortage of accessible simulators specifically for the analysis and testing of underwater gliders persists. This glaring gap in available simulation tools presents a significant challenge to researchers and engineers in the field. The absence of user-friendly, open-source simulators severely limits our ability to comprehensively study and validate glider behaviors.The analysis of the dynamics of AUGs is a highly sophisticated and challenging task. The core features may differ for different glider models, which can influence the dynamics and behaviors of the AUG, such as different mechanisms for internal mass actuation and ballast systems, use of external rudders, etc. This necessitates the need for a robust simulator that can be easily tailored to different AUG models depending on their application.In the future, the use of a simulator can contribute to the development of methods for improved performance in specific flight regimes, as well as the testing of novel flight trajectories, optimization of control algorithms, and conducting virtual experiments. In addition, the simulator can allow for the comparison of performance of glider models such as the Slocum, Spray, and Seaglider.Thus, this thesis aims to address this pressing need by developing an open-source simulator specifically for AUGs, thereby paving the way for the development of more efficient and advanced glider technologies.Existing Underwater SimulatorsWhile there do exist some underwater simulators, specifically for Autonomous Underwater Vehicles (AUVs), they are either not very suitable for AUGs, not very efficient, or not open-source.Every AUG model, such as the Slocum, and Spray, comes with a ‚ÄúSimulation‚Äù option that can help in planning missions. This however, carries out simulations in real-time, due to which a simulation to plan missions for a week will take a week to run.The following are a few existing open-source simulators:DAVEDAVE Aquatic Virtual Environment (DAVE) is a specialized simulation platform built on ROS and Gazebo, and is designed for the testing of underwater robots. It focuses on autonomous underwater vehicles (AUVs/UUVs) and their ability to carry out missions involving tasks like autonomous manipulation.With ROS being widely used among the robotics community, this simulator offers great functionality and robustness. However, it is based on the kinematic model of an AUG, which is not very reliable. Interfacing the physical glider‚Äôs simulation API with ROS is also a challenging task. Moreover, reading the glider data, such as position of center of mass, variation of internal mass actuators, etc. is not straightforward to do. Additionally, changing the glider model, or its system dynamics is tedious, which necessitates the need for a more simple and efficient simulator.Github: DAVE SimulatorWAVEThe WAVE simulator is a MATLAB based simulator that can be used to evaluate and improve the performance of an AUV by varying the blade geometry and braking strategies of the arms. It can be used in the development and testing of wing profiles and braking strategies.Github: WAVE SimulatorFISH GYMFish Gym is a physics-based simulation platform particularly for bionic underwater robots. It is integrated into the OpenAI Gym interface, thereby enabling the use of reinforcement learning algorithms and control techniques on the underwater agents.Underlying Principles of Glider BehaviorAs mentioned in Fossen‚Äôs ‚ÄúGuidance and Control of Ocean Vehicles‚Äù, the modeling of marine vehicles requires the study of statics and dynamics. Statics deals with the equilibrium of bodies, i.e., bodies at rest or moving with constant velocity. Dynamics is concerned with bodies having accelerated motion. Dynamics, in turn, is divided into Kinematics and Kinetics. Kinematics treats only the geometrical aspects of motion, and Kinetics is the analysis of forces that cause this motion.The overall kinematics and dynamics of AUGs are derived below, and the model presented here follows the glider model presented by Graver.KinematicsDefining two separate frames of reference makes it easier to model an AUG having 6 Degrees of Freedom (DOF). These are:Inertial FrameThe Inertial Frame serves as a global reference frame, as it is fixed to the surface of the Earth. This frame lets us understand the position of the glider relative to the Earth, enabling precise navigation and global planning. The accelerations of a point on the Earth‚Äôs surface can be neglected since the motion of the Earth hardly affects low speed marine vehicles.Let $xyz$ be the fixed inertial frame, such that the $x$ and $y$ axes lie in the horizontal plane, perpendicular to gravity. The $z$ axis points downward in the direction of gravity and denotes depth. The frame can be chosen on the surface of the water body such that $z = 0$.Body-Fixed FrameThe Body-Fixed Frame is attached to the glider‚Äôs body and moves along with it. It allows for measuring the internal dynamics of the glider, and control inputs. This local reference is vital for precise maneuvers and simplifying how we gather information from the on-board sensors.The Origin $O$ of the body-fixed frame usually coincides with the vehicle‚Äôs Center of Buoyancy (CoB). The body axes $X_{0}$, $Y_{0}$, $Z_{0}$ coincide with the principal axes of inertia: $X_{0}$ is the longitudinal axis - from aft to fore $Y_{0}$ is the transverse axis - directed to the starboard $Z_{0}$ is the normal axis - from top to bottomThe general motion of an AUG in 6 DOF can be described by the following vectors:\\[\\begin{align}{\\boldsymbol{\\eta}=[\\boldsymbol{b}^T,\\boldsymbol{\\eta_2}^T]^{T};\\ \\ \\ \\boldsymbol{b}=[x,y,z]^{T};\\ \\ \\ \\boldsymbol{\\eta_2}=[\\phi,\\theta,\\psi]^{T}}\\\\ {\\boldsymbol{\\nu}=[\\boldsymbol{\\nu}^{T},\\boldsymbol{\\Omega}^{T}]^{T};\\ \\ \\ \\boldsymbol{v}=[v_1,v_2,v_3]^{T};\\ \\ \\boldsymbol{\\Omega}=[\\Omega_1,\\Omega_2,\\Omega_3]^{T}}\\\\ {\\boldsymbol{\\tau}=[\\boldsymbol{\\tau_{1}}^{T},\\boldsymbol{\\tau_{2}}^{T}]^{T};\\ \\ \\ \\boldsymbol{\\tau_1}=[X, Y, Z]^{T};\\ \\ \\ \\boldsymbol{\\tau_{2}}=[K,M,N]^{T}}\\end{align}\\]Where $\\boldsymbol{\\eta}$ is the position and orientation in the inertial frame, $\\boldsymbol{\\nu}$ is the linear and angular velocity in the body-fixed frame, and $\\boldsymbol{\\tau}$ is the force and moment in the body-fixed frame.The body-fixed frame can be related to the earth-fixed frame with the linear velocity and angular velocity transformations, using the Euler angles: roll ($\\phi$), pitch ($\\theta$), and yaw ($\\psi)$.The vehicle‚Äôs vectors in the inertial frame is given by the velocity transformation:\\[\\begin{align} \\boldsymbol{\\dot{b}={J}_{1}({\\eta}_{2})v}\\end{align}\\]Where \\({J}_{1}({\\eta}_{2})\\) is the transformation matrix. For simple principal rotations,\\[\\begin{align} \\boldsymbol{J_1\\left(\\eta_2\\right)}=\\left[\\begin{array}{ccc}\\mathrm{c} \\psi \\mathrm{c} \\theta &amp; -\\mathrm{s} \\psi \\mathrm{c} \\phi+\\mathrm{c} \\psi \\mathrm{s} \\theta \\mathrm{s} \\phi &amp; \\mathrm{s} \\psi \\mathrm{s} \\phi+\\mathrm{c} \\psi \\mathrm{c} \\phi \\mathrm{s} \\theta \\\\\\mathrm{s} \\psi \\mathrm{c} \\theta &amp; \\mathrm{c} \\psi \\mathrm{c} \\phi+\\mathrm{s} \\phi \\mathrm{s} \\theta \\mathrm{s} \\psi &amp; -\\mathrm{c} \\psi \\mathrm{s} \\phi+\\mathrm{s} \\theta \\mathrm{s} \\psi \\mathrm{c} \\phi \\\\-\\mathrm{s} \\theta &amp; \\mathrm{c} \\theta \\mathrm{c} \\phi &amp; \\mathrm{c} \\theta \\mathrm{c} \\phi\\end{array}\\right]\\end{align}\\]The angular velocity transformation is given by\\[\\begin{align}\\boldsymbol{\\dot{\\eta}_2=J_2\\left(\\eta_2\\right) \\Omega}\\end{align}\\]\\[\\begin{align}\\boldsymbol{J_2\\left(\\eta_2\\right)}=\\left[\\begin{array}{ccc}1 &amp; s \\phi \\mathrm{t} \\theta &amp; c \\phi \\mathrm{t} \\theta \\\\0 &amp; c \\phi &amp; -s \\phi \\\\0 &amp; s \\phi / c \\theta &amp; c \\phi / c \\theta\\end{array}\\right]\\end{align}\\]Thus, the kinematic equations can be expressed in the vector form as\\[\\begin{align}\\left[\\begin{array}{c}\\boldsymbol{\\dot{b}} \\\\\\boldsymbol{\\dot{\\eta}_2}\\end{array}\\right]=\\left[\\begin{array}{cc}\\boldsymbol{J_1\\left(\\eta_2\\right)} &amp; \\boldsymbol{0_{3 \\times 3}} \\\\\\boldsymbol{0_{3 \\times 3}} &amp; \\boldsymbol{J_2\\left(\\eta_2\\right)}\\end{array}\\right]\\left[\\begin{array}{c}\\boldsymbol{v} \\\\\\boldsymbol{\\Omega}\\end{array}\\right]\\end{align}\\]Since we use \\(\\boldsymbol{J_{1}(\\eta_{2})}\\) very often, we replace this rotation matrix with $\\boldsymbol{R}$ to map vectors expressed in the body frame to the inertial frame.Vehicle ModelThe underwater glider is considered to be a rigid body fitted with hydrofoils, which are underwater wings that allow it to glide forward, and a tail. The glider dynamics are derived by considering the entire frame to be immersed in a fluid.The glider comprises of the following masses: The hull mass $m_{h}$, which is fixed and uniformly distributed throughout the body. The ballast mass $m_{b}$ which varies as water is pumped in and out of the system, and whose position is fixed with respect to the vehicle‚Äôs CoB. The internal moving mass $\\bar{m}$ which moves inside the glider‚Äôs body but has a fixed mass. The static offset mass $m_{w}$ which is a fixed point mass that can be offset from the vehicle‚Äôs CoB.The stationary mass of the glider is given by \\begin{align} m_{s} = m_{h} + m_{b} + m_{w} \\end{align}The total mass of the vehicle is then \\begin{align} m_{t} = m_{h} + m_{b} + m_{w} + \\bar{m} = m_{s} + \\bar{m}\\end{align}The positions of the internal point masses $m_{b}$ and $m_{w}$ inside the glider frame is given by the vectors $\\boldsymbol{r_{b}}$ and $\\boldsymbol{r_{w}}$ with respect to the Center of Buoyancy (CoB). The vector $\\boldsymbol{r_{p}(t)}$ gives the position of the movable mass $\\bar{m}$ in the body-fixed frame.The ballast mass $m_{b}$ changes as water is pumped in or emptied off of the buoyancy compartments in order to vary the vehicle‚Äôs buoyancy. The movable mass $\\bar{m}$ is used to vary the pitch of the glider, which together with the ballast mass $m_{b}$ helps the glider maneuver. The offset static point mass $m_{w}$ can be set to balance the rolling and pitching moment on the glider at equilibrium.We denote $m$ to be the mass of fluid displaced by the glider. As a result, the net buoyancy is given by \\begin{align} m_{0} = m_{t} - m \\end{align}When $m_{0}$ is positive, it means that the vehicle is negatively buoyant (it sinks), and similarly, when $m_{0}$ is negative, it means that the vehicle is positively buoyant (it floats).Ballast System and Controlled MassesThe ballast system is responsible for varying the AUGs depth within the water column. It comprises of volume compartments that can be flooded or emptied with water to change the glider‚Äôs buoyancy. By doing so, the glider can achieve positive, negative, or neutral buoyancy, and can either ascend or descend. Positive buoyancy - glider floats upward Negative buoyancy - glider sinks downward Neutral buoyancy - glider neither floats nor sinksThe type of ballast system used and its position inside the glider is motivated by the geometrical design of the AUG model. For most this thesis we model the Slocum glider, in which the ballast mass $m_{b}$ and internal moving mass $\\bar{m}$ are both well forward of the glider‚Äôs CoB. The Slocum design uses a syringe-type ballast tank to take in or pump out water.An important control input for the ballast mass is the ballast pumping rate $\\dot{m_{b}}$. We assume that when ballast is pumped into the glider, it doesn‚Äôt create any significant thrust or turning forces, similar to how a rocket‚Äôs thrust is generated by ejecting mass. For existing gliders used in oceanography, the ballast masses are very small compared to their total mass. Moreover, the ballast is ejected with very low relative velocity, considering the glider‚Äôs own movement.Moving the internal mass $\\bar{m}$ allows the glider to vary its pitch. When actuated in more than one direction, it can produce other motions such as roll and yaw. Some glider designs can also have two internal moving masses, with one mass actuated in the roll direction and the other in the pitch direction. Making additions like these to the system dynamics is a straightforward task.Specifying control to the internal masses $m_{b}$, $\\bar{m}$, and $m_{w}$ can be done two ways: Vector of forces Vector of accelerationsThe latter method of providing control inputs, that is, as a vector of accelerations of the internal point masses is preferred because of the following reasons: It provides a suspension system for the internal masses by preventing them from moving within the glider in response to its motions. It more closes approximates the internal mass systems of existing glider designs. It can fix some point masses in place by specifying their corresponding velocities and accelerations to zero.Forces and TorquesRestoring ForcesRestoring forces are the forces that tend to bring the glider back to its equilibrium position and counteract any deviations from the desired trajectories. There are two types of restoring forces acting on AUGs: Buoyant Force: This is the upward force acting on the immersed glider, at its Center of Buoyancy (CoB), by the surrounding fluid (water in our case) as per the Archimedes‚Äô Principle. It is dependent on the volume of water displaced by the glider and the difference in densities between the glider and the water. Gravitational Force: This is the downward force acting on the glider, at its Center of Mass (CoG), and counteracts the upward buoyant force.The gravitational and buoyant forces experienced by the glider are given by,\\[\\begin{align} \\boldsymbol{f_{gravity}} = m_{t}g(\\boldsymbol{R}^T\\boldsymbol{k}) \\\\ \\boldsymbol{f_{buoyancy}} = -mg(\\boldsymbol{R}^T\\boldsymbol{k})\\end{align}\\]Hydrodynamic ForcesHydrodynamic forces are the forces acting on the glider by the surrounding fluid as its moves through it, thereby influencing its motion. There are three components of hydrodynamic forces acting on the glider: Drag Force: It is the resistance experienced by the AUG as it moves through the water and acts opposite to the direction of velocity. It is dependent on the properties of the fluid, as well as the glider speed and surface area. Side Force: It acts horizontally, perpendicular to the glider‚Äôs longitudinal axis, and influences the yaw or turning motion of the glider. The amount of side force can be adjusted by controlling the angles of external surfaces like rudders that contribute to its generation. Lift Force: It acts perpendicular to the direction of the vehicle‚Äôs motion and is generated as the water flows over and around the glider‚Äôs surface. Although typically smaller in AUGs compared to airplanes, they can affect glider stability and influence its motion.The drag, lift and side forces act at the Center of Pressure (CoP) of the vehicle. The expressions for these forces experienced by the glider are given by,\\[\\begin{align} D &amp; =\\frac{1}{2} \\rho C_D(\\alpha) A V^2 \\approx\\left(K_{D_0}+K_D \\alpha^2\\right)\\left(V^2\\right)\\\\ SF &amp; =\\frac{1}{2} \\rho C_{SF}(\\beta) A V^2 \\approx\\left(K_{\\beta}\\beta\\right)\\left(V^2\\right) \\\\ L &amp; =\\frac{1}{2} \\rho C_L(\\alpha) A V^2 \\approx\\left(K_{L_0}+K_L \\alpha\\right)\\left(V^2\\right) \\end{align}\\]where $\\alpha$ is the angle of attack, $\\beta$ is the side-slip angle, $C_{D}$, $C_{SF}$ and $C_{L}$ are standard aerodynamic drag, side force and lift coefficients that are a function of $\\alpha$ or $\\beta$, $A$ is the maximum cross sectional area of the glider, and $\\rho$ is the density of the fluid. $K_{D}$, $K_{D_{0}}$ are the drag coefficients, $K_{\\beta}$ is the side force coefficient, and $K_{L}$, $K_{L_{0}}$ are the lift coefficients. $V$ is the velocity of the glider.Torques and MomentsThe restoring and hydrodynamic forces can generate torques and moments that affects the glider‚Äôs rotational motion.TorquesA torque is generated when there is an offset between the glider CoG and CoB. Ideally, the CoB of the glider should be directly above its CoG.The CoG is the weighted centroid of the vehicle and is given by,\\[\\begin{align} \\boldsymbol{r_{C G}}\\ =\\ \\frac{\\sum_{i}m_{i}r_{i}}{\\sum m_{i}}\\ =\\ \\frac{m_{h}r_{h}+m_{w}r_{w}+m_{b}r_{b}+\\bar{m}r_{p}}{m_{h}+m_{w}+m_{b}+\\bar{m}}\\end{align}\\]Since the CoG of the uniformly distributed hull mass always coincides with the CoB of the glider (which is also the origin of the body-fixed coordinate frame), $r_{h} = 0.$The torque on the glider due to the restoring forces is given by,\\[\\begin{align} \\boldsymbol{\\tau_{gravity}} = \\boldsymbol{r_{CG}} \\times m_tg(\\boldsymbol{R}^T\\boldsymbol{k})\\end{align}\\]Since the origin of the body-fixed frame coincides with the CoB and the torque due to buoyancy acts at the vehicle CoB, it is equal to zero.\\[\\begin{align} \\boldsymbol{\\tau_{buoyancy}} = 0\\end{align}\\]MomentsA moment is generated when there is an offset between the glider CoG and Center of Pressure (CoP). A glider whose CoP is forward of its CoG will have a positive hydrodynamic pitching moment at equilibrium.The expression for hydrodynamic moments experienced by a glider is given by,\\[\\begin{align}M_{D L_1} &amp; =K_{M R} \\beta V^2+K_{q 1} \\Omega_1 V^2 \\\\M_{D L_2} &amp; =\\left(K_{M 0}+K_M \\alpha+K_{q 2} \\Omega_2\\right) V^2 \\\\M_{D L_3} &amp; =K_{M Y} \\beta V^2+K_{q 3} \\Omega_3 V^2\\end{align}\\]where $\\beta$ is the side-slip angle, $K_{MR}$, $K_{M}$, $K_{M_{0}}$, $K_{MY}$ are the moment coefficients, $K_{q1}$, $K_{q2}$, $K_{q3}$ are the rotational damping coefficients, and $\\Omega_{1}$, $\\Omega_{2}$, $\\Omega_{3}$ are the components of the glider‚Äôs angular velocity.Ultimately, the viscous forces and moments are written as,\\[\\begin{align}\\boldsymbol{F}_{\\boldsymbol{e x t}}=\\left(\\begin{array}{c}-D \\\\S F \\\\-L\\end{array}\\right) \\text { and } \\boldsymbol{T}_{\\boldsymbol{e x t}}=\\left(\\begin{array}{c}M_{D L_1} \\\\M_{D L_2} \\\\M_{D L_3}\\end{array}\\right)\\end{align}\\]DynamicsEquations of MotionThe complete equations of motion of an AUG in three dimensions is given by\\[\\begin{align}\\left(\\begin{array}{c}\\dot{R} \\\\\\dot{b} \\\\\\dot{\\Omega} \\\\\\dot{v} \\\\\\dot{r}_p \\\\\\dot{r}_b \\\\\\dot{r}_{\\boldsymbol{w}} \\\\\\dot{P}_{\\boldsymbol{p}} \\\\\\dot{P}_b \\\\\\dot{P}_{\\boldsymbol{w}} \\\\\\dot{m}_b\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{R} \\hat{\\boldsymbol{\\Omega}} \\\\\\boldsymbol{R} \\boldsymbol{v} \\\\\\boldsymbol{J}^{-1} \\overline{\\boldsymbol{T}} \\\\\\boldsymbol{M}^{-1} \\overline{\\boldsymbol{F}} \\\\\\frac{1}{\\bar{m}} \\boldsymbol{P}_{\\boldsymbol{p}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}} \\\\\\frac{1}{m_b} \\boldsymbol{P}_{\\boldsymbol{b}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}} \\\\\\frac{1}{m_w} \\boldsymbol{P}_{\\boldsymbol{w}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}} \\\\\\boldsymbol{\\bar{u}} \\\\\\boldsymbol{u}_{\\boldsymbol{b}} \\\\\\boldsymbol{u}_{\\boldsymbol{w}} \\\\u_{ballastrate}\\end{array}\\right)\\end{align}\\]where\\[\\begin{align}\\overline{\\boldsymbol{T}}= &amp; \\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_p \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_b \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_w \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})\\nonumber \\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}}+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\nonumber \\\\&amp; +\\boldsymbol{T}_{\\boldsymbol{e x t}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\overline{\\boldsymbol{u}}-\\left(\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{u}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{u}_{\\boldsymbol{w}}\\right) \\\\\\overline{\\boldsymbol{F}}= &amp; \\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{ext}}-\\overline{\\boldsymbol{u}}-\\left(\\boldsymbol{u}_{\\boldsymbol{b}}+\\boldsymbol{u}_{\\boldsymbol{w}}\\right) .\\end{align}\\]and\\[\\begin{align}\\boldsymbol{F_{e x t}} &amp; =\\boldsymbol{R}^{T} \\sum \\boldsymbol{f_{ext_i}} \\\\ \\boldsymbol{T_{e x t}} &amp; =\\boldsymbol{R}^{T} \\sum\\left(\\boldsymbol{(x_i-b)})\\right (\\times \\boldsymbol{f_{e x t_i}})+\\boldsymbol{R}^{T} \\sum \\boldsymbol{\\tau_{e x t_j}}\\end{align}\\]where $\\boldsymbol{x_i}$ is the point in the inertial frame where $\\boldsymbol{f_{ext_i}}$ acts and represents the external forces and moments experienced by the glider w.r.t body-fixed frame. In the simulator code, these are expressed as a vector of the drag, lift, and hydrodynamic moments, as per the equations (2.14) - (2.16) and (2.20) - (2.23).Control TransformationIn equation (3.1), the control inputs $\\boldsymbol{\\bar{u}}$, $\\boldsymbol{u_b}$, and $\\boldsymbol{u_w}$ are equivalent to the forces acting on the point masses $\\bar{m}$, $m_b$ and $m_w$. However, as mentioned in Section 2.3, it is preferred to provide a vector of accelerations of the internal point masses instead.As a result, the controls need to be transformed and the state vector needs to be changed from $z = (R, b, \\Omega, v, r_p, r_b, P_p, P_b, P_w, \\dot m_b)^T$ to $x = (R, b, \\Omega, v, r_p, r_b, \\dot r_p, \\dot r_b, \\dot r_w, \\dot m_b)^T$.This changes the control vector from\\[\\begin{align} \\boldsymbol{u}=\\left(\\begin{array}{c}\\boldsymbol{\\bar{u}} \\\\\\boldsymbol{u_b} \\\\\\boldsymbol{u_w}\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{\\dot{P}_p} \\\\\\boldsymbol{\\dot{P}_b} \\\\\\boldsymbol{\\dot{P}_w}\\end{array}\\right)\\end{align}\\]corresponding to the forces on the internal point masses into\\[\\begin{align} \\boldsymbol{w}=\\left(\\begin{array}{c}\\boldsymbol{w_p} \\\\\\boldsymbol{w_b} \\\\\\boldsymbol{w_w}\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{\\ddot{r}_p} \\\\\\boldsymbol{\\ddot{r}_b} \\\\\\boldsymbol{\\ddot{r}_w}\\end{array}\\right)\\end{align}\\]In this control transformation, we specify $\\boldsymbol{Z = (Z_p, Z_b, Z_w)}^T$ as the drift vector field and $\\boldsymbol{F}$ as the control vector field. $\\boldsymbol{F}$ is computed to be\\[\\begin{align} \\boldsymbol{F} &amp; =\\left(\\begin{array}{ccc}\\boldsymbol{M}^{-1}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+\\frac{1}{\\bar{m}} \\mathcal{I} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\\\ \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+\\frac{1}{m_b} \\mathcal{I} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\\\ \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}+\\frac{1}{m_w} \\mathcal{I}\\end{array}\\right) \\end{align}\\]The determinant of $\\boldsymbol{F}$ is always greater than zero, hence $\\boldsymbol{H} = \\boldsymbol{F}^{-1}$.\\[\\begin{align}\\boldsymbol{Z_{p}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\text {ext }}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{p}} \\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right. \\nonumber\\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{p}} \\\\\\boldsymbol{Z}_{\\boldsymbol{b}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{e x t}}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{b}}\\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right.\\nonumber \\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{b}} \\\\\\boldsymbol{Z}_{\\boldsymbol{w}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{e x t}}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{w}}\\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right. \\nonumber\\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\end{align}\\]We can then transform the control inputs as\\[\\begin{align} \\left(\\begin{array}{c}\\bar{\\boldsymbol{u}} \\\\\\boldsymbol{u}_{\\boldsymbol{b}} \\\\\\boldsymbol{u}_{\\boldsymbol{w}}\\end{array}\\right)=\\boldsymbol{H}\\left(\\begin{array}{c}-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}} \\\\-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}} \\\\-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\end{array}\\right)=\\left(\\begin{array}{c}H_{11}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{12}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{13}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right) \\\\H_{21}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{22}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{23}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right) \\\\H_{31}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{32}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{33}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right)\\end{array}\\right) \\end{align}\\]Motion in Vertical PlaneThe equations of motion are derived by restricting the model to the vertical plane. This means that the Euler angles $\\phi = 0$ and $\\psi = 0$. It follows a sawtooth trajectory, which is basically a repetitive cycle of ascending and descending. The offset static point mass is eliminated $m_w=0$, and the ballast mass is fixed to the glider‚Äôs CoB, $\\boldsymbol{r_b=0}$. We also restrict the internal movable mass $\\bar{m}$ to move only along the $X_0$ axis. Thereby,\\[\\begin{aligned}\\boldsymbol{R}=\\left(\\begin{array}{ccc}\\cos \\theta &amp; 0 &amp; \\sin \\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin \\theta &amp; 0 &amp; \\cos \\theta\\end{array}\\right), \\boldsymbol{b}=\\left(\\begin{array}{c}x \\\\ 0 \\\\ z\\end{array}\\right), \\boldsymbol{v}=\\left(\\begin{array}{c}v_1 \\\\ 0 \\\\ v_3\\end{array}\\right), \\boldsymbol{\\Omega}=\\left(\\begin{array}{c}0 \\\\ \\Omega_2 \\\\ 0\\end{array}\\right), \\\\ \\boldsymbol{r}_{\\boldsymbol{p}}=\\left(\\begin{array}{c}r_{P 1} \\\\ 0 \\\\ r_{P 3}\\end{array}\\right), \\quad \\boldsymbol{P}_{\\boldsymbol{p}}=\\left(\\begin{array}{c}P_{P 1} \\\\ 0 \\\\ P_{P 3}\\end{array}\\right), \\quad \\overline{\\boldsymbol{u}}=\\left(\\begin{array}{c}u_1 \\\\ 0 \\\\ u_3\\end{array}\\right),\\end{aligned}\\]The glider speed $V$ is given by $V = \\sqrt{v_1^2 + v_3^2}$.The glide path angle $\\xi$ is given by $\\xi = \\theta - \\alpha$.Glide EquilibriumThe conditions for glide equilibrium are calculated at every extreme of the sawtooth trajectory. The desired trajectory is computed by specifying the desired glide path angle $\\xi_d$ and desired glide speed $V_d$.\\[\\begin{align} \\theta_{d} = \\xi_{d} + \\alpha_{d}, v_{1_d} = V_dcos\\alpha_{d}, v_{3_d} = V_{d}sin\\alpha_{d} \\end{align}\\]\\[\\begin{align} P_{P1_d} = \\bar{m}v_{1_d}, P_{P3_d} = \\bar{m}v_{3_d}\\end{align}\\]The admissible values of $\\xi_d$ should lie in the range,\\[\\begin{align} \\xi_d \\in\\left(\\tan ^{-1}\\left(2 \\frac{K_D}{K_L}\\left(\\frac{K_{L_0}}{K_L}+\\sqrt{\\left(\\frac{K_{L_0}}{K_L}\\right)^2+\\frac{K_{D_0}}{K_D}}\\right)\\right), \\frac{\\pi}{2}\\right)\\end{align}\\]or\\[\\begin{align} \\xi_d \\in\\left(-\\frac{\\pi}{2}, \\tan ^{-1}\\left(2 \\frac{K_D}{K_L}\\left(\\frac{K_{L_0}}{K_L}-\\sqrt{\\left(\\frac{K_{L_0}}{K_L}\\right)^2+\\frac{K_{D_0}}{K_D}}\\right)\\right)\\right)\\end{align}\\]The desired angle of attack,\\[\\begin{align} \\alpha_d=\\frac{1}{2} \\frac{K_L}{K_D} \\tan \\xi_d \\times\\left(-1+\\sqrt{1-4 \\frac{K_D}{K_L^2} \\cot \\xi_d\\left(K_{D_0} \\cot \\xi_d+K_{L_0}\\right)}\\right)\\end{align}\\]The desired ballast mass,\\[\\begin{align} m_{b_d}=\\left(m-m_h-\\bar{m}\\right)+\\frac{1}{g}\\left(-\\sin \\xi_d\\left(K_{D_0}+K_D \\alpha_d^2\\right)+\\cos \\xi_d\\left(K_{L_0}+K_L \\alpha_d\\right)\\right) V_d^2\\end{align}\\]The desired excess mass is given by,\\[\\begin{align} m_{0_d} = m_{b_d} + m_h + \\bar{m} - m\\end{align}\\]If we assume $r_{P3}$ to be fixed, that is, the mass $\\bar{m}$ has only 1 Degree of Freedom (DoF) and moves only along the axis $X_0$, we have\\[\\begin{align} r_{P 1_d}=-r_{P 3_d} \\tan \\theta_d+\\frac{1}{\\bar{m} g \\cos \\theta_d}\\left(\\left(m_{f 3}-m_{f 1}\\right) v_{1_d} v_{3_d}+\\left(K_{M_0}+K_M \\alpha_d\\right) V_d^2\\right)\\end{align}\\]Motion in Three DimensionsIn three dimensions, we also consider the yaw, sideslip and roll dynamics of the vehicle. The sideslip angle $\\beta$ is the lateral angle between the glider velocity and the longitudinal $X_0$ axis.\\[\\begin{align} \\beta = \\sin ^ {-1} (\\frac{v_2}{\\lVert \\boldsymbol{V} \\rVert})\\end{align}\\]The Euler angles $\\phi$ and $\\psi$ are no longer zero. The offset static point mass is still eliminated $m_w = 0$, and the ballast mass is fixed at the glider‚Äôs CoB, $\\boldsymbol{r_b = 0}$. According to the mode of turning, which is discussed in the subsequent section, the internal movable mass $\\bar{m}$ can have 1 Degree of Freedom (only along $X_0$) or 2 Degrees of Freedom (along $X_0$ and $Y_0$).TurningThere are two ways in which a glider can actuate its yaw: Using a Rudder: An external hydrodynamic surface such as a rudder can produce a turning moment on the glider. Rolling to cause a turn: The internal mass actuator has a degree of freedom along the lateral $Y_0$ axis and produces a roll which, in turn, induces a yaw and causes a banked turn.When considering the design aspects of a glider‚Äôs yaw control, it is important to consider the frequency of inflections during the glider‚Äôs operations. If roll is used to control the glider‚Äôs yaw, it becomes necessary to change the roll direction each time the glider transitions from an upward to a downward glide. This may be unfavourable in operations involving multiple inflections. On the other hand, in using a rudder, the relationship between the rudder angle and yaw rate remains independent of the glide direction.While designing a glider with roll actuation for yaw control, the roll/yaw relationship also plays a crucial role. Aircrafts typically have a positive roll/yaw relationship, where a roll to the right produces a yaw to the right when gliding downwards. In case of underwater gliders, we see examples of both positive and negative roll/yaw relations. In this thesis, we consider the positive roll/yaw relation of the glider while designing the simulator.Glide EquilibriumThe gliding equilibria are solutions of the three dimensional dynamic equations with \\(\\boldsymbol{\\dot{\\Omega}} = \\boldsymbol{\\dot{v}} = \\boldsymbol{\\dot{r}_p} = \\boldsymbol{\\ddot{r}_p} = \\dot{m}_b = 0\\). The same vertical plane equations for \\(\\theta_d, \\xi_d, \\alpha_d, m_{b_d}, m_{0_d}\\), and \\(r_{P1_d}\\) from Section 3.2.1 are used.To note the characteristics of spiral glides of AUGs, the thesis \\cite{bhatta2006nonlinear} provides proofs on observations about spiral motion equilibrium, which can be summarized as: The AUG moves with constant velocity along the circular helix. The pitch and roll angles are constant, while the yaw changes at a constant rate. The axis of the circular helix is aligned with the direction of gravity.The same can also be seen from the plots obtained from the simulator, which are displayed in Section 4.4.The radius of the spiral traversed by the glider is given by,\\[\\begin{align} R=\\frac{V \\cos (\\theta-\\alpha)}{\\Omega_3}\\end{align}\\]Simulation ResultsThe complete code for the AUG Simulator is made open-source and can be found at: AUG-Simulator.The code is written in the Python programming language.Simulator InterfaceThe following are the various arguments that can be given to the simulator:usage: main.py [-h] [-i] [-m MODE] [-c CYCLE] [-g GLIDER] [-a ANGLE] [-s SPEED] [-pid PID] [-r RUDDER] [-sr SETRUDDER] [-p [PLOT ‚Ä¶]] -h ‚Äìhelp show this help message and exit -i ‚Äìinfo give full information in each cycle -m MODE ‚Äìmode MODE set mode as 2D, 3D, or waypoint -c CYCLE ‚Äìcycle CYCLE number of desired cycles in sawtooth trajectory -g GLIDER ‚Äìglider GLIDER [‚Äòslocum‚Äô] -a ANGLE ‚Äìangle ANGLE desired glider angle -s SPEED ‚Äìspeed SPEED desired glider speed -pid PID ‚Äìpid PID enable or disable PID control -r RUDDER ‚Äìrudder RUDDER enable or disable rudder -sr SETRUDDER ‚Äìsetrudder SETRUDDER desired rudder angle. Defaults to 10 degrees -p [PLOT ‚Ä¶] ‚Äìplot [PLOT ‚Ä¶] variables to be plotted [3D, all, x, y, z, omega1, omega2, omega3, vel, v1, v2, v3, rp1, rp2, rp3, mb, phi, theta, psi] Parameter IdentificationThe values of parameters for the Slocum glider are shown in Table 4.2 and Table 4.3. Obtaining these values requires us to perform CFD analysis, and are hence picked up from research papers that have previously done the same. Body length 1.5 m Radius 0.11 m Hull mass $m_h$ 40 kg Ballast mass $m_b$ 1 kg Internal movable mass $\\bar{m}$ 9 kg Offset static point mass $m_w$ 0 kg Fluid displacement mass $m$ 50 kg $m_{f_{1,2,3}}$ 5, 60, 70 kg $J_{1,2,3}$ 4, 12, 11 kg.m^2 $K_{L_0}$ 0 $K_L$ 132.5 $K_{\\beta}$ 20 $K_{D_0}$ 2.15 $K_D$ 25 $K_{M_0}$ 0 $K_M$ -100 $K_{MY}$ 100 $K_{MR}$ -60 $K_{q1}$ -20 $K_{q2}$ -60 $K_{q3}$ -20 We specify the desired glide path angle to be $\\xi_d = \\deg{25}$ and desired glider speed as $V_d = 0.3 m/s$. These same values are used throughout to obtain plots for sawtooth as well as spiral motions.Upon calculating the values at glide equilibrium for the downward dive at $t=0$, we get the values shown in Table 4.4. $\\xi_d$ -25 deg $\\theta_d$ -23 deg $\\alpha_d$ 2 deg $v_{1_d}$ 0.299 m/s $v_{3_d}$ 0.0106 m/s $r_{p1_d}$ 0.0198 m $r_{p3_d}$ 0.05 m $m_{b_d}$ 1.047 kg Sawtooth MotionThe slide-slip angle $\\beta = 0$ deg since there is no component of velocity along the $Y_0$ axis, due to which the viscous side force $SF$ and moments $M_{DL_1}$, $M_{DL_3}$ are all equal to $0$.The images below show the sawtooth trajectory and the relevant plots for 4 cycles (1 cycle is considered to be 1 upward or downward motion).Notice in the above figure, $y$ remains zero at all times since the motion is restricted to the $xz$ plane. The roll $\\phi$ and yaw $\\psi$ are also zero, while the pitch $\\theta$ changes at each inflection.In the above figure, $\\Omega_1, \\Omega_3$ are zero since there is no rotation about the $X_0$ and $Z_0$ axes. $v_2$ is also zero since there is no component of velocity in the lateral direction.Here, we see that the component of \\(\\boldsymbol{r_p}\\) along $X_0$ axis, i.e., $r_{p1}$ reaches the desired position \\(r_{p1_d}\\) at each inflection. \\(r_{p2}\\) is zero while \\(r_{p3}\\) is fixed at \\(5 \\space cm\\). The image below shows the change in values of acceleration given to \\(\\bar{m}\\) to change its position \\(r_{p1}\\) along the \\(X_0\\) axis. A value of \\(0.02 \\space m/s^2\\) is given at the beginning of each inflection and remains zero at every other instant.The simulator also allows us to simulate the sawtooth motions for \\textit{n} number of cycles with the -c CYCLE or ‚Äìcycle CYCLE argument. The images below show the sawtooth trajectory and the relevant plots for 6 cycles. The plots are similar to the ones shown above.Spiral MotionThe presence of a side-slip angle, given by equation (3.22), results in a side force and hydrodynamic moments \\(M_{DL_1}, M_{DL_3}\\), which stimulates the glider‚Äôs lateral dynamics.As mentioned previously, gliders can actuate their heading in two ways - using a rudder or by rolling, and the subsequent plots obtained from the simulator are displayed below.Using a RudderThe presence of a rudder changes the hydrodynamic forces and moments, particularly the drag $D$, side force $SF$ and moment \\(M_{DL_3}\\). The hydrodynamic rudder coefficients are set as \\(K_{D_\\delta} = 2.0, K_{SF_\\delta} = 5.0, K_{MY_\\delta} = 1.0\\), and a rudder angle $\\delta$ is chosen.As a result, the viscous forces and moments equations change as follows,\\[\\begin{align}D &amp; =\\left(K_{D_0}+K_D \\alpha^2 + K_{D_\\delta} \\delta^2\\right)\\left(V^2\\right)\\\\ SF &amp; =\\left(K_{\\beta}\\beta + K_{SF_\\delta}\\delta\\right)\\left(V^2\\right) \\\\M_{D L_3} &amp; =\\left(K_{M Y} \\beta +K_{q 3} \\Omega_3 + K_{MY_\\delta}\\delta\\right)\\left(V^2\\right)\\end{align}\\]When rudder is set at $\\delta = 25$ deg:Equilibrium roll angle of glider: -0.006355428997639093 degEquilibrium pitch angle of glider: -23.18914824153443 degSideslip angle of glider: -0.14978467830958261 degEquilibrium glide speed: 0.26533944912351476 m/sRadius : 14.659688296097228 mFrom the above plots, we see that the roll and pitch reach stable values, while the yaw changes at a constant rate. $r_{p2}, r_{p3}$ are zero and the ballast mass remains constant throughout the glide. When the rudder is set at a high angle, it forces the glider to spiral down in a smaller circle, and hence makes more spirals in the given simulation time (2000s).When rudder is set at a smaller angle, the glider moves along a circle of larger radius, and hence makes lesser spirals in the given time. The plots when the rudder is set at $\\delta = 15$ deg are shown below:Equilibrium roll angle of glider: -0.061902809708920835 degEquilibrium pitch angle of glider: -23.41541331190702 degSideslip angle of glider: -0.04893320699757756 degEquilibrium glide speed: 0.28140084245628366 m/sRadius : 24.39382252404712 mRolling to TurnWhile in most AUGs, such as the Slocum glider, the internal movable mass $\\bar{m}$ has only 1 DOF and uses a rudder to turn, this simulator just offers an additional functionality to move $\\bar{m}$ even along the lateral $Y_0$ axis to produce a roll, and thereby induce a yaw. Here, $r_{p2_d}$ is set as 0.02 m and is the amount by which $\\bar{m}$ moves along $Y_0$.Equilibrium roll angle of glider: 17.338401948861858 degEquilibrium pitch angle of glider: -22.924819423213446 degSideslip angle of glider: 3.2097452502678503 degEquilibrium glide speed: 0.3110046819320383 m/sRadius : 13.07315069665681 mThe image above shows the plots of $r_{p1}$ and $r_{p2}$ until they reach their desired values. $r_{p3}$ is fixed at \\(0.05 \\space m\\) throughout the glide. The figure below shows the acceleration $w_{p2}$ given to $\\bar{m}$ to change $r_{p2}$.PID ControlTo maximize the benefits of underwater gliders in ocean sampling and other applications, a reliable control system is crucial. Efficient gliding, whether precise elevation or heading, depends on precise control. Improvements in control enhance gliders‚Äô utility in scientific missions, with feedback systems ensuring robustness against disturbances during prolonged missions.In this thesis, two types of PID (Proportional Integral Derivative) controllers are presented - one for pitch control and the other for yaw control.Sawtooth Pitch ControlEssentially a PD controller, the gain values are set as $K_p = 0.05, K_i = 0.0, K_d = 0.0005$. The controller takes in the current value of the pitch $\\theta$ and the desired reference value of pitch $\\theta_d$ as the inputs and outputs the acceleration of the internal movable mass $\\bar{m}$ to control its position $\\boldsymbol{r_p}$.The plots below are similar to the ones presented previously in the sawtooth motion results but with much lesser fluctuations of the glider pitch $\\theta$.Since the model is again restricted to the $xz$ plane, the values of $y, \\phi, \\psi, \\Omega_1, \\Omega_3, v_2$ are always zero.In the figure above, we see that $r_{p1}$ varies to obtain a more controlled pitch, as opposed to what we see in Figure 4.15. Figure 5.6 shows the acceleration $w_{p1}$ applied to the internal movable mass $\\bar{m}$ along $X_0$ axis, which is the output of the PID controller, and causes $r_{p1}$ to change.Heading Control with RudderA PD controller, the gain values are set as $K_p = 0.05, K_i = 0.0, K_d = 0.05$. The controller takes in current value of the yaw $\\psi$ and the desired reference value of yaw $\\psi_d$ as the inputs and outputs the desired rudder angle $\\delta$, which in turn, changes the yaw of the glider.In this example, the desired yaw to be achieved $\\psi_d$ is set as 40 deg, and the glide angle is the same as before. In the figure below, the blue line makes an angle of 40 deg with the x-axis and marks the desired direction of motion of the AUG. The red line shows the actual trajectory of the glider, which is fitted with the yaw controller.The image above shows the variation in the rudder angle, which is the output of the PID controller. Initially starting off at a high value, it varies until the yaw $\\psi$ matches with the desired value $\\psi_d$ and then stabilizes at zero, which means there is no further change in the yaw angle.The image above shows the variation of the yaw as it starts at 0 deg and reaches the desired value $\\psi_d = 40$ deg.The image below shows the variation of $r_{p1}$ to reach the desired pitch $\\theta_d$, as done previously." }, { "title": "Multi-Agent Traffic Signal Control using Reinforcement Learning", "url": "/posts/phase-duration-control-rl/", "categories": "Projects, RL", "tags": "rl", "date": "2023-11-11 11:00:00 -0500", "snippet": "The following work was carried out during my summer internship at the Multi-Agent Robotic Motion Laboratory, National University of Singapore under the supervision of Dr. Guillaume Sartoretti.Hybrid PPOHybrid Proximal Policy Optimization (Hybrid PPO) is an innovative actor-critic algorithm designed for learning in a parameterized action space. The key feature of H-PPO lies in its utilization of parallel actors, where discrete and continuous action selection are performed independently. This architecture decomposes the complex action space into simpler components, enhancing the learning efficiency of the model. The algorithm employs a global critic network to update the policy parameters of both the discrete and continuous actors.H-PPO operates in a parameterized action space, allowing the agent not only to select discrete actions but also to choose continuous parameters associated with those actions. This flexibility enables the model to make nuanced decisions, crucial for tasks with intricate action requirements.ArchitectureParallel Actors: Discrete Actor Network (œÄŒ∏d): Learns a stochastic policy for discrete actions. It outputs k values fa1 , fa2 , . . . , fak for the k discrete actions, and the discrete action ‚Äòa‚Äô is randomly sampled from the softmax(f) distribution. Continuous Actor Network (œÄŒ∏c): Learns a stochastic policy for continuous parameters associated with discrete actions xa1 , xa2 , . . . , xak. It generates the mean and variance of a Gaussian distribution for each parameter.The complete action to execute is the selected action a paired with the chosen parameter xa corresponding to action a. The two actor networks shares the first few layers to encode the state information.H-PPO incorporates a single critic network (V(s)) that estimates the state-value function. Using the state-value function instead of the action-value function mitigates over-parameterization issues in the parameterized action space.The discrete policy œÄŒ∏d and the continuous policy œÄŒ∏c are updated separately by minimizing their respective clipped surrogate objective.The probability ratio rtd (Œ∏d) only considers the discrete policy and rtc (Œ∏c) only considers the continuous policy. Even though the two policies work with each other to decide the complete action, their objectives are not explicitly conditioned on each other. œÄŒ∏d and œÄŒ∏c are viewed as two separate distributions instead of a joint distribution.RewardsIn traffic signal control, the choice of reward functions plays a pivotal role in shaping the behavior of reinforcement learning-based approaches. We investigate two prominent reward strategies: Max Pressure (MP) and Intensity-based Proximal Dynamic Adaptive Light (IPDALight). While MP prioritizes throughput and congestion minimization, IPDALight introduces a nuanced perspective by considering detailed vehicle dynamics, such as speed and position, to enhance traffic signal control policies.Max Pressure (MP) ControlThe objective is to minimize intersection pressure defined as the difference between the number of vehicles on incoming and outgoing lanes.Strengths: Throughput Maximization: MP aims to maximize the flow of vehicles through intersections, optimizing for high throughput. Simple and Effective: Known for its simplicity, MP often yields effective solutions for reducing congestion and delays.Limitations: Locally Optimal Solutions: MP can be greedy, leading to locally optimal solutions that may not be globally efficient. Lack of Vehicle Dynamics: The method does not consider vehicle dynamics, ignoring factors such as speed and position.IPDALight: Intensity-based Proximal Dynamic Adaptive LightThe objective is to minimize the intensity of intersections, considering detailed vehicle dynamics, including speed and position.Features: Intensity Calculation: Considers the speed, position, and distance of vehicles approaching an intersection to compute intensity. Logarithmic Scale: Mitigates the impact of large product values, offering a more balanced intensity metric. Dynamic Phase Duration: Proposes a heuristic to fine-tune phase duration based on dynamic traffic situations.\\[\\mathcal{T}_{v e h}=\\log \\left(\\frac{L-x}{L} \\times \\frac{\\delta \\times\\left(v_{\\max }-v\\right)}{v+1}+1\\right)\\]Intensity of vehicle increases when it approaches an intersection or the speed decreases due to traffic congestion.$(L-x)/L$ shows the impact of distance on intensity: closer the vehicle is to the intersection, greater is the intensity.The other term indicates the impact of slower vehicles on intensity. So by definition, vehicles crossing an intersection with higher speed will pose lower intensity on the intersection.Intensity of Lanes: sum of intensity of all vehicles on the lane.\\[\\mathcal{T}_{\\text {lane }}=\\sum_{\\text {veh }_i \\in \\text { lane }} \\mathcal{T}_{\\text {veh }_i}\\]Intensity of Intersections: It is the intensity difference between the incoming lanes and the outgoinglanes. It evaluates the overall traffic pressure suffered by the intersection.\\[\\mathcal{T}_{\\mathcal{I}}=\\sum_{\\text {lane }_i \\in \\text { lane }_{\\text {in }}} \\mathcal{T}_{\\text {lane }_i}-\\sum_{\\text {lane }_j \\in \\text { lane }_{\\text {out }}} \\mathcal{T}_{\\text {lane }_j}\\]Reward definition - The intensity of intersections reflects the average travel time of vehicles crossing the intersection more accurately than the MP-model because it considers more vehicle dynamics. So their reward is $r$ = $-$(Intensity of $I$)Advantages: Accurate Representation: Reflects average travel time more accurately by considering detailed vehicle dynamics. Adaptive Phase Duration: Provides a heuristic to adjust phase duration, adapting to changing traffic conditions.Challenges: Complexity: The inclusion of detailed vehicle dynamics adds computational complexity compared to MP. Implementation Challenges: Fine-tuning phase duration requires careful implementation and real-time adaptation mechanisms.Results" }, { "title": "Swarm Robot Coordination", "url": "/posts/swarm-robot-coordination/", "categories": "Projects, Robotics", "tags": "swarm, python, simulation", "date": "2023-05-21 12:00:00 -0400", "snippet": "AbstractSwarm robotics is an emerging field that enables a group of robots to collaborate and achieve complex tasks that are difficult for a single robot to accomplish. One of the key challenges in swarm robotics is the coordination of multiple robots‚Äô movements towards a common goal. This report provides a comprehensive overview of swarm robot coordination, with a specific focus on four common tasks: aggregation, dispersion, line formation, and shape formation.We discuss the different algorithms used for swarm robot coordination, including Potential Fields Algorithm, Centroid based Algorithm, and Line Marching Algorithm, and their respective advantages for each task. Furthermore, we present the simulation results for each task, demonstrating the effectiveness of these algorithms in achieving coordinated movements among the various swarm robots.AggregationSwarm aggregation refers to the process where a group of agents, often called a swarm, come together to form a single, cohesive group or cluster. This behavior is commonly observed in nature, such as with swarming insects, schooling fish, or flocking birds. In the context of robotics and artificial intelligence, swarm aggregation algorithms enable a group of robots or agents to converge to a single location or gather around a point of interest. The goal of these algorithms is to achieve robust, adaptive, and scalable aggregation, while maintaining simplicity and minimizing communication.Fields Algorithm Define constants: NUM_ROBOTS (number of robots), ROBOT_RADIUS (radius of each robot), SWARM_RADIUS (radius of the swarm area), and AGGREGATION_THRESHOLD (a distance threshold below which robots don‚Äôt move closer to the center of mass). Create the swarm of robots. For each robot, generate random initial positions within the swarm area. Main a. Calculate the center of mass of the swarm by summing the positions of all robots and dividing by the number of robots. b. Move each robot towards the center of mass: calculate the direction vector from the robot‚Äôs position to the center of mass. Update the positions of the robots in the scatter plot.In summary, this code demonstrates a simple swarm robot aggregation behavior by simulating the motion of robots towards the center of mass of the swarm.ResultsDispersionDispersion in swarm robotics refers to a group of robots‚Äô capacity to disperse over a wide region while still interacting and coordinating with one another. In other words, it requires evenly distributing robots around a place but still enabling them to cooperate as a group to achieve a shared objective.Dispersion is a crucial capability in swarm robotics, as it enables the robots to accomplish tasks such as environmental monitoring, search and rescue operations, and surveillance, among others. The robots can cover a broader area more rapidly and effectively when they are dispersed than when they are grouped together in one location. This can also lessen the chance of crashes or congestion, which may happen if the robots are too closely grouped together.Several algorithms have been developed to perform dispersion, with the Potential Fields Algorithm being one of the most commonly used.Potential Fields AlgorithmThe Potential Fields Algorithm is a popular method for controlling dispersion of swarm robots. In this method, each robot is represented as a point in a potential field, that is created by a combination of attractive and repulsive forces. While the repulsive forces assist the robots in avoiding objects or other robots, the attractive forces motivate them to go in the direction of a desired place.The following steps can be taken to implement the algorithm: Define the target dispersion area: First, the target dispersion area needs to be defined. This can be done by defining a virtual boundary around the area where the swarm robots need to be dispersed. Define the attractive force: Next, an attractive force needs to be defined to encourage the robots to move towards the target dispersion area. This can be done by setting up a virtual potential field with a low magnitude force that pulls the robots towards the center of the target area. Define the repulsive force: A repulsive force needs to be defined to prevent the robots from clustering together. This can be done by setting up a virtual potential field with a high magnitude force that pushes the robots away from each other. Update the forces: The attractive and repulsive forces need to be updated continuously based on the position of the robots and the obstacles in the environment. This can be done by using sensors or cameras to detect the position of the robots and obstacles and adjusting the virtual potential field accordingly. Move the robots: Finally, the robots can be moved based on the resultant force vector calculated from the attractive and repulsive forces. This can be done by using differential drive motors or other types of actuators.AdvantagesThe Potential Fields Algorithm has several advantages that make it a popular method for controlling the movement of swarm robots. Some of the key advantages of this algorithm are: Scalability: The algorithm is highly scalable, meaning that it can be used to control the movement of large numbers of robots in a swarm. This makes it ideal for applications where multiple robots need to work together to accomplish a task. Robustness: The algorithm is robust to changes in the environment, as it continuously adapts to changes in the position of obstacles or other robots. This makes it ideal for use in dynamic and unpredictable environments, such as disaster zones or construction sites. It is effective in complex environments with many obstacles and goals, as it provides a smooth and continuous way to move the robots. It can ensure that the robots converge to their goal location and avoid getting stuck in local minima.ResultsLine FormationLine formation in swarm robotics is a task in which a group of robots organizes themselves into a linear structure, following a predefined path or creating a path as they move. The goal of line formation is to achieve a linear arrangement of robots with a certain level of spacing between them. This task is useful in various scenarios, such as for search and rescue missions, inspection of long pipelines or cables, and monitoring of large areas.In line formation, robots must be able to move in a coordinated way to avoid collisions and maintain the desired distance between each other. They need to communicate with each other to share information about their positions, orientations, and velocities. Several algorithms have been developed to achieve line formation in swarm robotics, and one of the popular algorithms used for this task is the Line Marching Algorithm.Line Marching AlgorithmThe Line Marching Algorithm (LMA) is a swarm robotics algorithm used for line formation. It involves coordinating a group of robots to form a line that follows a predefined path. The algorithm involves dividing the line formation task into two sub-tasks: Moving forward along the line while maintaining the desired distance between robots. Correcting the deviations from the line caused by disturbances or errors in the robot‚Äôs motion.To use the Line Marching Algorithm, the following steps can be taken: Define the line: The first step is to define the line that the robots will form. The line can be defined using a series of points, with each point assigned a unique ID. The distance between each point should be small enough to ensure that the robots can move smoothly along the line. Initialize the swarm: Next, the swarm of robots is initialized. Each robot is given a unique ID and placed in a random position on the field. Calculate distance to line points: Each robot calculates its distance to all points on the line and determines the closest point. The robot then moves towards that point. Move towards the line: The robots move towards the closest point on the line using a simple proportional control system. The robot‚Äôs speed and direction are adjusted based on its distance to the line and the desired speed of the swarm. Maintain distance from neighbors: As the robots move towards the line, they must also maintain a minimum distance from their neighbors. This is done using a repulsion force, which pushes the robot away from any neighbors that are too close. The repulsion force is calculated based on the distance between the robots and the desired minimum separation. Adjust movement: If a robot encounters an obstacle or another robot, it must adjust its movement to avoid a collision. This is done using a simple obstacle avoidance algorithm. Continue to adjust: The swarm continues to adjust its movement until it forms a straight line that follows the path defined by the points. Maintain the line: Once the line is formed, the swarm must maintain it. This is done using a combination of attraction towards the line and repulsion from other robots. Each robot calculates its position on the line and moves towards the point that corresponds to that position. At the same time, it must also maintain a minimum distance from its neighbors.Advantages Decentralization: The Line Marching algorithm is a decentralized approach to formation control that does not require a centralized control unit. Each robot only needs to communicate with its nearest neighbors, which reduces the communication overhead and improves the scalability of the system. Robustness to communication delays: The Line Marching algorithm is robust to communication delays, which are common in wireless networks. This is because the algorithm uses local information to compute the desired positions of each robot in the formation, rather than relying on global information. Stability: The Line Marching algorithm is designed to maintain formation stability and avoid collisions between robots. This is achieved by imposing separation constraints between neighboring robots, which ensures that the robots maintain a safe distance from each other.Code StructureResultsShape FormationSwarm shape formation is a process in which a group of agents, often referred to as a swarm, organizes themselves into specific shapes or patterns. This is often observed in nature, such as with flocking birds or schooling fish, and has been extensively studied in the context of robotics and artificial intelligence. The goal of swarm shape formation algorithms is to enable the agents to coordinate and maintain the desired shape or pattern while maintaining a high degree of robustness, adaptability, and scalability.There are several algorithms and approaches to swarm shape formation, some of which are outlined below:Reynolds‚Äô Boids algorithm: It is based on three simple rules: separation (avoid collisions with neighbors), alignment (align velocity with the average velocity of neighbors), and cohesion (move toward the average position of neighbors). By adjusting the weights of these rules, a variety of swarm behaviors, including shape formation, can be achieved.Artificial potential fields: In this approach, agents in the swarm are influenced by potential fields that push or pull them toward specific positions. These fields can be generated based on the desired shape, and each agent calculates its movement based on the sum of forces from these fields. This method enables swarms to form shapes without explicit communication between agents.Graph-based methods: In these methods, the swarm is represented as a graph where nodes correspond to agents and edges represent connections or relationships between them. Shape formation can be achieved through graph manipulation techniques such as graph transformations, graph matching, or graph drawing. Agents can then follow specific rules to move toward their target positions, guided by the graph structure.We used Artificial Potential Fields AlgorithmThe algorithm works as below: Initialize the parameters, such as the number of robots (n_robots), size of the shape (size), number of iterations (iterations), step size (step_size), shape, and collision threshold (collision_threshold). Randomly initialize the positions of the robots in a 2D space. Define a function (generate_target_positions) to generate target positions for the robots based on the chosen shape. Calculate the target positions for the chosen shape. Define a function (avoid_collision) that computes the direction a robot should move to avoid collisions with other robots, based on a given threshold. Create an update function that executes the following steps for each robot: a. Find the closest target position. b. Calculate the direction towards the target position. c. Normalize the direction vector and scale it by the step size. d. Add the collision avoidance direction scaled by the step size. e. Update the robot‚Äôs position by adding the calculated direction. Create a matplotlib animation that iteratively calls the update function, which updates the robots‚Äô positions, and plots the updated positions on a 2D scatter plot.The forces of attraction and repulsion.In short, this algorithm moves robots towards their target positions while avoiding collisions with each other. The robots form the desired shape over a series of iterations while ensuring that they maintain a safe distance from one another.ResultsStart EndStart End" }, { "title": "Swarm Robot Tasks and Algorithms", "url": "/posts/swarm-algorithms/", "categories": "Blog, Robotics", "tags": "swarm, python, simulation", "date": "2023-05-21 12:00:00 -0400", "snippet": "The following are some of the algorithms that can be used to perform different tasks with swarm robots using Python:1. AggregationA Geometry Based Algorithm for Swarm AggregationsIn Geometry Based Algorithm (GBA), the robots rely on the relative positions of their neighboring robots to determine their movements towards a common goal. In general, the GBA approach involves defining a geometric shape that encloses the swarm robots, such as a circle or a polygon. The robots then use information about their relative positions within this shape to determine their movements towards the center or another predetermined location.One example of a GBA algorithm for swarm robot aggregation is the ‚Äúcentroid algorithm,‚Äù which involves calculating the centroid of the swarm robot positions and then moving each robot towards it at a predetermined speed. The speed of each robot can be proportional to the distance between the robot‚Äôs position and the centroid, such that robots that are farther away move faster than those that are closer. Another example is the ‚ÄúVoronoi-based algorithm,‚Äù which involves dividing the space around the swarm robots into Voronoi cells and then moving each robot towards the centroid of its cell.GBA algorithms have the advantage of being relatively simple and easy to implement, requiring only local communication between neighboring robots.2. DispersionThe Randomized Mobility Algorithm (RMA) is a popular algorithm used for dispersion in swarm robots. The goal of RMA is to spread the robots out evenly over a given area, while avoiding collisions with other robots and obstacles.In RMA, each robot moves randomly within the given area, with its movement direction chosen randomly at each time step. However, the movement of each robot is biased towards areas with lower robot density. Specifically, each robot maintains a virtual density map of the area, which is updated based on the positions of nearby robots. The robot then moves in a direction that is biased towards areas with lower density, which encourages it to move away from areas with high robot density.The degree of bias towards lower density areas can be adjusted based on the desired level of dispersion. For example, a higher bias towards lower density areas will result in a more dispersed swarm, while a lower bias will result in a more clustered swarm.One advantage of RMA is its simplicity and ease of implementation. It requires only local communication between neighboring robots and does not rely on complex algorithms or calculations.3. Line FormationLine Marching Algorithm For Planar Kinematic Swarm Robots: ADynamic Leader-Follower ApproachThe Line Marching Algorithm is a simple algorithm used for line formation in swarm robotics. In this algorithm, each robot in the swarm follows a simple set of rules to maintain a constant distance from its neighbors and to align its movement with the desired direction of the line.The Line Marching Algorithm can be implemented in a decentralized way, where each robot is only aware of its local neighbors, or in a centralized way, where a master robot controls the movements of the swarm.The algorithm is based on three main rules: Separation: each robot maintains a minimum distance from its neighbors to avoid collisions. Alignment: each robot aligns its movement with its neighbors to maintain the direction of the line. Cohesion: each robot moves towards the average position of its neighbors to maintain the formation. To implement these rules, each robot measures the distance and orientation to its neighbors and adjusts its speed and direction accordingly. The algorithm can be extended to include obstacles avoidance and other complex behaviors, such as rotation and splitting of the line.The Virtual Structure Algorithm is a decentralized algorithm for line formation in swarm robotics. In this algorithm, the swarm is divided into two groups: leaders and followers. The leaders are responsible for defining the virtual structure that the swarm should follow, while the followers adjust their movements to maintain the desired formation.The Virtual Structure Algorithm can be implemented in different ways, depending on the desired formation and the constraints of the system. One common approach is to use a line as the virtual structure, where the leaders are positioned at the ends of the line and the followers adjust their positions to maintain a constant distance from their neighbors and to align their movements with the line.The leaders can be controlled manually or by a centralized controller, or they can be selected from the followers based on certain criteria, such as connectivity or centrality. The leaders can communicate with their neighbors to coordinate their movements and to adjust the position and orientation of the virtual structure, if needed.The Virtual Structure Algorithm has some advantages over other line formation algorithms, such as scalability, flexibility, and adaptability to changes in the environment. It can also be extended to other formations, such as circles, squares, and polygons.4. Shape FormationThe Distributed Gradient Descent Algorithm is a centralized algorithm that can be used to form various shapes in a swarm of robots, including circles and other polygons. The algorithm works by iteratively adjusting the positions of the robots to minimize a cost function that captures the deviation from the desired shape.In the case of circle formation, the desired shape is a circle with a given radius and center. The cost function is defined as the sum of the squared distances between each robot and its desired position on the circle. The gradient of the cost function is then computed, which indicates the direction of steepest descent. Each robot is assigned a target position on the circle based on its current position and the gradient, and moves towards it at a certain speed. The process is repeated until the robots converge to the desired formation.In the case of polygon formation, the desired shape is a regular polygon with a given number of sides, radius, and center. The cost function is defined as the sum of the squared distances between each robot and its desired position on the polygon. The gradient is computed using a similar approach as in the case of circle formation, but with some modifications to account for the polygon geometry. Each robot is assigned a target position on the polygon based on its current position and the gradient, and moves towards it at a certain speed. The process is repeated until the robots converge to the desired formation.The Distributed Gradient Descent Algorithm can be extended to handle more complex shapes, such as irregular polygons and non-convex shapes.5. FlockingReynolds‚Äô Boids Algorithm is a decentralized algorithm that models the behavior of birds flocking. The algorithm is based on three simple rules that govern the movement of each robot or ‚Äúboid‚Äù in the flock: Alignment: The boid aligns its velocity with the average velocity of its neighbors within a certain radius. This means that the boid tends to move in the same direction as its neighbors. Cohesion: The boid moves towards the center of mass of its neighbors within a certain radius. This means that the boid tends to stay close to its neighbors. Separation: The boid avoids collisions with its neighbors by moving away from any neighbor that is too close within a certain radius. This means that the boid tends to avoid crowding or overlapping with its neighbors.These rules are implemented as simple mathematical equations, and the boid moves based on the net force acting on it. The net force is the sum of the forces generated by the alignment, cohesion, and separation rules. The algorithm can be implemented in a decentralized manner, where each boid only has access to local information about its neighbors.6. Particle Swarm OptimizationIn PSO, a group of particles or agents move in the search space to find the optimal solution to a given problem. Each particle has a position and a velocity, and its movement is governed by its own experience and the experience of the swarm. The algorithm iteratively updates the position and velocity of each particle based on a set of mathematical equations, which are derived from the social behavior of animals.PSO can be used in swarm robotics to optimize the behavior of a swarm of robots for various applications, including warehouse management, such as programming a swarm of robots with one-on-one communication for warehouse management tasks." }, { "title": "Path Planning", "url": "/posts/path-planning/", "categories": "Blog, Robotics", "tags": "planning, astar, dijkstra, search", "date": "2023-03-28 12:00:00 -0400", "snippet": "ResourcesProgrammingPath PlanningPath planning, also known as motion planning, is the process of finding a feasible path from a starting point to a desired goal point while avoiding obstacles in between. This is a fundamental problem in robotics, automation, and computer graphics.There are several types of algorithms used for path planning: Visibility graph algorithm: This algorithm constructs a graph of visibility between the starting point, the goal point, and all obstacles. The nodes of the graph are the starting point, goal point, and the vertices of the obstacles. The edges of the graph connect nodes that have a clear line of sight. This graph is then used to find the shortest path between the starting point and the goal point. Random-exploring algorithms: These algorithms randomly sample the configuration space (the space in which the robot can move) to find a path from the starting point to the goal point. Examples of random-exploring algorithms include Rapidly-exploring Random Trees (RRT) and Probabilistic Roadmaps (PRM). Optimal search algorithms: These algorithms use a search strategy to find the optimal path from the starting point to the goal point. Examples of optimal search algorithms include A* (A-star), Dijkstra‚Äôs algorithm, and Breadth-First Search (BFS). These algorithms use heuristics or cost functions to guide the search towards the goal point while minimizing the path length.Autonomy requires that the robot is able to plan a collision-free motion from an initial to a final posture on the basis of geometric information.Information about the workspace geometry can be entirely known in advance, called off-line planning gradually discovered by the robot, called on-line planningDiscrete PlanningThey are the simplest to describe because the state space will be finite (or countably infinite) in most cases. No forms of uncertainty will be considered.Formulation: A nonempty state space X, with finite or countably infinite set of states. For each state $x \\in X$, a finite action space $U(x).$ A state transition function $f$ that produces a state $f(x,u) \\in X$ for every $x \\in X$ and $u \\in U(x)$. The state transition equation is given by $x‚Äô=f(x,u)$. An initial state $x_I \\in X.$ A goal set $X_G \\subset X.$General Forward SearchPseudo code:The set of alive states is stored in a priority queue, Q, for which a priority function must be specified. The only significant difference between various search algorithms is the particular function used to sort Q. Therefore, assume for now that Q is a common FIFO (First-In First-Out) queue; whichever state has been waiting the longest will be chosen when $Q.GetFirst()$ is called. Initially, Q contains the initial state $x_I$. A while loop is then executed, which terminates only when Q is empty. This will only occur when the entire graph has been explored without finding any goal states, which results in a FAILURE (unless the reachable portion of X is infinite, in which case the algorithm should never terminate). In each while iteration, the highest ranked element, x, of Q is removed. If $x$ lies in $X_G$, then it reports SUCCESS and terminates; otherwise, the algorithm tries applying every possible action, $u \\in U(x)$. For each next state, x‚Äô = f(x,u), it must determine whether x‚Äô is being encountered for the first time. If it is unvisited, then it is inserted into Q; otherwise, there is no need to consider it because it must be either dead or already in Q.Breadth FirstBreadth-First Forward Search (BFS) is a graph traversal algorithm that systematically explores all the vertices (or nodes) of a graph in a breadth-first order, i.e., visiting all nodes at the same level before moving on to the next level.The algorithm starts by visiting the root node (or starting node) and then visits all the nodes at the next level before moving on to the nodes at the next level, and so on until all nodes have been visited. It uses a FIFO queue data structure to keep track of the nodes that have been visited but whose neighbors have not yet been visited. The queue is initialized with the root node, and the algorithm iteratively dequeues a node, visits all its neighbors, and enqueues them if they have not been visited before.The time complexity of BFS is $O(V+E)$, where V is the number of vertices (nodes) in the graph, and E is the number of edges. This is because the algorithm visits each node and each edge at most once. Also, $V=X, E=UX$ if the same actions U are available from every state. It is systematic. The worst-case performance of BFS is worse than that of A* and dynamic programming.Pseudo code: Initialise a queue data structure with the root node. Mark the root node as visited. Dequeue a node from the queue and visit it. Enqueue all the unvisited neighbors of the node. Mark all the enqueued neighbours as visited. Repeat steps 3-5 until the queue is empty.function BFS(start_node): Q = Queue() // Initialize a queue Q.enqueue(start_node) // Enqueue the start node visited = set() // Initialize a set of visited nodes while not Q.empty(): // While the queue is not empty node = Q.dequeue() // Dequeue the next node if node not in visited: // If the node has not been visited visited.add(node) // Mark the node as visited // Process the node (e.g., print its value) print(node) // Enqueue all unvisited neighbors of the node for neighbor in get_neighbors(node): if neighbor not in visited: Q.enqueue(neighbor)Depth FirstDepth First Forward Search (DFS) is a graph traversal algorithm that explores the vertices of a graph in a depth-first manner, starting from a given source vertex, and visiting all the vertices reachable from it.The DFS algorithm can be implemented using a LIFO stack data structure to keep track of the vertices being explored. When a vertex is visited, it is pushed onto the stack, and its unvisited neighbours are added to the stack in the order in which they are encountered. When there are no more unvisited neighbours, the vertex at the top of the stack is popped off and the algorithm backtracks to the previous vertex.The time complexity of DFS algorithm is $O(V+E)$, where V is the number of vertices in the graph and E is the number of edges. The space complexity of the algorithm is O(V) due to the stack used to keep track of the visited vertices. It is systematic. The search could easily focus on one direction and completely miss large portions of the search space as the number of iterations tends to infinity.Pseudo code: Start at the source vertex. Mark the source vertex as visited. Explore each unvisited neighbor of the source vertex in depth-first order. When there are no more unvisited neighbors, backtrack to the previous vertex and repeat step 3 for its unvisited neighbors. Continue this process until all reachable vertices have been visited.function depthFirstForwardSearch(graph, source): // Initialize a stack for DFS traversal and a set to keep track of visited vertices stack = new Stack() visited = new Set() // Push the source vertex onto the stack and mark it as visited stack.push(source) visited.add(source) while not stack.isEmpty(): // Pop the top vertex from the stack and print it currentVertex = stack.pop() print currentVertex // Explore the neighbors of the current vertex for neighbor in graph.getNeighbors(currentVertex): if neighbor not in visited: // Push the neighbor onto the stack and mark it as visited stack.push(neighbor) visited.add(neighbor)Best FirstBest First search is a heuristic search algorithm that is used to find the optimal path between two points in a graph. The algorithm expands the node with the lowest heuristic value first, i.e., the node that appears to be the closest to the goal. The search starts from the initial state and moves towards the goal state. It uses a heuristic function h(n) to evaluate the distance between a node n and the goal state. The algorithm maintains a priority queue (or a heap) of the nodes to be expanded, with the node with the lowest heuristic value at the front of the queue. It is not systematic.Pseudo code: Initialise the open list with the start node as the only element. Initialise the closed list as an empty set. While the open list is not empty, do the following: Remove the node with the lowest heuristic value from the open list. If the removed node is the goal node, return the path. Otherwise, expand the node by generating all its neighbouring nodes. For each neighbouring node, calculate its heuristic value h(n) and add it to the open list if it is not already in the closed list or the open list. Add the expanded node to the closed list. If the open list becomes empty and the goal node is not found, return failure.Iterative DeepeningIterative Deepening Algorithm (IDA) is a general search algorithm based on depth-first search (DFS), but unlike DFS, IDA performs multiple iterations, each with increasing depth limits. The algorithm starts with a depth limit of one and iteratively increases it until the goal is found.The iterative deepening algorithm uses a breadth-first search-like strategy to incrementally search the space by exploring each level of the search tree. The algorithm starts by searching the tree at a depth limit of one. If the goal is not found, the algorithm increases the depth limit to two and searches the tree again. This process is repeated until the goal is found.The iterative deepening algorithm is often used in cases where the search space is too large for a complete search. By limiting the search depth, the algorithm can focus on the most promising parts of the search space, while still being able to find the optimal solution. The time complexity of IDA is O(b^d), where b is the branching factor and d is the depth of the goal node. The space complexity of the algorithm is O(d). It has better worst-case performance than BFS. Furthermore, the space requirements are reduced because the queue in BFS is usually much larger than for DFS.Combinatorial Motion PlanningRoadmap Based Path Planning: Visibility Graph and Generalised Voronoi Diagrams as roadmapsCovers visibility graphs and voronoi diagrams.Combinatorial motion planning is a subfield of robotics and artificial intelligence that focuses on developing algorithms and techniques for planning the motion of robots in complex environments. The goal is to find feasible paths that enable robots to reach their desired goals while avoiding obstacles and adhering to various constraints.Unlike continuous motion planning, which deals with continuous trajectories, combinatorial motion planning involves searching through a discrete set of possible motions and selecting the optimal one. This involves exploring the space of all possible robot configurations and determining which configurations can be reached without colliding with obstacles or violating constraints.Some common techniques used in combinatorial motion planning include graph-based search algorithms, sampling-based planners, and cell decomposition methods. These techniques leverage computational tools and mathematical models to efficiently plan the motion of robots and ensure they operate safely and effectively.Continuous Motion Planning MethodsContinuous motion planning methods are algorithms and techniques used to plan the continuous motion of robots in complex environments. Unlike combinatorial motion planning, which deals with a discrete set of possible motions, continuous motion planning deals with continuous trajectories.Continuous motion planning methods use mathematical models and algorithms to determine optimal trajectories that enable robots to reach their desired goals while avoiding obstacles and adhering to various constraints. These methods often involve the use of calculus and optimization techniques to find the optimal path.They are often more computationally intensive than combinatorial motion planning methods. This is because they involve solving complex mathematical equations and optimizing continuous functions. However, they offer greater flexibility and precision in planning robot motion, enabling robots to operate more efficiently and effectively in complex environments.Examples of continuous motion planning techniques include: Trajectory optimization: This method involves finding the optimal trajectory for a robot by solving an optimization problem that minimizes a cost function. The cost function can be customized to include constraints on the robot‚Äôs motion, such as collision avoidance or energy consumption. Trajectory optimization can be solved using numerical optimization techniques, such as gradient descent or nonlinear programming. Control-based planning: This method involves using a control law to steer the robot along a desired trajectory. The control law can be designed using various techniques, such as feedback linearization or model predictive control. Control-based planning can be used to plan the motion of robots in dynamic environments, where the robot‚Äôs motion must be continuously updated in response to changing conditions. Model predictive control (MPC): MPC is a control-based planning method that involves predicting the future behavior of the robot and optimizing its motion accordingly. MPC uses a dynamic model of the robot and the environment to predict how the robot‚Äôs motion will affect its surroundings, and it optimizes the robot‚Äôs motion to achieve a desired goal while satisfying various constraints. Sampling-based planning: This method involves randomly sampling the robot‚Äôs configuration space to generate a set of feasible trajectories. The set of trajectories is then pruned to remove those that collide with obstacles or violate constraints. Sampling-based planning is computationally efficient and can be used to plan the motion of robots in high-dimensional spaces, but it may not always find the optimal trajectory.RoadmapsRoadmaps in combinatorial motion planning refer to a set of interconnected nodes and edges that are constructed in the configuration space of a robot or a group of robots. The configuration space represents all possible configurations of the robot(s) in the environment, including their position, orientation, and other relevant parameters.The roadmap construction process involves sampling the configuration space and identifying collision-free configurations that can be used as nodes in the roadmap. These nodes are then connected by edges that represent feasible paths between them. The resulting roadmap provides a discrete representation of the configuration space and enables efficient path planning for the robot(s).There are several types of roadmaps that can be used in combinatorial motion planning, including probabilistic roadmaps (PRMs), visibility graphs, and cell decomposition-based roadmaps. Each type has its strengths and weaknesses, and the choice of roadmap depends on the specific application and requirements.There are many roadmap-based approaches, and they primarily fall into 3 categories: Topological Model - Every location is represented by a node, which are connected by edges. Geometrical Model - The geometry of the environment, containing obstacles, is considered and a free space, which is a collection of all free configurations that do not collide with the obstacles, is created. Grid-based Model - The entire environment is divided in cells or grids and based on the connectivity between these grids, roadmaps are created.Formal definition: A union of one dimensional curves R is a roadmap if for all starting positions q and goal positions g, that can be connected by a path, the following properties hold - Accessibility - There exists a path from q to some point q‚Äô on R Departability - There exists a path from a point g‚Äô on R to g Connectivity - There exists a path in R from q‚Äô to g‚ÄôVisibility GraphThe visibility graph is a method used to represent the connectivity of a configuration space. A configuration space is the space that represents all possible positions and orientations of a robot or object.The basic idea is that if we stretched a rubber band from start to target, weaving in and out of the obstacles, the band would form a straight line between the obstacles, and would wrap around the obstacles themselves.The visibility graph is constructed by first placing a node at each obstacle boundary and at the starting and ending configurations of the robot. Then, an edge is added between two nodes if the line segment connecting them does not intersect any obstacle boundaries. This process creates a graph that represents all possible paths that the robot can take without colliding with any obstacles. Its edges are the edges of the obstacles and edges joining all pairs of vertices that can see each other.It is based on the principle that given a start point and a goal point, we look for all vertices of obstacles present in the environment which are visible from the given start location. We then traverse through those vertices to reach the destination in the shortest path.Start, goal, vertices of obstacles are graph nodes. Edges are ‚Äúvisible‚Äù connections between nodes, including obstacle edges.Reduced Visibility GraphA reduced visibility graph consists of very few edges to reduce its complexity. It makes use of supporting and separating lines. Supporting lines are edges in the visibility graph that connect two vertices on the boundary of the same obstacle. The obstacles lie on the same side of the supporting lines. These edges are important because they represent the possibility of the robot following a path along the boundary of an obstacle without colliding with it. Supporting lines can be used to construct a continuous path for the robot along the boundary of an obstacle, which is useful in situations where the robot cannot pass through the obstacle. Separating lines, on the other hand, are edges in the visibility graph that connect two vertices on the boundaries of different obstacles. The obstacles lie on opposite sides of the separating lines. These edges represent the possibility of the robot moving between two obstacles without colliding with either one. Separating lines are important because they define the regions of the configuration space that are accessible to the robot, and can be used to divide the configuration space into different regions, each of which can be explored separately.Neither supporting nor separating since they cut through the obstacles. They are avoided in constructing visibility graphs.Rotational Plane Sweep AlgorithmIt is used for constructing a visibility graph for a set of obstacles in a configuration space. The algorithm works by sweeping a ray around the configuration space and adding edges to the visibility graph whenever the ray intersects an obstacle boundary. Input: A set of vertices ${v_i}$(whose edges do not intersect) and a vertex v. Output: A subset of vertices from ${v_i}$ that are within the line of sight of v.Pseudo code:FOR each vertex v(i), calculate alpha(i)NEW vertexSet = set of alpha(i) which are increasing orderNEW activeSet = set of edge which intersect the horizontal half line emanating from vFOR all alpha(i)\tIF v(i) is visible v\t\tTHEN visibleSet.PUSH(edge(v,v(i))\tENDIF\tIF v(i) is the beginning of an edge, vertexSet, not in activeSet\t\tTHEN activeSet.PUSH(vertexSet)\tENDIF\tIF v(i) is the end of an edge in activeSet\t\tTHEN activeSet.DELETE(edge(v(i), v))\tENDIFENDFOROnce the visibility graph is built, then robot starts to find the path among visibility graph. Since the visibility map posses the properties of accessibility and departability by definition, if it is connected, then several kinds of searching algrithm can be implemented such as potential function and various star algorithm. Cost between nodes can be set to distance of nodes and heuristic cost function can be set to estimated cost of shortest path from node to goal.The shortest path being computed tries to stay as close as possible to the obstacles. Any execution error will lead to a collision. It also becomes more complex in higher dimensions.Cell DecompositionCell decomposition is a technique used in combinatorial motion planning to divide the workspace into smaller regions, called cells, that can be easily navigated by a robot. Each cell is defined as a region that is either free of obstacles or contains a single obstacle.The process of cell decomposition involves dividing the workspace into a grid of cells, where each cell is either classified as free or occupied by an obstacle. This grid can then be used to represent the workspace as a graph, where the cells are the nodes and the edges connect adjacent cells.One advantage of cell decomposition is that it simplifies the motion planning problem by reducing it to a discrete search problem. It also allows for the use of graph-based algorithms, such as Dijkstra‚Äôs algorithm or A*, to find a path from the starting cell to the goal cell.There are several types of cell decomposition techniques, including: Voronoi Diagrams: These are diagrams that divide the workspace into regions based on the distance to the obstacles. The cells in a Voronoi diagram represent the regions of the workspace that are closest to a particular obstacle. Binary Space Partitioning (BSP): This technique divides the workspace into two subspaces recursively, with each subspace divided by a line or plane. This creates a tree structure where each node represents a subspace and each leaf node represents a single cell. Quadtree Decomposition: This technique recursively divides the workspace into four quadrants, with each quadrant further subdivided until each cell contains at most one obstacle.Voronoi DiagramVoronoi diagrams are a geometric structure that partition a space into a set of regions based on the distance to a given set of points or objects. In the context of motion planning, Voronoi diagrams can be used to represent the free space and obstacles in a configuration space, and to compute paths or trajectories for a robot navigating through that space.The Voronoi diagram for a set of points in a space consists of a set of cells, where each cell represents the region of space that is closest to a particular point or object. The cells are defined by a set of lines or curves, called the Voronoi edges, which are equidistant from the neighboring points or objects. The Voronoi diagram is often visualized as a set of polygonal cells that cover the entire space, with the edges of the cells corresponding to the Voronoi edges.Generalized Voronoi DiagramsRobot Path Planning Using Generalized Voronoi Diagrams.The obstacles in the real world are never points but they‚Äôre large objects. Generalized Voronoi Diagrams are just like regular VDs, but instead of the regions around points, we have the regions around objects.Generalized Voronoi GraphCreate Voronoi regions with PythonVertical Strip Cell Decomposition The cells joining the start and target are shaded. The resolution of the decomposition is chosen to get a collision-free path dependent on the sensitivity of controlling robot‚Äôs motion.Planning Find the point \\(q*_{start}\\) of the Voronoi diagram closest to \\(q_{start}\\). Find the point \\(q*_{goal}\\) of the Voronoi diagram closest to \\(q_{goal}\\). Compute the shortest path from \\(q*_{start}\\) to \\(q*_{goal}\\) on the Voronoi diagram.Sampling based Motion PlanningThe state space for motion planning, C, is uncountably infinite. Yet a sampling based planning algorithm can consider at most a countable number of samples.Single Query and Multi Query PlanningMulti-query path planning and single-query path planning are two different types of path planning algorithms used in robotics and computer science.Single-query path planning refers to finding a path between two given points in a given environment, where the start and end points are fixed and do not change. In other words, it is the process of finding a path from a single source to a single destination in a given environment. This is a common problem in robotics, where robots need to navigate from one point to another without colliding with obstacles. It is only concerned about a portion of free C-space needed for a query.On the other hand, multi-query path planning refers to finding paths between multiple pairs of points in a given environment. Unlike single-query path planning, in multi-query path planning, the start and end points can vary from one query to another. For example, a robot may need to navigate to different locations in a warehouse to pick up items, and the start and end points will change with each new task. Th goal is to efficiently model the entire C-space so as to answer any query in that space. Example - Probabilistic Roadmap (PRM)Multi-query path planning is more complex than single-query path planning, as it requires the algorithm to efficiently handle multiple queries and avoid recalculating the entire path for each new query. However, it can be more efficient in situations where there are multiple tasks to be completed in the same environment.Probabilistic Roadmap of Tree (PRT) combines both ideas.Probabilistic Roadmap (PRM)It is a randomized algorithm that is designed to efficiently find collision-free paths for robots in complex environments. PRM algorithm involves the following components: Roadmap construction: The first step of PRM is to construct a roadmap of the environment. This is done by randomly sampling configurations of the robot in the environment and checking whether they are collision-free. Collision-free configurations are added to the roadmap as nodes, and connections are made between nodes that are within a certain distance of each other. Milestones: Milestones are the nodes in the roadmap that represent the important configurations of the robot in the environment. These configurations are sampled randomly from the configuration space of the robot, and they are checked for collision with the obstacles in the environment. If a configuration is collision-free, it is added as a milestone to the roadmap. Local paths: Local paths are the edges that connect the milestones in the roadmap. They represent the feasible paths that the robot can take between the milestones. The local paths are computed by planning a short path between each pair of milestones, while avoiding the obstacles in the environment. Roadmap search: The next step is to search the roadmap for a path between the start and goal configurations. The local paths are used to guide the robot along the roadmap, and the milestones serve as guideposts to ensure that the robot stays on a collision-free path. This is done using a standard graph search algorithm, such as A* search or Dijkstra‚Äôs algorithm. The algorithm tries to find the shortest path between the start and goal configurations while avoiding collisions with obstacles. Path refinement: The final step is to refine the path found by the roadmap search. This is done by iteratively attempting to move along the path and checking for collisions. If a collision is detected, the path is locally modified to avoid the obstacle. This process continues until a collision-free path is found.Features: PRMs don‚Äôt represent the entire free configuration space, but rather a roadmap through it. Roadmap is an undirected acyclic graph R = (N,E). Nodes N are robot configurations in free C-space, called milestones. Edges E represent local paths between configurations. Learning Phase Construction: reasonably connected graph covering C-space Expansion: improve connectivity Local paths not memorized (cheap to re-compute) Construction overview: R = (N,E) begins empty A random free configuration c is generated and added to N Candidate neighbours to c are partitioned from N Edges are created between these neighbours and c, such that acyclicity is preserved Repeat 2-4 until doneGeneral Local Planner: Connect the two configurations is C-space with a straight line segment Check the joint limits Discretize the line segment into a sequence of configurations c1, c2, ‚Ä¶, cm such that for every $(c_i, c_{i+1})$, no point on the robot at ci lies further than $\\epsilon$ away from its position at ci+1 For each ci, grow robot by $\\epsilon$, check for collisionsEdges are created between these neighbours and c, such that acyclicity is preserved.Random Bounce Walk: Pick a random direction of motion in C-space, move in this direction from c. If collision occurs, pick a new direction and continue. The final configuration n and the edge (c,n) are inserted into the graph. Attempt to connect n to other nodes using the construction step technique. Path between c and n must be stored, since process is non-deterministic.Select c such that P(c is selected) = w(c)Query Phase: Connect start and goal configurations to roadmap (say $\\hat{s}$ and $\\hat{g}$) Find path between $\\hat{s}$ and $\\hat{g}$ in roadmapPros: Once learning is done, queries can be executed quickly Complexity reduction over full C-space representation Adaptive - can incrementally build on roadmap Probabilisically complete, which is usually good enoughCons: $\\hat{s}$ and $\\hat{g}$ should be in same connected component, else failure Paths are not optimal. They can be long and indirect, depending on how the graph was created. Smoothing can be applied.Rapidly-Exploring Random Tree (RRT) [Path Planning with A* and RRT Autonomous Navigation, Part 4](https://www.youtube.com/watch?v=QR3U1dgc5RE&amp;t=45s) RRT, RRT* &amp; Random TreesRRT is a probabilistic algorithm that builds a tree of feasible paths in the search space.The RRT algorithm starts with an initial configuration of the robot or system being planned for, and then iteratively grows a tree by randomly selecting a new configuration, and connecting it to the closest node in the tree. The algorithm continues to grow the tree until a path is found from the initial configuration to the goal configuration, or until a certain maximum number of iterations is reached.The random sampling and connection of nodes in RRT allows it to quickly explore large areas of the search space, making it well-suited for solving complex path planning problems in high-dimensional spaces. Additionally, RRT can handle non-holonomic constraints, as well as dynamic obstacles and changes in the environment.RRT has several variants, including RRT* and RRT-Connect, which incorporate optimization and connectivity constraints to improve the quality of the resulting paths.RRTs have the advantage that they expand very fast and bias towards unexplored regions of the configuration space and so covering a wide area in a reduced time.When one of the leaves of the tree reaches a goal region, the algorithm stops and a path can directly be found by following the predecessors of the last added node.Features: It is implemented using tree data structure Tree is special case of directed graph Edges are directed from child node to parent Every node has one parent, except root Nodes represent physical states or configurations Edges represent feasible paths between states Each edge has cost associated traversing feasible pathPseudo code:1. Initialize tree T with a single node at the start state S2. while not goal_found and iterations &lt; max_iterations do3. randomly sample a new configuration Q4. find the node N in T that is closest to Q5. extend the tree from N towards Q by a maximum distance delta_q6. if Q is within delta_q of the goal state G, add a node at Q and goal_found = true7. iterations = iterations + 18. return the path from S to G if goal_found, otherwise return failureIn the above pseudocode, T represents the tree of feasible paths, S is the initial state, G is the goal state, and delta_q is a parameter that controls the maximum distance the algorithm can extend from the current node. The algorithm iteratively adds nodes to the tree T until either a path from S to G is found, or the maximum number of iterations is reached.A* AlgorithmA* Pathfinding (E01: algorithm explanation)Dijkstra‚Äôs AlgorithmDijkstra‚Äôs Algorithm - ComputerphileHow Dijkstra‚Äôs Algorithm Works" }, { "title": "Chess Engine using Reinforcement Learning", "url": "/posts/chess-engine-rl/", "categories": "Projects, RL", "tags": "rl, games", "date": "2023-03-28 12:00:00 -0400", "snippet": "AbstractMachine learning based chess engines have always proven themselves to outperform human capabilities in the strategic field of chess. The number of possibilities after every move someone makes increases exponentially. For example, after just two moves (two turns for white and two for black), there are 197,742 possible board positions. Certain grandmasters of chess are able to see 5 to 10 moves in the future in order to decide the best move to play, but that is in exchange for the lifetime of training they devote to this world of 64 squares.This is where computers come into play. While chess engines learn much faster and can calculate to much further depths than a human can, a perfect chess engine has still always been the biggest challenge to the world of Artificial intelligence. While the Deep Blue started making headlines in 1996, the top-level chess engines started beating human beings around the 1970s. However, needless to say, the chess engines have come a long way since then, with stockfish being the highest-rated chess engine to date, implementing supervised and unsupervised learning techniques with some pre-fed information contributed by top world grandmasters, and DeepMind‚Äôs Deep Reinforcement learning based AlphaZero, which learned to play excellent chess, even beating stockfish, entirely from self-play over a few hours of training.The aim of our project is to create a similar chess engine, which can at least beat average-level players and is more or less self-trained, using deep reinforcement learning techniques and neural network-based algorithms. The aim is to build an AlphaZero-based chess engine implementing deep neural network architectures paired with Monte Carlo Tree Search, but at the same time diverge from AlphaZero in that we‚Äôll avoid a full board representation and use a handcrafted feature vector instead, inspired by the Giraffe paper. We will also use Stockfish and a few other methods to accelerate training.Literature ReviewGiraffe: Using Deep Reinforcement Learning to Play Chess by Matthew LaiNEURAL NETWORK BASED EVALUATIONFeature Representation Side to Move Castling Rights Material Configuration Piece Lists Sliding Pieces Mobility Attack and Defend MapsTotal Features - 363Network ArchitectureTraining Set Generation Approach5 million positions randomly selected from databases and a random legal move to each position, introducing imbalances and more unusual positions.Network InitializationEvaluative function with only basic material knowledge used.TD-LeafImplementation of TD Leaf using following update rule:ResultPROBABILISTIC SEARCHLimit the search based on the probability of a move being a part of the main line.NEURAL NETWORK BASED PROBABILITY ESTIMATIONTrain a neural network to give $ P(child_i|parent)$Feature Representation Piece Type From Square To Square Promotion Type RankNetwork ArchitectureSimilar to the one used earlier.Training Positions GenerationTime Limited Search on the root positions from the training set for the evaluation network training and randomly sample from the positions encountered as internal nodes in those searches. 5 million positions.Network TrainingStochastic Gradient Descent with AdaDelta update rules using the cross-entropy loss function.RESULTS AND EVALUATIONDifference is 48+-12 Elo points.CONCLUSIONThe learned system performed at least comparably to the best expert-designed counterparts in existence at the time.Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess by Timo Bertram, Johannes Furnkranz and Martin MullerThe paper explores how to teach an artificial intelligence (AI) agent to play the imperfect information game of reconnaissance blind chess (RBC) using the AlphaGo methodology. RBC is an imperfect information game, unlike traditional board games like chess and go, where players have little knowledge of where the opponent‚Äôs pieces are.The research suggests a modification of the AlphaGo training method for RBC, which entails training a supervised agent on publicly accessible game records and then improving its performance through self-play using the Proximal Policy Optimisation reinforcement learning algorithm. Instead of a detailed description of the game state, the agent is trained to produce moves based on observations of the current state of the game. According to the article, using only observations is required to prevent issues brought on by the partial observability of game states in RBC.Network ArchitectureResultsThe suggested strategy led to an ELO of 1330 on the RBC leaderboard, moving the agent up to position 27 at the time of writing. The outcomes show that self-play considerably enhances the agent‚Äôs performance and that the agent is capable of playing respectably without search and without assuming the genuine game state.Reinforcement Learning in an Adaptable Chess Environment for Detecting Human-understandable ConceptsThis paper aims to make the engine master the game of chess via self play. The agent is a deep neural network with randomly initialised weights, which get better as it trains by playing against itself and learns to recognise board patterns. The environment consists of a board with user defined board size, with a simple interface for extracting useful information regarding the board state. The environment is also coupled with a standard variant of Monte Carlo Tree Search (MCTS). TensorFlow Lite is utilised to provide fast neural network guidance without having to rely on batching predictions for the GPU.Concepts are detected by learning a set of logistic probes for a data set representing the concept of interest, using the activation outputs for each intermediary layer in the neural network. The aim is to find the best-fitting logistic regressor with weights w and bias b so that$||œÉ (w ¬∑ Oi + b) ‚àí Pi||^2$ is minimised, where Oi are the activation outputs and Pi are the binary labels {0, 1}. Then a random sample of 10% of the data is tested for validation of the modelThe probed concepts for the 4x5 and 6x6 agents are:We can see that soon after in the training process, proceeding approximately 30 iterations, the 4x5-agent learns to represent whether it is threatening the opponent‚Äôs queen. . Later, after approximately 50 iterations, the agent learns to represent whether it has a potential mating attack. Surprisingly, the agent represents the state of being in check only weakly throughout the entire training process, despite learning to play optimally. All the probed concepts flatten out after about 100 training iterations, regardless of whether training continues. This indicates that this agent is highly unlikely to allocate more resources to represent these concepts provided more experience.For the 6x6-agent, many of the same trends as the 4x5 agent can be seen, the main difference being that material advantage as well as has mate threat are detectable already at the beginning of the training loop.In an RL context, most of the early learning takes place because the agent accidentally succeeds, i.e. delivers a checkmate. As a consequence, concepts connected to actions that are statistically more likely to win the game for a given agent, appear during the earliest stages of training.The implementation is limited to probing for linearly represented concepts, although there are no guarantees that the network represents its knowledge in a linearly separable manner. Also, it cannot highlight the differences between being able to detect concepts and knowing how the represented concepts are utilised within the given model. For example: it is easy for a human to understand that the player holding less material is more likely to lose. However, whether or not the model has made the same connection cannot be guaranteed.Network ArchitectureMastering Chess and Shogi by Self-Play with a General Reinforcement Learning AlgorithmThe paper introduces the AlphaZero reinforcement learning method, which trains a neural network to play board games. By teaching the algorithm to play chess and shogi (Japanese chess) without prior knowledge of the game‚Äôs rules or strategies, the authors demonstrated the algorithm‚Äôs effectiveness. The system eventually attained a superhuman level of performance, outperforming the top computer and human players in each game.The introduction to reinforcement learning, a subset of machine learning involving an agent interacting with its surroundings to learn how to complete a task, is the paper‚Äôs first section. The authors describe the AlphaZero approach, which combines a deep neural network with a Monte Carlo tree search to learn how to play a game through self-play.The algorithm for the proposed AlphaZero was - The parameters of the neural network are initialized randomly Games are played by selecting moves for both players by MCTS At the end of the game, the terminal position is scored according to the rules as +1 for a win, 0 for neutral, and -1 for a loss Neural network parameters are updated with 2 aims To minimize the error between predicted and game outcome To maximize the similarity of the policy vector to the search probabilities The author also discussed how their algorithm was different from the AlphaGo Zero algorithm AlphaGo Zero estimates and optimizes the probability of winning, assuming binary win/loss outcomes. AlphaZero estimates and optimizes the expected outcome, taking account of draws or potentially other outcomes In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. AlphaZero simply maintains a single neural network that is updated continually rather than waiting for an iteration to complete AlphaGo Zero tuned the hyper-parameters of its search by Bayesian optimization. In alphaZero, they reuse the same hyper-parameters for all games without game-specific tuningThe writers used chess, Shogi, and Go as test cases for AlphaZero. After only a few hours of practice, the algorithm was able to master all three games at a superhuman level. The results for chess and shogi are the main subject of the paper.The authors compared AlphaZero‚Äôs chess performance to that of Stockfish and Elmo, two more chess programs. Elmo was a forerunner of AlphaZero and was trained on a smaller sample of data than Stockfish, one of the greatest chess engines in the world. In a 100-game contest against Stockfish and Elmo, AlphaZero prevailed by winning 28 games and drawing the remaining 72. The unusual playing style of AlphaZero was also recognized, with a propensity for sacrifice attacks.The authors evaluated AlphaZero‚Äôs performance in shogi against the top human players and the YSS algorithm, the most powerful shogi program. In a contest of 100 games, AlphaZero defeated the YSS algorithm by winning 90 of them and drawing the remaining ten. The top human players were also used to compare AlphaZero‚Äôs performance, and it was discovered that AlphaZero‚Äôs style of play was distinct from both the human and YSS styles.Overall, the study offers a substantial advance in artificial intelligence and shows how well a broad reinforcement learning algorithm works for teaching players how to play challenging games. According to the authors, the technique might be used for other games and fields, such as robotics and optimization issues.MethodologyDeep Pepper SummaryThe Deep Pepper paper adapts methods from AlphaZero, deep networks and monte carlo tree search algorithms. It implements a neural network (value network) to predict state values and the optimal move (policy network) at any state using custom feature representation. This policy network is used together with an MCTS algorithm, and the optimal move is decided by an upper confidence tree selection process. Training is done by embedding some prior knowledge of chess, and using stockfish to early stop the games for an accelerated training.Deep Pepper uses MCTS for both policy evaluation and policy improvement.There are three main phases of the algorithm (each board state is stored as a node: Selection: The most promising child node for each root node is considered until we come across an unvisited node. Expansion and evaluation: This leaf node is expanded using the neural network, giving the probabilistic values for all possible board moves, and the value of the leaf, ‚Äòv‚Äô is found out using the value network. Back up: The statistics (action values and the number of times the move has been taken) of each explored move is updatedEach node contains N(s, a) (number of times each of its children were visited), W(s, a) (The total value of all descendent leaf nodes as per the value given by the neural network), Q(s, a) (the average value for each node =W(s, a)/N(s, a) ) and P(s, a) (the initial move probabilities that the neural network provides), with Dirichlet noise added to favour exploration. The states and actions are explored using a variant of Polynomial Upper Confidence Tree‚Äôs (PUCT), based on estimates on the upper bound of selection probabilities U(s, a) having an exploration factor Cpuct, and the child node is selected as the one that maximises the sum of Q(s, a) and U(s, a).At the end of each iteration (800 simulations of MCTS), MCTS provides a policy to be used in the current game state. The policy is given by: œÄ(s, a) = N(s, a)/Œ£a N(s, a)STATE REPRESENTATION: Complete board representation in the form of 8x8x73 poses certain issues, like having too many layers makes it harder and more time consuming to train,and CNNs can‚Äôt be trained to predict sudden changes in output to considerably small changes in input. In light of this, we take inspiration from the Giraffe paper, having a custom 353-dimensional feature vector, which also gives favorable results during pre-training. We also use stockfish embedded knowledge to accelerate training.NEURAL NETWORK ARCHITECTURE: Multiple architectures are experimented: PolicyValNetwork-parts: The global features are connected to h1a neurons, the piece centric features to h1b neurons and square centric features to h1c neurons in the first layer. All the concatenated outputs from the first layer are then connected to the h2p and h2v neurons in the second layer which give us the policy output and the value function respectively. PolicyValNetwork-Full: Instead of having distinct connections in the first layer, we have a fully connected layer allowing the network to learn more implicit connections between each of the features. An insignificant improvement in performance as compared to the increased training time is observed. PolicyNetwork-parts and Critic-parts: A minor improvement in performance is observed if we also train the first layer independently for policy and value networks (separate). PolicyNetwork-full and Critic-full: very slight improvement in performanceTRAINING ALGORITHM PIPELINE: The training procedure can be viewed as a CAPI. MCTS is used for game generation and improvement testing. Game generation phase: During this phase, deep pepper plays many games against itself. The current board state is used as a root node and the optimal policy is determined. After k half moves,in order to accelerate training, stockfish evaluation is used to indicate the probability for the player to win or lose, resulting in the player/opponent to resign Network training phase: The data is created during the game generation phase as a triplet of: {s, œÄ(s, a), ŒΩ}, where s is the board state, œÄ(s, a) is the policy calculated by MCTS, and v is the end game value determined by a draw, win or resignation. The network is trained via back propagation with gradient updates calculated using an Adam optimizer on mini-batches. Improvement Testing phase: an optional check to ensure that the new network has improved over the last iteration. Should the updated network defeat the older iteration by more than half the time, the newly trained network is kept and used in the next round of game generation.PRETRAINING: An optional phase added in order to offer early guidance in network training.Stockfish assigns values to the board positions taken from professional chess games, helping us to generate an approximate policy label. This is because training of the policy network doesn‚Äôt work directly on expert games. Then, supervised learning is used to train the networks using the generated labels.The Best Chess EnginesBackgroundStockfish chess engine was released in 2008, which was made by open-source developers. It is derived from Glaurang, an open-source engine which was developed in 2004. This chess engine was the strongest chess engine and had won many chess matches.In 2017, DeepMind, which is a subsidiary of Google, developed a chess engine, named AlphaZero, which defeated Stockfish multiple times and shocked the chess community. In Stockfish, the approach is to use sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades.But in AlphaZero, the only input given to it was rules of chess. It uses a deep neural network for evaluation which is trained by a reinforcement learning algorithm. This engine has also worked successfully on games like Shogi and Go.StockfishStockfish uses handcrafted features which consists of : Mobility or Trapped Pieces, Pawn Structures, King‚Äôs Safety, number of pieces left, outposts etc. Each of these features has a specific weight assigned to it and evaluation of a position is resulted from the sum of features multiplied to its respective weights. For evaluating a position, stockfish spans a tree of possible ways to move and evaluates using the evaluation function. The problem is that the size of complete chess tree could go to near 10123, and evaluating that would be difficult. So we limit the breadth and depth of search by using Alpha-Beta algorithm. This algorithm tries to go through the search which drops the worst options for the other player, instead of finding the highest value of search evaluation. This approach works best if we consider the best moves as early as possible. For example, if we conduct a search of all moves in ascending order, we would have to go through all possible positions. But if we consider strongest move first, then the algorithm would reduce the node that has to be evaluated to its root (if x is number of node, it will reduce it to ). Stockfish uses an opening book to choose moves in the first phase of the game as well an endgame tablebase that includes the best moves for all positions with six or less pieces left on the boardAlphaZeroIn the case of AlphaZero, instead of using handcrafted features for evaluation function, it uses a deep neural network which is trained by reinforcement learning.In this algorithm, the first step is to choose the initial position from where it should start and then to decide what moves it should make. The input to the neural network is all 64 squares in the chess board and all the move possibilities each piece would have on each of these squares. As each square is treated in the same way, it leads to certain illegal moves, which is then rectified by the chess rule which is given input to the engine by reducing their probability to zero. The output from this neural network is a vector containing 2 values. One signifies the percentage of search time and another is the win probability that the neural network assigns to this move. The training of this neural network is done by making it play with itself. The neural network is trained by Monte-Carlo Tree Search algorithm.Results and ImprovementsMonte Carlo Tree Search Accuracy:The accuracy of Monte Carlo Tree Search (MCTS) refers to the algorithm‚Äôs capacity to accurately predict the outcome of future moves in a game of chess. It depends on several factors such as the accuracy of the evaluation function used to estimate the value of a game position and the number of simulations performed by the algorithm. It is crucial in determining the effectiveness of the moves made by the AI agent. Poor accuracy can result in the agent performing poorly and making bad decisions. As a result, one of the key objectives of this project is to increase MCTS accuracy.Cyclic Learning Rates:Cyclic Learning Rates (CLR) is a technique used in reinforcement learning to tune the learning rate hyperparameter. The learning rate, in a conventional approach, is manually established and remains constant over the course of the training. However, this fixed learning rate may not be appropriate for different phases of the training process, which can result in suboptimal results.Cyclic Learning Rates vary the learning rate over multiple cycles, by initially starting with a low learning rate and gradually increasing it to a maximum value before lowering it once more. Throughout the training phase, this cycle is repeated numerous times.The main advantage of doing this is that the learning rates adapt to the dynamics of the training process and avoid getting stuck in local optima. They encourage the model to move out of the local minimum and instead identify the global one by periodically raising the learning rate.This figure shows how the learning rate and momentum is varied every iteration. It can also be noticed that the change in learning rate is minimal towards the end of training to accommodate the fact that the policy shouldn‚Äôt change too much towards the end.This figure shows the MCTS accuracy with and without cyclic learning rates. It can be seen that initially the model using cyclic learning rates has a higher accuracy but towards the end the difference is negligible.Squeeze and Excite:Squeeze and Excitation (SE) is a technique that can be used to improve the performance of convolutional neural networks (CNNs). It aims to capture the interdependence between channels by learning a channel-dependent weighting mechanism, which dynamically adjusts the channel activations based on their importance.The squeeze operation and the excitation operation are the SE module‚Äôs two main parts. In order to capture the global information for each channel, the squeeze operation decreases the spatial dimension (H x W) of feature maps to a single dimension. The excitation operation is in charge of learning a channel-wise weighting mechanism, which determines the significance of each channel at each point.The chess engine‚Äôs performance can be enhanced by including the SE module in its CNN architecture. It can be used to extract high-level features from the input data following a convolutional layer. The data from all channels are then combined into a vector via the SE module‚Äôs squeezing of the feature maps along the channel dimension.The excitation operation follows, which learns a set of parameters to model the dependencies between various channels in the feature maps. A weighted vector is created as a result, capturing the significance of each channel. Finally, the feature maps are obtained by multiplying the weighted vector with the original feature maps.This figure shows the MCTS accuracy trends with and without Squeeze and Excite. It can be seen that there are some early gains early during the training but there some losses later on. Overall, it does not affect the performance much and intuitively it should help.C-PUCT:C-PUCT extends the traditional MCTS algorithm by adding continuous progress estimates, which allow the algorithm to better allocate computational resources and improve search efficiency. To determine the optimum moves, it uses an Upper Confidence Bound (UCB) method with the additional benefit of updating the confidence bounds in real time while the search goes on.The C-PUCT algorithm can be used in a chess engine to find the best move by searching through the possible move sequences from the current position on the chessboard. The algorithm can reduce the search space and quickly determine the optimum move by using a search tree and a combination of deterministic and probabilistic heuristics.The quality of the input features used and the precise algorithmic parameters chosen will determine the move that the C-PUCT algorithm generates. To get the chess engine to function at its best, these parameters typically need to be adjusted.ConclusionThe observation of a limited application due to the significant hardware demands in this research spurred the need for effective and scalable improvements of Deep Pepper. Cyclic Learning Rates showed great potential while other methods like Squeeze and Excite and Hyperparameter Tuning showed decent increase in performance too. Overall, the hardware demands were reduced and the performance of the model was increased with these improvements.Bibliography Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting https://arxiv.org/pdf/1806.00683.pdf Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm https://arxiv.org/abs/1712.01815 Giraffe: Using Deep Reinforcement Learning to Play Chess https://doi.org/10.48550/arXiv.1509.01549 Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess https://doi.org/10.48550/arXiv.2208.02029 Reinforcement Learning in an Adaptable Chess Environment for Detecting Human-understandable Concepts https://doi.org/10.48550/arXiv.2211.05500" }, { "title": "Probabilistic Robotics", "url": "/posts/aifr/", "categories": "Blog, Robotics", "tags": "slam", "date": "2023-03-28 12:00:00 -0400", "snippet": "In progressResourcesProgrammingBasic Concepts of ProbabilityTerminologyThe environment, or world, is a dynamical system that possesses internal state. The robot can acquire information about its environment using its sensors. However, sensors are noisy, and there are usually many things that cannot be sensed directly. As a consequence, the robot maintains an internal belief with regards to the state of its environment.State State - It is a collection of all aspects of the robot and its environment that can impact the future. Dynamic states are state variables that tend to change over time, while static states are non-changing. The state at time $t$ is denoted as $x_t.$ Pose - The robot pose comprises of its location and orientation relative to a global reference frame. In the context of mobile robots, the pose is usually given by three variables, two location coordinates in the plane and one orientation w.r.t the vertical axis normal to the plane. The location and features of the surrounding objects in the environment are also state variables. Based on the quality of the state, robot environments can have even hundreds to billions of state variables. Some objects in the environment can be recognized reliably and are referred to as landmarks. Other state variables can include whether or not a sensor is broken, the level of battery charge, etc. These take discrete values. Markov chains - A state $x_t$ is said to be complete if the knowledge of past states, measurements or controls carry no additional information to help predict the future more accurately. The future may be stochastic but no variables prior to $x_t$ may influence the future states, unless this dependence is mediated through the state $x_t.$Environment Interaction Sensor Measurements - Perception is the process by which the robot uses its sensors to obtain information about the state of its environment. Typically, sensor measurements have some delay and provide information about the state a few moments ago. However, we will assume that measurements corresponds to a specific point in time. Measurement data at point $t$ will be denoted by $z_t.$\\[z_{t1:t2} = z_{t1},z_{t1+1},z_{t1+2},....,z_{t2}\\] Control Actions - They carry information about the change of state of the environments and they change the state by actively asserting forces on the robot‚Äôs environment. It is denoted by $u_t.$\\[u_{t1:t2} = u_{t1},u_{t1+1},u_{t1+2},....,u_{t2}\\] Hidden Markov ModelThe emergence of state $x_t$ might be conditioned on all past states, measurements and controls. This gives us a probability distribution of the following form:\\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t})\\]We assume that the robot executes the control action $u_1$ first and then takes the measurement $z_1$.If the state $x$ is complete, then\\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t}) = p(x_t|x_{t-1},u_t) \\implies State \\space transition \\space probability\\]Similarly, if $x_t$ is complete, we have\\[p(z_t | x_{0:t},z_{1:t-1},u_{1:t}) = p(z_t|x_{t}) \\implies Measurement \\space probability\\]The state at time $t$ is stochastically dependent on the state at time $t-1$ and the control $u_t.$ The measurement $z_t$ depends stochastically on the state at time $t.$Belief DistributionsBelief represents the robot‚Äôs internal knowledge about the state of the environment. We denote the belief over a state variable $x_t$ by $bel(x_t),$\\[bel(x_t) = p(x_t|z_{1:t},u_{1:t})\\]We assume that the belief is taken after incorporating the measurement $z_t$. Sometimes it is also useful to calculate the belief before incorporating $z_t$, just after executing $u_t.$\\[\\bar{bel(x_t)} = p(x_t|z_{1:t-1},u_{1:t})\\]Calculating $bel(x_t)$ from $\\bar{bel(x_t)}$ is called correction or measurement update.Bayes FilterPseudo-codeAlgorithm Bayes_filter ($bel(x_{t-1}),u_t,z_t):$for all $x_t$ do:\\[\\bar{bel(x_t)} = \\int{p(x_t|u_t,x_{t-1}) bel(x_{t-1})dx_{t-1}}\\]\\[bel(x_t) = \\eta p(z_t|x_t)\\bar{bel(x_t)}\\]endforreturn $bel(x_t)$Markov AssumptionAlso called the complete state assumption, it postulates that the past and future data are independent if one knows the current state $x_t$. However, the following factors may have a systematic effect on sensor readings and induce violations to the Markov assumption: Unmodeled dynamics in the environment are not included in $x_t$ Inaccuracies in probabilistic models - state transition probability and measurement probability Approximation errors when using approximate representations of belief functions Software variables in the robot control software that influence multiple controls Estimation is after the occurrence of the event i.e. posterior probability. Prediction is a kind of estimation before the occurrence of the event i.e. apriori probability.What is the difference between estimation and prediction?The $\\alpha-\\beta-\\gamma$ FilterOnline Kalman Filter TutorialNotation$x$ is the true value of the weight$z_n$ is the measured value of the weight at time¬†n$\\hat{x}_{n,n}$ is the estimate of¬†$x$¬†at time¬†n¬†(the estimate is made after taking the measurement¬†$z_n$)\\(\\hat{x}_{n+1,n}\\) is the estimate of the future state ( n+1¬†) of¬†$x$. The estimate is made at the time¬†n. In other words,¬†$\\hat{x}_{n+1,n}$¬†is a predicted state or extrapolated state\\(\\hat{x}_{n-1,n-1}\\) is the estimate of¬†xx¬†at time¬†n‚àí1¬†(the estimate is made after taking the measurement¬†$z_{n-1}$)$\\hat{x}_{n,n-1}$ is a prior prediction - the estimate of the state at time¬†n. The estimate is made at the time¬†n‚àí1\\[For \\space constant \\space dynamics: \\newline \\hat{x}_{n+1,n}= \\hat{x}_{n,n}\\]\\[State \\space Update \\space Equation: \\newline \\hat{x}_{n,n} = \\hat{x}_{n,n-1} + \\alpha_n \\left( z_{n} - \\hat{x}_{n,n-1} \\right)\\]$\\alpha_n$ is called the Kalman Gain$(z_n - \\hat{x}_{n,n-1})$ ¬†is the ‚Äúmeasurement residual‚Äù, also called¬†innovation. The innovation contains new information.$\\alpha - \\beta$ Filter\\[State \\space Extrapolation \\space Equations: \\newline\\]\\[x_{n+1}= x_{n}+ \\Delta t\\dot{x}_{n} \\newline \\dot{x}_{n+1}= \\dot{x}_{n}\\]The above system of equations is also called Transition Equation or¬†Prediction Equation, and is true for constant velocity dynamics.$\\alpha-\\beta$¬†track update equations¬†or $\\alpha-\\beta$ track filtering equations -\\[State \\space Update \\space Equation \\space for \\space position: \\newline\\]\\[\\hat{x}_{n,n} = \\hat{x}_{n,n-1}+ \\alpha \\left( z_{n}- \\hat{x}_{n,n-1} \\right) \\newline\\]\\[State \\space Update \\space Equation \\space for \\space velocity: \\newline\\]\\[\\hat{\\dot{x}}_{n,n} = \\hat{\\dot{x}}_{n,n-1}+ \\beta \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{ \\Delta t} \\right)\\]$\\alpha-\\beta-\\gamma$ Filter\\[State \\space Extrapolation \\space Equations: \\newline\\]\\[\\hat{x}_{n+1,n}= \\hat{x}_{n,n}+ \\hat{\\dot{x}}_{n,n} \\Delta t+ \\hat{\\ddot{x}}_{n,n}\\frac{ \\Delta t^{2}}{2} \\newline\\]\\[\\hat{\\dot{x}}_{n+1,n}= \\hat{\\dot{x}}_{n,n}+ \\hat{\\ddot{x}}_{n,n} \\Delta t \\newline\\]\\[\\hat{\\ddot{x}}_{n+1,n}= \\hat{\\ddot{x}}_{n,n}\\]\\[State \\space Update \\space Equations: \\newline\\]\\[\\hat{x}_{n,n}= \\hat{x}_{n,n-1}+ \\alpha \\left( z_{n}- \\hat{x}_{n,n-1} \\right) \\newline\\]\\[\\hat{\\dot{x}}_{n,n}= \\hat{\\dot{x}}_{n,n-1}+ \\beta \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{ \\Delta t} \\right) \\newline\\]\\[\\hat{\\ddot{x}}_{n,n}= \\hat{\\ddot{x}}_{n,n-1}+ \\gamma \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{0.5 \\Delta t^{2}} \\right)\\]The main difference between these filters is the selection of weighting coefficients¬†$\\alpha-\\beta-(\\gamma)$. Some filter types use constant weighting coefficients; others compute weighting coefficients for every filter iteration (cycle).The choice of the¬†$\\alpha,\\beta,\\gamma$¬†is crucial for proper functionality of the estimation algorithm.Kalman FilterOnline Kalman Filter TutorialThe Kalman Filter is an¬†optimal filter. It combines the prior state estimate with the measurement in a way that minimizes the uncertainty of the current state estimate.\\[For \\space constant \\space dynamics: \\newline p_{n+1,n}= p_{n,n}\\]\\[Covariance \\space Extrapolation \\space Equation: \\newline p_{n+1,n}^{x}= p_{n,n}^{x} + \\Delta t^{2} \\cdot p_{n,n}^{v} \\newline p_{n+1,n}^{v}= p_{n,n}^{v}\\]$p^x$ is the position estimate uncertainty, $p^v$ is the velocity estimate uncertainty, and the above is true for constant velocity dynamics.Note that for any normally distributed random variable¬†$x$¬†with variance ${\\sigma}^2$,¬†$kx$¬†is distributed normally with variance¬†$k^2{\\sigma}^2$, therefore the time term in the uncertainty extrapolation equation is squared.\\[\\hat{x}_{n,n} = \\hat{x}_{n,n-1} + \\frac{p_{n,n-1}}{p_{n,n-1} + r_{n}}\\left( z_{n} - \\hat{x}_{n,n-1} \\right)\\]Kalman Gain:\\[K_{n}= \\frac{Uncertainty \\quad in \\quad Estimate}{Uncertainty \\quad in \\quad Estimate \\quad + \\quad Uncertainty \\quad in \\quad Measurement}= \\frac{p_{n,n-1}}{p_{n,n-1}+r_{n}}\\]\\[0 \\leq K_{n} \\leq 1\\]\\[\\left( 1 - K_{n} \\right) = \\left( 1 - \\frac{p_{n,n-1}}{p_{n,n-1} + r_{n}} \\right) = \\left( \\frac{p_{n,n-1} + r_{n} - p_{n,n-1}}{p_{n,n-1} + r_{n}} \\right) = \\left( \\frac{r_{n}}{p_{n,n-1} + r_{n}} \\right)\\]\\[Covariance \\space Update \\space Equation: \\newline p_{n,n} = \\left( 1 - K_{n} \\right)p_{n,n-1}\\]This equation updates the estimate uncertainty of the current state. It is clear from the equation that the estimate uncertainty is constantly decreasing with each filter iteration, since¬†$\\left( 1-K_{n} \\right) \\leq 1$. When the measurement uncertainty is high, the Kalman gain is low. Therefore, the convergence of the estimate uncertainty would be slow. However, the Kalman gain is high when the measurement uncertainty is low. Therefore, the estimate uncertainty would quickly converge toward zero.The Kalman Gain is close to zero when the measurement uncertainty is high and the estimate uncertainty is low. Hence we give significant weight to the estimate and a small weight to the measurement.On the other hand, when the measurement uncertainty is low, and the estimate uncertainty is high, the Kalman Gain is close to one. Hence we give a low weight to the estimate and a significant weight to the measurement.If the measurement uncertainty equals the estimate uncertainty, then the Kalman gain equals 0.5.The Kalman Gain Defines the measurement‚Äôs weight and the prior estimate‚Äôs weight when forming a new estimate. It tells us how much the measurement changes the estimate.Summary of all 5 Kalman EquationsThe State Extrapolation Equation and the Covariance Extrapolation Equation depend on the system dynamics.SLAMLocalizationNavigation by odometry is prone to errors and can only give an estimate of the real pose of the robot. Further the robot moves, the larger is the error in the estimation of the pose. It can be compared to walking with eyes closed and counting our steps until we reach the destination. The further we walk with our eyes closed, the more uncertain we are about our location.For moving short distances odometry is good enough, but when moving longer distances the robot must determine its position relative to an external reference called a landmark. This process is called localization.LandmarksLandmarks, such as lines on the ground or doors in a corridor can be detected and identified by the robot and used for localization.Determining Position from Objects Whose Position Is KnownThe following are two methods by which a robot can determine its position by measuring angles and distances to an object whose position is known.From an Angle and a DistanceSuppose an object is placed at the origin $(x_0,y_0)$ of the coordinate system. The azimuth of the robot $\\theta$ is the angle between the north and the forward direction of the robot. A laser scanner is used to measure the distance s from the object and the angle $\\phi$ between the forward direction of the robot and the object.\\[\\Delta x = s \\sin(\\theta-\\phi) \\newline \\Delta y = s \\cos(\\theta-\\phi)\\]TriangulationTriangulation is based on the principle that from two angles of a triangle and the length of the included side, the lengths of the other sides can be computed.The robot measures the angles $\\alpha$ and $\\beta$ to the object from two positions separated by a distance $c$. This distance can be measured by odometry as the robot moves from one position to another, although this may be less accurate.Global Positioning SystemGPS navigation is based upon orbiting satellites. Each satellite knows its precise position in space and its local time. The position is sent to the satellite by ground stations and the time is measured by a highly accurate atomic clock on the satellite.A GPS receiver must be able to receive data from 4 satellites. For this reason, a large number of satellites (24-32) is needed so that there is always a line-of-sight between any location and at least four satellites. From the time signals sent by a satellite, the distances from the satellites to the receiver can be computed by multiplying the times of travel by the speed of light. These distances and the known locations of the satellites enable the computation of the three-dimensional position of the receiver: latitude, longitude and elevation.Advantages: Accurate Easily available - require only an electronic component which is small and inexpensiveDisadvantages: Positional error is roughly 10m. Not sufficient to perform tasks that need higher accuracy. GPS signals are not strong enough for indoor navigation and are subject to interference in dense urban environments.Probabilistic Localization Consider a robot that is navigating within a known environment for which it has a map. Suppose the map shows 5 doors (dark gray) and 3 areas where there is no door (light gray). The task of the robot is to enter a specific door. By odometry, the robot can determine its current position given a known starting position.MappingA robot can use its capability to detect objects to localize itself, and this information is usually provided by a map. However, building a map requires the robot to localize itself, but at the same time, solving the localization problem requires a map. Hence, we are now presented with a chicken-and-egg problem. This problem is overcome by using SLAM algorithms.Discrete and Continuous MapsA robot requires a non-visual representation of a map that it can store in its memory. There are 2 techniques for storing maps: discrete maps (or grid maps) and continuous maps.Sonar Sensor ModelFrontier AlgorithmMapping using Knowledge of the EnvironmentEven with bad odometry, the robot can construct a better map if it has some information on the structure of the environment. Suppose that the robot tries to construct the plan of a room by following its walls. If the robot knows in advance that the walls are straight and perpendicular to each other, the robot can correctly construct the map.There will also be an error when measuring the lengths of the walls and this can lead to the gap shown in the figure between the first and the last walls. If the robot is mapping a large area, the problem of closing a loop in a map is hard to solve because the robot has only a local view of the environment.Map correction can be improved by using sensor data that can give information on regular features in the environment. The regular features can be lines on the ground, a global orientation, or the detection of features that overlap with other measurements.Large area measurements facilitate identifying overlaps between the local maps that are constructed at each location as the robot moves through the environment. By comparing local maps, the localization can be corrected and the map can be updated accurately.SLAM PerceptionDue to odometry and perception error, there is a mismatch between the current map and the sensor data which should correspond to the known part of the map.How is this mismatch corrected? We assume that the odometry does give a reasonable estimation of the pose of the robot. For each relatively small possible error in the pose, we compute what the perception of the current map would be and compare it with the actual perception computed from the sensor data. The pose that gives the best match is chosen as the actual pose of the robot and the current map is updated accordingly.The similarity matrix is computed, and once we have this result, we correct the pose of the robot and use data from the perception map to update the current map stored in the robot‚Äôs memory.Probabilistic SLAMThe SLAM problem asks if it is possible for a mobile robot to be placed at an unknown location in an unknown environment and for the robot to incrementally build a consistent map of this environment while simultaneously determining its location within this map.Both the trajectory of the platform and the location of all landmarks are estimated on-line without the need for any a priori knowledge of location.SLAM problems possess a continuous and a discrete component: Continuous estimation problem: Deals with location of the objects in the map and the robot‚Äôs own pose variables. Discrete estimation problem: Deals with correspondence, i.e., the relation of the object to previously detected objects. The reasoning is discrete - whether the object is same as a previously detected object or not.Online SLAM: Involves estimating the posterior over the momentary pose along with the map.\\[p(x_t,m | z_{1:t}, u_{1:t})\\]Full SLAM: Involves calculating the posterior over the entire path along with the map, instead of just the current pose.\\[p(x_{1:t},m | z_{1:t}, u_{1:t})\\]The online SLAM problem is the result of integrating out past poses from the full SLAM problem:\\[p\\left(x_t, m \\mid z_{1: t}, u_{1: t}\\right)=\\iint \\cdots \\int p\\left(x_{1: t}, m \\mid z_{1: t}, u_{1: t}\\right) d x_1 d x_2 \\ldots d x_{t-1}\\]Solving the SLAM problem requires that a state transition model and an observation model be defined describing the effect of the control input and observation respectively. The observation model describes the probability of making an observation \\(z_k\\) when the vehicle location and landmark locations are known $$ P(z_k x_k,m) $$. The motion model for the vehicle described in terms of a probability distribution on state transitions $$ P(x_k x_{k-1},u_k) $$. The SLAM algorithm is now implemented in a recursive (sequential) prediction (time-update) correction (measurement-update) form.Time update:\\[\\begin{aligned}&amp; P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right) \\\\&amp; =\\int P\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\mathbf{u}_k\\right) \\times P\\left(\\mathbf{x}_{k-1}, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k-1}, \\mathbf{x}_0\\right) \\mathrm{d} \\mathbf{x}_{k-1}\\end{aligned}\\]Measurement update:\\[\\begin{aligned}&amp; P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right) \\\\&amp; =\\frac{P\\left(\\mathbf{z}_k \\mid \\mathbf{x}_k, \\mathbf{m}\\right) P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right)}{P\\left(\\mathbf{z}_k \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}\\right)}\\end{aligned}\\]EKF SLAMEKF-SLAM (Cyrill Stachniss)It takes in observed landmarks from the environment and compares them with the known landmarks to find associations and new landmarks. It then uses the association to correct the state and state covariance matricies.Maps in EKF SLAM are feature-based, which means they are composed of point landmarks. It tends to work well the less ambiguous the landmarks are, and hence requires significant engineering of feature detectors, sometimes using artificial beacons as features.EKF SLAM makes a Gaussian noise assumption for robot motion and perception.SLAM with Known CorrespondenceIt addresses only the continuous portion of the SLAM problem. In addition to estimating the robot pose, the algorithm also estimates the coordinates of all landmarks encountered along the way.Let the combined state vector comprising the robot pose and the map be denoted by $y_t$.\\[\\begin{aligned}&amp; y_t=\\left(\\begin{array}{l}x_t \\\\m\\end{array}\\right) \\\\&amp; =\\left(\\begin{array}{llllllllllll}x &amp; y &amp; \\theta &amp; m_{1, x} &amp; m_{1, y} &amp; s_1 &amp; m_{2, x} &amp; m_{2, y} &amp; s_2 &amp; \\ldots &amp; m_{N, x} &amp; m_{N, y} &amp; s_N\\end{array}\\right)^T \\\\&amp;\\end{aligned}\\]where $x,y,\\theta$ denote the robot‚Äôs coordinates at time t.$m_{i,x},m_{i,y}$ are the coordinates of the i-th landmark for i=1,2,‚Ä¶,N and $s_i$ is its signature. The dimension of this state vector is 3N+3, where N is the number of landmarks.Extended Kalman Filter AlgorithmInputs to the above algorithm are - the corrected estimate from the previous iteration, covariance or uncertainties, control input and measurements.Line 2 represents the motion model. Line 4 is calculating the Kalman Gain. Lines 5 and 6 are the update equations.Disadvantages of EKF SLAM: Linearization errors: The EKF SLAM algorithm relies on linearizing the nonlinear motion and measurement models, which can lead to approximation errors. These errors can accumulate over time, leading to inaccurate state estimates. Computational complexity: EKF SLAM is computationally intensive, and its complexity grows with the number of landmarks and the size of the state vector. This can make it difficult to use in real-time applications or on resource-limited devices. Limited observability: EKF SLAM assumes that all landmarks are observable, but in practice, this may not be the case. Landmarks that are not observed cannot be included in the state estimate, which can lead to inaccuracies. Sensitivity to initialization: The EKF SLAM algorithm is sensitive to the initial estimates of the robot‚Äôs pose and the landmark locations. Poor initial estimates can result in the algorithm converging to the wrong solution or getting stuck in a local minimum. Difficulty in handling loop closures: EKF SLAM assumes that the robot‚Äôs path is acyclic, which means it cannot handle loop closures. Loop closures occur when the robot revisits a previously visited location, and they are essential for accurate mapping. Handling loop closures requires more complex algorithms such as GraphSLAM.Loop ClosingIn loop closing, the SLAM algorithm attempts to identify and correct errors in the estimated robot trajectory by detecting and closing loops in the generated map. A loop is created when the robot revisits a previously mapped location after having moved through some other parts of the environment. When a loop is detected, the SLAM algorithm tries to reconcile the previously mapped location with the current robot pose estimate, usually by optimizing the robot‚Äôs trajectory using bundle adjustment or similar techniques.Loop closing is important in SLAM because it enables the creation of consistent and accurate maps of the environment. Without loop closing, SLAM algorithms can suffer from drift, in which errors accumulate over time, causing the estimated robot pose to diverge from the true pose. Loop closing helps to correct such errors and ensure that the generated map is globally consistent. Loop closing means revisiting and recognizing an already mapped area. It reduces uncertainty in robot and landmark estimates. Uncertainties collapse after a loop closure, whether the closure was correct or not. This can be exploited when exploring an environment for the sake of better and more accurate maps. However, wrong loop closures lead to filter divergence.GraphSLAMGraph-based SLAM using Pose Graphs (Cyrill Stachniss)GraphSLAM extracts from the data a set of soft constraints represented by a sparse graph. It obtains the map and the robot path by resolving these constraints into a globally consistent estimate.The constraints are generally nonlinear, but in the process of resolving them they are linearized and transformed into an information matrix.Each edge in the graph corresponds to an event: a motion event generates an edge between two robot poses, and a measurement event creates a link between a pose and a feature in the map.For a linear system, these constraints are equivalent to entries in an information matrix and an information vector of a large system of equations.Adding Confidence Measures Linear Least Squares allows us to include a weighting of each linear constraint. If we know something about how confident a measure is, we can include that in the computation. We weight each constraint by a diagonal matrix where the weights are 1/(variance for each constraint). Highly confident constraints have low variance; 1/variance is large weight, and vice-versa.Cyclic Dependence Features that are observed multiple times, with large time delays in between. This might be the case because the robot goes back and forth through a corridor, or because the world possesses cycles. In either situation, there will exist features that are seen at drastically different time steps." }, { "title": "Third-order Tchebyshev Low-pass Filter", "url": "/posts/chebyshev/", "categories": "Projects, Electronics", "tags": "emfme, microstrip, filter", "date": "2022-12-11 11:00:00 -0500", "snippet": "AbstractGiven the significance of microwave filters in real-world systems and the wide variety of potential applications, the domain of microwave filter design has been overgrowing. Modern computer-aided design (CAD) software based on the insertion loss approach is used for the majority of microwave filter designs. Microwave filter design is still a topic of active research due to ongoing developments in distributed element network synthesis, the use of low-temperature superconductors and other novel materials, and the integration of active devices in filter circuits.This report discusses the design and analysis of a third-order Tchebyshev low-pass microstrip filter. The prototype filter and the lumped element filter with a cut-off of 2.5 GHz and characteristic impedance of 50 Œ© are designed using standard formulas. The design and simulation of the low-pass filter are performed on CST Studio software. After the simulation, the final component is fabricated using Fr 4 substrate with er = 4.3 and thickness h = 0.8 mm and then tested using Vector Network Analyzer. The practical simulation and the measured results are reported and show a good agreement together.IntroductionThe filter is a device used to control the frequency response. Typical frequency responses include low-pass, high-pass, bandpass, and band-reject characteristics. A filter that allows low-frequency signals and rejects high-frequency signals, and transmits low-frequency signals from the input to the output port with minimum attenuation is called a low-pass filter. The amount of attenuation depends on the type of filter. The Low pass Chebyshev filter, made with LC combinations (Inductor and Capacitor), provides a faster transition from passband to stopband and has more ripples in the pass band.The Microstrip line is used for designing the filter as it is easy to fabricate and has low insertion loss. The microstrip line sections are equivalent to Inductors and Capacitors, and to obtain a filter with the desired cutoff frequency, length and width parameters are calculated. After getting the required specifications, the filter structure has been designed with the help of CST Studio. This is software used for designing, analyzing, and optimizing electromagnetic Components and Systems. The dxf files obtained from the design were imported into the Design Pro softwareAnd used for fabrication.Literature ReviewA number of research papers have been published on the design, simulation, and fabrication of low-pass filters using different software and design techniques. This review provides a brief overview of research papers relevant to the project title. Saxena, Shefali &amp; Porwal, Shikha &amp; Soni, Komal &amp; Chhawchharia, Pradeep. (2009). Design and Analysis of Chebyshev Low Pass Filter using Insertion Loss Method. This paper discusses the use of the insertion loss method for the design and analysis of a 5th Chebyshev low-pass filter with a cutoff frequency of 2.5 GHz, characteristic impedance of 50 Œ©, and passband ripple of 0.01 dB. PUFF software is used to simulate the lumped element filter, and the results are examined. The lumped element filter is then converted into a planar filter, and Micro-stripes software is used for its simulation and analysis. The microstrip is fabricated using GML100 substrate with er =3.2 and thickness h = 0.762 mm. The optimized low pass filter with end correction shows a measured cutoff frequency of 2.68 GHz and an impedance bandwidth of 2.82 GHz. Due to lead inductance and stray capacitance, the cutoff frequency of distributed and lumped elements exhibits a 700 MHz variance during analysis. R. Chaurasia, Mukesh &amp; Raval, Falguni. (2016). Third Order Low Pass Chebyshev Filter Characteristic Realization Using the Ansoft Designer.. Inventi Impact - Antennas &amp; Propagation. 2016. 1-4. This paper reports the design of a third-order low-pass Chebyshev filter with a cutoff frequency of 1 GHz and having a 1dB passband ripple. The lumped parameter filter is simulated in ANSOFT DESIGNER. The stepped impedance method is used to obtain the Microstrip filter, and the microstrip design and analysis are carried out using Fr 4 epoxy as substrate in ANSOFT HFSS. An insertion loss of -3.14 dB is obtained at 1.09 GHz for lumped network low pass filter design, and an insertion loss of -3.19 dB at 1.05 GHz for microstrip low pass filter design. Sheetal (2015). Stepped Impedance Microstrip Low-Pass Filter Implementation for S-band Application. This paper discusses the design of an S-band low pass filter using microstrip technology operating at 2.5 GHz for permittivity 4.1 with a substrate of thickness 1.6 mm and order n=6. The filter is designed using AWR microwave office simulation software, and the design and optimization of the lowpass filter is done using microstrip lines. The lowpass filter is then fabricated using photolithographic process and tested using vector network analyzer. The cutoff frequency obtained is lower than the desired value of 2.5 GHz, which may be due to imperfect fabrication and connection of SMA connector.MethodologyIn order to create practical filters, the lumped component filters must be transformed into distributed elements. In the design of microwave filters, we are mainly faced with two problems. The first is that only lumped parts function correctly within the constrained frequency range. The second is that while operating at microwave frequencies, there is no insignificant space between components. In order to turn lumped parts into transmission line sections, Richard‚Äôs transforms are utilized to solve the first issue. Using transmission elements to separate filter elements, Kuroda‚Äôs identities can be utilized to solve the second issue.The design of low-pass filters involves two steps: The first step is to choose the appropriate type of response and order of the filter based on the desired specifications. The lowpass prototype filters‚Äô element values are normalized to have a source impedance of gO = 1 and a cutoff frequency of wc = 1. They are then transformed to obtain the L-C elements for the required cutoff frequency and the source impedance. The next step is to find an appropriate microstrip realization that approximates the lumped element filter. The filter is then fabricated for a normalized cutoff frequency of wc on an Fr4 substrate with permittivity er and thickness h mm.The filter design steps are as follows:1) Filter specifications:Passband: lowpassRelative Dielectric Constant $\\epsilon_r$ = 4.3Height of substrate $h$ = 0.8 mmCutoff frequency = 2.5 GHzThe filter impedance $Z_o$ = 50 ‚Ñ¶The highest line impedance $Z_H$ = $Z_{OL}$ = 120 $\\Omega$The lowest line impedance $Z_L$ = $Z_{OC}$ = 20 $\\Omega$2) Order of the filter:For normalized LPF design with source impedance $g_0 = 1$ and cut-off frequency $w_c = 1$, the elemental values for equal-ripple low-pass filter prototypes are tabulated in table 1 for N=1 to N=10.Table 1: Element values for Equal-Ripple low-pass filter prototypes ( $g_0=1$ , $w_c = 1$, N=1 to N=10, 3.0 dB ripple)Figure 1: Attenuation characteristics for various N values versus normalized frequency.Since the order of the filter needed is N = 3, we obtain the following filter element coefficients - $g_0 = R_s$ $g_1 = L_1$ $g_2=C_2$ $g_3 = L_3$ $g_4 = R_L$ 1.000 3.3487 0.7117 3.3487 1.000 The actual values of the inductance Li and capacitance Ci can be calculated from the equations:\\[L_i = Z_0 g_i/2\\pi f_c \\newline C_i = g_i/Z_02\\pi f_c\\]The electrical length of the inductor\\[\\beta l = LZ_0/Z_H\\]The electrical length of the capacitor\\[\\beta l = CZ_L/Z_0\\]The L and C values are the normalized element values of the low pass prototype.The width of the high impedance line assuming $w/h \\leq 2$ and $w/h \\geq 2$ can be calculated using the formulas -\\[\\frac{W}{d}= \\begin{cases}\\frac{8 e^A}{e^{2 A}-2} &amp; \\text { for } W / d&lt;2 \\\\ \\frac{2}{\\pi}\\left[B-1-\\ln (2 B-1)+\\frac{\\epsilon_r-1}{2 \\epsilon_r}\\left\\{\\ln (B-1)+0.39-\\frac{0.61}{\\epsilon_r}\\right\\}\\right] &amp; \\text { for } W / d&gt;2\\end{cases}\\]where\\(\\begin{aligned}&amp; A=\\frac{Z_0}{60} \\sqrt{\\frac{\\epsilon_r+1}{2}}+\\frac{\\epsilon_r-1}{\\epsilon_r+1}\\left(0.23+\\frac{0.11}{\\epsilon_r}\\right) \\\\&amp; B=\\frac{377 \\pi}{2 Z_0 \\sqrt{\\epsilon_r}} . \\\\&amp; \\sqrt{\\varepsilon_e} k_0 l=\\beta l=\\frac{L R_0}{Z_n}=\\frac{C z_L}{R_0}\\end{aligned}\\)where $k_0=2 \\frac{\\pi f}{c}, c=3 \\times 10^8$ and\\(\\varepsilon_e=\\frac{\\varepsilon_R+1}{2}+\\frac{\\varepsilon_R-1}{2} \\frac{1}{\\sqrt{1+\\frac{12 d}{w}}}\\)3) Dimensions of the filter:The following are the dimensions of the filter, calculated using the formulas mentioned above - l0 5 mm W0 1.5559 mm l1 9.35 mm W1 0.5 mm l2 3.21235 mm W2 5.62716 mm l3 9.35 mm W3 0.5 mm l4 5 mm W4 1.5559 mm The values of $L_0$ and $L_4$ were chosen to be 5 mm so that the total length of the feedline, which is equal to 31.91235 could be close to $\\lambda / 4$, which is equal to 30 mm for 2.5 GHz frequency.4) Design of the filter:CST Studio software has been used for the design and simulation of the low-pass filter. The following picture shows the schematic -Figure 2: Schematic of microstrip filter with dimensions.5) Fabrication:The dxf files obtained from designing the low-pass filter prototype on CST Studio were used for the fabrication process. The dxf files were imported into the Design Pro software which is used for generating milling outlines, rubout, and contour routing. The Design View software is then used to fabricate boards with the desired design on a plate comprised of the substrate material and a copper layer.Figure 3: MITS PCB Machine6) Analysis:After obtaining the microstrip of the desired specifications, ports are soldered on each side, and the response of the fabricated low-pass filter is measured on a Vector Network Analyzer (VNA).Design and Simulation1) DesignThe software used for the design and simulation of the filter was CST (Computer Simulation Technology) Studio which is an electromagnetic wave analysis software. The software offers multiple templates across the EM spectrum, the ‚ÄòMicrowave and RF/Optical‚Äô template was used for designing the planar microstrip line filter in the frequency domain.Since the goal was to study the losses/ reflection, E-filed, H-field, Power loss were the parameters chosen for monitoring. Since the cut-off frequency was 2.5 GHz, the frequency range selected for modeling and analysis was 1.5-3GHz. The design procedure was as follows: First the bottom layer of the filter, that is, the ground plane was inserted [material from library: copper (lossy)] and dimensions were specified according to the calculations shown in the Methodology section. This was followed by the substrate layer made of FR4 substrate with ∆êr = 4.3. These two were added as separate elements. Next, the feedline layer was modeled, using copper(lossy). The 4 feedline parts were added in accordance to dimensions specified in the Methodology section. During the simulation, they were added as different elements. While fabrication, it was learnt that a better design strategy is to model the entire feedline as a single component. To simulate and study the s-parameters, port definition was required. The ports were created by using the face pick tool and the port extension coefficient calculator under macros solver. After this the simulation was run.The following is the model designed in CST studio:Figure 5: Microstrip filter modelFigure 6: Microstrip filter schematic with dimensions2) Simulation and s-parameter analysisOnce the simulation is run, the circuit can be analyzed through s-parameters (scattering parameters). S-parameters are network analysis parameters used for microwave circuits since it is not feasible to measure z-, abcd, h- parameters for RF circuits. Since the circuit under consideration is a low pass filter, which is a 2-port device, its s-parameters can be represented through a 2x2 reciprocal matrix.Figure 7: s-parameter results from CST StudioThe S11 parameter represents the return loss/ reflection coefficient of the transmission line. The ideal value of this parameter for a good transmission should be -15 to -20dB. The designed filter has low reflection loss for lower frequencies (below the cut off frequency 2.5GHz, the S11 value is below -10dB, which implies that at low frequencies, most of the power is absorbed by the load and there are very less reflections). At higher frequencies, especially beyond point 1 on the graph, the reflection loss is high, that is, the signal for higher frequencies is rejected by the filter, thereby showing the characteristics of a low pass filter.Figure 8:¬† S11 parameterThe S21 parameter represents the insertion loss, that is, the power loss in transmission from port 1 to port 2. The loss is close to 0dB for lower frequencies and cut-off occurs at 2.5 Ghz beyond which the losses are higher. Thus the designed filter shows low pass response.Figure 9:¬† S21 parameterMeasurement ResultsFigure 10:¬† s-parameter analysis obtained from VNAThe S11 and S21 Parameters crossover point was obtained nearly at -3db Point.In the results obtained from the VNA, the S11 parameter is indicated by the Yellow Line. The reflection/return loss value is well below -10dB for frequencies lower than 2.5 GHz and increases for higher frequencies. This implies that the cut-off for the filter is occurring close to 2.5GHz. This is in sync with the simulated values for reflection loss. The fabricated filter has more ripples in the passband as compared to the simulated one. These results are also reflected in the following table:The S21 Parameter (Indicated by Green Line) represents insertion loss, for lower frequencies loss is very low, and cut-off occurs nearly at 2.5 GHz beyond which the losses are higher. But the transition from passband to stopband is slower compared to simulation results.ApplicationsChebyshev filters (type I) exhibit a fast transition between pass-band and stop-band but this is at the expense of an in-band ripple. The Chebyshev filter is still essential to many RF applications even though the in-band ripple may render some applications unfeasible.¬† In order to significantly reduce undesirable out-of-band spurious transmissions like overtones or intermodulation, the steep roll-off is advantageously used. The optimal attenuation of undesirable signals can be obtained due to the quick transition between the pass-band and the stop-band.Chebyshev filters (type II) do not have a ripple in the passband, that is, the passband is flat which makes them very useful for low frequency and DC measurement systems, such as bridge sensors, loadcells, etc.Summary and ConclusionIn this report, we have designed a third-order Tchebyshev lowpass microstrip filter with a cut-off frequency of 2.5 GHz using CST Studio. All the calculations were performed using the mentioned standard formulas. After the verification of the design, the fabrication of the filter was done. The filter was then tested using the Vector Network Analyzer. From the return loss values, we observe that the cut-off frequency occurs at around 2.5 GHz, which is close to our desired result. The same can also be inferred from the insertion loss values. Thus, the results obtained for the designed filter match with the simulations in CST Studio." }, { "title": "Underwater Depth Estimation and Localization", "url": "/posts/underwater-depth-estimation/", "categories": "Projects, Robotics", "tags": "perception, underwater, sauvc, depth, camera", "date": "2022-12-11 11:00:00 -0500", "snippet": "AbstractSophisticated robots operating in an underwater environment require vision to perform different tasks. This project involves developing a reliable vision system by employing a depth camera, rather than a conventional binocular-stereo camera, for underwater depth estimation and localization. The project‚Äôs initial phase, until midsemester, consisted of testing the depth camera‚Äôs ability to estimate depth in the air. This report covers the second phase of the project, which involves underwater experimentation with objects for depth measurements.DEPTH ESTIMATION AND LOCALIZATIONCamera CalibrationCamera calibration is the process of estimating the parameters of a lens and camera image sensor. These calibrated camera parameters can be used to rectify the image by correcting lens distortion, estimating the depth of objects, and localization of the camera in the environment.The camera parameters, given by a camera matrix, includes the intrinsic, extrinsic and distortion parameters of the camera. The 3D world coordinates of the objects and their corresponding 2D image points are required to compute the camera matrix.The following are the components of the camera calibration matrix: Intrinsic parameters - The camera internal parameters such as the focal length, optical center, and the skew coefficient. Extrinsic parameters - The pose parameters of the camera given by a rotation and a translation vector. Distortion parameters - Radial distortion - When light rays bend differently near a lens‚Äôs edges and in its optical center, the result is radial distortion. The radial distortion coefficients model this type of distortion. Tangential distortion - Occurs when the lens and the image plane are not parallel. The tangential distortion coefficients model this type of distortion. Radial distortion:\\[\\begin{aligned}&amp; x_{\\text {corrected }}=x\\left(1+k_1 r^2+k_2 r^4+k_3 r^6\\right) \\\\&amp; y_{\\text {corrected }}=y\\left(1+k_1 r^2+k_2 r^4+k_3 r^6\\right)\\end{aligned}\\]Tangential distortion:\\[\\begin{aligned}&amp; x_{\\text {corrected }}=x+\\left[2 p_1 x y+p_2\\left(r^2+2 x^2\\right)\\right] \\\\&amp; y_{\\text {corrected }}=y+\\left[p_1\\left(r^2+2 y^2\\right)+2 p_2 x y\\right]\\end{aligned}\\]where,$x$, $y$ = undistorted pixel locations$r^2=x^2+y^2$$k_1,k_2,k_3 \\space -$ radial distortion coefficients$p_1,p_2 \\space -$ tangential distortion coefficientsThere exist quite a few methods for camera calibration, the most popular one being the Zhang Zhengyou calibration method.Zhang‚Äôs method:The Zhang Zhengyou technique is a calibration method that uses a checkerboard pattern. It runs a corner detector to find the points of interest and their positions on the checkerboard. The world coordinate system is set to the corner of the checkerboard, and all points are assumed to lie on the XY-plane.The method requires test patterns for camera calibration. The following shows the set of all test images used for calibration -The corners of the checkerboard are found using OpenCV, as shown below.Once the calibration is done, we can take an image and undistort it. The camera matrix and the distortion coefficients can then be stored. The following are the results obtained after running the code -Camera Matrix:\\[\\begin{bmatrix} 1.07443280e+03 &amp; 0.00000000 \\mathrm{e}+00 &amp; \\quad 4.32669818 \\mathrm{e}+02 \\\\ 0.00000000 \\mathrm{e}+00 &amp; \\quad 1.01862189 \\mathrm{e}+03 &amp; \\quad 2.63179677 \\mathrm{e}+02\\\\ 0.00000000 \\mathrm{e}+00 &amp; \\quad 0.00000000 \\mathrm{e}+00 &amp; \\quad 1.00000000 \\mathrm{e}+00\\\\\\end{bmatrix}\\]Distortion Parameters:\\[\\begin{bmatrix} -0.09265683 &amp; 0.80628656 &amp; -0.00354457 &amp; -0.00947235 &amp; -1.59645404\\end{bmatrix}\\]Total Error: $0.06509290538498035$Depth camera:The Intel Realsense D415 Depth camera uses stereo vision to calculate depth, and consists of a pair of depth sensors, RGB sensor, and an infrared projector. Image sensors - The set of image sensors enable capturing of disparity between images up to 1280 x 720 resolution. RGB sensor - Dedicated color image signal processor for image adjustments and scaling color data. Infrared sensor - Active infrared projector to illuminate objects to enhance the depth data.The camera data rectified in its Vision Processor D4 hardware component is streamed in most modes. The rectified data is then sent to the computer via the USB cable and made available for us to view. The rectified and unrectified images can be obtained by using the relevant IR stream modes, as follows -1) Y8 IR mode - provides calibrated and rectified images.2) Y16 IR mode - provides unrectified images (closest to what you would get from a true rawstream without calibration)The Intel Realsense SDK, however, does not allow simultaneous RGB stream of the left and right cameras. Thus the process of camera calibration in the depth camera is done by the on-board chip, and the calibrated data can be obtained by looking into the Y8 IR mode stream. The same can also be obtained using OpenCV and the provided librealsense API, as shown below.Depth EstimationUsing calibrated IR data:The calibrated IR image data from the depth camera can be used to perform depth estimation. The StereoSGBM class provided by OpenCV is used for computing the stereo correspondence using the Sum of Squared Differences (SSD) block-matching algorithm, and for disparity calculation by matching the blocks in left and right images as shown below.The image below shows the left and right IR images that are used as input to the stereo-depth algorithm.The following is the depth map, plotted on matplotlib, obtained from the stereo-depth algorithm. The map resembles the image on the right side to some extent, but needs to be improved by playing around with the values of the number of disparities and block size, which are given as input to the StereoSGBM class.Using default calibration data:The entire process of camera calibration, rectification, block matching, disparity calculation, and depth estimation is carried out by the On-board Chip of the Intel Realsense depth camera by default. Thus, the simpler way of performing depth estimation is by listening to the relevant streams and using them to find the depth. The depth stream, Z16 mode, and the color stream, BGR8 mode, are used for this purpose.The image on the left below shows the color frame and the corresponding distance at the point of placement of the cursor, while the image on the right shows the depth frame.OAK-D CameraThe OAK-D depth camera by Luxonis has three on-board cameras which can implement stereo and RGB vision used for depth and AI processing. The camera has a baseline length of 7.5cm, which is the distance between the left and the right stereo cameras. Stereo Cameras - The set of image sensors enable capturing of disparity between images up to 1280 x 800 (1MP) resolution. Color Camera - Dedicated color image signal processor that provides a resolution of up to 4032 x 3040 (12MP).The camera is capable of performing stereo depth perception with filtering, post-processing, RGB-depth alignment, and high configurability.UNDERWATER TESTINGThe depth camera calibrated in the air cannot be used for performing underwater experiments due to differences in disparities caused by the refraction of light. As a result, the depth camera needs to be calibrated underwater and then used for depth estimation.SetupThe setup to perform underwater depth measurement involves placing the OAK-D depth camera in a transparent container and holding it inside water partially submerged, as shown in the images below. The entire setup and all the experiments were performed in the swimming pool.Camera CalibrationThe camera is calibrated using a charuco board, in a process very similar to calibrating using the Zhang Zhengyou technique. The charuco board is printed onto a flat surface, and 13 different images are captured in different orientations.The following shows an image of the charuco board having a square size of 2.45 cm -The following shows the set of all test images (left + right + rgb) used for calibration. The numbers that appear on some images in the middle show the timer while capturing the image -Once the camera calibration is complete, we obtain the distortion coefficients, intrinsic parameters, extrinsic parameters, and the stereorectification data of the camera.\"extrinsics\": { \"rotationMatrix\": [ [ 0.9999324679374695, -0.008666034787893295, 0.007746713235974312 ], [ 0.008631718344986439, 0.9999528527259827, 0.0044522895477712154 ], [ -0.00778493145480752, -0.0043851216323673725, 0.9999600648880005 ] ], \"specTranslation\": { \"x\": -7.5, \"y\": 0.0, \"z\": 0.0 }, \"toCameraSocket\": 2, \"translation\": { \"x\": -7.581012725830078, \"y\": -0.0006467599887400866, \"z\": 0.040638044476509094 } },\"intrinsicMatrix\": [ [ 799.7395629882813, 0.0, 662.93359375 ], [ 0.0, 799.383056640625, 382.2420654296875 ], [ 0.0, 0.0, 1.0 ] ],\"rectifiedRotationLeft\": [ [ 0.9999605417251587, -0.008557096123695374, 0.002386769512668252 ], [ 0.00855177454650402, 0.9999609589576721, 0.0022310472559183836 ], [ -0.002405767561867833, -0.002210548147559166, 0.9999946355819702 ] ], \"rectifiedRotationRight\": [ [ 0.9999856352806091, 8.531191269867122e-05, -0.005360426381230354 ], [ -9.721628157421947e-05, 0.9999975562095642, -0.0022205670829862356 ], [ 0.005360223352909088, 0.0022210562601685524, 0.9999831914901733 ] ],\"distortionCoeff\": [ 9.491045951843262, -102.06877136230469, 0.0008228731458075345, 0.001999291591346264, 401.19476318359375, 9.264853477478027, -100.42776489257813, 394.56182861328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ],Depth EstimationThe StereoDepth node, provided by the OAK-D API, is used to calculate the disparity and depth from the stereo camera pair.The generation of the depth map from the StereoDepth node can be visualized using the following images, which are taken in air -When the object is placed closer to the camera -When the object is placed farther away from the camera -The same experiments are performed underwater, but now using the newly calibrated camera matrix. We obtain the depth values and the disparity map as follows -From the above images, we see that as the bottle is placed farther away from the depth camera setup, the value of the depth increases, and the disparity map changes accordingly. From the experiments performed above, we find that the depth camera can measure a depth of up to 1m (~3ft) underwater.Camera LocalizationAs done earlier, the position of the depth camera can be estimated using the extrinsic parameters of the camera.The following are the rotation and translation vectors obtained after calibration -\"rotationMatrix\": [ [ 0.9999324679374695, -0.008666034787893295, 0.007746713235974312 ], [ 0.008631718344986439, 0.9999528527259827, 0.0044522895477712154 ], [ -0.00778493145480752, -0.0043851216323673725, 0.9999600648880005 ] ], \"translation\": { \"x\": -7.581012725830078, \"y\": -0.0006467599887400866, \"z\": 0.040638044476509094 }ConclusionThe work done so far includes camera calibration, block matching, disparity calculation, and depth estimation in an underwater environment. While performing the experiments, the depth camera could measure the depth of obstacles only up to a distance of 1m. This can be further improved by building a better canister for housing the depth camera and by improving the lighting conditions using subsea LED lights.One major issue that I faced while working with the underwater camera setup was the problem of fogging. Due to the difference in temperatures at the back of the camera, which heats up, and the front of the camera, which is cold, fog begins to form on the left and right stereo cameras. This issue can also be solved by building a properly insulated canister." }, { "title": "The Singapore AUV Challenge!", "url": "/posts/sauvc-intro/", "categories": "Projects, Robotics", "tags": "", "date": "2022-09-01 12:00:00 -0400", "snippet": "AboutThe Singapore Autonomous Underwater Vehicle Challenge (SAUVC) is an annual international competition that aims to promote the development of autonomous underwater vehicles (AUVs) and encourage the exploration of innovative technologies in the field. The competition is open to undergraduate and graduate students from around the world, and provides a platform for teams to showcase their skills and contribute to the advancement of AUV technology.Participating teams are required to design and build an AUV that can successfully complete a series of tasks, including object identification and retrieval, navigation through an obstacle course, and acoustic localization. The tasks are designed to test the capabilities and efficiency of the AUVs, as well as the team‚Äôs understanding of the underlying technologies and principles.One of the main tasks of the SAUVC is object identification and retrieval. Teams are required to locate and retrieve a predetermined object from the competition area using their AUV. The AUV must be able to locate the object using its sensors and then retrieve it using a manipulator arm or other mechanism. This task requires the AUV to have precise navigation capabilities and the ability to identify and distinguish objects based on their characteristics. The object identification and retrieval task is particularly important as it simulates real-world scenarios in which AUVs may be used for search and rescue operations or to retrieve objects from the ocean floor.Another task of the SAUVC is navigation through an obstacle course. Teams are required to program their AUV to navigate through a series of gates and markers placed in the competition area. The AUV must be able to avoid obstacles and accurately follow the predetermined course. This task requires the AUV to have robust and reliable navigation capabilities, as well as the ability to adjust its course in real-time to avoid obstacles. The obstacle course task is designed to test the AUV‚Äôs ability to navigate complex environments and to adapt to changing conditions.Overall, the SAUVC is a valuable opportunity for teams to showcase their skills and contribute to the advancement of AUV technology. The competition provides a platform for students to gain practical experience in the design and development of AUVs, and to engage with experts in the field. The SAUVC is an important event for the AUV community, and continues to be a driving force in the development and promotion of this exciting and innovative technology." }, { "title": "AUV Simulation", "url": "/posts/sauvc-sim/", "categories": "Projects, Robotics", "tags": "simulation, ros, cad, gazebo, python", "date": "2022-08-31 12:00:00 -0400", "snippet": "Simulation is an important aspect of building an Autonomous Underwater Vehicle (AUV). It allows the team to test and validate the robot‚Äôs design and performance in a controlled environment before deploying it in the actual competition. Simulations also enable us to identify and troubleshoot potential issues before they occur in the field, which can save valuable time and resources.It helps us test various aspects of the robot‚Äôs design, such as its hydrodynamics, control algorithms, and sensor systems. By simulating the robot‚Äôs movement and behavior in different scenarios and environments, we can optimize its design for maximum performance.Additionally, simulation can be used to test the robot‚Äôs autonomy and decision-making capabilities. The simulated robot can be programmed to respond to different situations and tasks, such as obstacle avoidance, path planning, and target tracking. This can help the team to build a more robust and capable robot, which can improve its chances of success in the competition.All the simulations were carried out on Gazebo using Robot Operating System (ROS). The following were the components of the simulation/testing process:Environment - The initial environment consisted of an open ocean with objects such as a shipwreck. This environment was not enough to satisfy the simulations needed, so a pool environment called sauvc_pool which contained accurate information about the size of the pool as well as the obstacles and tasks inside the pool was implemented.Bluerov2(ffg) - The bot we chose to simulate was the bluerov2 from Bluerobotics, as it consisted of a 6-thruster design, and its thrusters would give us an accurate idea of our own thrusters.Bluerov in pool - The bot was integrated into the sauvc_pool environment, where it could freely interact with the tasks and obstacles. Since it was operated in ROV (Remotely Operated Vehicle) mode and not AUV mode, the bot had to be controlled with keyboard input.Simulation Video:6 thruster AUV in pool - Bluerov2 has 6 thrusters, but in our tests, we need to simulate the results for an AUV having anywhere from 1 to 6 thrusters and testing its DOF‚Äôs, thrust values, etc. For this, the bluerov2‚Äôs URDF files were tweaked to include/exclude thrusters as well as specify the directions they were pointed in.Simulation Video:ROV in pool After this, we imported the STL file of our own SAUVC entry into Gazebo for simulation in place of the bluerov2. The file followed the same format as that of the bluerov2 so that the thruster locations, thrust values, as well as ROV operation is possible for our bot the same way as the bluerov2." }, { "title": "Object Detection", "url": "/posts/sauvc-object-detection/", "categories": "Projects, Robotics", "tags": "dl, cnn, computer vision, python, yolo", "date": "2022-08-31 12:00:00 -0400", "snippet": "Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class in digital images and videos. One popular algorithm for object detection is YOLO (You Only Look Once). YOLO uses a single neural network to predict bounding boxes and class probabilities directly from full images in one evaluation. It‚Äôs become popular in real-time object detection because of its speed, as well as its ability to detect a large number of object classes.Using YOLO for object detection in an underwater environment, however, is a challenging task. The lighting conditions, turbidity, and low visibility can affect the performance of the algorithm. In addition, underwater images may have different characteristics than images taken in an above-water environment, such as color distortion, refraction, and reflection. To overcome these challenges, image pre-processing and training the model on a dataset of underwater images is crucial. This dataset should include a variety of underwater scenarios, lighting conditions, and different types of objects.In the context of the Singapore Autonomous Underwater Vehicle Challenge (SAUVC), YOLO can be used to detect and classify various objects in the underwater environment. This can help to improve the navigation and control of the autonomous underwater vehicle (AUV). For example, YOLO can be used to detect obstacles and map the environment, which can be used to plan a safe and efficient path for the AUV to follow. YOLO can also be used to identify targets of interest, such as underwater structures or objects of a specific class. Furthermore, object detection information can be used to trigger specific actions or behaviors of the AUV, such as obstacle avoidance, and capturing images or samples of a specific object.In the SAUVC competition, the vehicle is required to navigate in an unknown and unstructured environment. The object detection algorithm can be used for mission planning and localization and can help the vehicle perform more complex tasks, such as search and recovery." }, { "title": "Acoustic Localization", "url": "/posts/sauvc-acoustic/", "categories": "Projects, Robotics", "tags": "acoustics, localization", "date": "2022-08-31 12:00:00 -0400", "snippet": "Localization is required to complete the target acquisition and re-acquisition task in the challenge. The pingers fitted to the drums act as an acoustic source. A 2/3/4 acoustic sensor setup can be used for localization. There are different localization techniques, such as received signal strength indication (RSSI), angle of arrival (AOA), time difference of arrival (TDOA), and time of arrival (TOA), of which different methods are used in various applications.The angle of arrival estimates is found by using TDOA, and the far-field approach is followed for simplicity.Time difference of arrival estimation: We get a hyperbola with two possible locations with two hydrophones, and with four hydrophones, it is easier to find the location of the source. The time difference between the two microphones is computed and used in estimating the position of the acoustic source.Finding the time difference:The following methods are widely used, with each having its pros and cons: Normalized frequency-based TDOA: This method can be used when the source is of a single frequency(and the value is known). We convert the obtained signals into the frequency domain and use the phase difference to find the time difference, and this method is prone to noise. GCC-PHAT: This is based on the GCC algorithm. In order to compute the TDOA between the reference channel and any other channel for any given segment, it is usual to estimate it as the delay that causes the cross-correlation between the two signal segments to be maximum. This method is less prone to noise but is computationally expensive.Hilbert transformation:This method works well for narrow-band signals and can be simplified using FFT to reduce computational costs and gives good results when compared to cross-correlation methods." }, { "title": "Thruster Control of AUV Using LQR", "url": "/posts/lqr-control/", "categories": "Projects, Robotics", "tags": "control, lqr, pid, simulink, matlab", "date": "2022-08-11 12:00:00 -0400", "snippet": "Abstract To make two different control system models in Simulink on PID and LQR controllers respectively and to get the results for positional and velocity parameters of the ROV based on the desired reference inputs. To tune parameters related to both the controllers to achieve better results and finally test the model physically underwater.PID ControllerLiterature ReviewThe basic mathematical model of an ROV is given by:\\[\\dot{\\eta}=J(\\eta)v \\newline M\\dot{v}+C(v)v + D(v)v + g(\\eta)=\\tau\\] where M = Mass matrix (with added mass effect taken into account) C = Coriolis force matrix D = Damping forces matrix g = Restoring forces matrix $\\tau$ = Forces &amp; moments matrix1. Added Mass Effect:The added mass of an object is the effect in which some mass of fluid surrounding the object under observation is accelerated/decelerated along with it.2. Coriolis Effect:The Coriolis force is a fictitious force that comes into play whenever we are trying to explain the forces on an object with respect to a rotating frame.3. Ziegler-Nichols Method for tuning PIDs:This method can be used to tune our PID both in the case where we have a working model for our plant or even when we don‚Äôt. The first step to this method is measuring two parameters: KU which is the gain at which the system becomes marginally stable and TU which is the period of oscillation at marginal system response. These values are found by taking KI and KD values to be zero for that input and changing KP until marginal stability is achieved.After these parameters are evaluated controller gains can simply be calculated from the below table:4. Controller Models:The two controller models that we used are: Linear Model Non-linear ModelLinear ModelThe schematic of the simulink model created using the linear PID control looks like this:It consists of different functionalities in each block of the design:The PID controller uses an error signal, called the tracking error, generated from the difference between the desired position and the current position of the rover.\\[e = \\eta_d - \\eta\\]This error signal, in the world frame, is converted to the error signal in body frame ùëíb using the following equation:\\[e^b = J^T(\\eta)e\\]where the transformation matrix from the vehicle body frame to the world reference frame using Euler angle transformation is given by:\\[J_\\theta(\\eta) = \\begin{bmatrix} R^n_b(\\theta) &amp; 0_{3x3} \\\\ 0_{3x3}&amp; T_\\theta(\\theta) \\end{bmatrix}\\]Using the error signal in the body frame ùëíb, the torque generated by the PID can be calculated using the equation:\\[\\tau_{P I D}=K_P e^b(t)+K_I \\int_0^t e^b\\left(t^{\\prime}\\right) d t^{\\prime}+K_D \\frac{d e^b(t)}{d t}\\]In order to generalise the required control forces, control allocation calculates the control input signal u to apply to the thrusters. The control forces due to the control inputs applied to the thrusters can be expressed as:\\[\\tau = T(\\alpha)F = T(\\alpha)Ku\\]As a result, the control input vector can be derived as:\\[u = K^{-1}T^{-1}\\tau\\]For the linear model, the following values of KP, KI, and KD have been obtained using the Ziegler-Nichols method: 6-DoF PID Surge Sway Heave Roll Pitch Yaw KP 3 3 3 4 4 2 KI 0.2 0.2 0.2 0.3 0.3 0.1 KD 2.5 2.5 0.5 0.5 1 0.5 Using the control input vector, the thruster system generates the control forces in 6 DoFs with the help of the above mentioned equation $\\tau = T(\\alpha)F = T(\\alpha)Ku$Since the 8 thrusters of the ROV produce a maximum thrust of 40N at operating voltage of 16V, the thrust coefficients are approximated to 40. Thus the thrust coefficient matrix ùêæ is taken as$K = diag[40,40, 40,40,40,40,40,40]$After obtaining the torque vector, the kinetics is used to determine the acceleration in the body frame for the given forces using the state equation:\\[M\\dot{v}+C(v)v + D(v)v + g(\\eta)=\\tau\\]The kinematics block is then used to define the vehicle velocity in the world frame ùë£n. The position of the vehicle is then determined in the integrator and, using inverse kinematics, is converted to the body frame before being supplied back into the controller.Non-Linear ModelBecause of disturbances underwater like current speed, there will be non-linearities introduced into the system. This makes the linear-PID controller model inappropriate to use as there will be a lot of deviations from the desired o/p and noise in the system. So, we need to include the system dynamics to get the control input to the thrusters.In the nonlinear model-based PID control system design, the dynamic model of the ROV is utilized to produce a 6-DoF predictive force and the model-based PID is used to provide a corrective force in 6 DoFs to adjust the error in the model. This is advantageous in that the model error and non-linearities tend to be smaller than the dynamics themselves.In the predictive force generation, a virtual reference trajectory strategy is introduced for the design of trajectory tracking. With the use of a scalar measure of tracking in Fossen (Fossen, 1994), a virtual reference $x_r$ can be defined that satisfies:\\[\\dot{x_r}=\\dot{x_d}+\\lambda e^b\\]where $\\lambda$ &gt; 0 is the control bandwidth that describes the amount of tracking error to the overall tracking performance, and $e^b$ is the tracking error in the body frame.Since the velocity $v$ is the time derivative of the position (i.e. $v=\\dot{\\eta}$), for a defined virtual reference position $\\eta_r$, the following is satisfied:\\[v_r=v_d+\\lambda e^b\\]So, $\\lambda$ is used to tune the 6-DoF predictive force.This is shown in the following block:Where ‚ÄòA‚Äô is the new controller output.The PID controller gains(Kp, Ki and Kd) for the non-linear model are found out by Ziegler Nichols method and were found out to be as follows:Finally, the control law for the nonlinear model-based PID controller is computed given by:\\[\\begin{aligned}\\tau= &amp; M\\left(v_d+\\lambda\\left(v_d-v\\right)\\right)+C_{R B}(v) v+C_A\\left(v_w\\right) v_w+D\\left(v_w\\right)\\left(v_d+\\lambda e^b-v_c\\right)+g(\\eta) \\\\&amp; +K_P e^b(t)+K_I \\int_0^t e^b\\left(t^{\\prime}\\right) d t^{\\prime}+K_D \\frac{d e^b(t)}{d t}\\end{aligned}\\]The final model for this system is shown below:Here, we took desired position input as $[1;1;2;0;0;0]$.PID ImplementationWe implemented the models for both the controllers above and got the following results when we give a desired positional input:Linear Controller ModelUsing the above method, we have obtained the following results for a desired position input: $\\eta_d=[3;4;1;1.57;0;0]$.The output of the position and orientation control is obtained as follows:Position X:Desired output: 3 mObtained result:The steady state response final value = 3.Position Y:Desired output: 4 mObtained result:The steady state response final value = 4.Position Z:Desired output: 1 mObtained result:The steady state response final value = 1.Orientation $\\phi$:Desired output: 1.57 radiansObtained result:The steady state response final value = 1.57.Orientation $\\theta$:Desired output: 0 radiansObtained result:The steady state response final value = 0.Orientation $\\psi$:Desired output: 0 radiansObtained result:The steady state response final value = 3.The control inputs to the thrusters for the same are represented in the plots below:As it is evident from the plots, each thruster is instructed to provide the required thrust every instant for a finite period of time after which the control input eventually settles down to a final value.The model of the rover under study somewhat looks like this:Here, the thrusters 1,2,3,4 are used in the position control in the X,Y directions while 5,6,7,8 are used for the Z-directional control. If we see the plots for the thruster control inputs for 1,2,3,4:We can observe the steady state final value to be zero for all the thrusters 1,2,3,4.Reason: The above thrusters are used for controlling the position in X,Y directions and their respective control inputs drive them to work in a synchronised manner so as to reach the respective position. Once the rover has reached the required X,Y, coordinates of the position with the required orientation, there is no need for them to provide any more thrust given the lack of any external force acting in the X,Y directions.The interesting part comes when we observe the control input plots for the thrusters 5,6,7,8:As it is observed, each of the control inputs have a non-zero steady state value; positive for thrusters 6,7 and negative for thrusters 5,8.Reason: Thrusters 5,6,7,8 are used for positional control in the Z direction. When the rover moves to the desired Z coordinate position in the required orientation, the job of the thrusters is not done since the position has to be retained while neutralising an external force. This external force is the buoyancy acting upon the rover since it is designed to be positively buoyant. The values of forcing acting due to weight and buoyancy for the rover under study are:W = 112.8 NB = 114.8 N ; this implies that the net external force acting on the rover is 2N upwards.When we look at thrusters 5,8 ; they provide a thrust vertically upwards when rotated clockwise while thrusters 6,7 produce a thrust vertically downwards for the same. This means that for thrusters 5,8 positive thrust is upwards while negative thrust is downwards. This is opposite in the case of thrusters 6,7 From the plots of control inputs to the above thrusters, the steady state values are as follows:For thrusters 5,8: $-1.25*10^{-2}$ (approx.)For thrusters 6,7: $+1.25*10^{-2}$ (approx.)Thrust produced by thrusters 5,8 is negative, which implies that thrust is produced in the¬† vertically downwards direction.Thrust produced by thrusters 6,7 is positive, which again implies that thrust is produced in the vertically downwards direction.Total thrust produced $\\tau=Ku$where K= 40 for all the thrusters.Total thrust produced in steady state:\\[4*1.25*10^{-2}*40=2N\\]downwards.Since the external force in Z direction has been neutralised, the rover is stabilised once it reaches the desired position.This is how the positional control of the underwater rover has been established using the linear PID control method.Non-Linear Controller ModelFor desired positional input as $[1;1;2;0;0;0]$, we got the following results for positional coordinates in world frame:Position X:Desired output: 1 mPosition Y:Desired output: 1 mPosition Z:Desired output: 2 mOrientation $\\phi$:Desired output: 0 radOrientation $\\theta$:Desired output: 0 radOrientation $\\psi$:Desired output: 0 radThe controller input plots (controller input for each thruster will be as follows):So, we can see that the thrust control input for thrusters 5,6,7 and 8 will be: \\(-1.375*10^{-2}, 1.375*10^{-2}, 1.375*10^{-2}, -1.375*10^{-2}\\) respectively.Now, since propeller pair of 5 and 8 will be opposite to that of the pair 6 and 7, we get the total thrust on the ROV in Z-direction as:\\(-1.375*10^{-2}*4*40 = 2N\\) (approx.), where 40 is the gain.So, the force in the vertical direction is balanced.LQR ControllerLiterature ReviewThe PID controller has been one of the most commonly used controllers for a really long time. There have been numerous PID tuning techniques, such as the Ziegler-Nichols method but are insufficient for high-performance control applications. The Linear Quadratic Regulator (LQR) is an optimal control method based on full state feedback. It aims to minimise the quadratic cost function and is then applied to linear systems, hence the name Linear Quadratic Regulator.Why LQR controller?The use of LQR¬† over PID control comes from the higher robustness of the former in terms of tuning the parameters with varying conditions. PID control uses the error in the input parameter of the closed loop system and tunes the parameters to reduce the error to zero. LQR, on the other hand, uses the state space model of the system and takes complete state feedback: -Kx, to calculate the error. LQR uses the method of cost function to calculate the control input cost vs the importance of achieving desired states.State-Space Model with Full State Feedback Gain\\[\\dot{X}=AX+BU\\]\\[y=CX+DU\\]Cost FunctionThe cost function defined by the system equations is minimised by the LQR controller using an optimal control algorithm. The cost function involves the system‚Äôs state parameters and input (control) parameters, along with the Q and R matrices. For the optimal LQR solution, the overall cost function must be as low as possible. The weights given to the state and control parameters are represented by the Q and R matrices, which act as knobs whose values can be varied to adjust the total value of the cost function.The system must have a linearized state-space model to solve the LQR optimization problem. The cost function to be optimised is given by\\[J=\\int{(X^TQX+U^TRU) dt}\\]Algebraic Riccati Equation and Its Solution ($S$ matrix)The Q and R matrices are used to solve the Algebraic Riccati Equation (ARE) to compute the full state feedback matrix.\\[A^TS+SA-SBR^{-1}B^TS+Q=0\\]On solving the above equation, we obtain the matrix $S$.Feedback Gain (K) and Eigen Values from $S$ matrixThe matrix ùëÜ obtained from the above ARE is used to find the full state feedback gain matrix K using the relation,\\[K=R^{-1}B^TS\\]The control matrix U is then given by\\[U=-KX\\]Linearisation of a state-space modelThe linearization of a state-space model is needed while using the LQR technique since it works on linear systems. In our case, the state space model is in the form: ·∫ã = f(x); where f(x) is a nonlinear function of x. In such cases, to linearise the equation, we use the concept of linearising about a fixed point. The steps for the same are as follows: Find the fixed points $\\bar{x}$; where $f(\\bar{x})=0.$ Linearise about an xÃÑ, by calculating the Jacobian of dynamics at the fixed point xÃÑ; where the latter can be represented as:\\[\\mathbf{J}=\\left[\\begin{array}{ccc}\\frac{\\partial \\mathbf{f}}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial \\mathbf{f}}{\\partial x_n}\\end{array}\\right]=\\left[\\begin{array}{c}\\nabla^{\\mathrm{T}} f_1 \\\\\\vdots \\\\\\nabla^{\\mathrm{T}} f_m\\end{array}\\right]=\\left[\\begin{array}{ccc}\\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n}\\end{array}\\right]\\]where each term is a partial derivative of the dynamics with respect to a variable.This step is executed since the dynamics of a nonlinear system behave linearly at the fixed point, or, in a small neighbourhood around the fixed point.Changing the frame of reference to one with xÃÑ as the origin:\\[\\dot{x} - \\bar{\\dot{x}} = f(x) \\newline = f(\\bar{x}) + J.(x-\\bar{x})+J^2.{(x-\\bar{x})}^2 + ....\\]where¬†$J$ is calculated at $\\bar{x}$.The higher order terms from the third term $J^2.{(x-\\bar{x})}^2$ are neglected since they are really small, hence the equation reduces to:‚àÜ·∫ã = 0 +J.‚àÜx + 0‚áí ‚àÜ·∫ã = J.‚àÜx\\[\\Delta\\dot{x} = 0+ J.\\Delta x + 0 \\newline = J.\\Delta x\\]This is in the form¬†$\\dot{x} = Ax$.Note: The above method for linearization works only when the fixed point satisfies the condition for linearising a system given by the Hartman Grobman Theorem, which states that:‚Äúthe behaviour of a dynamical system in a domain near a hyperbolic equilibrium point is qualitatively the same as the behaviour of its linearisation near this equilibrium point, where hyperbolicity means that no eigenvalue of the linearisation has real part equal to zero. Therefore, when dealing with such dynamical systems one can use the simpler linearisation of the system to analyse its behaviour around equilibria.[1]‚ÄùHence, linearization works only when the fixed point is hyperbolic or put simply, has a non-zero real part.LQR ImplementationThe following is the Simulink model that we built:To implement this, we need the following: State-space model: A and B matrices: These relate the derivative of the states with the current states and the control input. But, since our model is non-linear in nature, we cannot get a direct relation consisting of A and B matrices. Also, the LQR controller works only for Linear Systems. So, we have to linearise our system around an operating point. For our system, we took the origin in the world frame (initial point of the bot) as the operating point. Also, we used the position of the bot in the world frame and velocity of the bot in its own frame as the two states. Meaning: \\[x = \\begin{bmatrix} \\eta \\newline v \\end{bmatrix}\\] (Here $x$ is the state but not the distance in x direction)\\[\\begin{aligned}\\dot{\\boldsymbol{x}} &amp; =\\left[\\begin{array}{c}\\dot{\\boldsymbol{\\eta}} \\\\\\dot{\\boldsymbol{v}}\\end{array}\\right]=f\\left(\\boldsymbol{x}, \\boldsymbol{u}, \\boldsymbol{\\tau}_d, t\\right) \\\\&amp; =\\left[\\begin{array}{c}J(\\eta) \\boldsymbol{v} \\\\M^{-1}\\left[K p(A \\boldsymbol{u})+\\tau_d-C(\\boldsymbol{v}) \\boldsymbol{v}-D(\\boldsymbol{v}) \\boldsymbol{v}-g(\\boldsymbol{\\eta})\\right]\\end{array}\\right]\\end{aligned}\\]Since,\\[\\dot{\\eta} = J(\\eta)v\\]The above state space model is clearly non-linear. So, we have to linearise to find A and B as follows:\\[A(t) \\equiv\\left[\\begin{array}{cccc}\\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\\\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\&amp; \\vdots &amp; \\\\\\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n}\\end{array}\\right]_0 \\newline \\newline B(t) \\equiv\\left[\\begin{array}{cccc}\\frac{\\partial f_1}{\\partial u_1} &amp; \\frac{\\partial f_1}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial u_m} \\\\ \\frac{\\partial f_2}{\\partial u_1} &amp; \\frac{\\partial f_2}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial u_m} \\\\ &amp; \\vdots &amp; \\\\ \\frac{\\partial f_n}{\\partial u_1} &amp; \\frac{\\partial f_n}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial u_m}\\end{array}\\right]_0\\]So, we obtain the A and B as follows:But we obtained the above results by taking weight W and buoyancy B as 110 N and 120 N respectively. We take the C matrix in state-space $y = CX+DU$ as $eye(12)$ so as to send back all the states as feedback to the summing point. Q and R matrices: Q stands for the importance of the states to reach their desired values. R stands for the cost of the control input. So, if we have an expensive control input compared to that of our needs to reach the states, we take the Q matrix to be dominating compared to R and vice versa. Then, we calculate the full-state feedback matrix, the Algebraic Riccati Equation and the eigenvalues by using the following command:\\[[K,S,P] = lqr(A,B,Q,R)\\] Then, we put the obtained value of the Gain matrix in the state-space model in Simulink. The following results are obtained for different R matrices (Q matrix being a 12x12 identity matrix) and desired states being $[1;2;1;[0;0;0;0;0;0;0;0;0]]$:\\[R = \\begin{bmatrix}1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;0.1 \\\\ 0.1 &amp; 1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 1 &amp; 0.1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;1 \\end{bmatrix}\\]For the above, the Poles of the closed loop and open loop transfer function are as follows:Poles:Closed loop (With the feedback):Open loop (Without the feedback):Making ‚ÄòR‚Äô more dominant\\[R = \\begin{bmatrix}3 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;1.1 \\\\ 1.1 &amp; 3 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 3 &amp; 1.1 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 3 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;3 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;3 \\end{bmatrix}\\]Making ‚ÄòR‚Äô less dominant\\[R = \\begin{bmatrix}0.11 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.01 \\\\ 0.01 &amp; 0.11 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.11 &amp; 0.01 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.11 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.11 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.11 \\end{bmatrix}\\]Results Observed states:More dominant ‚ÄòR‚ÄôLess dominant ‚ÄòR‚ÄôThrust i/p to the thrusters:More dominant ‚ÄòR‚ÄôLess dominant ‚ÄòR‚ÄôBand Limited White NoiseWe planned on modelling a system with White Noise. So, we used the following control system:Where:We included a low pass filter for removing noise with frequency greater than 1Hz. With low values of ‚ÄòQ‚Äô we are getting really unstable results as the importance to the states is reduced. Later, we increased the value of Q to allow the states to reach quickly. This made the response of our system better. Taking the reference input as: [0;0;2;1.57;1;0.8;0;0;0;0;0;0]; $Q = eye(12):$States:Thrust provided by each thruster:Poles of this system:$Q = 1000*eye(12)$:States:Thrust provided by each thruster:Poles of this system:Clearly, the response of the system is faster in the second case.Note: In the above two cases, the open loop and closed loop pole plots are drawn without taking the noise into account. The effect of noise is shown only in the ‚ÄòThrust‚Äô and ‚ÄòStates‚Äô plots.Also, in the paper that we referred to, they considered noise caused due to the ‚Äòcurrent velocity‚Äô which has a direct effect on the velocity of the rover instead of the torques. So, for that, a Random Number generator block was included in¬† the model as follows:The parameters of the noise block are as follows:Here, noise is added considering the velocity of current as: $[0.25,0.25,0.25]$ and taking the variance as 0.01 in all the three directions.Taking R matrix as: \\(R = 1.2*eye(6)+0.8*ones(6)\\)And Q matrix as: $Q = 1000*eye(12)$The results that we obtained for 1 min stop time are:States:Thrust provided by each thruster:Clearly, the thrust output involves too much vibrations.Analysis of EigenvaluesWhen we analyse the change in eigen values while considering the open loop and LQR controlled closed loop systems, for the same values of Q,R we see a clear change in their position in the pole zero plot.Considering the above plot, we can see a clear shift in the eigen values in the direction of the negative real axis, ensuring more stability of the system. As we write down the eigen values or the poles of the two systems, the difference becomes evident.For $Q=eye(12)$ &amp; \\(R = 0.1*ones(12)+0.9*eye(12)\\) Open loop poles Closed loop poles 0.0000 + 0.0000i ¬† ¬† -6.3274 + 0.0000i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-3.2047 + 3.1841i ¬†¬†-0.2740 + 4.3867i ¬†¬†-3.2047 - 3.1841i ¬†¬†-0.2740 - 4.3867i ¬†-3.4503 + 2.9329i ¬†¬†-0.2633 + 0.0000i ¬†¬†-3.4503 - 2.9329i ¬†¬†-0.3003 + 4.3834i ¬†¬†-0.9222 + 0.4604i ¬†¬†-0.3003 - 4.3834i ¬†¬†-0.9222 - 0.4604i ¬†¬†-0.4067 + 0.0000i ¬†¬†-0.8836 + 0.4709i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-0.8836 - 0.4709i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-1.0104 + 0.0000i ¬†¬†-0.4504 + 0.0000i ¬†¬†-0.2170 + 0.0000i ¬†¬†-0.4375 + 0.0000i ¬†¬†-0.4043 + 0.0000i As it can be observed, in the case of the open loop system, we have four eigen values or poles located at the origin which leads to the fact that the system is marginally stable. Whereas, for the closed loop system, most of¬† the poles have shifted towards the left side of the imaginary axis with all the poles lying on the left half of the s-plane. This gives enough proof to say that after implementing LQR control, the system has become more stable as compared to the open loop system and hence has helped in increasing performance in terms of reaching the end states.ConclusionGiven the results of implementing both PID and LQR control on the ROV system, we can see a better response for an LQR control over the PID when we include disturbances in the environment. Also, the LQR control is far more robust in adapting to the change in buoyancy as compared to the PID control, which broadens the spectrum of usage in the former as compared to the latter. When we have changes in the system, using a PID requires tuning all the three gain values for all the control inputs. Tuning the gain values for multiple parameters is not an easy task and there is a very narrow range of values which give the desired results for a specific situation. On the other hand, when we want to adapt to changes while working on LQR control, our tuning is dependent only on the Q,R cost matrices which can help in changing the¬† relative importance of the states and control inputs for a given circumstance. Here, our goal is achieved by changing the values of the matrices, so as to select the more important factor among the states and inputs,¬† where we achieve the desired results for a wide range of values which only differ in terms of speed of transient response and steady state error. Another advantage of LQR over PID control that we have come across was the ability to easily control velocity components along with the position coordinates, which would lead to much more complexity in the case of a PID based controller. For simulating in a noisy environment, we used nonlinear PID control where a water current velocity was added as a disturbance; its value being constant with time and we attained appreciable results. Whereas for the LQR setup, we have used a gaussian white noise as a source of disturbance, which gives a better look at the real world scenario. Upon increasing the Q matrix or simply, the cost of attaining the states, we have observed better performance in attaining the states and less erratic, more uniform thrust outputs produced by the thrusters; which is a more reliable result since it is realisable in a real world environment.However, one drawback of using the LQR control is its applicability to only linear systems, whereas most of the real world scenarios tend to have non linearity in them. However, this problem can be overcome by linearising the system near the fixed points and applying the control.Hence, we have come to the conclusion that using an LQR control is far more reliable than a PID control for the positional and velocity control of a 6-DOF Autonomous Underwater Vehicle.Future Work The model can be implemented with an Adaptive Controller based system which is expected to give better performance. We wish to implement our work on the actual physical system to know how it works in the real world. And by doing that, we will be able to understand how the sensor noise affects the system and how the controller will compensate for it." }, { "title": "Autonomous Ground Vehicle", "url": "/posts/transnomous/", "categories": "Projects, ML", "tags": "ml, dl, cnn, computer vision, python", "date": "2022-08-11 12:00:00 -0400", "snippet": "To be Updated" }, { "title": "Quadcopter Navigation", "url": "/posts/quadcopter-navigation/", "categories": "Projects, Electronics", "tags": "electronics, arduino, led", "date": "2022-08-11 12:00:00 -0400", "snippet": "Implementing Semi-Direct Visual Odometry (SVO) on drones for autonomous navigation.Code and ROS bag for sample implementation taken from: Github 1Github2" }, { "title": "CubeD", "url": "/posts/cubed/", "categories": "Projects, Electronics", "tags": "electronics, arduino, led", "date": "2022-08-11 12:00:00 -0400", "snippet": "CubeD is an interactive touch-based display that lets users engage and play games.Mechanical DesignCubeD consists of 200 squares hinged onto a 4 segment steel back panel. Its current dimensions are 3 meters x 2 meters. Each steel back panel measures 1meter x 1.5 meters and houses 50 squares. Each square has 12 - Neopixels arranged around the inner perimeter of the square, a diffuser ensures that that the light is uniformly projected to the user.The back panels are screwed onto a two-piece wooden frame with triangular support structures. Each square has a microswitch behind it on the panel, which is used as an input device to detect touch from the user.ElectronicsThe CubeD has been designed such that it can be assembled and dismantled without any wastage of resources, and hence the electronics and electrical wiring of the entire project is divided into modules that are easy to work on. The Power Supply for CubeD consists of the main power source and two junctions that are branched from the main source.NeopixelsThe WS2812 Integrated Light Source ‚Äî or NeoPixel in Adafruit parlance ‚Äî is the latest advance in the quest for a simple, scalable, and affordable full-colour LED. Red, green, and blue LEDs are integrated alongside a driver chip into a tiny surface-mount package controlled through a single wire. Neoxpixels use a Single-Wire Protocol to transfer data.Writing out WS2812 data requires some pretty tight timing. Tight enough that FastLED disables interrupts while it is writing out led data. This means that while the led data is being written out, any interrupts that happen will be delayed until all the led data is written out.NeoPixels receive data from a fixed-frequency 800 kHz datastream. One bit, therefore, requires 1/800,000 sec ‚Äî 1.25 microseconds. One pixel requires 24 bits (8 bits each for red, green blue) ‚Äî 30 microseconds. After the last pixel‚Äôs worth of data is issued, the stream must stop for at least 50 microseconds for the new colours to ‚Äúlatch.‚ÄùFor a strip of 2400 Pixels, that‚Äôs 2400*30 + 50 which is 72,050 microseconds, therefore 13.8 updates per second.Since the latching is done internally inside the Neopixel strip, the interrupts are only disabled for data transfer.For a strip of 2400 Pixels, that‚Äôs 2400*30 which is 72,000 microseconds. During this time interrupts are disabled on most boards.Shift RegistersThis sequential device loads the data present on its inputs and then move or ‚Äúshifts‚Äù it to its output once every clock cycle, hence the name Shift Register.A shift register consists of several single bit ‚ÄúD-Type Data Latches‚Äù, one for each data bit, either a logic ‚Äú0‚Äù or a ‚Äú1‚Äù, connected together in a serial type daisy-chain arrangement so that the output from one data latch becomes the input of the next latch and so on.PISO Shift RegistersThe parallel data is loaded into the register simultaneously and is shifted out of the register serially one bit at a time under clock control.The 8 inputs are translated into a series of HIGH and LOW pulses on the serial-out pin of the shift register. This pin should be connected to an input pin on your Arduino Board, referred to as the data pin. The transfer of information on the data pin is called ‚ÄúSynchronous Serial Output‚Äù because the shift register waits to deliver a linear sequence of data to the Arduino until the Arduino asks for it. Synchronous Serial communication, either input or output, is heavily reliant on what is referred to as a clock pin. The clock pin is the metronome of the conversation between the shift register and the Arduino, it is what keeps the two systems synchronous. Every time the Arduino changes the clock pin from LOW to HIGH the shift register changes the state of the Serial Output pin, indicating the value of the next switch.The third pin attached to the Arduino is a ‚ÄúParallel to Serial Control‚Äù pin. It is referred to as a latch pin. When the latch pin is HIGH the shift register is listening to its 8 parallel inputs. When the latch pin is LOW, it listens to the clock pin and passes information serially. That means every time the latch pin transitions from HIGH to LOW the shift register will start passing its most current switch information.In the current design, we have 4 sets of 7 PISO Registers daisy-chained with a UNO as a sub-unit to process the states of 50 microswitches.Micro SwitchesA Microswitch has three terminals NC, NO, and C (or COM). C is the signal to be switched. NC is normally closed and NO is normally open.A 5V signal is given to the Common terminal and the output signal from NO is monitored using a digitalRead. Each microswitch is placed behind a square box on the back panel, in such a way that the switch would be triggered when the box is touched. Each NO terminal is connected to the Parallel Input Pin of a PISO Shift Register with the help of a terminal box to maintain the modular structure.Power SupplySwitching Mode Power Supply units are used as the main power source for CubeD. Multiple SMPS units are used along with Junctions at appropriate places to distribute loads to keep the circuit stable. A key feature in the design is the fact that all the connections made to individual modules are in parallel which ensures the working of other modules in case a single module is down for maintenance.Each Neopixel grid is powered using one SMPS from both ends, this is to avoid reverse feed from one SMPS to another due to minute changes in the output voltage. All the grounds are interconnected.NeoPixels don‚Äôt care what end they receive power from. Though data moves in only one direction, electricity can go either way. You can connect power at the head, the tail, in the middle, or ideally distribute it to several points. Think of power distribution as branches of a tree rather than one continuous line.Power RequirementsEach individual NeoPixel draws up to 60 milliamps at maximum brightness white (red + green + blue). In actual use though, it‚Äôs rare for all pixels to be turned on that way. When mixing colours and displaying animations, the current draw will be much less.For a strip of 2400 Pixels at 5V, the current consumption is 144 Amps. Which is around 720 Watts. Therefore, each SMPS should be capable of giving out around 40 Amps and a rated power output of more than 200W.Terminal BoxEach of the 4 PISO Registers PCB contains around 56 PCB Terminals Soldered onto the PCB for easy connections between the microswitches and the registers.Junction BoxJunction Boxes are used to distribute power without directly tapping into the main power source and this also helps us keep all the connections modular.It is also used for interconnecting grounds across the circuits.Complete Frame" }, { "title": "Smart Agricultural Seeding Robot", "url": "/posts/agricultural-bot/", "categories": "Projects, Robotics", "tags": "arduino, electronics", "date": "2022-08-11 12:00:00 -0400", "snippet": "ElectronicsArduino Code#include &lt;Servo.h&gt;Servo myservo;int botrightpin = 5;int botleftpin = 6;int toprightpin = 9;int topleftpin = 10;int servopin = 11;int val=0;int buzzPin = 3;void setup(){ pinMode(botleftpin, OUTPUT); pinMode(botrightpin, OUTPUT); pinMode(topleftpin, OUTPUT); pinMode(toprightpin, OUTPUT); pinMode(buzzPin, OUTPUT); myservo.attach(servopin); Serial.begin(9600);}void loop(){ digitalWrite(botleftpin, HIGH); digitalWrite(botrightpin, HIGH); digitalWrite(topleftpin, HIGH); digitalWrite(toprightpin, HIGH); delay(7000); digitalWrite(botleftpin, LOW); digitalWrite(botrightpin, LOW); digitalWrite(topleftpin, LOW); digitalWrite(toprightpin, LOW); myservo.write(90); digitalWrite(buzzPin, HIGH); delay(200); myservo.write(0); digitalWrite(buzzPin, LOW); delay(2000);}FunnelChassis and Bot" }, { "title": "Recognizing Traffic Signs using CNNs", "url": "/posts/traffic-signs-recognition/", "categories": "Projects, ML", "tags": "ml, dl, cnn, object detection, computer vision", "date": "2022-08-11 12:00:00 -0400", "snippet": "IntroductionThe following project shows the implementation of a simple convolutional neural network (CNN). The model will be able to identify which signal it is when presented with a colour image of a traffic sign. Being my first project in deep learning, I gained extensive knowledge about how the dataset is composed, how to pre-process images, which deep network to use, and how to efficiently choose the number of layers and units.DatasetThe dataset used for this project can be obtained fromPublic Archive: daaeac0d7ce1152aea9b61d9f1e19370Data PreprocessingThe entire dataset contains images of different sizes. Since the very first operation of the model involves reading and standardizing the images, it is important to resize all the images to a predefined size, 32x32 in this case. The colorspace of the images is also converted from RGB to grayscale.Another important data preprocessing step is one-hot encoding. One-hot encoding refers to the process of converting categorical data variables to a numerical form, and this is done with a 43-dimensional array.Image Resizingimport numpy as npimport matplotlib.pyplot as pltfrom skimage.color import rgb2labfrom skimage.transform import resizeimport globfrom collections import namedtuple%matplotlib inlineN_CLASSES = 43RESIZED_IMAGE = (32,32)Dataset = namedtuple('Dataset',['X','y'])def read_dataset_ppm(rootpath, n_labels, resize_to): images = [] labels = [] for c in range(n_labels): full_path = rootpath + '/' + format(c,'05d') + '/' for img_name in glob.glob(full_path + '*.ppm'): img = plt.imread(img_name).astype(np.float32) img = rgb2lab(img/255.0)[:,:,0] img = resize(img, resize_to, mode='reflect').astype(np.float32) label = np.zeros(shape=(n_labels,), dtype=np.float32) label[c] = 1.0 labels.append(label) images.append(img) return Dataset(X = img_stack(/assets/images/L3D/a3/images), y = np.array(labels))def img_stack(imgs): return np.stack([img[:,:,np.newaxis] for img in imgs], axis=0).astype(np.float32)dataset = read_dataset_ppm('GTSRB_Final_Training_Images/GTSRB/Final_Training/Images', N_CLASSES, RESIZED_IMAGE)print(dataset.X.shape)print(dataset.y.shape)Train test splitfrom sklearn.model_selection import train_test_splitidx_train, idx_test = train_test_split(range(dataset.X.shape[0]), test_size=0.25)X_train = dataset.X[idx_train,:,:]X_test = dataset.X[idx_test,:,:]y_train = dataset.y[idx_train,:]y_test = dataset.y[idx_test,:]Creating minibatches of dataEvery training iteration would require the addition of a minibatch of randomly chosen samples taken from the practise set. Different minibatches of data in every generator will compel the model to learn the in-out connection rather than memorizing the sequence.n_samples = X_train.shape[0]def minibatcher(X, y, batch_size): i = np.random.permutation(n_samples) for j in range(int(np.ceil(n_samples/batch_size))): fr = j * batch_size to = (j+1) * batch_size yield X[i[fr:to],:,:,:], y[i[fr:to],:]Layersimport tensorflow.compat.v1 as tftf.disable_v2_behavior()def fc_no_activation(in_tensors, n_units): w = tf.get_variable(name=\"fc_W\", shape=[in_tensors.get_shape()[1], n_units], dtype=tf.float32, initializer=tf.keras.initializers.glorot_normal()) b = tf.get_variable(name=\"fc_B\", shape=[n_units,], dtype=tf.float32, initializer=tf.constant_initializer(0.0)) return tf.matmul(in_tensors,w) + b# Fully connected layerdef fc_layer(in_tensors, n_units): return tf.nn.leaky_relu(fc_no_activation(in_tensors, n_units))# COnvolution layerdef conv_layer(in_tensors, kernel_size, n_units): w = tf.get_variable(name=\"conv_W\", shape=[kernel_size, kernel_size, in_tensors.get_shape()[3], n_units], dtype=tf.float32, initializer=tf.keras.initializers.glorot_normal()) b = tf.get_variable(name=\"conv_B\", shape=[n_units,], dtype=tf.float32, initializer=tf.constant_initializer(0.0)) return tf.nn.leaky_relu(tf.nn.conv2d(input=in_tensors, filters=w, strides=[1,1,1,1], padding='SAME') + b)# Maxpool layerdef maxpool_layer(in_tensors, sampling): return tf.nn.max_pool(in_tensors, [1, sampling, sampling, 1], [1, sampling, sampling, 1], 'SAME')# Dropout layerdef dropout(in_tensors, keep_proba, is_training): return tf.cond(is_training, lambda: tf.nn.dropout(in_tensors, keep_proba), lambda: in_tensors)StructureThe model will be composed of the following layers: 2D convolution, 5x5, 32 filters 2D convolution, 5x5, 64 filters Flattenizer Fully connected later, 1,024 units Dropout 40% Fully connected layer, no activation Softmax outputdef model(in_tensors, is_training): # First layer: 5x5 2d-conv, 32 filters, 2x maxpool, 20% drouput with tf.variable_scope('l1', reuse=tf.AUTO_REUSE): l1_conv = conv_layer(in_tensors, 5, 32) l1_maxpool = maxpool_layer(l1_conv, 2) l1_drop = dropout(l1_maxpool, 0.8, is_training) # Second layer: 5x5 2d-conv, 64 filters, 2x maxpool, 20% drouput with tf.variable_scope('l2', reuse=tf.AUTO_REUSE): l2_conv = conv_layer(l1_drop, 5, 64) l2_maxpool = maxpool_layer(l2_conv, 2) l2_drop = dropout(l2_maxpool, 0.8, is_training) with tf.variable_scope('flatten', reuse=tf.AUTO_REUSE): l2_flatten = tf.layers.flatten(l2_drop) # Fully collected layer, 1024 neurons, 40% dropout with tf.variable_scope('l3', reuse=tf.AUTO_REUSE): l3 = fc_layer(l2_flatten, 1024) l3_drop = dropout(l3, 0.6, is_training) # OUTPUT with tf.variable_scope('output', reuse=tf.AUTO_REUSE): output_tensors = fc_no_activation(l3_drop, N_CLASSES) return output_tensorsTrainingfrom sklearn.metrics import classification_report, confusion_matrixdef train_model(X_train, y_train, X_test, y_test, learning_rate, max_epochs, batch_size): in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1)) in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES)) is_training = tf.placeholder(tf.bool) logits = model(in_X_tensors_batch, is_training) out_y_pred = tf.nn.softmax(logits) loss_score = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=in_y_tensors_batch) loss = tf.reduce_mean(loss_score) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) with tf.compat.v1.Session() as session: session.run(tf.global_variables_initializer()) for epoch in range(max_epochs): print(\"Epoch=\", epoch) tf_score = [] for mb in minibatcher(X_train, y_train, batch_size): tf_output = session.run([optimizer, loss], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True}) tf_score.append(tf_output[1]) print(\"train_loss_score=\",np.mean(tf_score)) print(\"TEST SET PERFORMANCE\") y_test_pred, test_loss = session.run([out_y_pred, loss], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False}) print(\" test_loss_score=\", test_loss) # CLASSIFICATION REPORT y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32) y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32) print(classification_report(y_test_true_classified, y_test_pred_classified)) \t\t# CONFUSION MATRIX\t\tcm = confusion_matrix(y_test_true_classified, y_test_pred_classified) plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) plt.colorbar() plt.tight_layout() plt.show() # And the log2 version, to enphasize the misclassifications plt.imshow(np.log2(cm + 1), interpolation='nearest', cmap=plt.get_cmap(\"tab20\")) plt.colorbar() plt.tight_layout() plt.show()tf.reset_default_graph()train_model(X_train, y_train, X_test, y_test, 0.001, 10, 256)ResultsAccuracy = 98%Performance ReportEpoch= 0train_loss_score= 4.8711586Epoch= 1train_loss_score= 0.83951813Epoch= 2train_loss_score= 0.3692537Epoch= 3train_loss_score= 0.2198893Epoch= 4train_loss_score= 0.1685863Epoch= 5train_loss_score= 0.11884567Epoch= 6train_loss_score= 0.10304754Epoch= 7train_loss_score= 0.07655481Epoch= 8train_loss_score= 0.072380714Epoch= 9train_loss_score= 0.059970096TEST SET PERFORMANCE test_loss_score= 0.0674421 precision recall f1-score support 0 1.00 1.00 1.00 45 1 0.99 0.98 0.98 545 2 0.92 1.00 0.96 561 3 0.99 0.98 0.99 357 4 0.98 0.98 0.98 484 5 0.99 0.94 0.97 472 6 1.00 1.00 1.00 104 7 1.00 0.97 0.98 347 8 1.00 0.97 0.98 372 9 0.99 0.99 0.99 370 10 0.99 1.00 1.00 500 11 1.00 0.97 0.99 352 12 0.99 1.00 0.99 538 13 1.00 0.99 1.00 549 14 0.99 1.00 0.99 193 15 0.99 0.99 0.99 166 16 0.99 0.99 0.99 96 17 1.00 0.99 1.00 293 18 0.97 1.00 0.99 276 19 1.00 0.98 0.99 44 20 0.95 0.99 0.97 93 21 0.96 0.99 0.97 77 22 0.99 1.00 1.00 119 23 0.97 0.98 0.98 127 24 0.96 0.99 0.97 72 25 0.98 0.98 0.98 396 26 0.97 0.97 0.97 155 27 1.00 1.00 1.00 68 28 0.96 0.99 0.98 133 29 0.98 0.89 0.93 64 30 1.00 0.91 0.95 113 31 1.00 0.98 0.99 195 32 1.00 1.00 1.00 53 33 1.00 0.96 0.98 170 34 1.00 0.98 0.99 98 35 1.00 1.00 1.00 263 36 0.99 0.98 0.98 98 37 1.00 1.00 1.00 48 38 0.99 1.00 0.99 519 39 1.00 1.00 1.00 64 40 0.95 0.99 0.97 89 41 1.00 0.98 0.99 61 42 1.00 0.97 0.98 64 accuracy 0.98 9803 macro avg 0.99 0.98 0.98 9803weighted avg 0.99 0.98 0.98 9803Confusion Matrix" }, { "title": "Persistence of Vision Wand", "url": "/posts/pov-wand/", "categories": "Projects, Electronics", "tags": "arduino, electronics, led", "date": "2022-08-11 12:00:00 -0400", "snippet": "IntroductionPOV (Persistence of Vision) is a kind of optical illusion in which a visual image seems to persist even when the light from it ceases to enter our eyes. This can be used to make POV displays where we can display text, images, gifs, etc.Hardware Components Quantity Arduino Uno 1 LED (Red) 19 Resistors 220 ohm 19 Perf board 1 Header pins 20 Toggle switch 1 Tools: Soldering ironConstruction Take a perf board and cut it to the required size for 20 LEDs. Solder the LEDs in a single line with all Anode on the same side. Attach 220 resistors beside each LED and solder them. Add toggle switch to turn on the display whenever the the switch is turned on. Solder wires to anode of each LED and common Gnd. Pin connections:Top of wand1¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 132¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 123¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 114¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 105¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 96¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 87¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 78¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 69¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 510¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 411¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 312¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 213¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 114¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 015¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 516¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 417¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 318¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 219¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 1Bottom of wandSoftwareThe code for the same can be found below.Each letter of the alphabet is generated by a collection of binary arrays that are stored on the Arduino. The Arduino matches each letter to one of its stored letters as it reads the message you want to display, and then it produces the stored array column by column.Results" }, { "title": "Disaster Management System for Floods using IoT and ML", "url": "/posts/iot-disaster-management-system/", "categories": "Projects, ML", "tags": "ml, iot", "date": "2022-08-11 12:00:00 -0400", "snippet": "IntroductionFloods are the most catastrophic and cataclysmic events of all-natural disasters. The World Meteorological Organization has stated that out of all the disasters in the world, floods are the most severe. Specifically, in India, about 12% of the land is vulnerable to flood conditions. Heavy unprecedented rainfall results in floods bringing everyday life to a standstill. Most floods occur during monsoons; however, floods can also occur due to dams &amp; levees breaking, which can be triggered by thunderstorms, cyclones, and low-pressure regions.With substantial technological innovations in sensing systems, communication networks, cloud computing, machine learning, and data analytics, it is readily possible to develop an integrated flood disaster management system that can effectively alert the flood affecting regions. Internet of Things (IoT) is one such technology that can not only help predict the occurrence of a flood but can also help provide an emergency floor plan using an AI approach.This project uses an IoT framework with AI to develop flood monitoring, prediction, and post-flood management systems.Basic OutlineOccurrencesPredictionA model will be designed to monitor the environmental parameters used for flood disaster prediction. The environmental parameters like temperature, relative humidity, atmospheric pressure, rainfall, etc., are sensed by an array of sensors, and the measured data is sent to the microcontroller via a suitable communication standard. Further, the relationships between the input data received and the output rainfall are modeled using ANN (Artificial Neural Network) techniques. Continuous monitoring of changes in the environment will be done by updating the old values with new ones after a specified time interval. A flood event is thus predicted using the ANN model, which will alert people to upcoming disasters according to the increase of rainfall and corresponding rising water levels in low-lying areas near river flow.Emergency Floor PlanPeople routing in an emergency is an exciting and complex problem due to the several challenges that affect the result of the provided solution. We would have to consider minute-by-minute changes in the emergency environment, which would require dynamic generation and management of the evacuation routes. Some possible hazards might include structural damages and collapses that block the route or limit route capacity while evacuating, low visibility, too much water logging, etc. Taking into consideration all the possible challenges, clear objectives have to be decided for the same.CommunicationUse of Intermediate NodesESP32 nodes can be used for effective communication of the sensor information and traversing floor plan information in times of an emergency. The range of a typical ESP32 board is limited to around 240 meters but can be extended up to 10km using a directional antenna, with a 4-kilometer range of efficient communication. When implemented over an extensive area, intermediate nodes can be utilized to deliver the information to the destination nodes and ensure data syncing.DeploymentThe smart buoy being built can be deployed in multiple ways depending on the city and the area where it is being placed. The purpose is flood detection. In coastal areas where floods can be caused by the water in the sea i.e. tsunamis and cyclones , the smart buoy can be placed near the sea region , where it can predict the chances of a flood judging the sea levels. Smart buoys can be deployed using drones , in case the water enters the cities.These buoys will communicate the info from various areas and can be used to analyze the situation in the entire city at once.End goal : To achieve efficient flood managementRescue Teams/Police StationRescue teams and police stations can act faster in case floods are predicted and an overall information is presented to them. The smart buoy can aid in better application of emergency procedures and can decrease the loss caused due to these disasters by a large scale.News MediaThe media can alert the citizens in case a flood is approaching and warn the areas which are at a high risk. The timely updates can help in efficient evacuation.Internet and SMSThe Internet and SMS play a major role in any disaster management situation. Notifications regarding the routes to be taken during evacuation, roads blocked, structural damage in various areas can be made available to the public.Requests for help so that the rescue teams arrive can also be made , by providing an emergency number to which sms sent will alert the authorities about the problem there.Protocols and ArchitectureIEEE 802.11IEEE 802.11 is a set of LAN standards, and specifies a set of MAC and PHY protocols for implementing Wireless Local Area Networks (WLAN) communication. It supports wireless connectivity for portable, moving devices (Wi-Fi devices) within a local area. Compared to theIEEE 802.16 standard, IEEE 802.11 standard supports shorter distance and higher data rate for the Wi-Fi devices. The data rate supported by IEEE 802.11 ranges from 11 Mbps to more than 1 Gbps.802.11b, 802.11g, and 802.11n-2.4 utilize the 2.400‚Äì2.500 GHz spectrum, one of the ISM bands. 802.11a, 802.11n, and 802.11ac use the more heavily regulated 4.915‚Äì5.825 GHz band. Each spectrum is subdivided into channels with a center frequency and bandwidth.The 2.4 GHz band is divided into 14 channels spaced 5 MHz apart, beginning with channel 1, which is centered on 2.412 GHz. The latter channels have additional restrictions or are unavailable for use in some regulatory domains.IEEE 802.15.4Unlike the 802.11 based networks, the IEEE 802.15 family of standards support short-range, low-power, low-rate communications.The IEEE 802.15.4 standard is designed to support low-data rate wireless connectivity withfixed, portable, and mobile devices with very limited battery consumption and relaxed throughput requirements. The standard through its energy-efficient link-layer technologies sup-ports networking of the constrained IoT devices. It supports longer network lifetime with periodic sleep cycles, low-rate, and low-power communications. Hence, it is one of the most widely adopted link-layer technologies for building IP-based IoT networks.Standards like ZigBee, WirelessHart and ISA100.11a define higher-layers on top of the IEEE 802.15.4 standard.IEEE 802.15.4 specifies three frequency bands of operation: 868 MHz, 915 MHz and the 2.4GHz unlicensed industrial, scientific and medical (ISM) band.IEEE 802.11ahIEEE 802.11ah is a wireless networking standard that uses 900 MHz license-exempt bands to provide extended-range Wi-Fi networks, compared to conventional Wi-Fi networks operating in the 2.4 GHz and 5 GHz bands. It also benefits from lower energy consumption, allowing the creation of large groups of stations or sensors that cooperate to share signals, supporting the concept of the Internet of things (IoT). The protocol‚Äôs low power consumption competes with Bluetooth and has the added benefit of higher data rates and wider coverage range.The sensor network standards (such as ZigBee, RFID, or Bluetooth) work over relatively short distances (i.e., tens of meters), with low data rates and low energy consumptions. On the other hand, standards like GPRS, LTE, WiMAX, etc., work over long distances and provide high throughput; however, they consume more energy, and demand an expensive and fixed infrastructure of base stations with proper line of sight. Owing to its low power consumption, the IEEE 802.15.4 is a suitable standard for many IoT applications. However, it is not suited for facilitating communication among a large number of IoT devices or for covering large areas.The 802.11ah standard enables single-hop communication over distances up to 1000 m. Relay Access Points can be used to extend the connectivity as well. The 802.15.4 standard, with a maximum range of 100m, alone cannot provide a communication framework for a larger coverage range.The 802.15.4 standard usually operates in the unlicensed 2.4 GHz band which can accommodate data rates up to 250 Kbps. On the other hand, the 802.11ah utilizes the sub-1 GHz license-exempt bands to provide an extended range to Wi-Fi networks.You can refer to sections 2.3, 2.5, 3.2, 3.3, 3.4 from here for more information.DevicesUtilitySensorsReference - https://flood.network/ Smart Cameras - Intelligent image processing and pattern recognition algorithms for coastal zone management, forecasting tidal waves and detecting overtopping waves, detecting smoke and fire caused by a flood, etc. Smart Buoys - Can be used to detect water level, flow velocity, tidal waves, etc. The floating object can carry a GPS and an acceleration sensor. Any sudden rise or gradual change in water level can be instantaneously used to alert nearby people through the web. Water Level Sensors - Used to measure water level in real-time near dams, rivers, reservoirs, etc. Weather Sensors - They measure humidity, wind speed, amount of rainfall, air pressure and temperature, the data from which can be used to predict the occurrence of a flood. Navigation Platforms - Real-time navigation platforms used by drivers/common people can provide hazard reports, making it possible to categorize the hazard into subcategories, such as a small flood, etc.Creating the Mesh Network and TestingThe initial setup consists of 9 ESP8266 boards, each loaded with the code and path-planning algorithm.Code can be found on GithubThe operating voltage range on an ESP8266 board is 3V-3.6V. The board comes with a low-dropout (LDO) voltage regulator to keep the voltage steady at 3.3V. It can reliably supply up to 600mA of current and has an operating current of 80mA. Power to the NodeMCU can be supplied via its USB connector, which is also used to load code into the board. Alternatively, we can use a 5V power supply by connecting it to the Vin pin in order to power up the board and its peripherals.The NodeMCU will not be connected to any other external electronic components but will just act as nodes in the mesh.Once all the boards are powered up, they will connect to a mesh of the following specifications (as directed by the code): MESH_SSID ‚ÄúdisasterManagement‚Äù MESH_PASSWORD ‚Äúpassword‚Äù MESH_PORT 5555Power connections and Hardware SetupFree Space Path LossFree Space Path Loss (FSPL) is the loss in signal strength of an electromagnetic wave (WiFi Signal in the present case). Free-space path loss is proportional to the square of the distance between the transmitter and receiver, and also proportional to the square of the frequency of the signal.\\[\\begin{align*}FSPL(dB)=10\\log_{10}((\\frac{4{\\pi}df}{c})^2)\\\\=20\\log_{10}(\\frac{4{\\pi}df}{c})\\end{align*}\\\\\\begin{align*}=20\\log_{10}(d)+20\\log_{10}(f)+20\\log_{10}(\\frac{4{\\pi}}{c})\\\\=20\\log_{10}(d)+20\\log_{10}(f)-147.55\\end{align*}\\]d- distance between emitter and receiverf- frequency of WiFi Signal (lies between 2400-2500 Hz)c- speed of light" }, { "title": "ROS Theory", "url": "/posts/ros-theory/", "categories": "Blog, Robotics", "tags": "ros", "date": "2022-07-28 12:00:00 -0400", "snippet": "SynopsisROS is an open-source project that provides a framework to your robot. It has become an integral part of robots today, and has massively impacted the Robotics Arena. ROS is extremely fascinating to study, but is not easy or beginner friendly.ROS Theory is an attempt to document the basics of ROS. This documentation will take you through a number of examples to better understand the concepts, and is neatly demonstarted by code and pictures of my terminal.Table of contents Introduction Tools Topics Messages Services Actions TF2Introduction1.1 What is ROS?ROS stands for Robot Operating System. Although the name implies that it is an OS, it is not. Rather, it is a framework that helps integrate the various parts of a robot by setting up communication between hardware and software of your robot, and between different processes going on within the software.Let us consider the following example in order to understand it better. Suppose you are building a simple robot for a ‚Äòfetch an item‚Äô task, in which your robot needs to navigate a given environment, find the required item and bring it back to a specified location. The robot has various parts that perform different functions such as locomotion and navigation (by the wheels of the robot), computer vision (by the camera on the robot), etc. However, these parts cannot talk to each other directly, due to which the robot will not know when it should stop/resume a certain task assigned to it. This is where ROS comes in. ROS helps integrate these various parts, thereby facilitating easy communication. The different parts now receive messages from other parts, thereby performing their functions more efficiently.1.2 ROS GraphROS Graph is a convenient way of representing the various programs, messages and message streams of a ROS system as a graph. The various ROS programs, called nodes, communicate with each other by sending/receiving messages. The nodes in a ROS Graph are connected by ‚Äútopics‚Äù, which represents a stream of messages that nodes use to communicate with each other.1.3 roscoreroscore is a service that provides connection information to nodes so that they can find and transmit/receive messages with other nodes. Every node connects to roscore at startup to register details of the message streams it publishes and the streams to which it wishes to subscribe. When a new node appears, roscore provides it with the information that it needs to form a direct connection with other nodes publishing and subscribing to the same message topics.1.4 catkincatkin is the ROS build system, which is a set of tools that ROS uses to generate executable programs, scripts, libraries, etc. catkin comprises a set of CMake macros and custom Python scripts. Every catkin directory will contain two files - CMakeLists.txt and package.xml, that you need to make changes in order for things to work properly.1.5 WorkspacesA workspace is a set of files and directories that contain ROS code. You can have multiple workspaces but you can work on only one workspace at a time.A catkin workspace is a directory where you build, modify, and install catkin packages.Running catkin_make will create two new directories - devel and build, along with the previously existing src directory. Build is where catkin will store the results of some of its work such as libraries, etc. Devel contains a number of files and directories, including setup files. Running these setup files configures the system to use the workspace and run the necessary code. Hence it is necessary to source your workspace every time you open a new shell.1.6 ROS PackagesDocumentation - http://wiki.ros.org/ROS/Tutorials/CreatingPackagePackages are projects that contain your ROS work. A workspace can have several packages, and all of them are located inside the src directory.For a package to be considered a catkin package, it must meet the following requirements- Must contain package.xml file - provides meta information about the package Must contain CMakeLists.txt file - Describes how to build the code and where to install it Each package must have its own folder All packages must be created inside ~/catkin_ws/src1.6.1 Creating a catkin packageTo create a package the following syntax can be used:catkin_create_pkg &lt;package_name&gt; [depend1] [depend2] [depend3]On creating a new package, the CMakeLists.txt file, package.xml file and the src directory come built-in automatically.1.6.2 Building the catkin workspaceOnce a package has been created, it needs to be compiled in order for it to work. catkin_make - Will compile the entire src directory and needs to be issued only in the catkin_ws directory. catkin_make --only-pkg-with-deps &lt;package_name&gt; - Will only compile the selected package. To add your workspace to the ROS environment, you need to source the generated setup file.Once the package has been created, the Python nodes can be saved in the src directory of the package.1.6.3 Package dependenciesrospack depends1 beginner_tutorials returns a list of the first-order dependenciesrospack depends beginner_tutorials returns a list of all dependencies, direct and indirect.These dependencies are stored in the package.xml file.1.7 package.xmlDocumentation: http://wiki.ros.org/catkin/package.xmlThe package.xml file provides meta information about the package such as package name, version number, authors, etc.What does a package.xml file contain? &lt;name&gt;¬†- The name of the package &lt;version&gt; - The version number of the package (required to be 3 dot-separated integers) &lt;description&gt; - A description of the package contents &lt;maintainer&gt; - The name of the person(s) that is/are maintaining the package &lt;license&gt; - The software license(s) under which the code is released.1.7.1 DependenciesThese four types of dependencies are specified using the following respective tags: &lt;buildtool_depend&gt;: Build Tool Dependencies specify build system tools which this package needs to build itself. Typically the only build tool needed is catkin. &lt;build_depend&gt;: Build Dependencies specify which dependencies are needed to build this package. &lt;run_depend&gt;: Run Dependencies specify which dependencies are needed to run code in this package, or build libraries against this package. &lt;test_depend&gt;: Test Dependencies specify only additional dependencies for unit tests. They should never duplicate any dependencies already mentioned as build or run dependencies.ROS Tools2.1 rosbashDocumentation- http://wiki.ros.org/rosbashrosbash is a package that contains some useful bash functions and adds tab-completion to a large number of the basic ROS utilities. When you source your setup file, you will implicitly get all bash-specific commands.rosbash includes the following command-line utilities: roscd - change directory starting with package, stack, or location name rospd - pushd equivalent of roscd rosd - lists directories in the directory-stack rosls - list files of a ros package rosed - edit a file in a package roscp - copy a file from a package rosrun - run executables of a ros package rospd:rospd is the pushd equivalent of roscd, that allows you to navigate between different ros directories by keeping the multiple locations in a directory-stack, and allowing you to jump back to a ros directory that you were previously working on. rosed:rosed allows you to edit files in a ROS package by typing the package name and the name of the file that you want to edit. roscp:roscp allows you to copy a file from a ROS package by specifying the package name and the name of the file that you want to copy. 2.2 Common File system tools rospack: Documentation- https://docs.ros.org/en/independent/api/rospkg/html/rospack.html rospack is a command-line tool that is used to get information about ROS packages available on the filesystem. Below are listed some common rospack options- rospack find - returns the absolute path to the package rospack depends - returns a list of all the package‚Äôs dependencies, direct and indirect rospack depends1 - returns a list of the package‚Äôs primary dependencies rospack depends-on - returns a list of all the packages that depend on the given package rospack export - returns flags necessary for building and linking against a package rospack list - returns a list of all ROS packages on the filesystem roscd: roscd allows us to change directory or subdirectory using a package name, stack name, or special location. You can only move to the packages installed into your ROS system. roscd log will take you to the ROS directory that contains log files. If no ROS program has been run yet, it will yield an error saying that it does not exist. rosls: rosls allows you to view the contents of a package, stack or location. rosparam: 2.3 rosrunrosrun is a ROS command-line utility that searches for a package for the requested program and runs it.Syntax-rosrun &lt;package&gt; &lt;executable&gt;rosrun displays a sequence of timestamps, which in the above picture, prints the string ‚ÄòHello World‚Äô 10 times per second. This reduced frequency helps in recognizing any changes in the messages.2.4 roslaunchAlthough rosrun is great for starting single ROS nodes, most robot systems end up running tens or hundreds of nodes simultaneously. Since it would not be practical to call rosrun on each of these, ROS includes a tool for starting collections of nodes, called roslaunch.roslaunch is a command line tool that helps run several nodes at a time, instead of using rosrun for each individual node.Syntax-roslaunch PACKAGE LAUNCH_FILEroslaunch operates on launch files instead of nodes. Launch files are XML files that are a collection of nodes along with their topic remappings and other parameters. These files have the suffix .launch.roslaunch includes several other features such as the ability to launch programs on other computers in the network via ssh, to automatically respawn nodes that crash, etc.roslaunch will also automatically run roscore, if it doesn‚Äôt already exist. However, pressing Ctrl+C will exit roscore as well, along with roslaunch.2.4.1 Creating a launch file&lt;launch&gt; &lt;group ns=\"turtlesim1\"&gt; &lt;node pkg=\"turtlesim\" name=\"sim\" type=\"turtlesim_node\"/&gt; &lt;/group&gt; &lt;group ns=\"turtlesim2\"&gt; &lt;node pkg=\"turtlesim\" name=\"sim\" type=\"turtlesim_node\"/&gt; &lt;/group&gt; &lt;node pkg=\"turtlesim\" name=\"mimic\" type=\"mimic\"&gt; &lt;remap from=\"input\" to=\"turtlesim1/turtle1\"/&gt; &lt;remap from=\"output\" to=\"turtlesim2/turtle1\"/&gt; &lt;/node&gt;&lt;/launch&gt;The launch tag &lt;launch&gt; is used to identify the file as a launch file.Lines 3-9: starts two instances of the same nodeLines 11-14: start the mimic node with the topics input and output renamed to turtlesim1 and turtlesim2. This renaming will cause turtlesim2 to mimic turtlesim1.2.5 ROS NodesA node is a ROS program that performs a certain task. Differnent nodes communicate with each other by sending messages through topics.2.5.1 Launching a nodeSyntax-rosrun &lt;package_name&gt; &lt;node_name&gt; __name:=new_node_name __ns:=name_space topic:=new_topic2.5.2 Controlling 2 turtles using different keyboards NOTE - We cannot have 2 turtlesims in the same namespace. Take a look at the below image to see what happens if we initialize 2 turtlebots in the same namespace.2.5.3 Using rosnodeThe rosnode command is used to display information about ROS nodes that are currently running. rosout documentation - http://wiki.ros.org/rosoutROS TopicsROS Topics represent a stream of messages that connect two or more nodes. The nodes act as publishers or subscribers of messages - publisher nodes send messages through topics and subscriber nodes receive the messages transmitted through a particular topic. A publisher node would have to register the topic name and the type of messages, and only then it can publish to a topic. A subscriber node would make a request to roscore to get details about a topic in order to receive the transmitted messages sent through it. In ROS, all messages on the same topic must have the same data type.Any number of nodes can publish to a topic as long as they have the same message type. A ROS Topic cannot be published without initializing a ROS Node.3.1 Using rostopicThe rostopic command is used to display information about ROS topics that are currently running.rostopic hz reports the rate at which data is being published. The below image tells us that turtlesim is publishing data about our turtle at the rate of 60 Hz.ROS Messages4.1 ROS msgThe message definition file has an extension of .msg and all such files need to be located inside the msg directory of a package.4.2 Creating a ROS msgNow we need to make sure that the msg files are turned into source code for Python, C++, and other languages.Open package.xml and make sure the following lines are uncommented.&lt;build_depend&gt;message_generation&lt;/build_depend&gt;&lt;build_export_depend&gt;message_runtime&lt;/build_export_depend&gt;&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;Now we need to make changes to the CMakeLists.txt file.# Modify the existing textfind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation)catkin_package( ... CATKIN_DEPENDS message_runtime ... ...)add_message_files( FILES Message1.msg Message2.msg)generate_messages( DEPENDENCIES std_msgs)Now since we have made a few changes to our package and created new files/directories, we need to compile out workspace by running catkin_make or catkin_make --only-pkg-with-deps &lt;package_name&gt;.4.3 Using rosmsgDocumentation - http://wiki.ros.org/rosmsg4.4 Publishing to a Topic#! /usr/bin/python2import rospyfrom std_msgs.msg import Stringimport timerospy.init_node('Node_name',anonymous=True)pub = rospy.Publisher('/orchis_dark', String, latch=True, queue_size=10)r = rospy.Rate(10)while True:\tpub.publish('Current time: {}'.format(time.time()))\tr.sleep()4.4.1 Code explanation:Line 1: #! /usr/bin/python2 is known as the shebang. It lets the kernel know that this is a Python file and that it should be passed into the Python interpreter.Line 3: import rospy appears in every ROS node and imports some basic functionalities, classes and functions.Line 4: this line allows us to reuse the std_msgs/String message type for publishing.Line 8: This is used to initialize a ROS node and the node takes the name that you give it. anonymous = True ensures that you have a unique name by adding random numbers to the end of name.Line 9: pub here is an object of the class Publisher, and allows us to publish to any topic of any message type that you give it. queue_size limits the amount of queued messages if any subscriber is not receiving them fast enough.Line 11: This line creates an object r of the class Rate. When r.sleep() is called, it sleeps just long enough for the loop to run at the desired rate. When the argument 10 is passed, it goes through the loop 10 times per second.Line 14: This is used to publish the desired message to the topic.rostopic hz tells us the rate at which messages are being published. Here we see that it is being published at a rate specified by the argument in rospy.Rate(arg).4.5 Subscribing to a Topic#! /usr/bin/python2import rospyfrom std_msgs.msg import Stringrospy.init_node('simple_subscriber', anonymous=True)def function(my_string):\tprint(my_string.data)\trospy.Subscriber('/force', String, function)rospy.spin()4.5.1 Code explanation:Line 1: #! /usr/bin/python2 is known as the shebang. It lets the kernel know that this is a Python file and that it should be passed into the Python interpreter.Line 3: import rospy appears in every ROS node and imports some basic functionalities, classes and functions.Line 4: this line allows us to reuse the std_msgs/String message type for publishing.Line 6: This is used to initialize a ROS node and the node takes the name that you give it. anonymous = True ensures that you have a unique name by adding random numbers to the end of name.Line 8-9: This is a callback function. Once a node has subscribed to a topic, everytime a message arrives on it, the associated callback function is called with the message as it‚Äôs parameter.Line 11: In this line, we subscribe to the topic, giving it the name of the topic, message type and callback function as its arguments.Line 13: This basically instructs ROS to loop over again, once a subscription has been made.Behind the scenes, the subscriber passes this information on to roscore and tries to make a direct connection with the publishers of this topic. If the topic does not exist, or if the type is wrong, there are no error messages: the node will simply wait until messages start being published on the topic. A subscriber callback function is not executed continuously, that is, it is not processing all the time. It will only process when new data is published.On VS Code - Output:ROS Services5.1 What are Services?Services are synchronous calls, which when called by one node executes a function in another node. They are used only when a function/task needs to be executed occassionally, say when a robot needs to perform a very discrete task such as taking a high resolution picture using a camera, etc. Synchronous refers to an intereference with time, and means the that functions are performed one after the other. Messages, on the other hand, are asynchronous which means that they branch out into dfferent functions that execute simultaneously.A Server provides a service by responding to a service call, and a Client requests for a service and accesses the service response. What is the difference between Services and Messges? ROS messages, which are transported using publishers and subscribers, are used whenever we need data to flow constantly and when we want to act on this data asynchronously.ROS services, on the other hand, are used only when we require data at specific time or when we want a task to be executed only at particular instances. To better understand this, take a look at the following example. We have a robot that simulates it‚Äôs environment in real-time. In such cases, we would use publishers/subscribers to send messages as that data flow needs to be constant. Also, we would want the robot to do other tasks as well, apart from just reading real-time data. If we use services, then the server/client would have to wait for a response/request and blocks the other code in the node, preventing other tasks from being executed, We have a robot that detects people in front of it. We would use services here as the node will wait for a person to come in front of it, then sends a request to the server and blocks the code while waiting for a response. Using messages here is pointless as we don‚Äôt want to continuously check for people in front of the robot (it‚Äôs just a one-time task). Reference: https://stackoverflow.com/questions/29458467/ros-service-and-message#:~:text=It%20is%20a%20similar%20example,the%20rest%20of%20its%20job.5.2 Service filesService files have an input and output call, and have the extension .srv. These files are present in the srv directory of a package. Here‚Äôs how a typical service file is definedint 64 aint 64 b---int64 sumIn order for the service files to run, a few changes have to be made in the package.xml file and CMakeLists.txt file.In package.xml# The following lines need to be added at the end of the file &lt;build_depend&gt;message_generation&lt;/build_depend&gt; &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;In CMakeLists.txtModify the file to make the following changes, if not done earlier# Modify the existing linefind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation) message_generation was added in both the files and is done while creating messages as well. It works for both msg and srv.add_service_files( FILES Service1.srv Service2.srv)After the necessary changes are made, we need to run catkin_make or catkin_make --only_pkg_with_deps &lt;package_name&gt; to compile the workspace or a particular package.Running catkin_make will create 2 additional classes for the service file as well. For example, if a service file is named add_two_int.srv, then the two classes add_two_intResponse and add_two_intRequest are created along with the previously existing add_two_int class. These classes allow us to interact with the service by calling for a Response or a Request.5.3 Using rossrvDocumentation - http://wiki.ros.org/ROS/Tutorials/CreatingMsgAndSrv5.4 Using rosservice Difference between rossrv and rosservice rossrv is a tool that provides us with information about all files ending in .srv rosservice is a tool that allows us to interact with Servers and Clients that are currently active. Reference - https://answers.ros.org/question/349148/rossrv-vs-rosservice/5.5 Creating a simple ServerBefore we get into creating a server, let us take a look at out service definition file.The same info can be gathered using the rossrv command, as shown below.Let us now create a server file, which would act as a python node for providing the service.Here‚Äôs the code for creating a simple server.Code:#! /usr/bin/python2import rospyfrom beginner_tutorials.srv import add_two_int,add_two_intResponseresponse = add_two_intResponse()def add_ints(req): print('Adding {} + {} '.format(req.a,req.b)) response.sum = req.a + req.b return response.sumrospy.init_node('add_two_int_server')srv = rospy.Service('add_two_int',add_two_int,add_ints)print('Server is ready')rospy.spin()5.5.1 Code ExplanationWe import the add_two_intResponse class as we are writing code for a Server, which provides a response for the service that is called.response is an object of the class add_two_intResponse().srv = rospy.Service('add_two_int',add_two_int,add_ints) This line creates an object named srv and declares/starts a service. The new service is given the name ‚Äòadd_two_int‚Äô and has the service type add_two_int. add_ints is a callback function that performs a desired task.The function add_ints takes a variable as a request and returns the sum of ‚Äòa‚Äô and ‚Äòb‚Äô.Execution:5.6 Creating a simple ClientLike all python nodes, the client service file should also be created under the src directory of a package.Code:#! /usr/bin/python2import rospyfrom beginner_tutorials.srv import add_two_int, add_two_intRequestreq = add_two_intRequest()req.a = 4req.b = 5rospy.wait_for_service('add_two_int')add_two_ints = rospy.ServiceProxy('add_two_int',add_two_int)response = add_two_ints(req)print(response.sum)5.6.1 Code ExplanationThe class add_two_intRequest is imported in a client node as we need to send a Request to the Server.req = add_two_intRequest() : req is an object of the class add_two_intRequest(). The inputs for the object are given in the next two lines, which are to be sent to the server.rospy.wait_for_service('add_two_int'): This is a method that blocks until the service named ‚Äòadd_two_int‚Äô is available.add_two_ints = rospy.ServiceProxy('add_two_int',add_two_int): This is how we call a service of the service name add_two_int and service type add_two_int. What is ServiceProxy? Before we get into that, what is a Proxy ?A proxy is a gateway between a client and a server (usually a website) that takes a request and performs a function. In a similar way, a ServiceProxy acts as an intermediary between a Client and a Server. It takes in the request from a Client and sends it to the Sever where it is passed into a function.response = add_two_ints(req): The request is passed into the service call line, and is sent to the server which returns the desired output.print(response.sum): The sum is then printed on the screen. NOTE - The server node needs to be running while a client node is being executed.Execution:Refer to the server code above to better understand how the service is being provided and why it returns the above outputs.5.7 A deeper dive into ServicesLet us take a look at a few other interesting things that we can do using services.5.7.1 Spawning 2 turtles in the same nodeHere‚Äôs a list of the services associated with turtlesim.The one which we are particularly interested in, to spawn 2 turtles in the same node, is the service named ‚Äò/spawn‚Äô. Here‚Äôs some info about the service ‚Äò/spawn‚Äô.In order to spawn two turtles, we call a service and pass in the necessary parameters, which is demonstrated below5.7.2 Getting laser scan data using gazeboSpawn your turtlebot3 on gazebo by running the following commandroslaunch turtlebot3_gazebo turtlebot3_empty_world.launchPlace a block in front of your turtlebot3, as shown belowLet us take a look at the various topics associated with gazeboHere, we find a topic named /scan which is pretty interesting. To learn more about this topic, let us do a rostopic info, as shown below.Let us now take a look at what this message sensors_msgs/LaserScan has to offer.Thus, this message provides us with several things related to the sensors output, and more importantly, gives us an array of all ranges of the laser scan. This is something that can be incredibly useful. However, subscribing to this would provide us with a list of 360 values (for 360 degrees), which is kinda unnecessary. Using services, we can have a synchronous callback function that provides us with data only for discrete inputs, such as giving us the range for a particular angle.Let us now call for a service, by creating a server and a client. Before we do this, we would obviously need a service definition file present in the srv directory.Create a file named gazebo_server.srv under the srv directory, and have the following lines.int64 direction---float32 distanceint64 direction would serve as an input while float32 distance serves as the output.We need to make the necessary changes to package.xml file and CMakeList.txt file. This has been discussed above.Creating a ServerCode#! /usr/bin/python2import rospyfrom sensor_msgs.msg import LaserScanfrom beginner_tutorials.srv import gazebo_server, gazebo_serverResponserospy.init_node('My_Node', anonymous=True)ls_obj = LaserScan()response = gazebo_serverResponse()def callback(val): global ls_obj ls_obj = val def func_serv(request): global ls_obj response.distance = ls_obj.ranges[request.direction] return response.distancesub = rospy.Subscriber('/scan', LaserScan, callback)serv = rospy.Service('my_service', gazebo_server, func_serv)rospy.spin()Explanationls_obj = LaserScan(), response = gazebo_serverResponse() : Objects are created for the respective classessub = rospy.Subscriber('/scan', LaserScan, callback) : Subscribing to the topic /scan of the message type LaserScan, and the data from this is passed into the function named callback which stores the data in the object ls_obj.serv = rospy.Service('my_service', gazebo_server, func_serv) : A service is started with the name my_service, service type gazebo_server, and a callback function named func_serv which returns the desired output based on the request.ExecutionCreating a ClientCode#! /usr/bin/python2import rospyfrom sensor_msgs.msg import LaserScanfrom beginner_tutorials.srv import gazebo_server, gazebo_serverRequestrospy.init_node('client_node')rospy.wait_for_service('my_service')req = gazebo_serverRequest()req.direction = 1srv = rospy.ServiceProxy('my_service', gazebo_server)r = rospy.Rate(5)while not rospy.is_shutdown(): result = srv(req) print(result.distance) r.sleep()Explanationrospy.wait_for_service('my_service') : Waits for the service to start, which in our case is initialized by the server.req = gazebo_serverRequest() : An object is created and the desired request is passedsrv = rospy.ServiceProxy('my_service', gazebo_server) : Calling for a service of the service name my_service and type gazebo_serverresult = srv(req) : The request is passed into the proxy, which is sent to the server and executes the desired function.print(result.distance) : The distance is printed on the screen. NOTE - The server node needs to be running while a client node is being executed.ExecutionROS Actions6.1 IntroductionActions are asynchronous calls in ROS, which means that you do not have to wait for a particular task to be finished and you can also do other tasks simultaneously.Reference: http://wiki.ros.org/actionlib What is the difference between Actions and Services? Actions are asynchronous calls which would perform almost the same functions as Services (kinda), but additionally allows you to work on other tasks in the node (i.e. no blocking code). It also allows you to get feedback, status and even cancel a call. You can better understand this with an analogy, as explained in this link. In some cases, if a service takes a long time to execute, the user might want the ability to cancel the request during execution or get periodic feedback about how the request is progressing. The actionlib package provides tools to create servers that execute long-running goals that can be preempted. It also provides a client interface in order to send requests to the server.A more detaied description: http://wiki.ros.org/actionlib/DetailedDescriptionThe ActionClient and ActionServer communicate via a ‚ÄúROS Action Protocol‚Äù, which is built on top of ROS messages. The client and server then provide a simple API for users to request goals (on the client side) or to execute goals (on the server side) via function calls and callbacks.There are 5 Topics provided by an Action Server: goal: User (or Client) sends a goal to the Server to initiate the action. cancel: User sends a signal under cancel topic to interrupt or stop an action from being executed. status: Tells the current status of the server. There are 10 status states - PENDING, ACTIVE, PREEMPTED, SUCCEEDED, ABORTED, REJECTED, PREEMPTING, RECALLING, RECALLED and LOST. result: Provides the final output after executing the action. feedback: Provides us with intermediate results about the action while it is being executed. NOTE - We cannot execute two actions at the same time. Doing so will cancel the previous action from being executed. If a new goal is sent to an action server that is already busy processing a goal, then the currently active goal will be pre-empted. However, this is not a hard limitation as we can have an action server that can process multiple goals.6.2 Action filesThe Action definition files have an extension of .action and is used to specify the format of goal, result, feedback message. These files are present in the action directory of a package. Here‚Äôs how a typical action file is definedint32 goal---int32 result---int32 feedbackIn order for the service files to run, a few changes have to be made in the package.xml file and CMakeLists.txt file.In package.xmlModify the file to make the following changes, if not done earlier&lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt; &lt;build_depend&gt;rospy&lt;/build_depend&gt; &lt;build_depend&gt;std_msgs&lt;/build_depend&gt; &lt;build_depend&gt;actionlib_msgs&lt;/build_depend&gt; &lt;build_depend&gt;actionlib&lt;/build_depend&gt; &lt;build_export_depend&gt;rospy&lt;/build_export_depend&gt; &lt;build_export_depend&gt;std_msgs&lt;/build_export_depend&gt; &lt;build_export_depend&gt;actionlib_msgs&lt;/build_export_depend&gt; &lt;build_export_depend&gt;actionlib&lt;/build_export_depend&gt; &lt;exec_depend&gt;rospy&lt;/exec_depend&gt; &lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;actionlib_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;actionlib&lt;/exec_depend&gt; &lt;build_depend&gt;message_generation&lt;/build_depend&gt; &lt;build_export_depend&gt;message_runtime&lt;/build_export_depend&gt; &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;In CMakeLists.txtModify the file to make the following changes, if not done earlier# Modify the existing linefind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation actionlib_msgs)# Generate actions in the 'action' folderadd_action_files( FILES my_action.action)## Generate added messages and services with any dependencies listed heregenerate_messages( DEPENDENCIES std_msgs actionlib_msgs)# Add actionlib_msgs as a catkin dependencycatkin_package( CATKIN_DEPENDS rospy std_msgs message_runtime actionlib_msgs)After the necessary changes are made, we need to run catkin_make or catkin_make --only_pkg_with_deps &lt;package_name&gt; to compile the workspace or a particular package.After compiling a package, ROS wil create additional messages for you. For example, if your action definition file is named robot.action, then the following messages will be created: robotAction.msg robotActionGoal.msg robotActionResult.msg robotActionFeedback.msg robotGoal.msg robotResult.msg robotFeedback.msgThese messages are used by actionlib to facilitate communication between ActionServer and ActionClient.The topics used in actions (mentioned above) will use one of these messages, whichever suits its function. In general, if you‚Äôre using the libraries in the actionlib package, you should not need to access the autogenerated messages with Action in their type name. The bare Goal, Result, and Feedback messages should suffice. The others are used internally by ROS, i.e., by the actionlib package. Let‚Äôs take a closer look at the counterAction message type.Once the goal is registered by the action server, the request is first given a time stamp and header information by actionlib indicating when and which action client requested this goal. Then the actionlib package also assigns a unique goal identifier to this goal along with a time stamp. Finally we have the goal message that we sent from our action client. Now the same goes with the action result.Inside the actionlib package, there is a state machine that is started to process the goal request that we sent. This state machine can lead to the requested goal being in different states and provides us with the status message.The same goes with the action feedback again, but now with a different state machine, and it also has the content of the feedback message.Let us create a custom action file and call it counter.action.# Goalint32 num_counts---# Resultstring result_message---# Feedbackint32 counts_elapsedCreating a simple ServerCode:#! /usr/bin/python3import rospy# import actionlib library used for calling actionsimport actionlib# import custom action file. Since actions are based on messages, notice how we import actions from the msg directory of a package.from beginner_tutorials.msg import counterAction, counterResult, counterFeedbackclass Server(): def __init__(self): # create a simple ActionServer by passing the name, action type, and callback function as its parameters. # auto_start has to be declared explicitly and always has to be set to False to prevent autostarting the server (can break your code otherwise). self.server = actionlib.SimpleActionServer('my_action_server', counterAction, self.counter, auto_start=False) # start server self.server.start() rospy.loginfo(\"Action server started\") def counter(self,goal): self.res = counterResult() self.feedback = counterFeedback() # initializing the feedback variable to 0 self.feedback.counts_elapsed = 0 # for 1s delay r = rospy.Rate(1) self.res.result_message = \"Counting complete!\" # start counting till the goal for i in range(0, goal.num_counts): success = True # check that preempt has not been requested by the user if self.server.is_preempt_requested(): rospy.loginfo(\"my_action_server: Preempted\") self.server.set_preempted() success = False break # publish the feedback self.feedback.counts_elapsed = i self.server.publish_feedback(self.feedback) # wait for 1s before incrementing the counter r.sleep() if success == True: # Once the necessary function is executed, the server notifies the client that the goal is complete by calling set_succeeded. self.server.set_succeeded(self.res)rospy.init_node(\"counter_action_server\")# initialize object called server to call the Server() class.server = Server()rospy.spin()Execution:rosrun &lt;package_name&gt; &lt;file_name&gt;Send goals via terminal:rostopic pub /action_server_name/goal /package_name/action_message_type parametersCreating a simple ClientCode:#! /usr/bin/python3import rospy# import actionlib libraryimport actionlib# import custom action filefrom beginner_tutorials.msg import counterAction, counterGoaldef Client(): # create client and specify the name of server and action message type client = actionlib.SimpleActionClient('my_action_server', counterAction) rospy.loginfo(\"Waiting for server\")\t# wait for server (name specified above) client.wait_for_server() goal = counterGoal() goal.num_counts = 20 \t# send goal to server client.send_goal(goal) # client.send_goal(goal, feedback_cb=feedback_func) rospy.loginfo(\"Goal has been sent to the action server\") # can perform other tasks here as well # wait for result\tclient.wait_for_result() return client.get_result()while not rospy.is_shutdown(): # initialize node rospy.init_node(\"counter_action_client\") r = rospy.Rate(1) \t# call the function and print result res = Client() print(res) r.sleep()Explanationclient.wait_for_result(): This will wait until the action is complete and blocks the remaining code. This won‚Äôt allow you to continue to work on your thread.client.get_state(): It returns an integer that specifies the state of the action. There are 10 possible states, a few of which are 0 implies PENDING 1 implies ACTIVE 2 implies PREEMPTED 3 implies SUCCEEDED 4 implies ABORTEDIf your get_state() is less than 2, it indicates that your action is still not complete.client.cancel_goal(): Used to cancel or preempt a goal.ExecutionTurtlesim exampleAs you can see below, running the following commands instructs turtlesim to move along a pentagon.On running rostopic list we see a list of all the current topics. /turtle_shape/goal is one such topic, which allows you to publish messages to the server through the terminal. It allows us to enter the number of edges and define the radius as well.GUIThe actionlib offers a graphical way to send goal to action server.Syntax-rosrun actionlib axclient.py /name_of_the_actionThe GUI has areas for Goal, Feedback and Result. You can press the SEND GOAL button to send goals with the relevant parameters, and you can also cancel or preempt a goal anytime with the CANCEL GOAL button. After the action finished successfully, the GUI shows the Result.ROS TF27.1 Introduction to TF2TF2 is the second generation of the transform library, which lets the user keep track of multiple coordinate frames over time. TF2 maintains the relationship between coordinate frames over time, and lets the user transform points, vectors, etc. between any two coordinate frames at any desired point in time.TF2 is not a centralized service, but is a distributed protocol with which many different robots will communicate about the state of the robot.In TF2, Publishers are known as Broadcasters, and Subscribers are known as Listeners. Listeners listen to /tf and cache all data heard up to the cache limit. Broadcasters publish transforms between coordinate frames on /tf.7.1.1 Applications of transformations between frames:The following are some of the applications where the tf2 library can be used: Compute inverse and forward kinematics of multi-joint robots Carry out obstacle avoidance Convey location of robot or parts of a robot Convert sensor data from one reference to another Control robot about a particular point in space Analyze multiple robot data in world frame Reference external objects w.r.t robot frame7.2 Demo using turtlesimEnter the following command to install the necessary dependencies and compile the demo package.sudo apt-get install ros-$ROS_DISTRO-turtle-tf2 ros-$ROS_DISTRO-tf2-tools ros-$ROS_DISTRO-tfAfter compiling the turtle_tf2 tutorial package, enter the following the run the demo:roslaunch turtle_tf2 turtle_tf2_demo.launchOnce the turtlesim is started you can drive the center turtle around in the turtlesim using the keyboard arrow keys. On moving the center turtle, you will see that the other turtle also moves continuously to follow the turtle that you are driving around.On entering the above command, you are essentially executing two main things: A TF2 broadcaster that is publishing the coordinate frames of both the turtles w.r.t the world frame. A TF2 listener that reads the transformations and uses it to calculate the direction in which turtle2 has to move to follow turtle1.7.2.1 TF2 toolsview_frames:view_frames is one of the TF2 tools that generates a diagram of the frames being broadcast by TF2 over ROS with the current TF2 tree. This diagram is stored as a pdf.Syntax to run view_frames:rosrun tf2_tools view_frames.pyThis is what you will see:Listening to tf data during 5 seconds...Generating graph in frames.pdf file...To view the tree diagram, we just need to open the pdf. If evince is your default document viewer, then run the command evince frames.pdfOn opening the pdf, we see a very simple tree diagram that depicts three frames - world frame, turtle1 frame, and turtle2 frame. The world frame is transformed to the turtle1 and to turtle2, seperately. We can see the Average rate, which is the publishing rate We can also see the most recent transform number, which should more or less coincide with the recording time (otherwise it is not publishing correctly)You can also listen to the TF being published, using the echo. There is a topic named /tf where ALL the TF are published. In simple systems like this one there is no problem, but as the system becomes more sophisticated, the quantity of data can be overwhelming. To tackle this, tf2 library provides you with tools that filters which tranformation you are interested in and just shows you that one.Running the following commands shows you the TF of the respective turtle only w.r.t the world frame.tf_echo:If you want to see the transform change as the two turtles move relative to each other, you can use tf_echo. tf_echo reports the transform between any two frames broadcast over ROS.Syntax:rosrun tf tf_echo [reference_frame] [target_frame]On running rosrun tf tf_echo turtle1 turtle2 you get the following information: Relative translation between the two turtles Relative rotation between the two turtles in terms of Quaternions, RPY (Roll Pitch Yaw)tf_monitor:7.2.2 Better visualization - rvizrviz is a visualization tool that is useful for examining tf2 frames.To open the turtle_tf2 file on rviz, enter the command: rosrun rviz rviz and open the relevant file. This is how your rviz environment will typically look like. Reference - https://youtu.be/Ra-nXIfPWdg7.3 Quaternion and Roll-Pitch-Yaw Conversion#! /usr/bin/python2import rospyimport tf2_rosimport tffrom geometry_msgs.msg import Twistfrom nav_msgs.msg import Odometryimport mathroll = math.radians(30)pitch = math.radians(50)yaw = math.radians(75)print('Roll: {}'.format(math.degrees(roll)))print('Pitch: {}'.format(math.degrees(pitch)))print('Yaw: {}'.format(math.degrees(yaw)))quaternion = tf.transformations.quaternion_from_euler(roll, pitch, yaw)print(\"\\nResulting quaternions:\")for i in range(0, 4): print(quaternion[i])ori = tf.transformations.euler_from_quaternion(quaternion)print('\\nEuler from quaternion:')for i in range(0, 3): print(math.degrees(ori[i]))\tExecution:7.4 TF2 Publisher/BroadcasterWe shall first create a new catkin package called learning_tf2 to better understand and demonstrate writing code for publishers and subscribers.Syntax to create the new package:catkin_create_pkg learning_tf2 tf2 tf2_ros roscpp rospy turtlesimCreate a new directory called nodes inside the learning_tf2 package. We will be storing all our python nodes here.To create a broadcaster, make a new file called tf2_broadcaster.py under the nodes directory.Code:#! /usr/bin/python2# import rospyimport rospy# import tf2 moduleimport tf2_ros# import tfimport tfimport geometry_msgs.msgimport turtlesim.msgdef turtle_func(msg, turtlename): # TransformBroadcaster makes publishing of transforms easy. To use the TransformBroadcaster, we need to import the tf2_ros module. br = tf2_ros.TransformBroadcaster() # We create a TransformStamped object which will contain the message to be published. t = geometry_msgs.msg.TransformStamped() # Before stuffing the actual transform values we need to give the TransformStamped object the appropriate metadata. t.header.stamp = rospy.Time.now() # We need to give the transform being published a timestamp, which in our case will be the current time t.header.frame_id = \"world\" # name of parent frame t.child_frame_id = turtlename # name of child frame t.transform.translation.x = msg.x t.transform.translation.y = msg.y t.transform.translation.z = 0.0 # convert angles from euler (radians/degrees) to quaternion q = tf.transformations.quaternion_from_euler(0, 0, msg.theta) t.transform.rotation.x = q[0] t.transform.rotation.y = q[1] t.transform.rotation.z = q[2] t.transform.rotation.w = q[3] br.sendTransform(t) # publish the transformif __name__=='__main__': rospy.init_node('tf2_broadcaster') # create node turtlename = rospy.get_param(\"~turtle\") rospy.Subscriber('%s/pose' % turtlename, turtlesim.msg.Pose, turtle_func, turtlename) rospy.spin()Execution:To run the broadcaster, we first need to create a launch file. Inside your package, create a folder called launch, create a file called start_demo.launch.&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt;&lt;/launch&gt;To execute, just run the command roslaunch learning_tf2 start_demo.launch.Rviz:7.5 TF2 Subscriber/Listener#! /usr/bin/python2# import rospyimport rospy# import tf2 moduleimport tf2_ros# import math moduleimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawn# initialize listener noderospy.init_node(\"tf2_listener\")# A listener object is created. Once the listener is created, it starts receiving tf2 transformations, and buffers them for up to 10 seconds.tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)# Spawn another turtle in the same turtlesim node (Refer to https://github.com/Bhaswanth-A/ROS-Theory/blob/main/Services.md#571-spawning-2-turtles-in-the-same-node )rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try: # Gets the transformation from source frame to target frame (change turtle1 to carrot1 for frames example) trans = tfBuffer.lookup_transform(turtle_name, 'turtle1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue # some math angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular # publish the pose pub.publish(cmd) r.sleep()Execution:Change the launch file start_demo.launch to this (only 1 line is added, nothing else is changed from before).&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt;&lt;/launch&gt;To execute, just run the command roslaunch learning_tf2 start_demo.launch.If you drive around turtle1 using your keyboard, you‚Äôll find the second turtle following the first one.Rviz:7.6 Adding framesCreate a new file called add_frame_1.py under the nodes directory.7.6.1 Example 1Broadcasterimport rospyimport tf2_rosfrom geometry_msgs.msg import TransformStampeddef frames(): br = tf2_ros.TransformBroadcaster() t = TransformStamped() t.header.stamp = rospy.Time.now() t.header.frame_id = \"turtle1\" t.child_frame_id = \"carrot1\" rate = rospy.Rate(10.0) while not rospy.is_shutdown(): t.header.stamp = rospy.Time.now() t.transform.translation.x = -2.0 t.transform.translation.y = 0.0 t.transform.translation.z = 0.0 t.transform.rotation.x = 0.0 t.transform.rotation.y = 0.0 t.transform.rotation.z = 0.0 t.transform.rotation.w = 1.0 br.sendTransform(t) rate.sleep()if __name__ == \"__main__\": rospy.init_node('frames') frames()We create a new transform, from the parent ‚Äúturtle1‚Äù to the new child ‚Äúcarrot1‚Äù. The carrot1 frame is 2 meters offset from the turtle1 frame.Listener#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawnrospy.init_node(\"tf2_listener\")tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try:\t\t# parent frame changed to carrot1 trans = tfBuffer.lookup_transform(turtle_name, 'carrot1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular pub.publish(cmd) r.sleep()Launch file&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt; &lt;node pkg=\"learning_tf2\" type=\"add_frame_1.py\" name=\"broadcaster_frames\" output=\"screen\" /&gt;&lt;/launch&gt;7.6.2 Example 2Broadcaster#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import TransformStampeddef frames(): br = tf2_ros.TransformBroadcaster() t = TransformStamped() t.header.stamp = rospy.Time.now() t.header.frame_id = \"turtle1\" t.child_frame_id = \"carrot1\" rate = rospy.Rate(10.0) while not rospy.is_shutdown(): x = rospy.Time.now().to_sec() * math.pi t.header.stamp = rospy.Time.now() t.transform.translation.x = 10 * math.sin(x) t.transform.translation.y = 10 * math.cos(x) t.transform.translation.z = 0.0 t.transform.rotation.x = 0.0 t.transform.rotation.y = 0.0 t.transform.rotation.z = 0.0 t.transform.rotation.w = 1.0 br.sendTransform(t) rate.sleep()if __name__ == \"__main__\": rospy.init_node('frames') frames()Instead of a fixed definition of our x and y offsets, we are using the sin and cos functions on the current time so that the offset of carrot1 is constantly changing.Listener#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawnrospy.init_node(\"tf2_listener\")tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try:\t\t# parent frame changed to carrot1 trans = tfBuffer.lookup_transform(turtle_name, 'carrot1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular pub.publish(cmd) r.sleep()Launch file&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt; &lt;node pkg=\"learning_tf2\" type=\"add_frame_1.py\" name=\"broadcaster_frames\" output=\"screen\" /&gt;&lt;/launch&gt;7.7 Time travelLet us see how we can get transformations from the past.7.7.1 Example 1Make the following changes in the listener file.while not rospy.is_shutdown(): try: past = rospy.Time.now() - rospy.Duration(5) trans = tfBuffer.lookup_transform(turtle_name,'turtle1',past) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continueThe above code asks for the pose of turtle1 5 seconds ago relative to turtle2 5 seconds ago. But this is not what we want. We want the pose of turtle1 5 seconds ago relative to the current pose of turtle2.7.7.2 Example 2while not rospy.is_shutdown(): try: past = rospy.Time.now() - rospy.Duration(5) trans = tfBuffer.lookup_transform_full(turtle_name, rospy.Time.now(), 'turtle1', past, 'world', rospy.Duration(1)) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continueThe above code computes the pose of turtle1 5 seconds ago relative to the current pose of turtle2." }, { "title": "ROS Navigation", "url": "/posts/ros-nav/", "categories": "Blog, Robotics", "tags": "ros, navigation", "date": "2022-07-28 12:00:00 -0400", "snippet": "ROS Navigation1.1 IntroductionThe ROS Navigation stack takes in information from odometry, sensor streams, and a goal pose and outputs safe velocity commands that are sent to a mobile base.The entire process of robot navigation can be divided into 3 major parts: Mapping Localization Path Planning1.2 MappingIn order to perform autonomous navigation, the robot must have a map of the environment. The robot will use this map for many things such as planning trajectories, avoiding obstacles, etc.Rviz is a very important tool that will be used extensively in the mapping process. For Mapping, you will basically need to use 2 displays of Rviz: LaserScan Display Map Display1.2.1 SLAMSimultaneous Localization and Mapping refers to building a map of an unknown environment while simultaneously keeping track of the robot‚Äôs location on the map that is being built.This scenario in Robotics is solved using the gmapping package on ROS.1.2.2 gmapping packageThe gmapping package implements a special SLAM algorithm called gmapping. We don‚Äôt need to know how to code the alogrithm ourselves but just need to learn how to configure the package for our robot to suit our needs.The gmapping package contains a ROS node called slam_gmapping that allows us to create a 2D map using the laser and odom data provided by the robot while navigating the given environment. This node basically reads data from the laser and the transforms of the robot, and turns it into an Occupancy Grid Map (OGM). Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch Launch the gmapping package with the robot (turtlebot3 in our case) by using a previously created launch file turtlebot3_gmapping.launch. To do so, run the command: roslaunch turtlebot3_slam turtlebot3_gmapping.launch This launch file starts the turtlebot3_slam_gmapping node from the gmapping package. The turtlebot3_slam_gmapping node subscribes to the Laser topic (/scan) and Transform topic(/tf) to get laser and odom data from the robot, which it needs to build the map. The generated map is published during the whole process into the /map topic, which reflects on Rviz. In order to visualize this map on Rviz, enter to following command: roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch Enter the following command to move the turtlebot3 and to produce a map of the complete environment. roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch The /map topic uses the message type nav_msgs/OccupancyGrid since it is an OGM. The occupancy is represented as an integer in the range {0,100}. 0 implies completely free 100 implies completely occupied -1 implies completely unknown.Rviz:On entering roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch, you will see something like this on the Rviz window:This however, does not have the map display and hence we cannot see the map of the environment produced by the robot. To get the map display, click on ‚ÄúAdd‚Äù and select ‚ÄúMap‚Äù.Now in the Map display, select the topic name /map to visualize the map being made in real-time by your robot.You can now navigate your robot the obtain a map of the complete environment.The finished map should look something like this.1.2.3 map_servermap_server is a package belonging to the ROS Navigation Stack.Saving a mapIt provides a map_saver node which allows us to access the map data from a ROS Service and save it into a file.When you request the map_saver to save the current map, the map data is saved into two files: YAML file - contains the image name and the metadata of the map Image_name.pgm - this contains the image itself, which has the encoded data of the OGM. pgm stands for Portable Gray MapTo save the built map, enter the following command: rosrun map_server map_saver -f &lt;map_name&gt;To open the map image, enter xdg-open &lt;map_name&gt;.pgmContents of YAML file: image: Name of the file containing the image of the generated map resolution: Resolution of the map (in meters/pixel) origin: Coordinates of the lower-left pixel in the map. The first two values indicate position and the third value indicates rotation. negate: Inverts the colors of the map. 0 by default occupied_thresh: Pixels which have a value greater than this value will be considered as completely occupied free_thresh: Pixels which have a value smaller than this value will be considered as completely freeProviding a mapThe map_server package also provides the map_server node that reads a map file and provides it to any other node that requests it via a ROS Service. This request is done, for example, by the move_base node to get data from a map and use it for Path Planning, or by the localization node in order to find the position of the robot in the map.The service to call to get the map is /static_map (nav_msgs/GetMap).Apart from requesting the map through the GetMap service, there are two topics that you can connect to in order to obtain ROS Messages about the map. /map_metadata (nav_msgs/MapMetaData): Provides map metadata /map (nav_msgs/OccupancyGrid): Provides the map occupancy dataEnter the following command to launch the map_server node in order to provide the map information:rosrun map_server map_server &lt;map_name&gt;.yaml The map that is created is a static map which means that the details of the map will not change or update with time. If the environment changes in the future, then these changes will not reflect in the map. The map that is created is a 2D map, which means that the obstacles that appear in the map don‚Äôt have any height. So these maps will be invalid in projects where you would want to navigate a drone or an underwater rover. For such cases, you would have to use 3D mappings.Creating a Service ClientThis Service Client will call the /static_map service in order to get the map data, and will print the dimensions and resolution of the map.#! /usr/bin/python2import rospyfrom nav_msgs.srv import GetMap, GetMapRequestrospy.init_node('call_map')rospy.wait_for_service('/static_map')map_data = GetMapRequest()srv = rospy.ServiceProxy('/static_map',GetMap)response = srv(map_data)print(response.map.info.resolution)print(response.map.info.width)print(response.map.info.height)rospy.spin()1.2.4 Creating a SLAM launch fileTill now we have used a previously created gmapping node to obtain a map of the environment. We shall now see how to create our own launch file. The main task to create this launch file is to correctly set the parameters for the turtlebot3_slam_gmapping node. This node is highly configurable and has lots of parameters that can be changed in order to improve the mapping performance. These parameters are read from the ROS Parameter Server, which can be set either in the launch file itself or in a separate YAML file.ParametersYou can refer to http://wiki.ros.org/gmapping for the list of various parameters available for the gmapping package.1.3 LocalizationIn the previous section, we have seen how to obtain a map of the given environment using the robot‚Äôs sensor data. When the robot uses this map to navigate, it is necessary for it to know its position withing the map, and its orientation as well. Determining the location and pose of a robot by using its sensor readings is known as Localization.1.3.1 Monte Carlo LocalizationMonte Carlo Localization (MCL) or particle filter localization is an algorithm used by robots to localize themselves in an environment. As the robot navigates the environment, the algorithm generates random guesses (called particles) about the next possible position of the robot. As the robot gathers more sensor data, the algo discards particles that don‚Äôt match with the readings and generates more particles closer to the probable sensor readings. So the more the robot moves, the more data we get from the sensors and the more precise is the localization. Read more https://en.wikipedia.org/wiki/Monte_Carlo_localization https://in.mathworks.com/help/nav/ug/monte-carlo-localization-algorithm.html1.3.2 amcl packageThe AMCL (Adaptive Monte Carlo Localization) package provides the amcl node which uses the MCL algorithm to track the localization of the robot in 2D space. The node subscribes to the laser data and transformations of the robot, and publishes the estimated position of the robot in the map. On startup, the amcl node initializes its particle filter according to the parameters provided. Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch We have previously saved the map of the gazebo-house environment in catkin_ws/src. In order to use a previously existing amcl node with turtlebot3 on our map, we need to copy the saved map into the directory in which the amcl node is located. Open the relevant launch file using the command code ~/catkin_ws/src/turtlebot3/turtlebot3_navigation/launch/turtlebot3_navigation.launch and change the map name in line 4 from map.yaml to my_map.yaml. Save the launch file, do catkin_make and source your workspace. Now to launch the amcl node with gazebo running, use the command roslaunch turtlebot3_navigation turtlebot3_navigation.launch Rviz:On entering the above commands, you will see something like this.For now, you can remove a few of the displays and keep only the ones shown in the image below.In order to visualize localization, we use the PoseArray display. Using the 2D Pose Estimate Tool, set an approximate initial position and orientation for the robot on Rviz (need not be accurate).On moving your robot around using your keyboard, you will notice that the particle cloud shrinks in size due to the scan data allowing amcl to refine its estimate of the robot‚Äôs pose.Topics: The initial pose set up using the 2D Pose Estimate tool on Rviz is published into the /initialpose topic. The amcl node read data published into the /scan topic (laser readings), /map topic and /tf topic, and published the estimate pose of the robot into the /amcl_pose and /particlecloud topics.1.3.3 ServicesThe amcl node provides the following services: /global_localization (std_srvs/Empty): amcl node provides this service wherein all particles are dispersed randomly throughout the free space in the map. /static_map (nav_msgs/GetMap): when called, this service is used to retreive map info for localizationWhen you call the /global_localization service, you see something like this on Rviz.Syntax: rosservice call /global_localization \"{}\"Creating a Service ServerCode for a service server that returns the position and orientation of the robot at the moment when the service is called.#! /usr/bin/python2import rospyfrom geometry_msgs.msg import PoseWithCovarianceStampedfrom std_srvs.srv import Empty, EmptyResponsebot_pose = PoseWithCovarianceStamped()def serv_func(req): print(\"Pose:\") print(bot_pose) return EmptyResponse()def sub_func(msg): global bot_pose bot_pose = msg.pose.poserospy.init_node('service_server')srv = rospy.Service('my_pose_service',Empty,serv_func)sub = rospy.Subscriber('/amcl_pose',PoseWithCovarianceStamped,sub_func)rospy.spin()Creating a Service ClientCode for a service client which performs a call to the /global_localization service in order to disperse the particles.#! /usr/bin/python2import rospyfrom std_srvs.srv import Empty, EmptyRequestrospy.init_node('client_global_localization')rospy.wait_for_service('global_localization')req = EmptyRequest()srv = rospy.ServiceProxy('global_localization',Empty)result = srv(req)rospy.spin()1.3.4 Creating a launch fileTill now we have used a previously created amcl node (slightly modified) to obtain a map of the environment. We shall now see how to create our own launch file. As mentioned before, the main task to create a launch file is to correctly set the parameters for the amcl node.The basic structure of the launch files will be the same as shown above. All you have to do is change the parameters to obtain different outputs.ParametersYou can refer to http://wiki.ros.org/amcl for the list of various parameters available for the amcl package.1.4 Path Planning1.4.1 move_base packageThe move_base package provides the move_base node, which is one of the most important elements of the ROS Navigation Stack that links all processes that take place in the navigation process. The main function of the move_base node is to move the robot from the current position to the target position. It is an implementation of a Simple Action server. Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch We now need to make a few small changes to a previously existing launch file. To do so, first go to the ROS directory turtlebot3_navigation, and open the launch file turtlebot3_navigation.launch present under the launch folder. After that, uncomment the following lines in the code Save the file, go back to your ROS Workspace, do a catkin_make and source your workspace. Now with gazebo running, launch the file containing the required move_base node using the command roslaunch turtlebot3_navigation turtlebot3_navigation.launch. Rviz:Using the 2D Nav Goal tool you can goals to your robot, instructing it to move to a particular location in the map.Topics:The goals are sent to the robot using the 2D Nav Tool on Rviz through the /move_base/goal topic.On doing a rostopic echo on the above mentioned topic, we can listen to the messages being published through it. Initially, when no messages are being published, we do not see anything. But if we send a target location to the robot using the 2D Nav Tool on Rviz, we get a message of the following typeWe can now send goals to the robot directly, without using Rviz, using the command rostopic pub /move_base/goal move_base_msgs/MoveBaseActionGoal &lt;parameters&gt;.Other relevant topics:When the move_base node receives a goal pose, it links other components such as the global planner, local planner, recovery behaviours, and costmaps, and generates an output which is a velocity command with the message type geometry_msgs/Twist, and sends it to the /cmd_vel topic in order to move the robot.1.4.2 ActionsAs mentioned before, the move_base node is an implementation of an Action Server. This makes it possible for us to send goals, receive feedback and results, and even cancel a goal if necessary. The action server takes a goal pose with the message type geometry_msgs/PoseStamped. This allows us to send goals to the server using a simple Action Client.The Action server provides the topic move_base/goal, which is the input of the navigation stack, and uses this topic to provide the goal pose.Action ClientEx 4.41.4.3 Creating a launch fileWe shall now see how to create our own move_base launch file. This node also has a lot of parameters associated with it that can be configured.ParametersYou can refer to http://wiki.ros.org/move_base for the list of various parameters available for the move_base package.1.4.4 The Global PlannerWhenever a new goal is received by the move_base node, the goal messages are also sent to the global planner. The lobal planner is responsible for calculating a safe path in order to arrive at the goal pose. This path is calculated before the robot starts moving, so it will not take into account the sensor data of the robot while moving.There are different types of global planners that different robots use to navigate an environment. Some of the important global planners are:NavfnThe Navfn planner is one of the most commonly used global planners in the ROS Navigation Stack. It uses the Djikstra‚Äôs algorithm to calculate the shortest path between the initial and target pose of the robot.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/navfnGlobal PlannerThe global planner is a more flexible replacement to the Navfn planner. It allows you to change the algorithm used for path planning (Djikstra‚Äôs in case of Navfn). These options include support for A*, toggling quadratic approximation, and toggling grid path.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/global_plannerCarrot PlannerThe Carrot planner takes the goal pose and checks if this goal is an obstacle. If it is an obstacle, it walks back along the vector between the goal and the robot until a goal point where the obstacle is not found. This planner is useful if you require your robot to move close to the given goal, even if the goal is unreachable.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/carrot_planner1.4.5 The Local PlannerOnce the global planner has calculated the path to follow, this path is sent to the local planner. The local planner will then execute each segment of the global plan. With the help of the plan provided by the global planner and a map, the local planner will send velocity commands in order to move the robot.The local planner monitors the odometry and laser data from the robot and chooses a collision-free local plan. It can recompute the robot‚Äôs path while it is moving in order to keep the robot from colliding with obstacles.base_local_plannerThe base_local_planner provides an implementation of the Trajectory Rollout and Dynamic Window Approach (DWA). The basic idea of how the algorithm works is as follows: Samples the robot space. For each sampled velocity, it performs simulations to predict the outcomes when the sampled velocity is applied. It evaluates the trajectory resulting from the simulation and discards the unwanted trajectories. It picks the best trajectory and sends the associated velocity commands for the robot to moveTrajectory Rollout samples are from a set of achievable velocities ove the entire simulation, while DWA samples are from the set of achievable velocities for just one simulation step.dwa_local_plannerThe dwa_local_planner provides an implementation of the Dynamic Window Approach (DWA) algorithm. It is basically a cleaner version of the base local planner‚Äôs DWA option.You can find the list of parameters associated with the DWA planner here.1.4.6 CostmapsA costmap is a map that represents place that are safe for the robot to be in a grid of cells. Unusally the values in the costmap are binary, representing either free space orplaces where the robot would be in collision. Each cell in a costmap has an integer value in the range {0,255}. 255 (NO_INFORMATION): Cells that do not contain enough information 254 (LETHAL_OBSTACLE): Indicates presence of obstacle in a cell 253 (INSCRIBED_INFLATED_OBSTACLE): Indicates no obstacle but moving the robot to this location will result in a collision 0 (FREE_SPACE): No obstacle in the cellThere are 2 types of costmaps: Global Costmap - created from static map Local Costmap - created from sensor readingsCostmap parameters are defined in 3 different files: A YAML file that sets the parameters for the global costmap - global_costmap_params.yaml A YAML file that sets the parameters for the local costmap - local_costmap_params.yaml A YAML file that sets the parameters for both the global and local costmaps - costmap_common_params.yamlGlobal CostmapThe global costmap is created from a user-generated static map and is used by the global planner. It is initialized to match the width, height, and obstacle information provided by the static map.The global costmap has its own set of parameters defined in a YAML file.See Parameters section of http://wiki.ros.org/costmap_2dLocal CostmapThe local costmap is created from the sensor readings taken from the robot and is used by the local planner.The local costmap too has its own set of parameters defined in a YAML file." }, { "title": "Robotic Arm on ROS", "url": "/posts/ros-manipulator/", "categories": "Projects, Robotics", "tags": "ros, urdf", "date": "2022-07-28 12:00:00 -0400", "snippet": "IntroductionThe aim of this project is to build a robot manipulator on ROS (Robot Operating System). A robot manipulator is a basically a set of links and joints that together forms an arm. By controlling the movement of the joints and links, the robotic arm can perform tasks such as picking up objects. I made use of URDF (Unified Robot Description Format) in order to represent the various components of the manipulator and simulated the same on Rviz and Gazebo.In order to go about designing the arm, my work is broadly divided into 2 parts: Exploring URDF and Xacro to make a simple arm Making a seven degree of freedom robotic armBasic manipulatorThis model of the manipulator comprises of 6 links and 5 joints, as follows:Code&lt;?xml version=\"1.0\"?&gt;&lt;robot name=\"arm\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\"&gt;&lt;!-- Materials --&gt;&lt;material name=\"Black\"&gt; &lt;color rgba=\"0.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"Red\"&gt; &lt;color rgba=\"1.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"White\"&gt; &lt;color rgba=\"1.0 1.0 1.0 1.0\"/&gt;&lt;/material&gt;&lt;link name=\"base_link\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"1 1 1\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt;&lt;/link&gt;&lt;joint name=\"base_link_link_01\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.5\"/&gt; &lt;parent link=\"base_link\"/&gt; &lt;child link=\"link_01\"/&gt; &lt;axis xyz=\"0 0 1\"/&gt; &lt;limit effort=\"300\" lower=\"-3.14\" upper=\"3.14\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_01\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.2\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.35\" length=\"0.4\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.2\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.35\" length=\"0.4\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_01_link_02\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;parent link=\"link_01\"/&gt; &lt;child link=\"link_02\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-0.67\" upper=\"0.67\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_02\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt;&lt;/link&gt;&lt;joint name=\"link_02_link_03\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_02\"/&gt; &lt;child link=\"link_03\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_03\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_03_link_04\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_03\"/&gt; &lt;child link=\"link_04\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_04\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_04_link_05\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_04\"/&gt; &lt;child link=\"link_05\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_05\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.25\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;/robot&gt;SimulationThe image below shows the simulation of the robotic arm on Rviz, which also provides a GUI that allows me to control the movements of the joints.The following image shows the different transformation frames and axes of the links, with the X-axis indicated in red, Y-axis in green and Z-axis in blue.Seven DOF ArmThe seven DOF robotic arm is a serial link manipulator having multiple serial links. It is kinematically redundant, which means it has more joints and DOF than required to achieve its goal position and orientation. The advantage of redundant manipulators are, we can have more joint configuration for a particular goal position and orientation. It will improve the flexibility and versatility of the robot movement and can implement effective collision free motion in a robotic workspace. Joint number Joint name Joint type Limits (rad) 0 fix_world FIxed - - - 1 shoulder_pan_joint Revolute -3.14 to 3.14 2 shoulder_pitch_joint Revolute 0 to 1.899 3 elbow_roll_joint Revolute -3.14 to 3.14 4 elbow_pitch_joint Revolute 0 to 1.5 5 wrist_roll_joint Revolute -1.57 to 1.57 6 wrist_pitch_joint Revolute -1.5 to 1.5 7 gripper_roll_joint Revolute -2.6 to 2.6 8 finger_joint1 Prismatic 0 to 3cm 9 finger_joint_2 Prismatic 0 to 3cm Code&lt;?xml version=\"1.0\"?&gt;&lt;robot name=\"seven_dof_arm\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\"&gt;&lt;!-- Materials --&gt;&lt;material name=\"Black\"&gt; &lt;color rgba=\"0.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"Red\"&gt; &lt;color rgba=\"1.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"White\"&gt; &lt;color rgba=\"1.0 1.0 1.0 1.0\"/&gt;&lt;/material&gt;&lt;xacro:macro name=\"inertial_matrix\" params=\"mass\"&gt; &lt;inertial&gt; &lt;mass value=\"${mass}\"/&gt; &lt;inertia ixx=\"1.0\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.5\" iyz=\"0.0\" izz=\"1.0\"/&gt; &lt;/inertial&gt;&lt;/xacro:macro&gt;&lt;link name=\"world\"/&gt;&lt;joint name=\"fix_world\" type=\"fixed\"&gt; &lt;parent link=\"world\"/&gt; &lt;child link=\"base_link\"/&gt; &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;&lt;/joint&gt;&lt;link name=\"base_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.1 0.1 0.1\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.05\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.1 0.1 0.1\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"shoulder_pan_joint\" type=\"revolute\"&gt; &lt;origin xyz=\"0 0 0.05\" rpy=\"0 0 0\"/&gt; &lt;axis xyz=\"0 0 1\"/&gt; &lt;parent link=\"base_link\"/&gt; &lt;child link=\"shoulder_pan_link\"/&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-3.14\" upper=\"3.14\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"shoulder_pan_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0.02\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.02\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"shoulder_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"shoulder_pan_link\"/&gt; &lt;child link=\"shoulder_pitch_link\"/&gt; &lt;origin xyz=\"-0.04 0.0 0.04\" rpy=\"0 0 -${3.14/2}\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"0\" upper=\"1.89994105047\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"shoulder_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"-0.002 0 0.07\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.14 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.002 0 0.07\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.14 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"elbow_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"shoulder_pitch_link\"/&gt; &lt;child link=\"elbow_roll_link\"/&gt; &lt;origin xyz=\"0.0 0 0.14\" rpy=\"${3.14} ${3.14/2} 0\" /&gt; &lt;axis xyz=\"-1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-3.14\" upper=\"3.14\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"elbow_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"-0.03 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.06\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.03 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.06\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"elbow_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"elbow_roll_link\"/&gt; &lt;child link=\"elbow_pitch_link\"/&gt; &lt;origin xyz=\"-0.06 0 0\" rpy=\"0 ${3.14/2} 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"0\" upper=\"1.5\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"elbow_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0 -0.11\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.22 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0.0 0 -0.11\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.22 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"wrist_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"elbow_pitch_link\"/&gt; &lt;child link=\"wrist_roll_link\"/&gt; &lt;origin xyz=\"0.0 0.0 -0.22\" rpy=\"0 0 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-1.57\" upper=\"1.57\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"wrist_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0.0 -0.02\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.02 0 0.00\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"wrist_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"wrist_roll_link\"/&gt; &lt;child link=\"wrist_pitch_link\"/&gt; &lt;origin xyz=\"0.0 0.0 -0.04\" rpy=\"0 0 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-1.5\" upper=\"1.5\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"wrist_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0.0 -0.03\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.06 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 -0.03\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.06 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"gripper_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"wrist_pitch_link\"/&gt; &lt;child link=\"gripper_roll_link\"/&gt; &lt;origin xyz=\"0 0 -0.06\" rpy=\"-${1.5*3.14} ${.5*3.14} 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-2.61799387799\" upper=\"2.6128806087\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"gripper_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0.0\" rpy=\"0 -${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.02\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.02\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"finger_joint1\" type=\"prismatic\"&gt; &lt;parent link=\"gripper_roll_link\"/&gt; &lt;child link=\"gripper_finger_link1\"/&gt; &lt;origin xyz=\"0.0 0 0\" /&gt; &lt;axis xyz=\"0 1 0\" /&gt; &lt;limit effort=\"100\" lower=\"0\" upper=\"0.03\" velocity=\"1.0\"/&gt; &lt;safety_controller k_position=\"20\" k_velocity=\"20\" soft_lower_limit=\"${-0.15 }\" soft_upper_limit=\"${ 0.0 }\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"gripper_finger_link1\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.04 -0.03 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.08 0.01 0.01\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"finger_joint2\" type=\"prismatic\"&gt; &lt;parent link=\"gripper_roll_link\"/&gt; &lt;child link=\"gripper_finger_link2\"/&gt; &lt;origin xyz=\"0.0 0 0\" /&gt; &lt;axis xyz=\"0 1 0\" /&gt; &lt;limit effort=\"1\" lower=\"-0.03\" upper=\"0\" velocity=\"1.0\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt; &lt;link name=\"gripper_finger_link2\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.04 0.03 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.08 0.01 0.01\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;!-- Transmission block --&gt;&lt;xacro:macro name=\"transmission_block\" params=\"joint_name\"&gt;\t&lt;transmission name=\"tran1\"&gt;\t &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;\t &lt;joint name=\"${joint_name}\"&gt;\t &lt;hardwareInterface&gt;hardware_interface/PositionJointInterface&lt;/hardwareInterface&gt;\t &lt;/joint&gt;\t &lt;actuator name=\"motor1\"&gt;\t &lt;hardwareInterface&gt;hardware_interface/PositionJointInterface&lt;/hardwareInterface&gt;\t &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;\t &lt;/actuator&gt;\t&lt;/transmission&gt;&lt;/xacro:macro&gt;&lt;!-- Transmissions for ROS Control --&gt;&lt;xacro:transmission_block joint_name=\"shoulder_pan_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"shoulder_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"elbow_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"elbow_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"wrist_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"wrist_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"gripper_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"finger_joint1\"/&gt;&lt;xacro:transmission_block joint_name=\"finger_joint2\"/&gt;&lt;!-- Colors in gazebo --&gt;&lt;gazebo reference=\"base_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"shoulder_pan_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"shoulder_pitch_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"elbow_roll_link\"&gt; &lt;material&gt;Gazebo/Black&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"elbow_pitch_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"wrist_roll_link\"&gt; &lt;material&gt;Gazebo/Black&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"wrist_pitch_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_roll_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_finger_link1\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_finger_link1\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;!-- ros_control plugin --&gt;&lt;gazebo&gt; &lt;plugin name=\"gazebo_ros_control\" filename=\"libgazebo_ros_control.so\"&gt; &lt;robotNamespace&gt;/seven_dof_arm&lt;/robotNamespace&gt; &lt;/plugin&gt;&lt;/gazebo&gt;&lt;/robot&gt;Configuring the joint parametersIn a separate configuration file, we add all the joint state and joint position parameters.seven_dof_arm: # Joint State Controllers joint_state_controller: type: joint_state_controller/JointStateController publish_rate: 50 # 50Hz # Position Controllers assigned to 7 joints and define PID gains joint1_position_controller: type: position_controllers/JointPositionController joint: shoulder_pan_joint pid: { p: 100, i: 0.01, d: 10 } joint2_position_controller: type: position_controllers/JointPositionController joint: shoulder_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint3_position_controller: type: position_controllers/JointPositionController joint: elbow_roll_joint pid: { p: 100, i: 0.01, d: 10 } joint4_position_controller: type: position_controllers/JointPositionController joint: elbow_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint5_position_controller: type: position_controllers/JointPositionController joint: wrist_roll_joint pid: { p: 100, i: 0.01, d: 10 } joint6_position_controller: type: position_controllers/JointPositionController joint: wrist_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint7_position_controller: type: position_controllers/JointPositionController joint: gripper_roll_joint pid: { p: 100, i: 0.01, d: 10 }Launch file&lt;launch&gt; &lt;!-- these are the arguments you can pass this launch file, for example paused:=true --&gt; &lt;arg name=\"paused\" default=\"false\"/&gt; &lt;arg name=\"use_sim_time\" default=\"true\"/&gt; &lt;arg name=\"gui\" default=\"true\"/&gt; &lt;arg name=\"headless\" default=\"false\"/&gt; &lt;arg name=\"debug\" default=\"false\"/&gt; &lt;!-- We resume the logic in empty_world.launch --&gt; &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt; &lt;arg name=\"debug\" value=\"$(arg debug)\" /&gt; &lt;arg name=\"gui\" value=\"$(arg gui)\" /&gt; &lt;arg name=\"paused\" value=\"$(arg paused)\"/&gt; &lt;arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/&gt; &lt;arg name=\"headless\" value=\"$(arg headless)\"/&gt; &lt;/include&gt; &lt;!-- Load the URDF into the ROS Parameter Server --&gt; &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro '$(find robot_manipulator)/urdf/seven_dof_arm.xacro'\" /&gt; &lt;!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --&gt; &lt;node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model seven_dof_arm -param robot_description\"/&gt; &lt;!-- Load joint controller configurations from yaml file --&gt; &lt;rosparam file=\"$(find robot_manipulator)/config/seven_dof_arm_control.yaml\" command=\"load\"/&gt; &lt;!-- Load controllers --&gt; &lt;node name=\"controller_spawner\" pkg=\"controller_manager\" type=\"spawner\" respawn=\"false\" output=\"screen\" ns=\"/seven_dof_arm\" args=\"joint_state_controller joint1_position_controller joint2_position_controller joint3_position_controller joint4_position_controller joint5_position_controller joint6_position_controller joint7_position_controller\"/&gt; &lt;!-- convert joint states to TF transforms for rviz --&gt; &lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" output=\"screen\" respawn=\"false\" &gt; &lt;remap from=\"/joint_states\" to=\"/seven_dof_arm/joint_states\"/&gt; &lt;/node&gt;&lt;/launch&gt;List of available topics:SimulationLaunching the robot: roslaunch robot_manipulator gazebo_seven_dof_arm.launchControlling the joints using the terminal:rostopic pub /seven_dof_arm/joint4_position_controller/command std_msgs/Float64 1.0rostopic pub /seven_dof_arm/joint1_position_controller/command std_msgs/Float64 1.57ROS ControllersA ROS Controller mainly consists of a feedback mechanism, usually a PID, that lets us control manipulator joints using feedback from the actuators.ROS controllers are provided by the ros_control package. The ros_control packages have the implementation of robot controllers, controller managers, hardware interface, different transmission interface, and control toolboxes.Some standard ROS controllers: joint_position_controller: implementation of the joint position controller joint_state_controller: publishes joint states joint_effort_controller: implementation of the joint effort (force) controllerCommon hardware interfaces in ROS: Joint Command Interfaces: sends commands to the hardware EffortJointInterface: sends the effort command VelocityJointInterface: sends the velocity command PositionJointInterface: sends the position command Joint State Interfaces: retrieves join states from the actuators encoderWriting ROS Controllers" }, { "title": "Robot Kinematics", "url": "/posts/robot-kinematics/", "categories": "Blog, Robotics", "tags": "kinematics", "date": "2022-07-28 12:00:00 -0400", "snippet": "JointsRobot manipulators consist of links connected by joints.Higher pair joint: The two connecting elements are in contact along a line or at a point.Lower pair joint: Constrains contact between a surface in a moving body to a corresponding surface in the fixed body.There are 6 types of lower pair joint designs possible. Revolute joint (R): This joint is like a hinge and allows for relative rotation between the two links. DOF = 1 Prismatic joint (P): This joint allows for relative linear motion between the two links. DOF = 1 Spherical joint (S): This joint allows for motion along all the three axes. DOF = 3 Cylindrical joint (C): This joint allows for both rotation and linear motion between the two links. DOF = 2 Helical joint (H): This joint can be thought of as a screw-nut and allows for motion only along one axis. The linear and rotatory motions are not independent of each other. DOF = 1. Planar joint (E): This joint allows for motion between two planes. DOF = 3Gruebler‚Äôs equation\\[M = 3L - 2J-3G\\] M = degree of freedom or Mobility L = number of links J = number of full joints (1 DOF) G = number of grounded links (G is always 1)TransformationsA linear transformation is one that preserves the linear and scalar multiplication rules of vectors.A projective transformation shows how an object is perceived as the observer‚Äôs viewpoint changes.An affine transformation is one that preserves collinearity (i.e., all points lying on a line continue to lie on the line after the transformation) and the ratio of distances between any 2 points on it remains the same. They are used for scaling, skewing and rotation.Source: https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htmRigid transformations, also called Euclidean transformation, is one that preserves the Euclidean distance between every pair of points. It includes rotations, translations, reflections, or their combination. Scaling and shear are excluded from such a type of transformation. Sometimes reflections are also excluded by imposing that the transformation also preserves the handedness of figures.Rotational TransformationsA transformation matrix called the Rotation Matrix is used to specify the coordinate vectors for the axes of a frame $o_1x_1y_1z_1$ *with respect to coordinate frame $o_0x_0y_0z_0$.* Thus, $R_1^0$ is a rotation matrix whose column vectors are the coordinates of the axes of frame $o_1x_1y_1z_1$ *expressed relative to coordinate frame $o_0x_0y_0z_0$.*\\[R_1^0 = \\begin{bmatrix}x_1.x_0 &amp; y_1.x_0 &amp; z_1.x_0\\\\x_1.y_0 &amp; y_1.y_0 &amp; z_1.y_0\\\\x_1.z_0 &amp; y_1.z_0 &amp; z_1.z_0\\end{bmatrix}\\]For a positive rotation about the z-axis by an angle Œ∏, we have\\[x_1.x_0=cosŒ∏\\]\\[x_1.y_0=sinŒ∏\\]\\[y_1.x_0=-sinŒ∏\\]\\[y_1.y_0=cosŒ∏\\]\\[z_1.z_0=1\\]Thus the rotation matrix equals\\[R_1^0=\\begin{bmatrix} cosŒ∏ &amp; -sinŒ∏ &amp; 0\\\\sinŒ∏ &amp; cosŒ∏ &amp; 0\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\]If we wish to describe the orientation of frame $o_0x_0y_0z_0$ with respect to frame $o_1x_1y_1z_1$, we would have $R_0^1$ such that\\[R_0^1 = (R_1^0)^T = (R_1^0)^{-1}\\]The rotation matrix $R_1^0$ can not only be used to represent the orientation of the coordinate frame $o_1x_1y_1z_1$ with respect to frame $o_0x_0y_0z_0$, but also to transform the coordinates of a point from one frame to the other. Thus if a given point is expressed with respect to the frame $o_1x_1y_1z_1$ as $p^1$then $R_1^0p^1$ represents the same point with respect to frame $o_0x_0y_0z_0$.\\[p^0=R_1^0p^1\\]Instead of relating the coordinates of a fixed vector with respect to two different coordinate frames, the rotation matrix is also used to represent the coordinates of a vector $v_1$ obtained from a vector $v$ by a given rotation in the same coordinate frame.Composition of RotationsRotation about current frameIn order to transform the coordinates of a point $p$ from its representation $p^2$ in the frame $o_2x_2y_2z_2$ to its representation $p^0$ in the frame $o_0x_0y_0z_0$ , we may first transform to its coordinates $p^1$ in the frame $o_1x_1y_1z_1$ using $R_2^1$ and then transform $p^1$ to $p^0$ using $R_1^0$. Thus,\\[R_2^0=R_1^0R_2^1\\]Rotation about fixed frameIf we wish to perform successive rotations with respect to a fixed frame $o_0x_0y_0z_0$ instead of the current frame, then the above equation does not hold true anymore. Rather we need to multiply the rotation matrices in the reverse order.Let $o_0x_0y_0z_0$ be the reference frame. Let the frame $o_1x_1y_1z_1$ be obtained by performing a rotation with respect to the reference frame, and let this rotation be denoted by $R_1^0$. Now let $o_2x_2y_2z_2$ be obtained by performing a rotation of frame $o_1x_1y_1z_1$ with respect to the reference frame (not with respect to frame $o_1x_1y_1z_1$ itself). Let this rotation be denoted by the matrix $R$. Let $R_2^0$ be the matrix that denotes the orientation of frame $o_2x_2y_2z_2$ w.r.t frame $o_0x_0y_0z_0$. Now\\[R_2^0 ‚â† R_1^0R\\]Instead,\\[R_2^0=RR_1^0\\]Derivation:We will convert the rotation about the fixed frame into one about the current frame. First we will rotate the frame $o_1x_1y_1z_1$ w.r.t the frame $o_0x_0y_0z_0$. This can be done by postmultiplying with the inverse of $R_1^0$. Now since the current frame is aligned with the reference frame, we can simply postmultiply with $R$ to get the frame $o_2x_2y_2z_2$. We now need to undo the initial rotation by postmultiplying with $R_1^0$. Therefore, we have\\[R_2^0=R_1^0R_2^1\\\\R_2^0=R_1^0[(R_1^0)^{-1}RR_1^0]\\\\R_2^0=RR_1^0\\]Euler AnglesRoll, Pitch and Yaw AnglesA rotation by $\\phi$ about $z_0$ is the roll angle, a rotation by $\\theta$ about $y_0$ is the pitch angle, and a rotation by $\\psi$ about $x_0$ is the yaw angle.For the rotations yaw-pitch-roll w.r.t a fixed frame or for the rotations roll-pitch-yaw w.r.t current frame, we get the following transformation matrix\\[R_1^0=R_{z,\\phi}R_{y,\\theta}R_{x,\\psi}\\\\=\\begin{bmatrix}c_\\phi &amp; -s_\\phi &amp; 0\\\\s_\\phi &amp; c_\\phi &amp; 0\\\\0&amp;0&amp;1 \\end{bmatrix}\\begin{bmatrix}c_\\theta&amp; 0&amp;s_\\theta\\\\0&amp;1&amp;0\\\\-s_\\theta&amp;0&amp;c_\\theta\\end{bmatrix}\\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;c_\\psi&amp;-s_\\psi\\\\0&amp;s_\\psi&amp;c_\\psi\\end{bmatrix}\\\\\\]\\[=\\begin{bmatrix}c_\\phi c_\\theta&amp;-s_\\phi c_\\psi+c_\\phi s_\\theta s_\\psi&amp;s_\\phi s_\\psi+c_\\phi s_\\theta c_\\psi\\\\s_\\phi c_\\theta&amp;c_\\phi c_\\psi+s_\\phi s_\\theta s_\\psi &amp; -c_\\phi s_\\psi + s_\\phi s_\\theta c_\\psi \\\\ -s_\\theta&amp;c_\\theta s_\\psi&amp;c_\\theta c_\\psi\\end{bmatrix}\\]Homogeneous TransformationsIf frame $o_1x_1y_1z_1$ is obtained from frame $o_0x_0y_0z_0$ by first applying a rotation specified by$R_1^0$ followed by a translation given (with respect to $o_0x_0y_0z_0$) by $d_1^0$ , then the coordinates $p^0$ are given by\\[p^0=R_1^0p^1+d_1^0\\]In general,\\[r_p^G=R_B^Gr_p^B+d^G\\]where $d^G$ is the displacement or translation of B w.r.t G.If we have 2 rigid motions,\\[First \\space motion: p^0=R_1^0p^1+d_1^0\\]\\[Second \\space motion: p^1=R_2^1p^2+d_2^1\\]On combining the two, we get\\[p^0=R_1^0(R_2^1p^2+d_2^1)+d_1^0 \\\\ p^0=R_1^0R_2^1p^2+d_1^0+R_1^0d_2^1\\]This can also be expressed as\\[p^0=R_2^0p^2+d_2^0\\]where\\[R_2^0=R_1^0R_2^1\\\\d_2^0=d_1^0+R_1^0d_2^1\\]\\[\\begin{bmatrix}R_1^0&amp;d_1^0\\\\0&amp;1\\end{bmatrix}\\begin{bmatrix}R_2^1&amp;d_2^1\\\\0&amp;1\\end{bmatrix}=\\begin{bmatrix}R_1^0R_2^1&amp;d_1^0+R_1^0d_2^1\\\\0&amp;1\\end{bmatrix}\\]\\[T_B^G=\\begin{bmatrix}R_B^G&amp;d^G\\\\0&amp;1\\end{bmatrix}\\]\\[T_G^B=\\begin{bmatrix}R_B^G&amp;d^G\\\\0&amp;1\\end{bmatrix}^{-1}=\\begin{bmatrix}{R_B^G}^T&amp;-{R_B^G}^Td^G\\\\0&amp;1\\end{bmatrix}\\]Forward KinematicsIn Forward Kinematics, we are to determine the position and orientation of the end-effector given the values for the joint variables of the robot. The joint variables are the angles between the links in the case of revolute or rotational joints, and the link extension in the case of prismatic or sliding joints.The following conventions are defined to perform kinematic analysis: A robot manipulator with n joints will have n+1 links, since each joint connects to two links. Joints are numbered from 1 to n. Links are numbered from 0 to n, starting from the base. Joint i connects link i-1 to link i. When joint i is actuated, link i moves. A coordinate from $o_ix_iy_iz_i$ is rigidly attached to link i. So when a joint i moves, link i and its attached coordinate frame also moves.Let $A_i$ be a homogeneous transformation matrix that expresses the position and orientation of frame $o_ix_iy_iz_i$ w.r.t frame $o_{i-1}x_{i-1}y_{i-1}z_{i-1}$.$T_j^i$ is the transformation matrix that expresses the position and orientation of frame $o_jx_jy_jz_j$ w.r.t frame $o_ix_iy_iz_i$.\\[T_j^i=A_{i+1}A_{i+2}...A_{j-1}A_j \\space \\space if \\space i&lt;j\\]\\[T_j^i=I \\space \\space if\\space i=j \\\\ T_j^i = (T_i^j)^{-1} \\space if \\space j&gt;i\\]Denavit-Hartenberg ConventionThe following steps need to be followed to derive the forward kinematics for a manipulator:Step 1: Locate and label the joint axes $z_0,z_1,‚Ä¶,z_{n-1}$. We assign $z_i$ to be the axis of actuation for joint $i+1$.Step 2: Choose the base coordinate frame. Set the origin anywhere on the $z_0$ axis.Step 3: Locate the origin $o_i$ where the common normal to $z_i$ and $z_{i-1}$ intersects $z_i$. If $z_i$ intersects $z_{i-1}$, $o_i$ is located at the point of intersection. If $z_i$ and $z_{i-1}$ are parallel, locate $o_i$ in any point along $z_i$.Step 4: The x-axis is defined along the common normal between $z_{i-1}$ and $z_i$ axes through $o_i$, pointing from $z_{i-1}$ to $z_i$ axis. When the two axes are intersecting, $x_i$ is in the direction of $z_{i-1}$x $z_i$.Step 5: Define $y_i$ to complete the right hand coordinate system.Step 6: Choose $o_nx_ny_nz_n$ to be the end-effector frame.Step 7: Create a table of the following parameters $a_i,d_i,\\alpha_i,\\theta_i$. Link length $a_i$: distance between $z_i$ and $z_{i-1}$axes along the $x_i$ axis. Joint distance $d_i$: distance between $x_{i-1}$ and $x_i$ axes along the $z_{i-1}$ axis. Link twist $\\alpha_i$: required rotation of $z_{i-1}$ axis about the $x_i$ axis to become parallel to $z_i$ axis. Joint angle $\\theta_i$: required rotation of $x_{i-1}$ axis about the $z_{i-1}$ axis to become parallel to $x_i$ axis.Step 8: Form the homogeneous transformation matrix $A_i$ by substituting the above parameters.\\[A_i=Rot_{z,\\theta_i}Trans_{z,d_i}Trans_{x,a_i}Rot_{x,\\alpha_i} \\\\ = \\begin{bmatrix} c_{\\theta_i} &amp; -s_{\\theta_i}c_{\\alpha_i} &amp; s_{\\theta_i}s_{\\alpha_i} &amp; a_ic_{\\theta_i} \\\\ s_{\\theta_i} &amp; c_{\\theta_i}c_{\\alpha_i} &amp; -c_{\\theta_i}s_{\\alpha_i} &amp; a_is_{\\theta_i}\\\\0&amp;s_{\\alpha_i}&amp;c_{\\alpha_i}&amp;d_i\\\\0&amp;0&amp;0&amp;1 \\end{bmatrix}\\]Step 9: Form the transformation matrix $T_n^0=A_1A_2‚Ä¶A_n$. This gives the position and orientation of the end-effector w.r.t the base frame.Stanford Manipulator:DH-parameters: Link ‚Åç ‚Åç ‚Åç ‚Åç 1 0 0 -90 * 2 ‚Åç 0 90 * 3 * 0 0 0 4 0 0 -90 * 5 0 0 90 * 6 ‚Åç 0 0 * We can plug in the above values into the $A_i$ matrix to get $A_1,A_2,A_3,A_4,A_5,A_6$. This gives us $T_6^0 = A_1A_2‚Ä¶A_6.$SCARA ManipulatorHas 4 DOF - 3R+1PDH-parameters: Link ‚Åç ‚Åç ‚Åç ‚Åç 1 0 0 0 * 2 0 ‚Åç 0 * 3 ‚Åç ‚Åç 0 0 4 0 0 0 * Reference: https://www.youtube.com/watch?v=O8nzaDcjTiYInverse Kinematics" }, { "title": "Numpy", "url": "/posts/numpy/", "categories": "Blog, Python", "tags": "numpy, python, programming", "date": "2022-07-28 12:00:00 -0400", "snippet": "What is Numpy?Numpy is the python package that allows us to work with multidimensional array objects. It is extremely useful for scientific computing using Python, and is widely used in various fields such as Machine Learning for Data Analysis etc.ArrayAn Array in Numpy is a matrix of elements having the same datatype. An array class in Numpy is known as ndarray (N-dimensional array).Creating a Numpy Array# Using array() methodimport numpy as nparr = np.array([[1, 4, 7], [7, 2, 9], [5, 0, 1]])print('Created Array:\\n {}'.format(arr))Created Array: [[1 4 7] [7 2 9] [5 0 1]]# Using empty() method# numpy.empty(shape, dtype = float, order = ‚ÄòC‚Äô) : Return a new array of given shape and type, with random values.import numpy as nparr = np.empty((3, 3), dtype=int)print('Empty Array:\\n {}'.format(arr))Empty Array: [[1 4 7] [7 2 9] [5 0 1]]# Using eye() method# numpy.eye(R, C = None, k = 0, dtype = type &lt;‚Äòfloat‚Äô&gt;) : The eye tool returns a 2-D array with 1‚Äôs as the diagonal and 0‚Äôs elsewhere.# The diagonal can be main, upper, lower depending on the optional parameter k.# A positive k is for the upper diagonal, a negative k is for the lower, and 0 k (default) is for the main diagonal.import numpy as nparr1 = np.eye(4, 4, k=0, dtype=int)print('Matrix: \\n {}'.format(arr1))arr2 = np.eye(4, 4, k=1, dtype=int)print('Matrix: \\n {}'.format(arr2))arr3 = np.eye(4, 4, k=-1, dtype=int)print('Matrix: \\n {}'.format(arr3))Matrix: [[1 0 0 0] [0 1 0 0] [0 0 1 0] [0 0 0 1]]Matrix: [[0 1 0 0] [0 0 1 0] [0 0 0 1] [0 0 0 0]]Matrix: [[0 0 0 0] [1 0 0 0] [0 1 0 0] [0 0 1 0]]# Using identity() method# numpy.identity(n, dtype = None) : Return a identity matrix i.e. a square matrix with 1's on the main diagonal.import numpy as nparr = np.identity(4)print('Identity Matrix: \\n {}'.format(arr))Identity Matrix: [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]# Using ones() method# numpy.ones(shape, dtype = None, order = 'C') Returns a new array of given shape and type, with ones.import numpy as nparr_one = np.ones([3, 3])print('Matrix with 1 as elements: \\n {}'.format(arr_one))# Using zeros() method# numpy.zeros(shape, dtype = None, order = 'C') Returns a new array of# given shape and type, with zeros.arr_zero = np.zeros([3, 3])print('Matrix with 0 as elements: \\n {}'.format(arr_zero))Matrix with 1 as elements: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]Matrix with 0 as elements: [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]# Using asarray() method# numpy.asarray()function is used to convert input to an array. # Input can be lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays.import numpy as nparr_tuple1 = ([1,6,5],[2,7,6])print('Input: {}'.format(arr_tuple1))arr1 = np.asarray(arr_tuple1)print('Array: \\n {}'.format(arr1))# Uncomment these lines to see what happens if you try to convert inputs having different lengths into an array# arr_tuple2 = ([1,6,5],[2,7])# print('Input: {}'.format(arr_tuple2))# arr2 = np.asarray(arr_tuple2)# print('Array: \\n {}'.format(arr2))Input: ([1, 6, 5], [2, 7, 6])Array: [[1 6 5] [2 7 6]]# Using arange() method# arange([start,] stop[, step,][, dtype]) : Returns an array with evenly spaced elements as per the interval.import numpy as nparr_1 = np.arange(0, 20)print('Array 1: {}'.format(arr_1))arr_2 = np.arange(2, 20, 3)print('Array 2: {}'.format(arr_2))Array 1: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19]Array 2: [ 2 5 8 11 14 17]# Using linspace() method# Returns number spaces evenly w.r.t interval. Similar to numpy.arange() function but instead of step it uses sample number.import numpy as nparr_1 = np.linspace(start=0, stop=10, num=5, retstep=True, dtype=int)print('Arr 1:\\n {}'.format(arr_1))arr_2 = np.linspace(start=0, stop=10, num=5, retstep=False, dtype=int)print('Arr 2:\\n {}'.format(arr_2))Arr 1: (array([ 0, 2, 5, 7, 10]), 2.5)Arr 2: [ 0 2 5 7 10]# Using diag() method# Extracts and construct a diagonal array# k=0 denotes main diagonal. k&gt;0 denotes diagonal above main diagonal. k&lt;0 denotes diagonal below main diagonalimport numpy as nparr = np.array([[1, 2, 6], [4, 8, 1], [6, 9, 0]])print('Array:\\n {}'.format(arr))diag_1 = np.diag(arr, k=0)print('Main diag: {}'.format(diag_1))diag_2 = np.diag(arr, k=1)print('Upper diag: {}'.format(diag_2))diag_3 = np.diag(arr, k=-1)print('Lower diag: {}'.format(diag_3))Array: [[1 2 6] [4 8 1] [6 9 0]]Main diag: [1 8 0]Upper diag: [2 1]Lower diag: [4 9]# Using diagflat() method# Create a two-dimensional array with the array_like input as a diagonal to the new output array.import numpy as npprint(\"diagflat use on main diagonal : \\n\", np.diagflat([1, 7, 6], k=0))print(\"\\ndiagflat above main diagonal : \\n\", np.diagflat([1, 7, 6], k=1))print(\"\\ndiagflat below main diagonal : \\n\", np.diagflat([1, 7, 6], k=-1))diagflat use on main diagonal : [[1 0 0] [0 7 0] [0 0 6]]diagflat above main diagonal : [[0 1 0 0] [0 0 7 0] [0 0 0 6] [0 0 0 0]]diagflat below main diagonal : [[0 0 0 0] [1 0 0 0] [0 7 0 0] [0 0 6 0]]# RANDOM NUMBERS# randint(low, high=None, size=None, dtype=) Uniformly distributes integers in a given rangeimport numpy as npnp.random.seed(101) # enter an arbitrary number (called seed number)arr1 = np.random.randint(0,100,10) # Returns random numbers from 'low'(inclusive) to 'high'(exclusive)print(arr1)[95 11 81 70 63 87 75 9 77 40]Indexing:Idexing is done in order to obtain slices of the main array. This can be done in the following ways- Basic slicing and indexing: using the form [start: stop: step] Advanced indexing: done by pure integer indexing or boolean indexing # Basic slicing and indexingarr_1 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])print('Array 1:\\n {}\\n'.format(arr_1))print('Indexing 1: \\n {}\\n'.format(arr_1[:, :, 2]))print('Indexing 2: \\n {}\\n'.format(arr_1[:, :, 0]))arr_2 = np.arange(0, 10)print('Array 2: {}\\n'.format(arr_2))print('Indexing: \\n {}\\n'.format(arr_2[2:9:2]))arr_3 = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])print('Array 3:\\n {}\\n'.format(arr_3))index_1 = arr_3[1:4, 0:2]print('Indexing: {}'.format(index_1))index_2 = arr_3[1:4, [2, 0]]print('Indexing:\\n {}\\n'.format(index_2))Array 1: [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]]Indexing 1: [[ 3 6] [ 9 12]]Indexing 2: [[ 1 4] [ 7 10]]Array 2: [0 1 2 3 4 5 6 7 8 9]Indexing: [2 4 6 8]Array 3: [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]Indexing: [[ 3 4] [ 6 7] [ 9 10]]Indexing: [[ 5 3] [ 8 6] [11 9]]# Advanced slicingimport numpy as nparr_1 = np.array([[3, 4, 7], [5, 1, 8], [2, 4, 7]])print('Array 1:\\n {}\\n'.format(arr_1))ind_1 = arr_1[[1, 2, 1], [2, 0, 1]] # index inputs are column-wiseprint('Index 1:\\n {}\\n'.format(ind_1))# Type of rearrangement (See carefully)imp = arr_1[np.array([[1, 2, 1], [2, 0, 1]])]print('Imp:\\n {}'.format(imp))# Boolean indexingarr_2 = np.array([[2, 5, 8], [1, 9, 0], [3, 5, 4]])print('Array 2:\\n {}\\n'.format(arr_2))ind_2 = arr_2[arr_2 &gt;= 3]print('Indexing 2:\\n {}\\n'.format(ind_2))arr_3 = np.array([10, 40, 80, 50, 100])print('Indexing 3:\\n {}'.format((arr_3[arr_3 % 40 == 0])**2))Array 1: [[3 4 7] [5 1 8] [2 4 7]]Index 1: [8 2 1]Imp: [[[5 1 8] [2 4 7] [5 1 8]] [[2 4 7] [3 4 7] [5 1 8]]]Array 2: [[2 5 8] [1 9 0] [3 5 4]]Indexing 2: [5 8 9 3 5 4]Indexing 3: [1600 6400]Using .reshape(rows,columns) methodThis method allows us to convert a given list into an array with required number of rows and columns.# Using reshape() methodimport numpy as nparr = np.arange(0, 20)print('Array 1: {}\\n'.format(arr))arr_2 = arr.reshape(4, 5)print('Array 2:\\n {}'.format(arr_2))Array 1: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19]Array 2: [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]]Using numpy.nditer()This method allows us to iterate over multi-dimensional arrays.# Using numpy.nditer() method# https://youtu.be/XawR6CjAYV4import numpy as nparr = np.arange(0, 12).reshape(3, 4)print('Reshaped Array 1:\\n {}\\n'.format(arr))# Using a for loop# for row in arr:# for cell in row:# print(cell)# Both the codes are equivalent# for row in arr.flatten():# print(row)# Using nditer row-wisefor x in np.nditer(arr, order='C'): print(x)print()# Using nditer column-wisefor x in np.nditer(arr, order='F'): print(x)Reshaped Array 1: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]0123456789101104815926103711# Using flags with nditer()import numpy as npa = np.arange(0, 12).reshape(3, 4)print('Array:\\n {}\\n'.format(a))# Doing this will allow you to print your entire column at oncefor x in np.nditer(a, order='F', flags=['external_loop']): print(x)print()# Doing this will allow you to print your entire row at oncefor x in np.nditer(a, order='C', flags=['external_loop']): print(x)print()# Using optional flags with op_flags option# readwrite option is used to modify the elements of the array, and in this case, it gives us the square of all elementsfor x in np.nditer(a, op_flags=['readwrite']): x[...] = x*x # [...] is ellipsis.print(a)Array: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]][0 4 8][1 5 9][ 2 6 10][ 3 7 11][ 0 1 2 3 4 5 6 7 8 9 10 11][[ 0 1 4 9] [ 16 25 36 49] [ 64 81 100 121]]# Iterating through two arrays using nditer()# To do this, either both arrays should have the same shape, or one of the# dimensions in one of the arrays should be 1a = np.arange(0, 12).reshape(3, 4)print('Array 1:\\n {}\\n'.format(a))b = np.arange(3, 15, 4).reshape(3, 1)print('Array 2:\\n {}\\n'.format(b))for x, y in np.nditer([a, b]): print(x, y)Array 1: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]Array 2: [[ 3] [ 7] [11]]0 31 32 33 34 75 76 77 78 119 1110 1111 11Binary Operations in Numpy numpy.bitwise_and(arr1,arr2): Computes the bitwise AND of two arrays, element wise numpy.bitwise_or(arr1,arr2): Computes the bitwise OR of two arrays, element wise numpy.bitwise_xor(arr1,arr2): Computes the bitwise XOR of two arrays, element wise numpy.invert(arr): Computes the bitwise NOT of an array, element wise numpy.left_shift(arr,bit_shift): Shifts the bits of an integer to the left and returns the corresponding decimal representation. numpy.right_shift(arr,bit_shift): Shifts the bits of an integer to the right and returns the corresponding decimal representation. numpy.unpackbits(array,axis): Unpacks elements of a uint8 array into a binary-valued output array. numpy.packbits(array,axis): Packs the elements of a binary-valued array into bits in a uint8 array. numpy.binary_repr(number,width): Gives the binary represntation of the array element. For negative numbers, if width is not given, a minus sign is added to the front. If width is given, the two‚Äôs complement of the number is returned, with respect to that width. # BINARY OPERATIONSimport numpy as nparr_1 = np.array([2, 8, 125])arr_2 = np.array([3, 3, 115])print('Array 1: {}\\n'.format(arr_1))print('Array 2: {}\\n'.format(arr_2))# Bitswise AND Operationarr_and = np.bitwise_and(arr_1, arr_2)print('Bitwise AND: {}\\n'.format(arr_and))# Bitwise OR Operationarr_or = np.bitwise_or(arr_1, arr_2)print('Bitwise OR: {}\\n'.format(arr_or))# Bitwise XOR Operationarr_xor = np.bitwise_xor(arr_1, arr_2)print('Bitwise XOR: {}\\n'.format(arr_xor))Array 1: [ 2 8 125]Array 2: [ 3 3 115]Bitwise AND: [ 2 0 113]Bitwise OR: [ 3 11 127]Bitwise XOR: [ 1 11 14]# BINARY OPERATIONSimport numpy as nparr_1 = np.array([2, 8, 15])print('Array 1: {}\\n'.format(arr_1))# Bitwise NOT Operationarr_not = np.invert(arr_1)print('Bitwise NOT: {}\\n'.format(arr_not))# Bit shift Operationsbit_shift_1 = np.array([3, 4, 5])print('Bit shift array 1: {}\\n'.format(bit_shift_1))# Left shiftarr_left_shift = np.left_shift(arr_1, bit_shift_1)print('Left shift: {}\\n'.format(arr_left_shift))print()arr_2 = np.array([24, 48, 16])print('Array 2: {}\\n'.format(arr_2))bit_shift_2 = np.array([3, 4, 2])print('Bit shift array 2: {}\\n'.format(bit_shift_2))# Right shiftarr_right_shift = np.right_shift(arr_2, bit_shift_2)print('Right shift: {}\\n'.format(arr_right_shift))Array 1: [ 2 8 15]Bitwise NOT: [ -3 -9 -16]Bit shift array 1: [3 4 5]Left shift: [ 16 128 480]Array 2: [24 48 16]Bit shift array 2: [3 4 2]Right shift: [3 3 4]# BINARY OPERATIONSimport numpy as npin_arr = [5, -8]print()print(\"Input array : \", in_arr)# Without using width parameterout_num = np.binary_repr(in_arr[0])print(\"Binary representation of 5\")print(\"Without using width parameter : \", out_num)# Using width parameterout_num = np.binary_repr(in_arr[0], width=5)print(\"Using width parameter: \", out_num)print(\"\\nBinary representation of -8\")# Without using width parameterout_num = np.binary_repr(in_arr[1])print(\"Without using width parameter : \", out_num)# Using width parameterout_num = np.binary_repr(in_arr[1], width=5)print(\"Using width parameter : \", out_num)Input array : [5, -8]Binary representation of 5Without using width parameter : 101Using width parameter: 00101Binary representation of -8Without using width parameter : -1000Using width parameter : 11000Linear Algebra using NumpyThe Linear Algebra module of NumPy offers various methods to apply linear algebra on any numpy array.One can find: rank, determinant, trace, etc. of an array. eigen values of matrices matrix and vector products (dot, inner, outer,etc. product), matrix exponentiation solve linear or tensor equations# LINEAR ALGEBRAimport numpy as npA = np.array([[6, 1, 1], [4, -2, 5], [2, 8, 7]])# Rank of a matrixprint(\"Rank of A:\", np.linalg.matrix_rank(A))# Trace of matrix Aprint(\"\\nTrace of A:\", np.trace(A))# Determinant of a matrixprint(\"\\nDeterminant of A:\", np.linalg.det(A))# Inverse of matrix Aprint(\"\\nInverse of A:\\n\", np.linalg.inv(A))print(\"\\nMatrix A raised to power 3:\\n\", np.linalg.matrix_power(A, 3))Rank of A: 3Trace of A: 11Determinant of A: -306.0Inverse of A: [[ 0.17647059 -0.00326797 -0.02287582] [ 0.05882353 -0.13071895 0.08496732] [-0.11764706 0.1503268 0.05228758]]Matrix A raised to power 3: [[336 162 228] [406 162 469] [698 702 905]]# LINEAR ALGEBRA# Matrix and Vector productsimport numpy as np# Using numpy.dot(): returns the dot product of vectors a and b# Scalarsproduct = np.dot(5, 4)print(\"Dot Product of scalar values : \", product)# 1D arrayvector_a = 2 + 3jvector_b = 4 + 5jproduct = np.dot(vector_a, vector_b)print(\"Dot Product : \", product)# Using numpy.vdot(): Returns the dot product of vectors a and b. If first# argument is complex the complex conjugate of the first argument is used# for the calculation of the dot productvector_a = 2 + 3jvector_b = 4 + 5jproduct = np.vdot(vector_a, vector_b)print(\"VDot Product : \", product)Dot Product of scalar values : 20Dot Product : (-7+22j)VDot Product : (23-2j)# LINEAR ALGEBRA# Solving equations# numpy.linalg.solve(): Solves a system of linear scalar equations.# Computes the ‚Äúexact‚Äù solution, x, of the linear matrix equation ax = b.import numpy as npa = np.array([[1, 2], [3, 4]])b = np.array([8, 18])print(\"Solution of linear equations:\", np.linalg.solve(a, b))Solution of linear equations: [2. 3.]Using numpy.sum():numpy.sum(arr, axis, dtype, out) : This function returns the sum of array elements over the specified axis.out : Different array in which we want to place the result. The array must have same dimensions as expected output. Default is None.Using numpy.add():numpy.add(arr1, arr2, /, out=None) : This function is used when we want to compute the addition of two array. It adds arguments element-wise.If shape of two arrays are not same, they must be broadcastable to a common shape.Broadcastable arrays - should have same dimensions or the dimension of one of the arrays should be 1.# Using numpy.sum()import numpy as np# 2D arrayarr = [[14, 17, 12, 33, 44], [15, 6, 27, 8, 19], [23, 2, 54, 1, 4, ]]print(\"\\nSum of arr : \", np.sum(arr))print(\"\\nSum of arr : \", np.sum(arr, axis=0))print(\"\\nSum of arr : \", np.sum(arr, axis=1))Sum of arr : 279Sum of arr : [52 25 93 42 67]Sum of arr : [120 75 84]# Using numpy.add()import numpy as npin_arr1 = np.array([[2, -7, 5], [-6, 2, 0]])in_arr2 = np.array([[5, 8, -5], [3, 6, 9]])print(\"1st Input array : \\n\", in_arr1)print(\"2nd Input array : \\n\", in_arr2)out_arr = np.add(in_arr1, in_arr2)print(\"output added array : \\n\", out_arr)1st Input array : [[ 2 -7 5] [-6 2 0]]2nd Input array : [[ 5 8 -5] [ 3 6 9]]output added array : [[ 7 1 0] [-3 8 9]]numpy.concatenate()numpy.concatenate() function concatenate a sequence of arrays along an existing axis.It is used to join two or more arrays# Concatenateimport numpy as nparr1 = np.array([[2, 4], [6, 8]])arr2 = np.array([[3, 5], [7, 9]])print('Array 1: \\n', arr1)print('Array 2: \\n', arr2)gfg_1 = np.concatenate((arr1, arr2), axis=1)print('Horizontally: \\n', gfg_1)gfg_2 = np.concatenate((arr1, arr2), axis=0)print('Vertically: \\n', gfg_2)Array 1: [[2 4] [6 8]]Array 2: [[3 5] [7 9]]Horizontally: [[2 4 3 5] [6 8 7 9]]Vertically: [[2 4] [6 8] [3 5] [7 9]]numpy.append()numpy.append(array, values, axis = None) : appends values along the mentioned axis at the end of the array.It is used to add items/elements/arrays.# numpy.append()import numpy as nparr = np.array([[2, 3, 1], [4, 8, 1]])print('Original Array: \\n', arr)mylist = [[3, 1, 0], [4, 5, 1]] # Dimensions should be the same.print('List: ', mylist)new_arr = np.append(arr, mylist, axis=1)print('\\nAppended array: \\n', new_arr)print()arr_2 = np.append(arr, [[3, 4, 1], [1, 4, 0]], axis=0)print(arr_2)# Error when only one list is given.Original Array: [[2 3 1] [4 8 1]]List: [[3, 1, 0], [4, 5, 1]]Appended array: [[2 3 1 3 1 0] [4 8 1 4 5 1]][[2 3 1] [4 8 1] [3 4 1] [1 4 0]]SortingArranging data in a specific order. We can sort a numpy array in the following ways: numpy.sort(): Returns sorted copy of the array numpy.argsort(): Returns indices that would sort an array numpy.sort_complex(): Sorts an array with real part first, then the imaginary part # SORTING# Using numpy.sort()import numpy as npa = np.array([[12, 15], [10, 1]])print('Array 1: \\n{}\\n'.format(a))arr1 = np.sort(a, axis=0)print(\"Column wise (axis=0) : \\n\", arr1)a = np.array([[10, 15], [12, 1]])print('\\nArray 2: \\n{}\\n'.format(a))arr2 = np.sort(a, axis=1) # Axis=1 and Axis=-1 are the sameprint(\"Row wise (axis=1/-1) : \\n\", arr2)arr1 = np.sort(a, axis=None)print(\"\\nAlong none axis : \\n\", arr1)Array 1: [[12 15] [10 1]]Column wise (axis=0) : [[10 1] [12 15]]Array 2: [[10 15] [12 1]]Row wise (axis=1/-1) : [[10 15] [ 1 12]]Along none axis : [ 1 10 12 15]# SORTING# Using numpy.argsort()import numpy as npa = np.array([9, 3, 1, 7, 4, 3, 6])print('Original array:\\n', a)b = np.argsort(a)print('Sorted indices of original array-&gt;', b)Original array: [9 3 1 7 4 3 6]Sorted indices of original array-&gt; [2 1 5 4 6 3 0]# SORTINGimport numpy as nparr_1 = np.array([4, 8, 1, 2, 9])names = np.array(['A', 'B', 'C', 'D', 'E'])# Low to Highsort_1 = np.sort(arr_1)print('Sorted Array 1: {}\\n'.format(sort_1))# High to Lowsort_2 = np.sort(arr_1)[::-1]print('Sorted Array 2: {}\\n'.format(sort_2))# Sorting namessort_names = names[np.argsort(arr_1)[::-1]]print('Sorted Names: {}'.format(sort_names))Sorted Array 1: [1 2 4 8 9]Sorted Array 2: [9 8 4 2 1]Sorted Names: ['E' 'B' 'A' 'D' 'C']SearchingSearching is an operation or a technique that helps finds the place of a given element or value in the list. This can be done in some of the following ways: numpy.argmax(): Returns indices of the max element of the array in a particular axis. numpy.nanargmax(): Returns indices of the max element of the array in a particular axis ignoring NaNs.The results cannot be trusted if a slice contains only NaNs and Infs. numpy.argmin(): Returns the indices of the minimum values along an axis. numpy.where(): Returns the indices of elements in an input array where the given condition is satisfied. # SEARCHINGimport numpy as nparray = np.arange(12).reshape(3, 4)print(\"INPUT ARRAY : \\n\", array)print(\"\\nMax element : \", np.argmax(array))print(\"Indices of Max element: \", np.argmax(array, axis=0))print(\"Indices of Max element: \", np.argmax(array, axis=1))print(\"Indices of min element : \", np.argmin(array, axis=0))print()# numpy.where()a = np.array([[1, 2, 3], [4, 5, 6]])print('Array 2: \\n', a)print('Indices of elements &lt;4')b = np.where(a &lt; 4)print(b)print(\"Elements which are &lt;4\")print(a[b])INPUT ARRAY : [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]Max element : 11Indices of Max element: [2 2 2 2]Indices of Max element: [3 3 3]Indices of min element : [0 0 0 0]Array 2: [[1 2 3] [4 5 6]]Indices of elements &lt;4(array([0, 0, 0]), array([0, 1, 2]))Elements which are &lt;4[1 2 3]Countingnumpy.count_nonzero() : Counts the number of non-zero values in the array .import numpy as np# Counting a number of non-zero valuesa = np.count_nonzero([[0, 1, 7, 0, 0], [3, 0, 0, 2, 19]])b = np.count_nonzero([[0, 1, 7, 0, 0], [3, 0, 0, 2, 19]], axis=0)print(\"Number of nonzero values is :\", a)print(\"Number of nonzero values is :\", b)Number of nonzero values is : 5Number of nonzero values is : [1 1 1 1 1]" }, { "title": "Analysis of Battery in Low Power Electric Vehicles using Machine Learning", "url": "/posts/ev/", "categories": "Projects, ML", "tags": "ml, ev, matlab", "date": "2022-07-28 12:00:00 -0400", "snippet": "The following project has been done as part of my Practice School-1 cirriculum under CSIR - Central Electronics Engineering Research Institute (CEERI), Pilani. It enabled me to work on real-world issues and provided me with an opportunity to obtain industrial experience under the guidance of Dr. Bhausaheb Ashok Botre, Principal Scientist.AbstractDue to its numerous favorable effects on the environment and society, electric vehicles (EVs) are growing in popularity around the world. This Project Report discusses the use of Machine Learning techniques for the analysis of batteries in low power Electric-tricycle. The report talks about the use of machine learning techniques such as Random Forest (RF) and Gradient Boosting Regression Tree (GBRT) for State of Charge (SOC) estimation and load forecasting. MATLAB simulations for speed control of a BLDC motor using PID controller and a Lithium-ion temperature dependent battery model are used to predict the SOC and the power of the battery. The data generated from these simulations is used to train the ML models.Project Areas: Electric Vehicles, Machine Learning.Keywords: Machine Learning, State of Charge, BMS, Load Forecasting, BLDC motor.IntroductionElectric Vehicles are becoming increasingly integrated in several cities. With increasing EV technology and smart power grids, they have emerged as an alternative to the conventional gas driven vehicles. Being more environmentally friendly, people are rapidly moving towards EVs.Battery charging in EVs is still a major challenge and continuous research efforts are being made to build an efficient system to analyze the various parameters of the batteries being used. Accurate analysis needs to be performed to prevent any battery failure and occurrence of any accidents.Problem StatementThe aim of this project is to analyze batteries in low power Electric Vehicles using Machine Learning techniques. CEERI Pilani has developed an electric assisted tricycle for the outdoor mobility of differently abled persons and aims to improve it by incorporating an efficient Battery Management System using IoT and ML techniques.Challenges AddressedA number of challenges are involved in analyzing the batteries of Electric Vehicles, which include: Prediction of driving range Performance of battery Charge load conditions State of charge (SOC) estimationThe absence of high-quality, full-scale datasets is one of the most critical challenges faced in the accurate prediction of batteries in EVs. But in recent times, one major starting point in the prediction of battery safety is the simulation of the loading conditions. My personal objective is to focus on the EV load conditions and SOC estimation techniques to help build an efficient battery management system (BMS).Analysis of EV charge load conditions implies analyzing the power of the battery during peak demands. It also looks at how charging the EV affects the battery. It is important because EV load balancing instructs the chargers to deliver the right amount of energy and manages the energy flow. So analyzing the energy trends of the battery would tell us its charge load characteristics.The most critical inputs for controlling EV charging and improving power system operation and performance are accurate early knowledge of the EV load demand. Moreover, factors such as driving and travel patterns of each EV lead to a stochastic nature of the EV charging demand.SOC is a number between 0 and 1 that indicates the amount of charge available in the cell relative to the maximum amount of charge. A major challenge with SOC is that it cannot be measured directly but it needs to be inferred based on measurements of voltage, current, temperature and the use of a model of the cell. However, some Simulink models of batteries let us obtain SOC data directly which is extremely beneficial.Machine LearningThere exist a number of traditional techniques for EV load analysis and SOC estimation. But most of them have been proved less efficient in recent times, and with the rise of machine learning, we have a lot more options to help in load forecasting and SOC estimation.A popular way of modeling batteries with the purpose of system design and charge load analysis is to create an electro-thermal analogy for the cell‚Äôs behavior, i.e, to create an equivalent circuit. The topology and parameters of the equivalent circuit should allow us to load it in the same way we would load a battery and observe the same response a battery would have. This includes voltage drop, relaxation times and open circuit voltage, as functions of temperature and state of charge (SOC).The easiest way to estimate SOC is integrating current. It is a computationally cheap method but the disadvantage is that any error in the measurement of current will accumulate over time, making the SOC diverge from its true value.A more advanced method of estimating SOC is the use of observers such as Kalman filters. The Kalman filter takes in the model of the cell and uses a recursive algorithm that continuously predicts a future state and corrects it using measurements performed on the system. But this is computationally very expensive.An alternative would be to use Machine Learning models that take the relevant parameters and use them to predict the desired features. Trained with a sufficiently large amount of data measured under sufficiently varied conditions, ML models would be able to learn the patterns of change in the measurable quantities of the battery and link them to predict the SOC and power/energy trends.Machine learning algorithms provide a variety of approaches for forecasting EV load conditions and SOC estimation. Random Forest (RF) and Gradient Boosting Regression Tree (GBRT) are two such techniques that have lately proven to be useful.Random Forest (RF):The Random Forest algorithm is based on decision trees. A decision tree is a model that follows a path from the branched observations to the outcomes. We use regression trees since the outcomes (power/energy consumption and SOC) have continuous values.Random Forest Regression is a supervised learning approach for regression that employs ensemble learning methods. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.The diagram above shows the structure of a RF. The trees run in parallel with no interaction amongst them. An RF operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.Random Forest Regression is a powerful and precise model. It usually works well on a wide range of issues, including those with non-linear relationships.Gradient Boosting Regression Tree (GBRT):GBRT is also based on decision trees and ensemble learning. It also works on the concept of Boosting, which works on the principle of improving mistakes of the previous learner through the next learner. In boosting, weak learners are used which perform only slightly better than a random chance.In GBRT, we combine many weak learners to come up with one strong learner. The weak learners here are the individual decision trees. All the trees are connected in series and each tree tries to minimize the error of the previous tree. Due to this sequential connection, boosting algorithms are usually slow to learn, but also highly accurate.Overfitting owing to the inclusion of too many trees is a problem that we may face in GBRT but not in Random Forests. Overfitting will not occur in RF if you add too many trees. Although the model‚Äôs accuracy does not improve after a certain point, there is no issue with overfitting. However, when using GBRT, we must be cautious about the amount of trees we choose, as having too many weak learners in the model can lead to data overfitting. As a result, hyperparameter optimization is quite important.SimulationsMATLAB is a great software platform that provides us with a number of tools that can be used to model an EV system. Two Simulink models were created to obtain data used for SOC estimation and load forecasting. Lithium-Ion Temperature Dependent Battery Model Speed Control of a BLDC Motor using PID ControllerThese two individual models were then interfaced together to create a single large model that predicts SOC and power trends in the presence of a BLDC motor as load.Lithium-Ion Temperature Dependent Battery ModelThis model shows the variation in performance of a 7.2 V, 5.4 Ah Lithium-Ion battery. Throughout a discharge and charge operation, the model is exposed to an environment with varying ambient temperature. Its performance is contrasted with the scenario in which temperature effects are disregarded. The temperature dependent battery model functions reasonably well, as seen from the scope. The output voltage and capacity change together with the cell‚Äôs internal temperature as a result of charge (or discharge) heat losses and variations in the outside temperature.The data from the scope can be extracted into an excel sheet, as shown below, containing all the necessary parameters. By using data of only the relevant parameters, ML models can be trained to estimate the SOC and power trends (for load forecasting).Speed Control of a BLDC Motor using PID ControllerThis model displays the functionality of a Brushless DC motor coupled to a PID controller for precision and stability. Ignoring the effects of temperature, a Lithium-Ion battery is connected to the motor. We use the scope to monitor changes in stator currents, motor speed, and electromagnetic torque. The PID controller ensures that the motor‚Äôs speed is consistently maintained at the required rpm specified in the input.Complete ModelThe two models generated above are combined to obtain one single large model. This demo shows the performance of the Lithium-Ion battery in the presence of a load, i.e., BLDC motor, its variation with change in temperature and allows us to measure motor speed as well.The information from the scope can be retrieved into an excel file that includes all the relevant parameters. This complete data can then be used to train the ML models that will accurately predict the SOC and the power/energy consumed for load forecasting.ResultsThe complete MATLAB simulation, however, is extremely heavy and time consuming, and requires higher specs and computational power to run. So the ML models were trained based on data generated from the battery without the presence of a load.DatasetsThe data obtained from the MATLAB simulation in an excel sheet is shuffled randomly and divided into training sets and testing sets.Random ForestScikit learn library is used to run the Random Forest model and is trained on the training dataset. The trained model is then tested on the test dataset and the accuracy is measured.Code:# Importing the librariesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Train settrain = pd.read_csv('train_parameters_updated.csv')energy = train.iloc[:,4].valuessoc = train.iloc[:,2:3].values time = train.iloc[:,0:1].values # X# Test settest = pd.read_csv('test_parameters_updated.csv')energy_test = test.iloc[:,4].valuessoc_test = test.iloc[:,2:3].values time_test = test.iloc[:,0:1].values # X# Predict SOCfrom sklearn.ensemble import RandomForestRegressorregressor_1 = RandomForestRegressor(n_estimators = 10, random_state = 0)regressor_1.fit(time,soc)# Visualizing the SOC Set X_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, soc_pred, color = 'red')plt.plot(X_grid, regressor_1.predict(X_grid), color = 'blue')plt.title('Random Forest Regression')plt.xlabel('Time')plt.ylabel('SOC')plt.show()# Predict Energyfrom sklearn.ensemble import RandomForestRegressorregressor_2 = RandomForestRegressor(n_estimators = 10, random_state = 0)regressor_2.fit(time,energy)energy_pred = regressor_2.predict(time_test)# Visualizing the Energy Set X_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, energy_pred, color = 'red')plt.plot(X_grid, regressor_2.predict(X_grid), color = 'blue')plt.title('Random Forest Regression')plt.xlabel('Time')plt.ylabel('Energy')plt.show()# Accuracy of SOC estimationfrom sklearn.metrics import r2_scorer2_score(soc_pred,soc_test)# Accuracy of Energy estimationfrom sklearn.metrics import r2_scorer2_score(energy_pred,energy_test)Predicting SOC:Accuracy: 99%Predicting Energy:The data for energy is obtained by multiplying the voltage and the current. The trends in energy are used for load forecasting.Accuracy: 99%Model Summary:import sklearn.metrics as metricsmae = metrics.mean_absolute_error(soc_test, soc_pred)mse = metrics.mean_squared_error(soc_test, soc_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(soc_test,soc_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)mae = metrics.mean_absolute_error(energy_test, energy_pred)mse = metrics.mean_squared_error(energy_test, energy_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(energy_test,energy_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)For SOC:MAE: 0.0007033089960029275MSE: 7.885088251654824e-07RMSE: 0.0008879801941290596R-Squared: 0.9999999861769471For Energy:MAE: 0.0042136681167083585MSE: 0.15557361977356302RMSE: 0.3944282187845629R-Squared: 0.9995374972380205Gradient Boosting Regression TreeThis model also uses the Scikit learn library to train the model on the training data. The trained model is then tested on the test dataset and the accuracy is measured.Code:# Load librariesfrom sklearn.ensemble import GradientBoostingRegressorimport numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_errorfrom sklearn.metrics import r2_scoreimport warningswarnings.filterwarnings('ignore')# Train setdata = pd.read_csv('train_parameters_updated.csv')energy = data.iloc[:,4].valuessoc = data.iloc[:,2:3].values time = data.iloc[:,0:1].values # Xgradientregressor = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)# Test settest = pd.read_csv('test_parameters_updated.csv')energy_test = test.iloc[:,4].valuessoc_test = test.iloc[:,2:3].values time_test = test.iloc[:,0:1].values # X# Predict SOCmodel = gradientregressor.fit(time,soc)soc_pred = model.predict(time_test)# Visualizing the SOC Set import matplotlib.pyplot as plt%matplotlib inlineX_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time, soc, color = 'red')plt.title('GBRT')plt.xlabel('Time')plt.ylabel('SOC')plt.show()# Predict Energymodel_2 = gradientregressor.fit(time,energy)energy_pred = model_2.predict(time_test)# Visualizing the Voltage Set import matplotlib.pyplot as plt%matplotlib inlineX_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, energy_pred, color = 'red')plt.plot(X_grid, model_2.predict(X_grid), color = 'blue')plt.title('GBRT')plt.xlabel('Time')plt.ylabel('Energy')plt.show()# Accuracy of SOC estimationr2_score(soc_pred,soc_test)# Accuracy of Energy estimationr2_score(energy_pred,energy_test)Predicting SOC:Accuracy: 93%Predicting Energy:Accuracy: 99%Model Summary:import sklearn.metrics as metricsmae = metrics.mean_absolute_error(soc_test, soc_pred)mse = metrics.mean_squared_error(soc_test, soc_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(soc_test,soc_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)mae = metrics.mean_absolute_error(energy_test, energy_pred)mse = metrics.mean_squared_error(energy_test, energy_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(energy_test,energy_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)For SOC:MAE: 1.594852472588293MSE: 3.6870569829698217RMSE: 1.9201710816929365R-Squared: 0.9353635847153323For Energy:MAE: 0.13236938942063645MSE: 0.18590540743040299RMSE: 0.4311674934760307R-Squared: 0.9994473242666165Comparison:By comparing the accuracies of the two models, RF and GBRT, we see that the RF algorithm performs better than the other over the same training set. While GBRT gives a comparable accuracy as that of RF in predicting the energy, the main difference comes in predicting the SOC.So the RF model would be a better choice for data estimation, as seen from the experiments performed on the data generated from the MATLAB simulations. Regression Algorithm Accuracy (R^2 score) ¬† Mean Absolute Error (MAE) ¬† Root Mean Square Error (RMSE) ¬† ¬† SOC Energy SOC Energy SOC Energy Random Forest 99.9% 99.9% 0.0007 0.0042 0.0008 0.3944 Gradient Boosting Regression Tree 93.5% 99.9% 1.5948 0.1324 1.9202 0.4312 ConclusionThe development of an efficient battery management system using the previously-mentioned methods would help build safer EVs. By making data analysis more practical, the research being conducted would help reduce the gap between the tests in labs and the real-time conditions. This will help prevent any accidents that can be caused by faults in batteries.The data generated from the MATLAB simulations were extracted and the essential parameters served as the input to the two different machine learning algorithms used to predict the SOC and power/energy consumed. Promising results were obtained with more than 90% accuracy in the regression models, and I believe that this can be increased with the help of more data." }, { "title": "Basic Electronics", "url": "/posts/electronics/", "categories": "Blog, Electronics", "tags": "arduino", "date": "2022-07-28 12:00:00 -0400", "snippet": "IntroductionThe following notes were created for Techtainment, an event that I conducted to teach the basics of electronics and programming to people who hadn‚Äôt previously explored the field of robotics.Arduino Uno Tech Specs Operating voltage = 5V Recommended input voltage = 7V-12V Input voltage (limit) = 6-20V DC current per I/O pin = 40mA DC current per 3.3V pin = 50mA Flash memory = 32 kB of which 0.5 kB is used by the bootloader SRAM = 2 kB EEPROM = 1 kB Clock speed = 16 MHz The ATmega328P chip has 28 pins.You can check out the datasheet of the ATmega328P chip if interested.What is the deal with Arduino Uno‚Äôs digital pin 13 LED?Digital pin 13 is harder to use as a digital input than the other digital pins because it has an LED and a resistor attached to it that‚Äôs soldered to the board on most boards. If you enable its internal 20k resistor, it will hang at around 1.7V instead of the expected 5V because the onboard LED and series resistor pull the voltage level down, meaning it always returns a LOW. If you must use digital pin 13 as a digital input, set its pinMode() to INPUT and use an external pull-down resistor.TX/RXOn the Arduino board, pin 0 (RX) and 1 (TX) are used for communicating with the computer. Connecting anything to these pins can interfere with the communication, and can cause failed uploads to the boards.Purpose of TX and RX LEDs:The TX and RX LEDs blink whenever there is a communication between the onboard microcontroller and the computer through the USB to Serial Converter chip present near the USB port.The lighting up of LEDs indicates the direction of flow of data. When a bit of data goes from the Arduino board to the computer, the TX LED glows. When a bit of data goes from the computer to the Arduino, the RX LED glows.When the computer uploads a code to the Arduino, it sends data to the board to set its fuse bits, determines the inputs and outputs to be used and other necessary instructions to make the code work. The Arduino board replies each time data is sent during the upload to tell the comp that it is receiving the instructions. So when the computer uploads code to the board, the TX and RX LEDs flash rapidly, showing the data exchange process. It is so fast that these LEDs seem to be turned on steadily.AREF pinUsed as an analog reference pin for analog inputs.Watch the following video to get a good understanding of why we use the AREF pin: https://youtu.be/qUySekwhXwsDocumentation: analogReference()Crystal OscillatorWhat is an oscillator?In general form, an oscillator is something that creates oscillation, which means that something is moving or swinging back &amp; forth. Now in the context of electrical &amp; electronic engineering oscillations that we talk about is the swinging back &amp; forth of the voltages. An oscillator not only can generate sine waves but it can also generate squarewaves, triangle waves, etc.What are oscillators used for?Oscillators are always used in electrical designs but commonly used for generating radio waves,tone generators,generating counters to keep track of time, and generating clock signals to maintain the speed of the digital processors including computers that we are regularly using.What is the use of a crystal oscillator in Arduino?Arduino crystals are used because it helps when Arduino is dealing with time issues, for example if we build a project that turns a switch A OFF after 15 minutes ON and turn ON switch B, C &amp; D one after another with a 20 minutes step. We can use Arduino software to program Arduino to do that, but how does Arduino calculate the time?The answer is by using a crystal oscillator. The number on the top of the Arduino crystal is 16.000H9H this gives us information about the frequency which is 16,000,000 Hertz or 16 Mhz. This small component can make 16 millions cycles per second.Why does Arduino have a 16 Mhz crystal instead of 32 Mhz or more?The datasheet of the ATmega328P chip says that the maximum frequency should not exceed 20MHz. Hence a clock of 16 Mhz is used.What is the accuracy of the crystal?Arduino crystal has 100ppm accuracy, and these crystals have an error margin of 100 cycles -or+ in each 1 million cycles. This means that the maximum -or+ error in time calculated by Arduino is 30 sec.Blocking CodeWhen a program ‚Äúhalts‚Äù to execute some code, that code that holds up the program is called blocking code. The delay() function is an example of blocking code. This function decreases the tightness of a loop.Blocking code is a generic term given to code that is going to take some time to execute and is going to stop other parts of our program from running while it executes.If you have two repetitive events and those events overlap, then you may find that using the delay function is not the best solution for programming it. We can use an alternative function called millis() instead.The millis() function will be discussed in detail in the upcoming sessions.Does the delay() function pause all activities?Certain things do go on while the delay() function is controlling the ATmega chip, because the delay function does not disable interrupts. Serial communication that appears at the RX pin is recorded, PWM (analogWrite) values and pin states are maintained, and interrupts will work as they should.We will discuss interrupts in detail in the upcoming sessions.LDRLight Dependent Resistor (LDR) is a device whose resistivity varies with the incident EM radiation. Hence they are light-sensitive devices. Also called photoconductors or photoconductive cells. They do not have a polarity.When light is incident on the LDR, electrons from the valence band jump to the conduction band, resulting in a decrease in resistance.We cannot connect an LDR directly to an Arduino pin. This is because on doing so, the current through the LDR changes due to change in its resistance. But the Arduino cannot measure current and it measures only changes in voltage. Therefore in order to use an LDR, we need to connect it in series with another resistor and measure the voltage across that resistor.Ways to Power Up Your Arduino UnoRefer to the following link: Power Scheme MethodsPWM from non-PWM pinsGo through the following simulation: TinkerCADCode explanation:The oscilloscope has a time per division equal to 100ms. The user gives the required duty cycle as input.Let‚Äôs say the duty cycle required is 50%. This means that the signal should stay HIGH for 50% of the time and stay LOW for the remaining 50% of the time. So in one time division, the signal will stay HIGH for 50ms (50% of 100ms = 50ms) and LOW for the remaining 50ms. This can be achieved by keeping a delay equal to the duty cycle, i.e., 50ms in this case.Now let‚Äôs say the duty cycle required is 25%. This means that the signal should stay HIGH for 20% of the time and stay LOW for the remaining 75% of the time. So in one time division, the signal will stay HIGH for 25ms (25% of 100ms = 25ms) and LOW for the remaining 75ms (100ms - 25ms). This can be achieved by keeping a delay equal to the duty cycle, i.e., 25ms in this case, for the HIGH signal, and a delay equal to 75ms (100 - duty cycle) for the LOW signal.So in general, if a particular duty cycle is required, you can give a delay equal to the duty cycle for the HIGH signal, and a delay equal to 100 - duty cycle for the LOW signal.We are using 100 here because the time per division of the oscilloscope is 100ms. This number can be changed depending on the time per division of the oscilloscope.You can also notice how the brightness of the LED changes with change in duty cycle.Servo MotorClosed loop system based on position feedback to control the motion and position of the shaft. The feedback signal is generated by comparing the output signal and reference input signal.The actual position of the shaft is captured by the potentiometer and is fed back to the error detector where it is compared to the target position. The controller tries to minimize this error and corrects the actual position of the motor to match with the target position.Components: DC Motor - high speed, low torque Gearbox - increase torque, reduces rpm Potentiometer - attached to the servo shaft. As the motor rotates, the potentiometer rotates as well. This produces a variable voltage related to the absolute angle of the shaft. Control circuit - potentiometer voltage is compared to the target voltage signal. If needed, it activates an internal H-bridge which enables the motor to rotate in either direction, until the two signals reach a difference of 0.When the shaft of the motor is at the desired position, power supplied to the motor is stopped. If not, the motor is turned in the appropriate direction.The motor‚Äôs speed is proportional to the difference between its actual position and desired position. So if the motor is near the target position, it will turn slowly, otherwise it will turn fast. This is called proportional control.How are Servos controlled?Servos are controlled by sending PWM signals. The PWM sent to the motor determines the position of the shaft, and based on the duration of the pulse sent, the shaft will turn to the desired position.The servo expects to see a pulse every 20ms (frequency=50Hz) and the length of the pulse will determine how far the motor turns, as shown below.When servos are controlled to move, they will move to the position and hold that position. If an external force pushes against the servo while it is holding its position, it will resist from moving out of its position.Torque Ratinghttps://automaticaddison.com/how-to-determine-what-torque-you-need-for-your-servo-motors/HC-SR04Sensor features: Operating voltage = 5V Theoretical measuring distance: 2cm to 450cm Practical measuring distance: 2cm to 80cm Accuracy: 3mm Measuring angle covered: &lt;15 degrees Pulse frequency: 40 kHzWorking:To start measurement, trig pin of the sensor must be given 5V (HIGH) for 10 microseconds. This will initiate the sensor, transmit out 8 cycles of ultrasonic pulse at 40 kHz and wait for the reflected pulse.When the pulse is sent, the echo pin is set from LOW to HIGH automatically. On receiving the reflected signal, the echo pin will go back to LOW and produce a pulse. The length/width of the pulse is proportional to the time it took for the transmitted signal to be detected. The width of the pulse varies between 150us-25ms.If the pulses are not reflected back, the echo signal will timeout after 38ms and return LOW, indicating no presence of an obstacle within the range of the sensor.The trigger of 10us is to allow the firmware in the microcontroller to recognize the input.Why transmit 8 cycles?pulseIn()PIRWorking:The PIR sensor has 2 slots in it, each made up of a special material sensitive to IR. When the sensor is idle, both slots detect the same amount of IR. When a warm body passes by, it first intercepts one half of the sensor, which causes a positive differential change between the two halves. When the warm body leaves the sensing area, the reverse happens,i.e., it generates a negative differential change. These change pulses are what is detected.The BISS0001 decoder chip is used in the PIR sensor. It takes the signal from the sensor and does some minor processing on it to emit a digital output pulse from the analog sensor.UARTThe protocol used by Arduino to communicate with the computer is called UART. UART stands for Universal Asynchronous Receiver Transmitter. It defines a protocol for exchanging serial data between two devices, in this case, between the Arduino and computer.It uses only 2 wires between the transmitter and receiver, TX and RX. The communication can be done is different modes- Simplex - data is sent in one direction only Half duplex - data is sent in both directions but only one at a time Full duplex - data is sent in both directions simultaneouslyUART is Asynchronous, which means that the transmitter and receiver do not share a common clock. The transmitter and receiver must therefore Transmit at the same known speed inorder to have the same bit timing. Most common baud rates used are 4800, 9600, etc. Use the same frame structureFrame structure:In the idle state, when no data is being transmitted, the line is held HIGH. This allows for easy detection of a damaged line/transmitter.The start bit indicates data is coming. It is a transition from the ideal HIGH state to LOW state. User data bits come immediately after the start bit.After the data bits are finished, the stop bits indicate the end of user data.The stop bit is either a transition back to the HIGH or idle state or will remain at the HIGH state for an additional bit time. A second optional stop bit can be configured, usually to give the receiver time to get ready for the next frame, but this is uncommon in practice.Length of data bits: 5-9 bits (usually 7 or 8)Data is typically sent with the least significant bit (LSB) first.An optional parity bit can be used for error detection. It is inserted between the end of the data bits and the stop bit.Even parity: bit is set such that the total no. of 1‚Äôs is evenOdd parity: bit is set such that the total no. of 1‚Äôs is oddIt can be used to detect only a single flipped bit. If more than one bit is flipped, there is no reliable way to detect these using a single parity bit." } ]
