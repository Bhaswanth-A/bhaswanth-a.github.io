[ { "title": "Wheeled Biped", "url": "/posts/wheeled-biped/", "categories": "Projects, Robotics", "tags": "rl, biped, wheeled", "date": "2025-09-03 21:30:00 +0530", "snippet": "In progressWheeled Biped" }, { "title": "Introduction to Reinforcement Learning", "url": "/posts/intro-to-rl/", "categories": "Blog, Robotics", "tags": "learning, rl", "date": "2025-06-09 21:30:00 +0530", "snippet": "In ProgressIntroduction to Reinforcement LearningThis blog is a collection of my notes based on the book ‚ÄúReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto‚Äù.Finite Markov Decision ProcessesContextThe agent-environment interactionReinforcement learning is built around the interaction between an agent and an environment over time. At each step: The agent observes the current situation (state). It chooses an action based on a policy. The environment responds by providing a reward and transitioning to a new state.This back-and-forth loop defines the learning problem.Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized.üí°$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots¬†= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$where $0 \\le \\gamma \\le 1,$ is called the discount rate.The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. If $\\gamma = 0,$ the agent is ‚Äúmyopic‚Äù in being concerned only with maximizing immediate rewards.In case of episodic tasks that have a ‚Äútermination state‚Äù, the discounted rewards can be written asüí°$$G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$$Markov Decision ProcessIn the most general case, the environment‚Äôs response at time $t+1$ can depend on everything that has happened earlier. Thus, the dynamics can be defined by the complete probability distribution:\\[\\Pr\\{ R_{t+1} = r , S_{t+1} = s' \\mid S_0, A_0, R_1, \\ldots, S_{t-1}, A_{t-1}, R_t, S_t, A_t \\}\\]If the state signal has the Markov property, then the environment‚Äôs response at time $t+1$ depends only on the current state and action. In this case, the environment‚Äôs dynamics are simplified to:\\[p(s', r \\mid s, a) = \\Pr\\{ R_{t+1} = r, S_{t+1} = s' \\mid S_t = s, A_t = a \\}\\]Given these dynamics, we can compute everything else we might want to know about the environment.Expected reward for a state‚Äìaction pair:üí°$$r(s,a)= \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a] = \\sum_{r\\in\\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a).$$From definition of conditional expectation\\[\\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a] = \\sum_{r\\in\\mathcal{R}} r \\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a\\}\\]Using law of total probability\\[\\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a\\}= \\sum_{s'\\in\\mathcal{S}} \\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\} \\\\= \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a).\\]Plugging back in, we get\\[r(s,a)= \\sum_{r\\in\\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s',r\\mid s,a)\\]State transition probability:üí°$$p(s'\\mid s,a)= \\sum_{r\\in\\mathcal{R}} \\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\}= \\sum_{r\\in\\mathcal{R}} p(s',r\\mid s,a).$$Expected rewards for state‚Äìaction‚Äìnext-state triples:üí°$$r(s,a,s') = \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a, S_{t+1}=s']= \\frac{\\sum_{r\\in\\mathcal{R}} r p(s',r\\mid s,a)}{p(s'\\mid s,a)}$$Start from conditional expectation\\[r(s,a,s') = \\mathbb{E}[R_{t+1}\\mid S_t=s, A_t=a, S_{t+1}=s'] \\\\ =\\sum_{r\\in\\mathcal{R}} r \\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a, S_{t+1}=s'\\}\\]Then using Bayes‚Äô rule\\[\\Pr\\{R_{t+1}=r \\mid S_t=s, A_t=a, S_{t+1}=s'\\}=\\frac{\\Pr\\{S_{t+1}=s', R_{t+1}=r \\mid S_t=s, A_t=a\\}}{\\Pr\\{S_{t+1}=s' \\mid S_t=s, A_t=a\\}}=\\frac{p(s',r\\mid s,a)}{p(s'\\mid s,a)}.\\]Plugging this in above, we get\\[r(s,a,s') = \\frac{\\sum_{r\\in\\mathcal{R}} r p(s',r\\mid s,a)}{p(s'\\mid s,a)}\\]Value FunctionsState-Value Functionüí°$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]= \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s \\right]$$This is the expected long-term return (sum of discounted rewards) if you start in state $s$ and follow policy $\\pi$.Intuition: ‚ÄúIf I‚Äôm standing in this state, and I keep behaving according to my current policy, how good is this situation in the long run?‚ÄùAction-Value Functionüí°$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]= \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right]$$This is the expected long-term return if you start in state $s,$ take action $a$ immediately, and then afterwards follow policy $\\pi$.Intuition: ‚ÄúIf I‚Äôm in this state and I try this particular move right now, and then keep following my usual policy, how good will things turn out?‚ÄùRelationship between state-value and action-value functionsüí°$$v_\\pi(s) = \\sum_a\\pi(a|s)\\space q_\\pi(s,a)$$The value functions $v_\\pi$ and $q_\\pi$ can be estimated from experience. For example, if an agent follows policy $\\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state‚Äôs value, $v_\\pi(s),$ as the number of times that state is encountered approaches infinity. If separate averages are kept for each action taken in a state, then these averages will similarly converge to the action values, $q_\\pi(s,a).$ These estimation methods are called Monte Carlo methods, and are discussed in the sections below.Bellman Equation\\[v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\\]Consider random variables $X,Y,Z$\\[\\mathbb{E}[X \\mid Z] = \\sum_y \\mathbb{E}[X \\mid Y=y, Z] \\space \\Pr(Y=y \\mid Z)\\]Let\\[X = R_{t+1} + \\gamma G_{t+1}, \\quad Y = A_t, \\quad Z = \\{ S_t=s \\}\\]Then\\[\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s] = \\sum_a \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a] \\Pr(A_t=a \\mid S_t=s) \\\\ v_\\pi(s) = \\sum_a \\pi(a \\mid s) \\space \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a]\\]Applying the same law again\\[\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a]\\\\= \\sum_{s'} \\sum_r \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] \\space p(s',r \\mid s,a)\\]Given $R_{t+1} = r$ we know the first term is just $r.$For the second term, by Markov property, the future return $G_{t+1}$ depends on the future only through $S_{t+1},$ so\\[\\mathbb{E}_{\\pi}[G_{t+1} \\mid S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] = \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s']\\]Therefore the inner expectation becomes\\[r + \\gamma \\space \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s']\\]Plugging this back in, we get\\[v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) \\Big[ r + \\gamma \\mathbb{E}_{\\pi}[ G_{t+1} \\mid S_{t+1}=s'] \\Big]\\]Finally, recognise that $\\mathbb{E}{\\pi}[ G{t+1} \\mid S_{t+1}=s‚Äô] = v_\\pi(s‚Äô)$. So we haveüí°$$v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) \\Big[ r + \\gamma v_{\\pi}(s') \\Big]$$Intuition:In a nutshell, the Bellman equation says that: The value of a state = immediate reward + future value. But instead of computing everything into the infinite future directly, it breaks the problem down recursively.The value of a state $v_\\pi(s)$ is obtained by: Looking at all possible actions $a$ that policy $\\pi$ might choose For each action, look at all possible next states $s‚Äô$ and rewards $r$ that the environment could produce Weight each outcome by how likely it is Add up the immediate reward $r$ plus the discounted value of the next state $\\gamma v_\\pi(s‚Äô)$Optimal Value FunctionsThere exists at least one policy that is better than or equal to all other policies. This is called the optimal policy.Optimal state-value function:üí°$$v_*(s) = \\max_{\\pi} v_\\pi(s)$$for all $s \\in S$.Optimal action-value function:üí°$$q_*(s,a) = \\max_\\pi q_\\pi(s,a)$$for all $s \\in S, \\space a \\in \\mathcal{A}(s).$For the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_$ in terms of $v_$ as:üí°$$q_*(s,a) = \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_{t}=s, A_t=a] $$Bellman Optimality EquationBellman Optimality for state-value function:It expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\\[{v_{*}(s)} = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_{}}(s,a) \\newline = \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[G_t \\middle| S_t = s, A_t = a \\right] \\newline = \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right] \\newline= \\max_{a} \\mathbb{E}_{\\pi_{*}}\\left[ R_{t+1} + \\gamma \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+2} \\middle| S_t = s, A_t = a \\right] \\newline= \\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1}) \\mid S_t = s, A_t = a] \\newline= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s',r} p(s',r \\mid s,a) \\big[ r + \\gamma v_{*}(s') \\big]\\]So,üí°$$v_*(s)= \\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1}) \\mid S_t = s, A_t = a]\\\\ = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s',r} p(s',r \\mid s,a) \\big[ r + \\gamma v_{*}(s') \\big]$$Bellman Optimality for action-value function:üí°$$q_*(s,a) = \\mathbb{E}\\big[R_{t+1} + \\gamma \\max_{a'}q_*(S_{t+1},a') \\mid S_t=s, A_t=a\\big] \\\\= \\sum_{s',r}p(s',r \\mid s,a) \\big[r + \\gamma \\max_{a'}q_*(s',a') \\big]$$Backup diagrams for (a) v‚àó and (b) q‚àóOptimal Value Functions to Optimal PoliciesUsing the Optimal State-Value Function $v_*$:Once we know $v_*,$ we can derive an optimal policy by: Looking at the actions that achieve the maximum in the Bellman optimality equation Any policy that assigns probability only to these maximizing actions is optimalThis process is like a one-step lookahead search ‚Äî $v_*$ already accounts for all long-term returns, so just looking at the immediate action + expected next-state values is enough.Simply put, a policy that is greedy w.r.t $v_{*}$ (chooses the best action based only on short-term consequences evaluated via $v_{*}$) is optimal in the long run, because $v_{*}$ already encodes all future returns.Using the Optimal Action-Value Function $q_*$:With $q_*$ the process is even simpler. For any state $s$, just pick the action $a$ that maximizes $q_*(s,a)$.This is easier because: $q_*$ already represents the cached results of the one-step lookahead No need to know transition probabilities $p(s‚Äô,r \\mid s,a)$ or successor states Directly gives the optimal expected return for each state‚Äìaction pairKey insights: $v_* \\implies$need one-step lookahead to find best actions $q_* \\implies$no lookahead needed, just take the $\\max_a q_*(s,a)$Practical Limits True optimality is rarely achievable Computing the exact optimal policy is usually too expensive (requires solving the Bellman optimality equation exactly). Even with a complete model of the environment, tasks like chess are too complex to solve optimally. Constraints in practice Computation per step is limited (agent can‚Äôt spend forever planning) Memory is limited (can‚Äôt store values for every possible state) Tabular vs Function Approximation Tabular methods: possible when state/action space is small (store values in arrays/tables) Function approximation: required when state space is huge or continuous (use compact parameterized functions, e.g. neural networks) Approximating optimal behavior Not all states matter equally Agents can focus on frequent states and ignore rare states with little effect on overall performance Dynamic ProgrammingPolicy EvaluationPolicy ImprovementPolicy IterationValue IterationAsynchronous Dynamic ProgrammingGeneralized Policy IterationEfficiencyMonte Carlo MethodsTemporal Difference Learning" }, { "title": "Deep Reinforcement Learning", "url": "/posts/deep-rl/", "categories": "Blog, Robotics", "tags": "learning, rl", "date": "2025-06-09 21:30:00 +0530", "snippet": "In ProgressThis blog will only contain pseudocodes and important pointers for my reference.Deep Q Network (DQN)Double DQNActor-Critic NetworkDeterministic Policy Gradient (DPG)Deep Deterministic Policy Gradient (DDPG)Advantage Actor-Critic Network (A2C)Asynchronous Advantage Actor-Critic Network (A3C)Proximal Policy Optimization (PPO)" }, { "title": "Deep Learning - Advanced", "url": "/posts/deep-learning-advanced/", "categories": "Blog, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "In Progress" }, { "title": "Visual Learning and Recognition", "url": "/posts/visual-learning-recognition/", "categories": "CMU MRSD, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "" }, { "title": "Planning and Decision Making", "url": "/posts/planning-decision-making/", "categories": "CMU MRSD, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "" }, { "title": "Milwaukee Tool", "url": "/posts/milwaukee-tool/", "categories": "Projects, Robotics", "tags": "robotics, wheeled, manipulator, planning, control, perception, industry", "date": "2025-06-09 21:30:00 +0530", "snippet": "In ProgressMilwaukee Tool" }, { "title": "Deep Learning", "url": "/posts/deep-learning/", "categories": "CMU MRSD, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "Deep LearningThis series of blogs are my notes from the class 11-785 Introduction to Deep Learning, taught by Bhiksha Raj at CMU. For my own sake of understanding and simplicity, the blog has been divided into 3 broad categories: Perceptrons CNNs, RNNs, and Language Models Advnaced" }, { "title": "Deep Learning - CNNs, RNNs, & Language Models", "url": "/posts/deep-learning-cnn-rnn-lang/", "categories": "Blog, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "In Progress" }, { "title": "Deep Learning - Perceptrons", "url": "/posts/deep-learning-perceptrons/", "categories": "Blog, Robotics", "tags": "learning, nnets", "date": "2025-06-09 21:30:00 +0530", "snippet": "In ProgressNeural NetworksDepth - length of longest path from source to sinkLayer - Set of all neurons which are all at the same depth with respect to the sourceGradientFor a scalar function $f(X)$ with a multivariate input $X$:\\[df\\left( x\\right) =\\nabla _{x}f\\left( x\\right) dx\\]\\[\\nabla _{x}f\\left( x\\right) =\\left[ \\dfrac{\\partial f}{\\partial x_{1}}\\dfrac{\\partial f}{\\partial x_{2}}\\ldots \\dfrac{\\partial f}{\\partial x_{n}}\\right]\\]Because it is a dot product, for an increment $dX$ of any given length, $df$ is maximum if the increment $dX$ is aligned with the gradient direction $\\nabla _{x}f\\left( x\\right)^T$. So the gradient is the direction of steepest ascent.So if you want a function to decrease, your increment should be in the direction of $-\\nabla _{x}f\\left( x\\right)^T$.To find a maximum move in the direction of gradient:\\[x^{k+1} = x^k + \\eta^k \\nabla_x f(x^k)^T \\\\[1em]\\]To find a minimum move in the direction of gradient:\\[x^{k+1} = x^k - \\eta^k \\nabla_x f(x^k)^T \\\\[1em]\\]There are many solutions to choosing step size $\\eta^k$.Hessian\\[\\nabla^2_{x} f(x_1, \\ldots, x_n) = \\begin{bmatrix}\\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2}\\end{bmatrix}\\]NetworkA continuous activation function applied to an affine function of the inputs\\[y = f\\left( \\sum_i w_i x_i + b \\right) \\\\[1em]y = f(x_1, x_2, \\ldots, x_N; W)\\]Activation Functions and DerivativesSigmoid:\\[f(z) = \\frac{1}{1 + \\exp(-z)} \\\\f'(z) = f(z)(1 - f(z)) \\\\[1.5em]\\]Tanh:\\[f(z) = \\tanh(z) \\\\f'(z) = 1 - f^2(z)\\]ReLU:\\[f(z) =¬†\\begin{cases}z, &amp; z \\geq 0 \\\\0, &amp; z &lt; 0\\end{cases} \\\\f'(z) =¬†\\begin{cases}1, &amp; z \\geq 0 \\\\0, &amp; z &lt; 0\\end{cases}¬†\\]Softplus:\\[f(z) = \\log(1 + \\exp(z)) \\\\f'(z) = \\frac{1}{1 + \\exp(-z)}\\]Softmax:\\[f(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\\\f'(z_i) = \\begin{cases}f(z_i)(1 - f(z_i)), &amp; \\text{if } i = j \\\\- f(z_i) f(z_j), &amp; \\text{if } i \\ne j\\end{cases}\\]KL DivergenceMeasures how different two probability distributions are. In classification, $Y$ is the predicted distribution and $d$ is the ground truth, usually a one-hot encoding.\\[KL(Y, d) = \\sum_i d_i \\log \\frac{d_i}{y_i} \\\\\\]For one-hot, $d$ simplifies to\\[KL(Y, d) = -\\log y_c\\]This means that it is minimized when correct class is predicted with the highest probability $y_c \\rightarrow 1$.Gradient:\\[\\frac{d}{dy_c} KL(Y, d) = -\\frac{1}{y_c}\\]Training by Back PropagationForward PassSetting $y_i^{(0)} = x_i$ and $w_{0j}^{(k)} = b_j^{(k)}$. Let the bias equal $1$ for simplicity.For layer 1,\\[z_j^{(1)} = \\sum_i w_{ij}^{(1)}y_i^{(0)} \\\\ y_j^{(1)} = f_1(z_j^{(1)})\\]For layer 2,\\[z_j^{(2)} = \\sum_i w_{ij}^{(2)}y_i^{(1)} \\\\ y_j^{(2)} = f_2(z_j^{(2)})\\]Similarly,\\[z_j^{(N)} = \\sum_i w_{ij}^{(N)}y_i^{(N-1)} \\\\ y_j^{(N)} = f_N(z_j^{(N)})\\]Backward PassStep 1: Initialize Gradient at Output LayerWe start by computing the gradient of the loss w.r.t. the network output:\\[\\frac{\\partial Div}{\\partial y^{(N)}_i} = \\frac{\\partial Div(Y, d)}{\\partial y_i}\\]Then, propagate this to the pre-activation output $z^{(N)}$:\\[\\frac{\\partial Div}{\\partial z^{(N)}_i} = \\frac{\\partial y_i^{(N)}}{\\partial z_i^{(N)}} \\cdot \\frac{\\partial Div}{\\partial y^{(N)}_i} =f_N'\\left(z^{(N)}_i\\right) \\cdot \\frac{\\partial Div}{\\partial y^{(N)}_i}\\]In case of a vector activation function, such as the softmax function, $y_i^{(N)}$ is influenced by every $z_i^{(N)}$:\\[\\frac{\\partial Div}{\\partial z_i^{(N)}} = \\sum_j \\frac{\\partial y_j^{(N)}}{\\partial z_i^{(N)}} \\cdot \\frac{\\partial Div}{\\partial y_j^{(N)}}\\]Step 2: Backpropagation Through LayersLoop from layers $k = (N-1) \\rightarrow 0$:For each layer $k$ and for each neuron $i$ in that layer:Compute gradient of loss w.r.t. activation:\\[\\frac{\\partial Div}{\\partial y^{(k)}_i} = \\sum_j w_{ij}^{(k+1)} \\cdot \\frac{\\partial Div}{\\partial z^{(k+1)}_j}\\]Chain through activation function:\\[\\frac{\\partial Div}{\\partial z^{(k)}_i} = \\frac{\\partial y_i^{(k)}}{\\partial z_i^{(k)}} \\cdot \\frac{\\partial Div}{\\partial y^{(k)}_i} = f_k'\\left(z^{(k)}_i\\right) \\cdot \\frac{\\partial Div}{\\partial y^{(k)}_i}\\]Step 3: Gradient w.r.t. WeightsFor each weight connecting neuron $i$ in layer $k$ to neuron $j$ in layer $k$:\\[\\frac{\\partial Div}{\\partial w_{ij}^{(k)}} = y^{(k-1)}_i \\cdot \\frac{\\partial Div}{\\partial z^{(k)}_j}\\]Step 4: Updating WeightsActual loss is the sum of the divergence over all training instances:\\[Loss = \\frac{1}{|\\{X\\}|}\\sum_X Div(Y(X), d(X))\\]Actual gradient is the average of the derivatives computed for each training instance:\\[\\nabla_W Loss = \\frac{1}{|\\{X\\}|}\\sum_X \\nabla_W Div(Y(X), d(X))\\]\\[W \\leftarrow W - \\eta \\nabla_W Loss^T\\]SummaryVector Formulation\\[\\mathbf{z}_k =¬†\\begin{bmatrix}z^{(k)}_1 \\\\z^{(k)}_2 \\\\\\vdots \\\\z^{(k)}_{D_k}\\end{bmatrix}\\qquad\\mathbf{y}_k =¬†\\begin{bmatrix}y^{(k)}_1 \\\\y^{(k)}_2 \\\\\\vdots \\\\y^{(k)}_{D_k}\\end{bmatrix}\\]\\[\\mathbf{W}_k =\\begin{bmatrix}w^{(k)}_{11} &amp; w^{(k)}_{21} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}1} \\\\w^{(k)}_{12} &amp; w^{(k)}_{22} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}2} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\w^{(k)}_{1D_k} &amp; w^{(k)}_{2D_k} &amp; \\cdots &amp; w^{(k)}_{D_{k-1}D_k}\\end{bmatrix}\\qquad\\mathbf{b}_k =¬†\\begin{bmatrix}b^{(k)}_1 \\\\b^{(k)}_2 \\\\\\vdots \\\\b^{(k)}_{D_k}\\end{bmatrix}\\]\\[\\mathbf{z_k} = \\mathbf{W_ky_{k-1} + b_k} \\\\ \\mathbf{y_k} = \\mathbf{f_k(z_k)}\\]Setup: Let $\\mathbf{y_n = Y}$, the network output Let $\\mathbf{y_0 = X}$, the input Initialize:\\[\\nabla_{\\mathbf{y}_N} Div = \\nabla_{\\mathbf{Y}} Div\\]For each layer $k = N \\rightarrow 1$:Compute the Jacobian of activation:\\[J_{\\mathbf{y}_k}(\\mathbf{z}_k) = \\frac{\\partial \\mathbf{y}_k}{\\partial \\mathbf{z}_k}\\]This is a matrix of partial derivatives:\\[J_{\\mathbf{y}}(\\mathbf{z}) =\\begin{bmatrix}\\frac{\\partial y_1}{\\partial z_1} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial z_D} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial y_M}{\\partial z_1} &amp; \\cdots &amp; \\frac{\\partial y_M}{\\partial z_D}\\end{bmatrix}\\]Backward recursion step:\\[\\nabla_{\\mathbf{z}_k} Div = \\nabla_{\\mathbf{y}_k} Div \\cdot J_{\\mathbf{y}_k}(\\mathbf{z}_k)\\]\\[\\nabla_{\\mathbf{y}_{k-1}} Div = \\nabla_{\\mathbf{z}_k} Div \\cdot \\mathbf{W}_k\\]Gradient Computation:\\[\\nabla_{\\mathbf{W}_k} Div = \\mathbf{y}_{k-1} \\cdot \\nabla_{\\mathbf{z}_k} Div\\]\\[\\nabla_{\\mathbf{b}_k} Div = \\nabla_{\\mathbf{z}_k} Div\\]SummaryImportant Points Backpropagation will often not find a separating solution even though the solution is within the class of functions learnable by the network, because the separable solution is not a feasible optimum for the loss function. It is minimally changed by new training instances ‚Äî doesn‚Äôt swing wildly in response to small changes to the input It prefers consistency over perfection, due to which it works better even if there are outliers It is a low-variance estimator, at the potential cost of bias Minimizing the differentiable loss function does not imply minimizing the classification error.Convergence of Gradient DescentCovergence RateIt measures how quickly an iterative optimization algorithm approaches the solution.\\[R = \\left| \\frac{f(x^{(k+1)}) - f(x^*)}{f(x^{(k)}) - f(x^*)} \\right|\\]Where: $x^{(k)}$: point at iteration $k$ $x^*$: optimal point (solution) $f(x)$: objective functionIf $R&lt;1$, that means that the function value is getting closer to the optimum and is hence converging. The smaller the $R$, the faster the convergence.If $R$ is a constant or upper-bounded, then the algorithm has linear convergence. This means that the difference between the function value and the optimum shrinks exponentially with iterations.\\[|f(x^{(k)}) - f(x^*)| \\leq R^k \\cdot |f(x^{(0)}) - f(x^*)|\\]Convergence for Quadratic SurfacesWhat is the optimal step size $\\eta$ to reach the minimum value of the error fastest? Consider a general quadratic function:\\[E(w) = \\frac{1}{2}aw^2 + bw + c\\] Gradient descent update:\\[w^{(k+1)} = w^{(k)} - \\eta \\frac{dE(w^{(k)})}{dw}\\] Taylor expansion of $E(w)$ around $w^{(k)}$:\\[E(w) \\approx E(w^{(k)}) + E'(w^{(k)})(w - w^{(k)}) + \\frac{1}{2} E''(w^{(k)})(w - w^{(k)})^2\\] Newton‚Äôs method gives:\\[w_{\\text{min}} = w^{(k)} - \\left(E''(w^{(k)})\\right)^{-1} E'(w^{(k)})\\] Optimal learning rate:\\[\\eta_{\\text{opt}} = \\left(E''(w^{(k)})\\right)^{-1} = a^{-1}\\]Effect of step size $\\eta$: $\\eta &lt; \\eta_{opt} \\rightarrow$ monotonic convergence $\\eta = \\eta_{opt} \\rightarrow$ fast convergence in one step $\\eta_{opt}&lt;\\eta &lt; 2\\eta_{opt} \\rightarrow$ oscillating convergence $\\eta \\geq 2\\eta_{opt} \\rightarrow$ divergenceConvergence for Multivariate Quadratic Functions General form of a quadratic convex function:\\[\\mathbf{w} = [w_1, w_2, ..., w_N] \\\\ E(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T A \\mathbf{w} + \\mathbf{w}^T \\mathbf{b} + c\\] If $A$ is diagonal:\\[E(\\mathbf{w}) = \\frac{1}{2} \\sum_i (a_{ii} w_i^2 + b_i w_i) + c\\]This means the cost function is a sum of independent univariate quadratics. Each direction $w_i$ is uncoupled.Equal-Value Contours: For diagonal $A$, the contours are ellipses aligned with coordinate axes. Each coordinate‚Äôs behavior is independent of the others.Optimal Step Size:For each dimension $i$:\\[\\eta_{i,\\text{opt}} = \\frac{1}{a_{ii}} = \\left( \\frac{\\partial^2 E}{\\partial w_i^2} \\right)^{-1}\\]Each coordinate has a different optimal learning rate.Problem with Vector Update Rule:Conventional update applies the same step size $\\eta$ to all coordinates. An issue with this is that one direction might converge optimally while another direction might diverge due to too large a step.\\[\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\eta \\nabla_{\\mathbf{w}} E\\]Safe Learning Rate Rule to Avoid Divergence:\\[\\eta &lt; 2 \\cdot \\min_i \\eta_{i,\\text{opt}}\\]This guarantees convergence but slows down overall learning.Generic Convex Functions We can apply Taylor expansion to approximate any smooth convex function:\\[E(\\mathbf{w}) \\approx E(\\mathbf{w}^{(k)}) + \\nabla_{\\mathbf{w}} E(\\mathbf{w}^{(k)})(\\mathbf{w} - \\mathbf{w}^{(k)})+ \\frac{1}{2} (\\mathbf{w} - \\mathbf{w}^{(k)})^T H_E(\\mathbf{w}^{(k)})(\\mathbf{w} - \\mathbf{w}^{(k)})\\] $H_E$ is the Hessian of second derivatives and measures the curvature of loss function Optimal step size is inversely proportional to eigenvalues of the Hessian The eigenvalues give curvature in orthogonal directions For the smoothest convergence, the eigenvalues must all be equal Convergence Challenges In high dimensions, convergence becomes harder to control Ideally, the step size $\\eta$ should work for both:\\[\\max_i \\eta_{i,\\text{opt}}, \\quad \\min_i \\eta_{i,\\text{opt}}\\] If the following condition number is large, then convergence is slow and unstable.\\[\\frac{\\max_i \\eta_{i,\\text{opt}}}{\\min_i \\eta_{i,\\text{opt}}}\\]Decaying Learning RateThe loss surface has many saddle points and gradient descent can stagnate on saddle points Start with a large learning rate to explore fast Gradually reduce step size to fine-tune near the minimum Prevents overshooting and bouncing around the optimumCommon Decay Schedules:\\[\\text{Linear:} \\quad \\eta_k = \\frac{\\eta_0}{k + 1}\\]\\[\\text{Quadratic:} \\quad \\eta_k = \\frac{\\eta_0}{(k + 1)^2}\\]\\[\\text{Exponential:} \\quad \\eta_k = \\eta_0 \\cdot e^{-\\beta k}, \\quad \\beta &gt; 0\\]Common Approach for Neural Networks: Train with a fixed learning rate until the validation loss stagnates Reduce learning rate:\\[\\eta \\leftarrow \\alpha \\eta \\quad \\text{(e.g., } \\alpha = 0.1 \\text{)}\\] Resume training from the same weights, repeat as neededRProp Resilient Propagation is a first-order optimization algorithm that adjusts step size independently for each parameter It doesn‚Äôt rely on the gradient magnitude, rather only on the sign of the gradient It is more robust than vanilla gradient descent Does not need a global learning rate Doesn‚Äôt require Hessian or curvature information and doesn‚Äôt assume convexityThe core idea is that at each step: If the gradient sign has not changed ‚Üí increase step size in the same direction If the gradient sign has flipped ‚Üí reduce step size and reverse direction (overshoot detected)Algorithm: For each layer $l$, for each parameter $w_{l,i,j}$:\\[\\Delta w_{l, i, j} &gt; 0 \\\\ \\text{prevD}(l, i, j) = \\frac{d \\, \\text{Loss}(w_{l, i, j})}{d w_{l, i, j}} \\\\ \\Delta w_{l, i, j} = \\text{sign(prevD}(l, i, j)) \\cdot \\Delta w_{l, i, j}\\] While not converged: Update parameter: \\[w_{l, i, j} = w_{l, i, j} - \\Delta w_{l, i, j}\\] Recompute gradient: \\[D(l, i, j) = \\frac{d \\, \\text{Loss}(w_{l, i, j})}{d w_{l, i, j}}\\] Check sign consistency: If:\\[\\text{sign(prevD}(l, i, j)) == \\text{sign}(D(l, i, j))\\] Then,\\[\\Delta w_{l, i, j} = \\min(\\alpha \\cdot \\Delta w_{l, i, j}, \\Delta_{\\max}) \\\\ \\text{prevD}(l, i, j) = D(l, i, j)\\] Else undo step:\\[w_{l, i, j} = w_{l, i, j} + \\Delta w_{l, i, j} \\quad \\\\ \\Delta w_{l, i, j} = \\max(\\beta \\cdot \\Delta w_{l, i, j}, \\Delta_{\\min})\\] " }, { "title": "Robot Autonomy", "url": "/posts/robot-autonomy-25/", "categories": "CMU MRSD, Robotics", "tags": "manipulators, franka", "date": "2025-03-09 21:30:00 +0530", "snippet": "Adaptive Pixel Art using a Franka ArmDemoMinimum Viable ProductStretch GoalImages Ground Truth Franka Pixel Art Report Your browser does not support PDFs. Please download the report here.Presentation Your browser does not support PDFs. Please download the report here." }, { "title": "Learning for 3D Vision", "url": "/posts/learning-3d-vision-25/", "categories": "CMU MRSD, Robotics", "tags": "computer vision, deep learning, gaussian splatting, nerf, multi-view geometry", "date": "2025-03-09 21:30:00 +0530", "snippet": "Assignment 1: Rendering Basics with PyTorch3DQuestions: Github Assignment 11. Practicing with Cameras1.1 360-degree RendersUsage:python -m submissions.src.render_360 --num_frames 100 --fps 15 --output_file submissions/360-render.gif360-degree gif video that shows many continuous views of the provided cow mesh:1.2 Re-creating the Dolly ZoomUsage:python -m starter.dolly_zoom --num_frames 20 --output_file submissions/dolly.gifMy recreated Dolly Zoom effect:2. Practicing with Meshes2.1 Constructing a TetrahedronUsage:python -m submissions.src.mesh_practice --shape tetrahedron --num_frames 50 --fps 15 --output_file submissions/tetrahedron.gif --image_size 512360-degree gif animation of tetrahedron:Number of vertices = 4 Number of faces = 42.2 Constructing a CubeUsage:python -m submissions.src.mesh_practice --shape cube --num_frames 50 --fps 15 --output_file submissions/cube.gif --image_size 512360-degree gif animation of cube:Number of vertices = 8 Number of faces = 123. Re-texturing a meshChosen colors: color1 = [0, 1, 1] color2 = [1, 1, 0]Usage:python -m submissions.src.retexturing_mesh --num_frames 50 --fps 15 --output_file submissions/retexture_mesh.gif --image_size 512Gif of rendered mesh:4. Camera Transformations$1)$ Rotate about z-axis by -90 degrees:$R_{relative} = [[\\cos(-\\pi/2), -\\sin(-\\pi/2), 0], [\\sin(-\\pi/2), \\cos(-\\pi/2), 0], [0, 0, 1]]$Use original translation matrix: $T_{relative} = [0, 0, 0]$$2)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$Move along z-axis by 2: $T_{relative} = [0, 0, 2]$$3)$ Keep original rotation matrix: $R_{relative} = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$Move along x-axis by 0.5 and along y-axis by -0.5: $T_{relative} = [0.5, -0.5, 0]$$4)$ Rotate along y-axis by 90 degrees: $R_{relative} = [[\\cos(\\pi/2), 0, \\sin(\\pi/2)], [0, 1, 0], [-\\sin(\\pi/2), 0, \\cos(\\pi/2)]]$Move along x-axis by -3 and along z-axis by 3: $T_{relative} = [-3, 0, 3]$5. Rendering Generic 3D Representations5.1 Rendering Point Clouds from RGB-D ImagesUsage:python -m submissions.src.pcl_render --image_size 512Gif of point cloud corresponding to the first image:Gif of point cloud corresponding to the second image:Gif of point cloud formed by the union of the first 2 point clouds:5.2 Parametric FunctionsUsage:python -m submissions.src.torus_render --function parametric --image_size 512 --num_samples 500Parametric equations of Torus:\\[x = (R + r\\cos\\theta)\\cos\\phi \\\\y = (R + r\\cos\\theta)\\sin\\phi \\\\z = r\\sin\\theta\\]where\\(\\theta \\in [0,2\\pi) \\\\\\phi \\in [0,2\\pi)\\)The major radius $R$ is the distance from the center of the tube to the center of the torus and the minor radius $r$ is the radius of the tube360-degree gif of torus, with visible hole:Parametric equations of Superquadric Surface:\\[x = a(\\cos\\theta)^m(\\cos\\phi)^n \\\\y = b(\\cos\\theta)^m(\\sin\\phi)^n \\\\z = c(\\sin\\theta)^m\\]where\\(\\theta \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}] \\\\\\phi \\in [0,2\\pi)\\)360-degree gif of Superquadric Surface:5.3 Implicit SurfacesUsage:python -m submissions.src.torus_render --function implicit --image_size 512Implicit equation of torus:\\[F(X,Y,Z) = (R - \\sqrt{X^2+Y^2})^2 + Z^2 - r^2\\]360-degree gif of torus, with visible hole:Implicit equation of Superquadric Surface:\\[F(X,Y,Z) = \\bigg(\\bigg(\\bigg\\rvert\\frac{X}{a}\\bigg\\rvert \\bigg)^\\frac{2}{n} + \\bigg(\\bigg\\rvert\\frac{Y}{b}\\bigg\\rvert \\bigg)^\\frac{2}{n} \\bigg)^\\frac{n}{m} + \\bigg(\\bigg\\rvert\\frac{Z}{c}\\bigg\\rvert \\bigg)^\\frac{2}{m} - 1\\]360-degree gif of Superquadric Surface:Tradeoffs between rendering as a mesh vs a point cloud: Method of Generation: Point Clouds: Formed by directly sampling a parametric function. Meshes: Built by voxelizing a 3D space, sampling an implicit function, and then extracting surfaces using the Marching Cubes algorithm. Rendering Speed: Point Clouds: Faster to render since they simply use sampled points without extra processing. Meshes: Slower because they need additional steps like voxelization and surface extraction before rendering. Accuracy &amp; Visual Quality: Point Clouds: More accurate at capturing fine details because each point represents a sampled location. However, they don‚Äôt have surfaces, making shading and texturing more difficult. Meshes: Can be less accurate due to voxelization, but increasing the resolution can improve precision. They also provide continuous surfaces, which makes them easier to texture and shade. Computational Efficiency: Point Clouds: Easier to rotate, scale, and modify since they are just a collection of points. Meshes: More computationally expensive to modify because updating a mesh requires adjusting vertex positions and their connections. 6. Do Something FunHere is a 360 degree view of a cottage and also a dolly zoom view:Usage:python -m submissions.src.fun --function full --image_size 512 --output_file submissions/cottage_render_360.gifUsage:python -m submissions.src.fun --function dolly --image_size 512 --output_file submissions/cottage_dolly.gif(Extra Credit) 7. Sampling Points on MeshesAssignment 2: Single View to 3DQuestions: Github Assignment 20. SetupDownloaded the shapenet single-class dataset. Unzipped the dataset and set the appropriate path in dataset_location.py.1. Exploring loss functions1.1. Fitting a voxel gridTo align a predicted voxel grid with a target shape, I used a binary cross-entropy (BCE) loss function. A 3D voxel grid consists of 0 (empty) and 1 (occupied) values, making this a binary classification problem where we predict occupancy probabilities of each voxel.Implementation:def voxel_loss(voxel_src,voxel_tgt):\t# voxel_src: b x h x w x d\t# voxel_tgt: b x h x w x d\tvoxel_src.unsqueeze(1)\tvoxel_tgt.type(dtype=torch.LongTensor)\t\tloss = torch.nn.functional.binary_cross_entropy(voxel_src, voxel_tgt)\treturn lossUsage:python fit_data.py --type 'vox' Ground Truth Optimized Voxel I trained the data for 10000 iterations.1.2. Fitting a point cloudUsage:python fit_data.py --type 'point' Ground Truth Optimized Point Cloud def chamfer_loss(point_cloud_src,point_cloud_tgt):\t# point_cloud_src, point_cloud_src: b x n_points x 3 \tp1 = knn_points(point_cloud_src, point_cloud_tgt)\tp2 = knn_points(point_cloud_tgt, point_cloud_src)\tloss_chamfer = torch.mean(torch.sum(p1.dists + p2.dists))\t\treturn loss_chamfer1.3. Fitting a meshUsage:python fit_data.py --type 'mesh' Ground Truth Optimized Mesh def smoothness_loss(mesh_src):\tloss_laplacian = mesh_laplacian_smoothing(mesh_src)\treturn loss_laplacian2. Reconstructing 3D from single view2.1. Image to voxel grid Input RGB Ground Truth Mesh Ground Truth Voxel Predicted 3D Voxel I implemented the decoder architecture from the paper Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images.For the encoder, I used the pre-trained ResNet-18 model, which computes a set of features for the decoder to recover the 3D shape of the object.The decoder is responsible for transforming information of 2D feature maps into 3D volumes. I specifically implemented a slightly modified version of the Pix2Vox-F architecture from the above paper. The input of the decoder is of size [batch_size x 512] and the output is [batch_size x 32 x 32 x 32]. The decoder contains five 3D transposed convolutional layers. The first four transposed convolutional layers are of kernel size $4^3$, with stride of $2$ and padding of $1$. The last layer has a kernel of size $1^3$. Each transposed convolutional layer is followed by a LeakyReLU activation function, except for the last layer which is followed by a sigmoid activation function. The number of output channels for each layer follows the Pix2Vox-F configuration: 128 -&gt; 64 -&gt; 32 -&gt; 8 -&gt; 1.I trained the model for 10000 iterations, with the default batch size of 32 and learning rate of 4e-4.Usage:python train_model.py --type 'vox' --max_iter 10000 --save_freq 500 python eval_model.py --type 'vox' --load_checkpoint2.2. Image to point cloud Input RGB Ground Truth Mesh Ground Truth Point Cloud Predicted 3D Point Cloud I used an approach similar to the Pix2Vox-F decoder that I implemented above. The ResNet-18 model encodes the input images into feature maps, and a decoder reconstructs the 3D shape of the object from them.The decoder takes in an input of size [batch_size x 512] and gives an output of size [batch_size x n_points x 3]. The decoder architecture comprises of 4 fully connected layers, three of which are followed by a LeakyReLU activation function.I trained the model for 2000 iterations, with the default batch size of 32 and learning rate of 4e-4.Usage:python train_model.py --type 'point' --max_iter 2000 --save_freq 500 --n_points 5000 python eval_model.py --type 'point' --load_checkpoint --n_points 50002.3. Image to mesh Input RGB Ground Truth Mesh Predicted Mesh Instead of encoding an image like I did in case of image to voxel and image to point cloud, the meshes are constructed from an icosphere mesh. The purpose of the decoder is to refine this initial mesh by giving per-vertex displacement vector as an ouput.The decoder architecture that I implemented is very similar to that in case of image to point cloud, as mentioned above. It takes an input of size [batch_size x 512] and gives an output of size [batch_size x num_vertices x 3]. It comprises of 4 fully connected layers, three of which are followed by a ReLU activation function.I trained the model for 2000 iterations, with the default batch size of 32, learning rate of 4e-4.Usage:python train_model.py --type 'mesh' --max_iter 2000 --save_freq 500python eval_model.py --type 'mesh' --load_checkpoint2.4. Quantitative comparisionsF1-score curves at different thresholds: Voxel Grid Point Cloud Mesh From the above plots, we can infer that the point cloud model performed the best, giving the highest F1-score, followed by the mesh model and the voxel model.Intuitively, I think the reason the point cloud outperformed the voxel and mesh models is because it aligns well with the evaluation method, which compares points directly from the network output to the ground truth. Since point clouds don‚Äôt need to define surfaces or connections, they are more flexible and avoid errors caused by surface sampling. This makes them easier to optimize and more accurate in reconstruction.The mesh model performed slightly worse primarily due to the challenges associated with sampling points from a continuous surface. Unlike point clouds, where each output point is directly predicted, meshes require proper face orientation and connectivity. Due to this, the sampled points might not always align perfectly with the ground truth, especially when dealing with complex geometries like thin structures (legs of the chair).The voxel model has the lowest F1-score because representing 3D space as voxels limits a lot of detail and accuracy. Fine details can be lost due to this fixed resolution and sampled points may not always align perfectly with the object‚Äôs actual surface, affecting evaluation results.2.5. Analyse effects of hyperparams variationsI have chosen to vary the w_smooth hyperparameter and analyze the changes in the mesh model prediction. The default value of w_smooth is 0.1, and its results have been shown in Section 2.3. I sampled 4 other values of w_smooth - 0.001, 0.01, 1, 10. Input RGB Ground Truth Mesh w_smooth=0.001 w_smooth=0.01 w_smooth=0.1 (Default) w_smooth=1 w_smooth=10 F1-score curves for different variations in w_smooth for the mesh model: w_smooth=0.001 w_smooth=0.01 w_smooth=0.1 (Default) w_smooth=1 w_smooth=10 Avg F1-score@0.05 = 72.977 Avg F1-score@0.05 = 72.133 Avg F1-score@0.05 = 70.951 (Default) Avg F1-score@0.05 = 71.834 Avg F1-score@0.05 = 72.337 For low values of w_smooth = 0.001, 0.01: They preserve the fine details but introduce noise and distortions It results in rough and fragmented surfaces They show a slightly higher F1-score because they retain geometric detailsFor high values of w_smooth = 1, 10: They produce cleaner and more smooth meshes with reduced artifacts It over-smooths the surface, causing loss of sharp details The F1-score improves slightly likely because they more likely fall within the threshold radius of the ground truthThe default value of w_smooth = 0.1 falls in between the above two categories.2.6. Interpret your modelPer-Point Error Visualization Input RGB Ground Truth Point Cloud Predicted 3D Point Cloud Per-Point Error I used per-point error to gauge how well each predicted 3D point matches its corresponding point in the ground truth. In my approach, I compute the distance between each point in my reconstructed point cloud and its nearest neighbor in the ground truth. Then, I color-code these distances such that points with very small errors appear in cool colors (blue), while those with larger errors show up in warm colors (red).From the above gifs, we can clearly see that some regions are rendered in cool tones, which tells me that my model is accurately capturing those parts of the object, such as the seat surface or the main body of the chair. On the other hand, areas highlighted in warm colors reveal where the model struggles, like along the thin chair legs or at complex curves of the backrest.This visualization pinpoints the exact regions that need improvement.Failure Case AnalysisIn analyzing my 3D voxel model‚Äôs predictions, I noticed that while it reconstructs the backrest of chairs quite well, it struggles significantly with legs, seats, and unusual shapes. These failure cases provide valuable insight into the model‚Äôs learning behavior and what its limitations are. Legs:Chair legs vary widely in shape, thickness, and placement across different samples in the dataset. Some chairs have four standard legs, while others may have a single central support or a complex curved base. Because the model tries to generalize patterns across the dataset, it struggles to reconstruct legs consistently. Additionally, legs are usually thin and small compared to the rest of the chair, and this makes them more prone to voxelization errors. Seats:Many chair designs have gaps in them, which makes it challenging for the model to learn and also more prone to voxelization errors. Since the model tries to reconstruct a smoothed version of objects, it often fails to represent holes correctly, either closing them off entirely or introducing unexpected artifacts. Unusual shapes:Some chairs in the dataset have very unique designs. Since the model is trained on a limited dataset, it may not have seen enough similar examples to generalize well. 3. Exploring other architectures / datasets3.3 Extended dataset for trainingdataset_location.py updated to include the extended dataset containing three classes (chair, car, and plane). category #instances airplane 36400 car 31460 chair 61000 total 128860 I trained and evaluated the point cloud model with n_points = 5000.Quantitative evaluation: Point Cloud trained on one class Point Cloud trained on three classes Qualitative evlautaion by comparing ‚Äútraining on one class‚Äù vs ‚Äútraining on three classes‚Äù: Input RGB Ground Truth Point Cloud Predicted 3D Point Cloud for 1 Class Training Predicted 3D Point Cloud for 3 Classes Training 3D consistency and diversity of output samples:Training the model on a single class, like chairs, results in more consistent and refined reconstructions. Since the model only sees one object type during training, it gets really good at capturing the details and structure unique to that class. However, this also means that the model becomes highly specialized. So when it is faced with a completely new object type (such as an airplane or car), it struggles because it hasn‚Äôt learned to handle the variation.On the other hand, training on multiple classes (airplanes, cars, and chairs) allows the model to adapt better to different object shapes. Instead of focusing on one type, it learns general patterns that apply across different categories. This makes it more versatile when reconstructing new objects.So in conclusion, single-class models tend to produce more uniform outputs because they have learned a very specific structural representation but lack adaptability. Multi-class models generate more diverse outputs because they have seen various object types and have learned to adapt to different shapes but at the cost of some fine-grained details.Assignment 3: Part-1 Neural Volume RenderingQuestions: Github Assignment 30. Transmittance Calculation1. Differentiable Volume Rendering1.3. Ray samplingUsage:python volume_rendering_main.py --config-name=box1.4. Point samplingUsage:python volume_rendering_main.py --config-name=box1.5. Volume renderingUsage:python volume_rendering_main.py --config-name=box2. Optimizing a basic implicit volume2.1. Random ray samplingdef get_random_pixels_from_image(n_pixels, image_size, camera): xy_grid = get_pixels_from_image(image_size, camera) # Random subsampling of pixel coordinaters xy_grid_sub = xy_grid[np.random.choice(xy_grid.shape[0], n_pixels)].to(\"cuda\") return xy_grid_sub.reshape(-1, 2)[:n_pixels]2.2. Loss and trainingloss = torch.nn.functional.mse_loss(rgb_gt, out['feature'])Usage:python volume_rendering_main.py --config-name=train_boxBox center: (0.2502, 0.2506, -0.0005) Box side lengths: (2.0051, 1.5036, 1.5034)2.3. VisualizationUsage:python volume_rendering_main.py --config-name=train_box3. Optimizing a Neural Radiance Field (NeRF)Usage:python volume_rendering_main.py --config-name=nerf_lego4. NeRF Extras4.1 View DependenceUsage:python volume_rendering_main.py --config-name=nerf_materialsTrade-offs between increased view dependence and generalization quality: Adding view dependence allows the model to capture complex lighting effects like reflections, and translucency. But excessive view dependence can create inconsistencies when interpolating between unique views, which will make the rendering look unnatural. If the network heavily relies on viewing direction, it may overfit to the specific camera angles in the training data. This can lead to poor generalization to unseen viewpoints. It can increase the network‚Äôs complexity, requiring more parameters and training time.Assignment 3: Part-2 Neural Surface Rendering5. Sphere TracingMy implementation of the SphereTracingRenderer class uses the sphere tracing algorithm to find intersections between rays and an implicit surface of a torus defined by a signed distance field. The algorithm iteratively updates points along each ray by moving in the direction of the ray by the amount of the SDF value at the current point. This process continues until the maximum number of iterations is reached or the SDF value becomes very close to zero (threshold of 1e-6), indicating a surface intersection.Usage:python -m surface_rendering_main --config-name=torus_surface6. Optimizing a Neural SDFUsage:python -m surface_rendering_main --config-name=points_surface Input lr=0.0001 lr=0.001 lr=0.00001 Loss 0.001279 0.000428 0.001635 eikonal_loss = ((gradients.norm(2, dim=1) - 1.0) ** 2).mean()Implementation:Input (XYZ points) -&gt; Harmonic Embedding -&gt; Layer 1 (Linear + ReLU) -&gt; Layer 2 (Linear + ReLU) -&gt; Layer 3 (Linear + ReLU) -&gt; ... -&gt; Layer N (Linear + ReLU) -&gt; Linear SDF (Output: Signed Distance Function)7. VolSDFUsage:python -m surface_rendering_main --config-name=volsdf_surface Alpha: Scales the overall density. A higher value increases the density, while a lower value reduces it. Beta: Controls how quickly the density changes with distance from the surface. A smaller beta results in a sharper transition, while a larger beta smooths the transition.def sdf_to_density(signed_distance, alpha, beta): return torch.where( signed_distance &gt; 0, 0.5 * torch.exp(-signed_distance / beta), 1 - 0.5 * torch.exp(signed_distance / beta), ) * alpha Geometry Result Loss 0.006958 The above renders are for values alpha=10.0 and beta=0.05.When alpha=10.0 and beta is changed: beta beta=0.05 beta=0.1 beta=0.5 Geometry Render Loss 0.006958 0.010227 0.020789 When beta=0.05 and alpha is changed: alpha alpha=1 alpha=10 alpha=50 Geometry Render Loss 0.022317 0.006958 0.004329 How does high beta bias your learned SDF? What about low beta?High beta makes the transition between occupied and free space more gradual, leading to a smoother SDF. This can cause a bias where surfaces appear more diffused rather than sharp.Low beta results in a sharper transition, meaning the SDF will be more precise in distinguishing surfaces, but it can also lead to unstable gradients and more difficult optimization.Would an SDF be easier to train with volume rendering and low beta or high beta? Why?An SDF is generally easier to train with volume rendering when using a high beta. This is because high beta values cause a larger number of points along each ray to have non-zero density, allowing gradients to be backpropagated through more points simultaneously. This leads to denser gradients and faster convergence during training.Training with a low beta can be more challenging because it forces the network to learn very sharp transitions, which means only points very close to the surface contributes significantly to the rendering. This can lead to sparse gradients and slower convergence.Would you be more likely to learn an accurate surface with high beta or low beta? Why?You are more likely to learn an accurate surface with a low beta. A low beta encourages sharp boundaries and a more precise surface representation, as the density function closely approximates a step function. High beta values, on the other hand, lead to smoother surfaces, which can be less accurate.Implementation:Input (SDF Feature + XYZ Embedding) -&gt; Layer 1 (Linear + ReLU) -&gt; Layer 2 (Linear + ReLU) -&gt; Layer 3 (Linear + ReLU) -&gt; ... -&gt; Layer N (Linear + ReLU) -&gt; Linear RGB (Output: 3D Color Prediction) 8. Neural Surface Extras8.3 Alternate SDF to Density ConversionsLogistic density distribution function:\\[\\phi_s(x) = \\frac{se^{-sx}}{(1+e^{-sx})^2}\\]def neus_sdf_to_density(signed_distance, s): return s * torch.exp(-s * signed_distance) / ((1 + torch.exp(-s * signed_distance))**2) s s=10 s=50 Geometry Render Loss 0.005590 0.006529 Low s results in a more blurry render, while a higher value of s makes it look sharp.Assignment 4Questions: Github Assignment 41. 3D Gaussian Splatting1.1 3D Gaussian RasterizationRun:python render.py --gaussians_per_splat 1024Output GIF:1.2 Training 3D Gaussian RepresentationsRun:python train.py --gaussians_per_splat 2048 --device cudaI modified the run_training function to improve performance and reduce CUDA memory usage. Specifically, I: Reduced the number of Gaussians from 10k to 7k by subsampling the points file, lowering the overall GPU memory footprint. Ensured key Gaussian parameters (opacities, scales, colours, and means) were trainable and set up an optimizer with different learning rates for each parameter group (as mentioned in 1.2.1) Wrapped the forward pass in autocast and used GradScaler to scale the loss, which both reduced memory usage and accelerated computation I called the scene.render method to generate the predicted image from the input camera parameters, using the specified image size, background color, and the number of Gaussians per splat. I then computed the L1 loss between the rendered (predicted) image and the ground truth image.Learning rates that I used for each parameter: pre_act_opacities = 0.001 pre_act_scales = 0.001 colours = 0.02 means = 0.0002Number of iterations that I trained the model for = 1000Mean PSNR: 27.356Mean SSIM: 0.915Training final render GIF:Training progress GIF:1.3 Extensions1.3.1 Rendering Using Spherical HarmonicsRun:python render.py --gaussians_per_splat 1024GIF: Original Spherical Harmonics RGB image comparisons of the renderings obtained from both the cases: Frame Original Spherical Harmonics 000 015 021 030 Differences that can be observed: Frame 000 and 015: The spherical harmonics rendering looks more photorealistic because the angular variations in color better capture how the material responds to illumination from different directions. Frames 021 and 030: The spherical harmonics rendering looks more glossy and reflective because the added directional sensitivity leads to more dynamic and detailed shading.2. Diffusion-guided Optimization2.1 SDS Loss + Image OptimizationRun:python Q21_image_optimization.py --sds_guidance 1 Prompt Without Guidance With Guidance Iterations 400 2000 ‚Äúa hamburger‚Äù ‚Äúa standing corgi dog‚Äù ‚Äúa fish in a pan‚Äù ‚Äúa mansion‚Äù 2.2 Texture Map Optimization for MeshRun:python python Q22_mesh_optimization.pyIn order to reduce the CUDA memory footprint, I reduced the image size to 256x256. Prompt Initial Mesh GIF Final Mesh GIF ‚Äúa tiger‚Äù ‚Äúa zebra‚Äù 2.3 NeRF OptimizationI perfomed no CUDA memory optimization here. All values were the same as default as pulled from GitHub.Prompt: ‚Äúa standing corgi dog‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a standing corgi dog\" --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa hamburger‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a hamburger\" --iters 2500 --lambda_entropy 1e-3 --lambda_orient 1e-2 --latent_iter_ratio 0.2 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa mansion‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a mansion\" --iters 2500 --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.2.4 Extensions2.4.1 View-dependent text embeddingPrompt: ‚Äúa standing corgi dog‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a standing corgi dog\" --lambda_entropy 1e-2 --lambda_orient 1e-2 --latent_iter_ratio 0.1 --view_dep_text 1Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Prompt: ‚Äúa hamburger‚ÄùRun:python Q23_nerf_optimization.py --prompt \"a hamburger\" --iters 2500 --lambda_entropy 1e-3 --lambda_orient 1e-2 --latent_iter_ratio 0.2 --view_dep_text 1Rendered RGB video: Your browser does not support the video tag.Rendered depth video: Your browser does not support the video tag.Comparing with 2.3:In 2.3, the NeRF model used one fixed text embedding for all views. This led to consistent but somewhat flat results and sometimes produced artifacts like multiple front faces (standing corgi dog example) because the model didn‚Äôt know how to adjust for different angles.With view-dependent text conditioning, different embeddings (like front, side, and back) are used based on the camera angle. This helps the model adjust lighting, highlights, and reflections for each view, resulting in more realistic and 3D-consistent images.The ‚Äúhamburger‚Äù looks really good with view-dependent conditioning, while the ‚Äústanding corgi dog‚Äù did not turn out as well, likely because it needs more training to capture all its details.Assignment 5Questions: Github Assignment 51. Classification ModelRun:python train.py --task clspython eval_cls.pyThe model was trained for 250 epochs, and the best model was saved at epoch 115.Train loss = 1.1288Test accuracy of best model = 0.9696Visualization of a few random test point clouds and their predicted classes: Correct Prediction Chairs Vases Lamps Point Cloud Point Cloud Visualization of a failure prediction for each class: Correct Label Vase Lamp Vase Point Cloud Incorrect Prediction Lamp Vase Lamp The successful results show that the model is able to pick out important features. For example, it correctly identifies the distinct structure of chairs, vases, and lamps. But in some cases, the shapes can be ambiguous or similar, which causes the model to misclassify objects. This can happen when the point sampling misses some important details or when the features are too similar between classes.2. Segmentation ModelRun:python train.py --task segpython eval_seg.pyThe model was trained for 250 epochs, and the best model was saved at epoch 225.Train loss = 13.0442Test accuracy of best model = 0.8966Visualization of correct segmentation predictions along with their corresponding ground truth: Accuracy 0.9317 0.9147 0.9308 Ground Truth Good Segmentation Predictions Visualization of incorrect segmentation predictions along with their corresponding ground truth: Accuracy 0.5604 0.5572 0.4702 Ground Truth Bad Segmentation Predictions The good segmentation predictions show that the model can distinguish the chair‚Äôs seat, back, and legs fairly accurately. In the failure cases, we can see that certain parts like the seat or back merges with the legs or other regions. A possible reason could be that if the training data includes ambiguous or poorly differentiated boundaries, the model may struggle to learn to differentiate segments in these areas.3. Robustness Analysis3.1 Rotate Input Point Clouds3.1.1 Classification ModelVisualization of a few random test point clouds and their predicted classes: Angle Accuracy Chairs Vases Lamps 10 degrees 0.958 Successful Successful Successful 10 degrees 0.958 Successful Successful Successful 20 degrees 0.894 Successful Successful Successful 20 degrees 0.894 Wrong predicted as lamp Successful Successful 30 degrees 0.679 Successful Successful Successful 30 degrees 0.679 Wrong predicted as lamp Wrongly predicted as lamp Successful 60 degrees 0.299 Wrongly predicted as vase Successful Wrongly predicted as chair 60 degrees 0.299 Wrong predicted as lamp Wrongly predicted as lamp Successful From the observations above, we see that the model classifies most rotated objects correctly when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), the accuracy drops and the model starts confusing objects. This happens because the model wasn‚Äôt trained with any rotational variations and so the learned features become orientation-dependent.3.1.2 Segmentation Model Angle Model X X X X 10 degrees Ground Truth 10 degrees Prediction Success: 0.9185 Success: 0.9134 Success: 0.9055 Failure: 0.4798 20 degrees Ground Truth 20 degrees Prediction Success: 0.9396 Success: 0.9191 Success: 0.9307 Failure: 0.5906 30 degrees Ground Truth ¬† 30 degrees Prediction Success: 0.9093 Success: 0.9429 Failure: 0.4707 ¬† 60 degrees Ground Truth ¬† 60 degrees Prediction Failure: 0.4707 Failure: 0.5283 Failure: 0.5478 ¬† From the observations above, we see that the model segments most portions of the rotated objects well when the rotation is small (10 or 20 degrees). However, as the rotation angle becomes larger (30 or 60 degrees), we see a massive accuracy drop. This happens because the model wasn‚Äôt trained with any rotational variations and so the learned features become orientation-dependent. The rotations make it difficult for the network to correctly differentiate between segments.3.2 Different Number of Point Points per Object3.2.1 Classification Model Number of Points Accuracy Chairs Vases Lamps 5000 0.968 Successful Successful Successful 1000 0.9695 Successful Successful Successful 500 0.9622 Successful Successful Successful 100 0.882 Successful Successful Successful We see that the classification model performs well even with fewer points. The drop in accuracy when the number of points is reduced to 100 or below is expected because fewer points reduce geometric detail, which makes it harder to distinguish specific features. Still, even at 100 points, the outline structure is preserved, allowing the model to classify successfully in many cases.3.2.2 Segmentation Model Number of Points Model X X X X 5000 Ground Truth 5000 Prediction Success: 0.9448 Success: 0.992 Success: 0.9658 Failure: 0.5366 1000 Ground Truth 1000 Prediction Success: 0.952 Success: 0.988 Success: 0.904 Failure: 0.515 500 Ground Truth 500 Prediction Success: 0.934 Success: 0.998 Success: 0.926 Failure: 0.484 100 Ground Truth 100 Prediction Success: 0.92 Success: 0.96 Success: 0.91 Failure: 0.58 We see that the segmentation model performs well even with fewer points. While the model performs well even with 500 points, the performance becomes more unstable at 100 points, especially in complex or ambiguous regions. This is because fewer points reduce the structural detail, making it harder for the model to distinguish fine boundaries and specific object features like thin legs and armrests.4. Bonus Question - LocalityI implemented a general Transformer framework for both the classification model and the segmentation model.The classification model predicts a single label for an entire point cloud. Each point‚Äôs 3D coordinates are passed through two linear layers: one to embed the input features, and another to learn positional information. These two embeddings are added together and fed into a standard Transformer Encoder. After processing, I apply max pooling across all points to extract a global feature vector. This is then passed through fully connected layers with batch normalization, ReLU activation, and dropout. The log softmax over the class scores is then returned.The segmentation model /assets/images/L3D/a5/outputs a class label for each point. I embed the 3D points and add positional information before passing them through a Transformer Encoder. Instead of pooling the features, I use 1D convolution layers to generate per-point predictions. These /assets/images/L3D/a5/outputs are also passed through a log softmax to get per-point class probabilities.4.1 Classification Model using TransformersRun:python train.py --task clspython eval_cls.py --load_checkpoint best_model --/assets/images/L3D/a5/output_dir \".//assets/images/L3D/a5/output/Q4/cls\"The model was trained for 100 epochs, and the best model was saved at epoch 18.Train loss = 30.6177Test accuracy of best model = 0.9454Visualization of a few random test point clouds and their predicted classes using Transformers: Correct Prediction Chairs Vases Lamps Point Cloud Point Cloud Visualization of a failure prediction for each class using Transformers: Correct Label Vase Lamp Lamp Point Cloud Incorrect Prediction Lamp Vase Chair The model was run only for 100 epochs because it was taking a lot of time. I also did not have the time to run the segmentation Transformer model. However, there were no errors in starting the training and it only had to be left running for ~8 hours on my laptop.Moreover, the Transformer model gave a very high accuracy of 94% for just training for 100 epochs. The classification model without Transformer gave an accuracy of 97% for training for 250 epochs." }, { "title": "Introduction to Robotics Business", "url": "/posts/intro-robotics-business-25/", "categories": "CMU MRSD, Robotics", "tags": "business, construction, startup", "date": "2025-03-09 21:30:00 +0530", "snippet": "Company Analysis: Advanced Construction Robotics Your browser does not support PDFs. Please download the report here.Patent Analysis Your browser does not support PDFs. Please download the report here." }, { "title": "Coursework at CMU", "url": "/posts/cmu-blog/", "categories": "Blog, Robotics", "tags": "cmu, robotics, masters", "date": "2025-02-06 21:30:00 +0530", "snippet": "This blog serves as a comprehensive collection of notes, assignments and insights from my coursework in the Master‚Äôs in Robotic Systems Development program at Carnegie Mellon University.Links will redirect to CMU MRSD.Fall 2025 Introduction to Deep Learning Visual Learning and Recognition Planning and Decision MakingSpring 2025 Learning for 3D Vision Robot Autonomy Introduction to Robotics BusinessFall 2024 Manipulation, Estimation, and Control Robot Mobility on Air, Land, and Sea Introduction to Computer Vision Systems Engineering and Project Management for Robotics" }, { "title": "Micro-ROS, RCL, & RCLC", "url": "/posts/micro-ros/", "categories": "Blog, Robotics", "tags": "mujoco, manipulation, ik, fk", "date": "2025-02-01 21:30:00 +0530", "snippet": "Micro-ROS Publisher#include &lt;micro_ros_arduino.h&gt; #include &lt;stdio.h&gt;#include &lt;rcl/rcl.h&gt;#include &lt;rcl/error_handling.h&gt;#include &lt;rclc/rclc.h&gt;#include &lt;rclc/executor.h&gt;#include &lt;std_msgs/msg/int32.h&gt;rcl_publisher_t publisher;std_msgs__msg__Int32 msg;rclc_executor_t executor;rclc_support_t support;rcl_allocator_t allocator;rcl_node_t node;rcl_timer_t timer;#define LED_PIN 13#define RCCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){error_loop();}}#define RCSOFTCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){}}void error_loop(){ while(1){ digitalWrite(LED_PIN, !digitalRead(LED_PIN)); delay(100); }}void timer_callback(rcl_timer_t * timer, int64_t last_call_time){ RCLC_UNUSED(last_call_time); if (timer != NULL) { RCSOFTCHECK(rcl_publish(&amp;publisher, &amp;msg, NULL)); msg.data++; }}void setup() { set_microros_transports(); pinMode(LED_PIN, OUTPUT); digitalWrite(LED_PIN, HIGH); delay(2000); allocator = rcl_get_default_allocator(); //create init_options RCCHECK(rclc_support_init(&amp;support, 0, NULL, &amp;allocator)); // create node RCCHECK(rclc_node_init_default(&amp;node, \"micro_ros_arduino_node\", \"\", &amp;support)); // create publisher RCCHECK(rclc_publisher_init_default( &amp;publisher, &amp;node, ROSIDL_GET_MSG_TYPE_SUPPORT(std_msgs, msg, Int32), \"micro_ros_arduino_node_publisher\")); // create timer, const unsigned int timer_timeout = 1000; RCCHECK(rclc_timer_init_default( &amp;timer, &amp;support, RCL_MS_TO_NS(timer_timeout), timer_callback)); // create executor RCCHECK(rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator)); RCCHECK(rclc_executor_add_timer(&amp;executor, &amp;timer)); msg.data = 0;}void loop() { delay(100); RCSOFTCHECK(rclc_executor_spin_some(&amp;executor, RCL_MS_TO_NS(100)));}#include &lt;micro_ros_arduino.h&gt;#include &lt;rcl/rcl.h&gt;#include &lt;rcl/error_handling.h&gt;#include &lt;rclc/rclc.h&gt;#include &lt;rclc/executor.h&gt;#include &lt;std_msgs/msg/int32.h&gt; micro_ros_arduino.h: Includes Micro-ROS Arduino-specific functions. rcl/rcl.h and rclc: Provides core ROS client library (RCL) and RCLC utilities. std_msgs/msg/int32.h: Includes the ROS std_msgs::msg::Int32 message type for publishing integers.rcl_publisher_t publisher;std_msgs__msg__Int32 msg;rclc_executor_t executor;rclc_support_t support;rcl_allocator_t allocator;rcl_node_t node;rcl_timer_t timer; executor: Manages callbacks (like timers). support: Manages initialization options for Micro-ROS. allocator: Allocates memory for the Micro-ROS framework. node: A Micro-ROS node (basic ROS entity that performs tasks). timer: Periodically triggers the timer_callback.#define RCCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){error_loop();}}#define RCSOFTCHECK(fn) { rcl_ret_t temp_rc = fn; if((temp_rc != RCL_RET_OK)){}} RCCHECK: Ensures critical operations succeed. If an error occurs, it enters the error_loop. RCSOFTCHECK: Similar, but doesn‚Äôt halt execution on failure.void error_loop(){ while(1){ digitalWrite(LED_PIN, !digitalRead(LED_PIN)); delay(100); }} Blinks the onboard LED continuously to indicate an error.void timer_callback(rcl_timer_t * timer, int64_t last_call_time){ RCLC_UNUSED(last_call_time); if (timer != NULL) { RCSOFTCHECK(rcl_publish(&amp;publisher, &amp;msg, NULL)); msg.data++; }} This function is called each time the timer triggers. Increments msg.data and publishes it to the topic.void setup() { set_microros_transports(); // Initialize communication (e.g., Serial, WiFi, etc.) pinMode(LED_PIN, OUTPUT); // Set LED_PIN as output. digitalWrite(LED_PIN, HIGH); delay(2000); // Wait for 2 seconds for initialization. allocator = rcl_get_default_allocator(); // Set up the memory allocator. // Initialize Micro-ROS support and components RCCHECK(rclc_support_init(&amp;support, 0, NULL, &amp;allocator)); RCCHECK(rclc_node_init_default(&amp;node, \"micro_ros_arduino_node\", \"\", &amp;support)); RCCHECK(rclc_publisher_init_default( &amp;publisher, &amp;node, ROSIDL_GET_MSG_TYPE_SUPPORT(std_msgs, msg, Int32), \"micro_ros_arduino_node_publisher\")); const unsigned int timer_timeout = 1000; // Timer interval in milliseconds. RCCHECK(rclc_timer_init_default( &amp;timer, &amp;support, RCL_MS_TO_NS(timer_timeout), timer_callback)); RCCHECK(rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator)); RCCHECK(rclc_executor_add_timer(&amp;executor, &amp;timer)); msg.data = 0; // Initialize the message value.}Key setup steps: Transport Setup: Configures the communication method (e.g., serial or WiFi). Micro-ROS Initialization: Initializes the support structure. Creates a node (\"micro_ros_arduino_node\"). Sets up the publisher for the topic \"micro_ros_arduino_node_publisher\". Timer Setup: Calls timer_callback every second (1000 ms). Executor Setup: Manages the timer callback execution. Message Initialization: Starts with msg.data = 0.void loop() { delay(100); // Short delay. RCSOFTCHECK(rclc_executor_spin_some(&amp;executor, RCL_MS_TO_NS(100)));} The executor handles the timer callback, ensuring timer_callback is executed as needed.RCLROS Client LibraryCommon Types and Return Codes rcl_ret_t : The type that holds an rcl return code Codes: RCL_RET_OK: Success return code RCL_RET_ERROR: Unspecified error return code RCL_RET_NOT_INIT: rcl_init()¬†not yet called return code RCL_RET_ Allocatorallocator.h rcl_allocator_t : Encapsulation of an allocator RCL_CHECK_ALLOCATOR : Check that the given allocator is initialized. If the allocator is not initialized, run the fail_statement RCL_CHECK_ALLOCATOR_WITH_MSG(allocator,¬†msg,¬†fail_statement) : Check that the given allocator is initialized, or fail with a message. If the allocator is not initialized, set the error to msg, and run the fail_statement rcl_get_default_allocator : Return a properly initialized rcl_allocator_t with default valuesrcl_allocator_t allocator = rcl_get_default_allocator();if (!allocator.allocate) { printf(\"Allocator not initialized\\n\");}Timertimer.h rcl_timer_t: Structure which encapsulates a ROS Timer Macros: RCL_MS_TO_NS : Convenience macro to convert milliseconds to nanoseconds rcl_timer_init: Initializes a timer with a callback rcl_ret_t rcl_timer_init( rcl_timer_t *timer, rcl_clock_t *clock, rcl_context_t *context, int64_t period, const rcl_timer_callback_t callback, rcl_allocator_t allocator ) Publisherpublisher.h rcl_publisher_t : Structure which encapsulates a ROS Publisher Initialization: rcl_ret_t rcl_publisher_init( rcl_publisher_t *publisher, const rcl_node_t *node, const rosidl_message_type_support_t *type_support, const char *topic_name, const rcl_publisher_options_t *options ) Publish messages: rcl_ret_t rcl_publish( const rcl_publisher_t *publisher, const void *ros_message, rmw_publisher_allocation_t *allocation ) Stop publisher: After calling, the node will no longer be advertising that it is publishing on this topic (assuming this is the only publisher on this topic) rcl_ret_t rcl_publisher_fini( \t\tconst rcl_publisher_t *publisher, \t\tconst rcl_node_t *node ) Nodenode.h rcl_node_t : Structure which encapsulates a ROS Node rcl_node_impl_t rcl_service_t Initialization: rcl_ret_t rcl_node_init( \t\trcl_node_t *node, \t\tconst char *name, \t\tconst char *namespace_, \t\trcl_context_t *context, \t\tconst rcl_node_options_t *options ) rcl_ret_t rcl_node_fini(rcl_node_t¬†*node): Destroys any automatically created infrastructure and deallocates memory. After calling, the rcl_node_t can be safely deallocatedRCLCRCLCSupport RCLC_UNUSED rclc_support_t : Initializes RCL and creates foundational data structures rclc_support_init: Initializes rcl and creates some support data structures. Initializes clock as RCL_STEADY_TIME rcl_ret_t rclc_support_init( \t\trclc_support_t *support, \t\tint argc, \t\tchar const *const *argv, \t\trcl_allocator_t *allocator ) Node rcl_node_t : Structure which encapsulates a ROS Node rclc_node_init_default: Creates a default RCL node rcl_ret_t rclc_node_init_default( \t\trcl_node_t *node, \t\tconst char *name, \t\tconst char *namespace_, \t\trclc_support_t *support ) Timer rcl_timer_t : Structure which encapsulates a ROS Timer rclc_timer_init_default: Creates an rcl timer rcl_ret_t rclc_timer_init_default( \t\trcl_timer_t *timer, \t\trclc_support_t *support, \t\tconst uint64_t timeout_ns, \t\tconst rcl_timer_callback_t callback ) Executor rclc_executor_t: Handles callback execution for nodes, timers, and other handles Initialization: rcl_ret_t rclc_executor_init( \t\trclc_executor_t *executor, \t\trcl_context_t *context, \t\tconst size_t number_of_handles, \t\tconst rcl_allocator_t *allocator ) Add Timer to Executor: Adds time to an executor rcl_ret_t rclc_executor_add_timer( \t\trclc_executor_t *executor, \t rcl_timer_t *timer ) Publisher rclc_publisher_init_default: Creates an rcl publisher rcl_ret_t rclc_publisher_init_default( \t\trcl_publisher_t *publisher, \t\tconst rcl_node_t *node, \t\tconst rosidl_message_type_support_t *type_support, \t\tconst char *topic_name ) Quick Initialization Flow Support Initialization: rclc_support_t support; rclc_support_init(&amp;support, argc, argv, &amp;allocator); Node Initialization: rcl_node_t node; rclc_node_init_default(&amp;node, \"node_name\", \"namespace\", &amp;support); Publisher Creation: rcl_publisher_t publisher; rclc_publisher_init_default(&amp;publisher, &amp;node, type_support, \"topic_name\"); Timer and Executor: rcl_timer_t timer; rclc_timer_init_default(&amp;timer, &amp;support, RCL_MS_TO_NS(1000), my_callback); rclc_executor_t executor; rclc_executor_init(&amp;executor, &amp;support.context, 1, &amp;allocator); rclc_executor_add_timer(&amp;executor, &amp;timer); " }, { "title": "MuJoCo Basics", "url": "/posts/mujoco-basics/", "categories": "Blog, Robotics", "tags": "mujoco, manipulation, ik, fk", "date": "2025-02-01 21:30:00 +0530", "snippet": "Overview of mjModel and mjData mjModel: Holds static information about the simulation model (e.g., geometry, joint limits, sensor definitions). Think of it as a blueprint for the robot/environment. mjData: Holds dynamic state information for the simulation (e.g., joint positions, velocities, applied forces). It tracks how the simulation evolves over time.mjModel Function Description Example Usage mj_name2id Converts a name (e.g., of a joint, body, or sensor) to its corresponding ID. joint_id = mj_name2id(model, mjOBJ_JOINT, \"joint1\") mj_id2name Converts an ID to its corresponding name. joint_name = mj_id2name(model, mjOBJ_JOINT, joint_id) mj_getGeomId Retrieves the ID of a geometry by name. geom_id = mj_getGeomId(model, \"geom1\") mj_getBodyId Retrieves the ID of a body by name. body_id = mj_getBodyId(model, \"robot_body\") mj_getJointId Retrieves the ID of a joint by name. joint_id = mj_getJointId(model, \"joint1\") mj_getSiteId Retrieves the ID of a site by name. site_id = mj_getSiteId(model, \"end_effector_site\") mj_printModel Dumps the entire model structure to a file. mj_printModel(model, \"model.txt\") mj_setGeomMass Dynamically changes the mass of a geometry. mj_setGeomMass(model, geom_id, 1.5) mj_setBodyMass Dynamically changes the mass of a body. mj_setBodyMass(model, body_id, 10.0) mj_setLengthRange Sets the length range of a tendon. mj_setLengthRange(model, tendon_id, 0.1, 0.5) Use mj_name2id and mj_id2name to access objects in your model dynamically, when you don‚Äôt hardcode object IDs. Functions like mj_setGeomMass and mj_setBodyMass allow you to tweak physical properties during runtime.Attributes Function Description Example Usage qpos0 Default joint positions (initial state). print(model.qpos0) qpos_spring Rest positions of joints with springs. print(model.qpos_spring) jnt_range Joint position limits, given as [min, max] for each joint. joint_limits = model.jnt_range jnt_type Type of each joint (hinge, slide, etc.). print(model.jnt_type) body_pos Positions of all bodies in the model‚Äôs local frame. body_positions = model.body_pos body_quat Orientations of bodies in quaternion format. body_orientations = model.body_quat geom_pos Positions of geometries relative to their parent body. geom_positions = model.geom_pos geom_size Dimensions of geometries (e.g., radius for spheres, size for boxes). geom_sizes = model.geom_size geom_type Type of each geometry (sphere, box, capsule, etc.). print(model.geom_type) dof_damping Damping coefficients for degrees of freedom (DOF). damping_values = model.dof_damping dof_armature Armature values for rotational DOF. armature_values = model.dof_armature actuator_ctrlrange Control range ([min, max]) for each actuator. ctrl_ranges = model.actuator_ctrlrange actuator_forcerange Force range ([min, max]) for each actuator. force_ranges = model.actuator_forcerange actuator_trntype Type of actuator transmission (joint, tendon, etc.). print(model.actuator_trntype) sensor_type Type of each sensor (force, torque, etc.). sensor_types = model.sensor_type sensor_dim Dimension of each sensor output (e.g., 3 for a 3D force sensor). sensor_dimensions = model.sensor_dim sensor_addr Index in the sensordata array where each sensor‚Äôs data starts. print(model.sensor_addr) The mjOBJ_JOINT is a constant identifier in MuJoCo that represents the object type ‚Äújoint‚Äù.Some common object types and their constants include: mjOBJ_BODY: Refers to a body. mjOBJ_JOINT: Refers to a joint. mjOBJ_GEOM: Refers to a geometry. mjOBJ_SITE: Refers to a site. mjOBJ_SENSOR: Refers to a sensor. mjOBJ_ACTUATOR: Refers to an actuator. mjOBJ_TENDON: Refers to a tendon.mjData Function Description Example Usage mj_resetData Resets the simulation to the initial state. mj_resetData(model, data) mj_step Advances the simulation by one timestep. mj_step(model, data) mj_forward Computes forward dynamics without advancing time. mj_forward(model, data) mj_inverse Computes inverse dynamics (forces needed for desired accelerations). mj_inverse(model, data) mj_applyFT Applies a force/torque to a body at a specific point. mj_applyFT(model, data, force, torque, body_id, point) mj_getControl Retrieves the current control inputs. controls = mj_getControl(data) mj_setControl Sets the control inputs for actuators. mj_setControl(data, control_values) mj_contactForce Returns the forces at a specific contact point. force = mj_contactForce(model, data, contact_id) mj_time Returns the current simulation time. current_time = mj_time(data) mj_energyPos Computes the system‚Äôs potential energy. potential_energy = mj_energyPos(model, data) mj_energyVel Computes the system‚Äôs kinetic energy. kinetic_energy = mj_energyVel(model, data) Use mj_step to progress the simulation. You can modify data (e.g., joint positions or control inputs) before calling mj_step. mj_contactForce is useful for analyzing interaction forces in contact-rich environments.Attributes Function Description Example Usage qpos Joint positions, representing the current state of the system. print(data.qpos) qvel Joint velocities. print(data.qvel) qacc Joint accelerations. print(data.qacc) ctrl Control inputs applied to actuators. data.ctrl[:] = [0.1, 0.2, 0.3] qfrc_applied Forces applied directly to joints. data.qfrc_applied[joint_id] = 1.0 xfrc_applied External forces/torques applied to bodies. data.xfrc_applied[body_id][:3] = [1.0, 0.0, 0.0] xpos Global positions of bodies in the simulation. print(data.xpos[body_id]) xquat Global orientations of bodies in quaternion format. print(data.xquat[body_id]) xmat Global orientations of bodies in rotation matrix format. print(data.xmat[body_id]) cvel Combined spatial velocity (angular + linear) for all bodies in the local frame. spatial_velocity = data.cvel[body_id] \\angular_velocity = data.cvel[body_id][:3]\\ linear_velocity = data.cvel[body_id][3:] geom_xpos Global positions of geometries. print(data.geom_xpos[geom_id]) geom_xmat Global orientations of geometries. print(data.geom_xmat[geom_id]) sensordata Sensor data output. print(data.sensordata) subtree_com Center of mass of each kinematic subtree. print(data.subtree_com[body_id]) subtree_mass Mass of each kinematic subtree. print(data.subtree_mass[body_id]) cfrc_ext Contact forces/torques acting on bodies. print(data.cfrc_ext[body_id]) cfrc_int Internal forces/torques acting on bodies. print(data.cfrc_int[body_id]) ten_length Current lengths of tendons. print(data.ten_length[tendon_id]) ten_velocity Velocities of tendons. print(data.ten_velocity[tendon_id]) Other MuJoCo Functions Function Description Example Usage mj_loadXML Loads an XML model into MuJoCo. model = mj_loadXML(\"model.xml\") mj_makeData Creates a new mjData object for the loaded model. data = mj_makeData(model) mj_deleteModel Frees memory allocated for mjModel. mj_deleteModel(model) mj_deleteData Frees memory allocated for mjData. mj_deleteData(data) mjv_updateScene Updates the visual representation of the model. mjv_updateScene(model, data, options, scene, camera) mjr_render Renders the scene to a viewport. mjr_render(viewport, scene) " }, { "title": "Introduction to Computer Vision", "url": "/posts/computer-vision-24/", "categories": "CMU MRSD, Robotics", "tags": "computer vision", "date": "2024-12-05 21:30:00 +0530", "snippet": "Image Processing and Representation Your browser does not support PDFs. Please download the report here.Cameras and Image Formation Your browser does not support PDFs. Please download the report here.Multi-view Geometry and 3D Reconstruction Your browser does not support PDFs. Please download the report here.Deep Learning and Segmentation Your browser does not support PDFs. Please download the report here.Motion Your browser does not support PDFs. Please download the report here.Physics Based Vision Your browser does not support PDFs. Please download the report here." }, { "title": "Robot Mobility on Air, Land, and Sea", "url": "/posts/mobility-24/", "categories": "CMU MRSD, Robotics", "tags": "autonomous driving, wheeled mobility, air mobility, legged mobility, marine mobility", "date": "2024-12-04 21:30:00 +0530", "snippet": "Wheeled Mobility Your browser does not support PDFs. Please download the report here.Aerial Mobility Your browser does not support PDFs. Please download the report here." }, { "title": "Manipulation, Estimation, and Control", "url": "/posts/mec-24/", "categories": "CMU MRSD, Robotics", "tags": "estimation, manipulation, control, matlab, kalman filter, efk, particle filter, kinematics, inverse kinematics, dynamics", "date": "2024-12-04 21:30:00 +0530", "snippet": "ControlsEstimation Your browser does not support PDFs. Please download the report here.Manipulation" }, { "title": "Lunar ROADSTER", "url": "/posts/lunar-roadster-cmu/", "categories": "Robotics", "tags": "space, navigation, localization, perception, control, planning, electronics, design", "date": "2024-12-04 21:30:00 +0530", "snippet": "Lunar Robotic Operator for Autonomous Development of Surface Trails and Exploration Routes (ROADSTER)Supervisor: Dr. William ‚ÄúRed‚Äù WhittakerTeam: Bhaswanth Ayapilla, Ankit Aggarwal, Deepam Ameria, Simson D‚ÄôSouza, Boxiang (William) FuVisit the Lunar ROADSTER Website Lunar-ROADSTER GitHub Spring Validation DemoDemoSVD PresentationDate: $17^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.EncoreDate: $24^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Design ReviewCritical Design Review Report (CDRR)Date: $3^{rd}$ May, $2025$ Your browser does not support PDFs. Please download the report here.Critical Design Review Presentation (CDRP)Date: $29^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Preliminary Design Review (PDR)Date: $11^{th}$ March, $2025$ Your browser does not support PDFs. Please download the report here.Conceptual Design Review Report (CoDRR)Date: $12^{th}$ December, $2024$ Your browser does not support PDFs. Please download the report here.Conceptual Design Review Presentation (CoDRP) Your browser does not support PDFs. Please download the slides here.Test PlansSpring Test Plan Your browser does not support PDFs. Please download the report here.Spring Validation Demo Test Plan Your browser does not support PDFs. Please download the report here.Fall Validation Demo Test Plan Your browser does not support PDFs. Please download the report here.Progress Review PresentationsProgress Review 1Date: $13^{th}$ February, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 2Date: $27^{th}$ February, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 3Date: $20^{th}$ March, $2025$ Your browser does not support PDFs. Please download the report here.Progress Review 4Date: $8^{th}$ April, $2025$ Your browser does not support PDFs. Please download the report here.Individual Lab ReportsILR 01Date: $7^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 02Date: $14^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 03Date: $28^{th}$ February, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 04Date: $21^{th}$ March, $2025$ Your browser does not support PDFs. Please download the slides here.ILR 05Date: $9^{th}$ April, $2025$ Your browser does not support PDFs. Please download the slides here." }, { "title": "Docker for Robotics", "url": "/posts/docker-for-robotics/", "categories": "Blog, Robotics", "tags": "docker, software, programming, development, robotics", "date": "2024-12-01 21:30:00 +0530", "snippet": "Docker OverviewDocker is a tool that helps you create, share, and run applications in containers. Containers are small, lightweight packages that include everything your application needs, like the code, libraries, and settings. They are faster and use fewer resources compared to virtual machines (VMs).Difference Between Containers and Virtual Machines: Virtual Machines (VMs): Full operating systems running on top of a hypervisor. They are resource-heavy and slow to start. Docker Containers: Share the host OS kernel, making them faster, more lightweight, and more resource-efficient.Key components: Images: Think of an image like a recipe. It tells Docker what to include in the container (software, libraries, etc.). Containers: These are the actual ‚Äúlive‚Äù versions of the image. It‚Äôs like cooking from the recipe ‚Äî your container is the meal ready to eat. Dockerfile: A script that defines how an image is built, including base images, commands, and configurations. Volumes: Persistent storage that containers can use to save data. Docker Hub: A cloud-based repository where pre-built Docker images are shared.Why Use Docker in Robotics?Working on robots means dealing with lots of software and tools, which can sometimes conflict or break things. Docker helps solve these problems: Easily Switch Between Setups: Robots often use specific software versions, like different ROS distributions (Robot Operating System) or Ubuntu versions. For example, if you‚Äôre using an Nvidia Jetson board with JetPack (their software toolkit), Docker lets you use the version you want, even if Nvidia hasn‚Äôt updated JetPack in a while. Consistent Building and Testing: Docker ensures you build and test your software in the same environment every time, no matter where you run it. Same Environment for Everyone: All developers on a team can work with the exact same setup using Docker. This avoids problems where something works on one computer but not on another. Easy to Update Robots: When you need to update a robot‚Äôs software, Docker lets you make small, controlled changes without messing up the entire system. Code Defines the Environment: Docker lets you describe your software environment in code, so it‚Äôs easy to share, version, and reproduce. Cloud Development: Docker makes it easy to set up powerful computers in the cloud to develop and test your robotics software. You can even connect to these remote setups with tools like VS Code. Docker CommandsImages Build an image from a Dockerfile: $ docker build -t &lt;image_name&gt; Pull an image from a Docker Hub: $ docker image pull &lt;image_name&gt;:&lt;tag&gt; Search Hub for an image: $ docker image search &lt;image_name&gt; List local images: $ docker image ls $ docker images Delete an image: $ docker image rm &lt;image_name&gt; $ docker rmi &lt;image_name&gt; Remove all unused images: $ docker image prune Containers Create and run a container from an image, with a custom name: $ docker run --name &lt;container_name&gt; &lt;image_name&gt; Run a container with terminal: $ docker run -it &lt;image_name&gt; Start or stop an existing container: $ docker start|stop &lt;container_name&gt; (or &lt;container-id&gt;) Start an existing container with terminal: $ docker start -i &lt;container_name&gt; List running containers: $ docker container ls $ docker ps List all containers (even the stopped ones): $ docker container ls -a $ docker ps -a Remove a stopped container: $ docker rm &lt;container_name&gt; Remove all available containers: $ docker container prune Open terminal inside a running container: $ docker container exec -it &lt;container_name&gt; /bin/bash For any commands within a running container: $ docker container exec -it &lt;container_name&gt; &lt;command&gt; Working with Volumes Mount Host Directory to Container: This is how we can make a directory on host available inside the container$ docker run -it -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; &lt;image_name&gt;$ docker run -it --network=host --ipc=host -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; &lt;image_name&gt;Any files created in a container in a shared volume will be locked ‚Äî can be accessed only by the root.Note: docker run always creates a new container. We lose any changes we make to the environment every time we run the container.Setting up a DockerfileThe Dockerfile contains the steps for creating an image. It typically starts with a base image and includes commands for installing software, setting environment variables, and defining the container‚Äôs entrypoint.# FROM &lt;base_image_name&gt;FROM osrf/ros:humble-desktop-full# Commands to perform on base imageRUN apt-get -y update \\ &amp;&amp; apt-get -y install some_package \\ &amp;&amp; git clone https://github.com/some_user/some_repository some_repo \\ &amp;&amp; cd some_repo \\ &amp;&amp; mkdir build \\ &amp;&amp; cd build \\ &amp;&amp; cmake .. \\ &amp;&amp; make -j$(nproc) \\ &amp;&amp; make install \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# Install additional packagesRUN apt-get update &amp;&amp; apt-get install -y \\ python3-pip \\ ros-humble-turtlebot3-simulations# Set up workspaceENV WS_DIR=\"/root/ros2_ws\"WORKDIR ${WS_DIR}RUN /bin/bash -c \"source /opt/ros/humble/setup.bash &amp;&amp; colcon build\"# Default commandCMD [\"/bin/bash\"]# COPY &lt;configuration_file to copy&gt; &lt;direction in image to be copied into&gt;COPY config/ site_config/# Define the script that should be launched upon start of the containerENTRYPOINT [\"/root/ros2_ws/src/my_script.sh\"] All commands run in the docker container will run as root. The COPY command assumes paths are relative to the build context specified in the docker-compose.yml or the docker build command. To build and run the Docker image, go into the directory which will be the new image$ docker image build -t &lt;new_image_name&gt; &lt;directory&gt;$ docker run -it &lt;new_image_name&gt;Entrypoint ScriptsEntrypoint scripts automate container setup at runtime. It runs every time the container is brought up. Create a new file called entrypoint.sh inside the directory.#!/bin/bashsource /opt/ros/humble/setup.bashexec \"$@\" Add it to the DockerfileCOPY entrypoint.sh /entrypoint.shENTRYPOINT [\"/entrypoint.sh\"]GUI in Docker$ docker run -it --network=host --ipc=host -v &lt;absol_path_on_host&gt;:&lt;absol_path_in_container&gt; -v /tmp/.X11-unix:/tmp/.X11-unix:rw --env=DISPLAY &lt;image_name&gt;Docker ComposeDocker Compose simplifies multi-container applications by allowing you to define and manage them through a YAML file (docker-compose.yml).For newer versions (Docker v2.0 and later), use docker compose instead of docker-compose.Basic Commands Start and run all services defined in the docker-compose.yml file:$ docker compose up Start services in detached mode (background):$ docker compose up -d Stop all running services:$ docker compose stop Stop and remove all services, networks, and volumes:$ docker compose down Restart all services:$ docker compose restartConfiguration Management Validate the docker-compose.yml file:$ docker compose config View the service logs (real-time streaming):$ docker compose logs View logs of a specific service:$ docker compose logs &lt;service_name&gt; Build or rebuild services$ docker compose build Build a specific service:$ docker compose build &lt;service_name&gt; Pull service images defined in the docker-compose.yml file:$ docker compose pullService ManagementA service represents a single containerized application or component in a multi-container setup. Each service corresponds to a container, and the docker-compose.yml file is used to define the configuration for these services. Start a specific service$ docker compose up &lt;service_name&gt; Stop a specific service:$ docker compose stop &lt;service_name&gt; Remove stopped service containers:$ docker compose rm Remove a specific service container:$ docker compose rm &lt;service_name&gt;Network and Volume Management View networks created by Docker Compose:$ docker network ls View volumes created by Docker Compose:$ docker volume ls Remove unused networks:$ docker network prune Remove unused volumes:$ docker volume pruneWriting and launching a Docker-Compose fileExample docker-compose.yml for a ROS 2 project:version: '3.8'services: ros-master: image: osrf/ros:humble-ros-core container_name: ros-master networks: - ros-network turtlebot-sim: image: osrf/ros:humble-desktop container_name: turtlebot-sim depends_on: - ros-master networks: - ros-networknetworks: ros-network: driver: bridgeAfter having created both a¬†Dockerfile¬†as well as a¬†docker-compose.yml¬†you can launch them with:$ docker compose -f docker-compose.yml build$ docker compose -f docker-compose.yml upwhere with the option¬†-f¬†a Docker-Compose file with a different filename can be provided. If not given it will default to¬†docker-compose.yml.More general docker-compose.yml:version: \"3.9\"services: some_service: # Name of the particular service (Equivalent to the Docker --name option) build: # Use Dockerfile to build image context: . # The folder that should be used as a reference for the Dockerfile and mounting volumes dockerfile: Dockerfile # The name of the Dockerfile container_name: some_container stdin_open: true # Equivalent to the Docker -i option tty: true # Equivalent to the Docker docker run -t option volumes: - /a_folder_on_the_host:/a_folder_inside_the_container # Source folder on host : Destination folder inside the container another_service: image: ubuntu/20.04 # Use a Docker image from Dockerhub container_name: another_container volumes: - /another_folder_on_the_host:/another_folder_inside_the_containervolumes: - ../yet_another_folder_on_host:/a_folder_inside_both_containers # Another folder to be accessed by both imagesIf instead you wanted only to run a particular service you could do so with:$ docker compose -f docker-compose.yml run my_serviceThen similar to the previous section, we can connect to the container from another console with$ docker compose exec &lt;docker_name&gt; shwhere¬†&lt;docker_name&gt;¬†is given by the name specified in the¬†docker-compose.yml¬†file and¬†sh¬†stands for the type of comand to be execute, in this case we open a¬†shell.Docker Registry Build image locally:$ docker compose build &lt;servie_name&gt; Tag the resulting image for Docker Hub:$ docker tag &lt;service_name&gt; &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt; Push the image to Docker Hub:$ docker push &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt;In case you‚Äôre not logged in, use$ docker login -u &lt;username&gt;And then enter the password.It is necessary to include your Docker Hub username in the tag.Building Docker Images for Multiple Architectures Ensure qemu emulation is enabled: You need to have qemu-user-static installed and properly configured for cross-platform builds.$ sudo apt-get install -y qemu-user-static$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yesThis ensures that the qemu emulator is registered for the required architectures.When building or running Docker images for a different architecture: Build Process: QEMU emulates the target architecture (e.g., arm64) on the host (e.g., amd64), enabling you to compile binaries and packages for the target system. Run Process: QEMU interprets arm64 instructions so that the container can run on an amd64 host without errors. Setup buildx$ docker buildx create --name multiarch --use$ docker buildx inspect --bootstrap Tag the image that you want to push$ docker tag &lt;service_name&gt; &lt;your_dockerhub_username&gt;/&lt;name&gt;:&lt;tag&gt; Build the multi-arch image$ docker buildx build --platform linux/amd64,linux/arm64/v8 \\ -t your-dockerhub-username/your-image-name:tag \\ --push \\ -f /path/to/Dockerfile /path/to/context The image will be pushed to Docker HubAdditional Resources Docker for Development Docker for Robotics YouTube" }, { "title": "Systems Engineering and Project Management for Robotics", "url": "/posts/systems-engineering-24/", "categories": "CMU MRSD, Robotics", "tags": "systems engineering, project management, v-model", "date": "2024-12-01 21:30:00 +0530", "snippet": "The content below are my notes for the class \"16-650 Systems Engineering and Management for Robotics‚Äù Fall 2024, by Dr. Dimi Apostolopoulos. The credit for the full content goes to Professor Dimi along with several internet sources. You can see a clear relation of the below concepts with my MRSD Capstone Project, the Lunar ROADSTER ‚Äì check Conceptual Design Review Report (CoDRR).SystemDefinitionA system is an arrangement of parts or elements that work together toexhibit behavior or meaning that is not obtainable by the individualelements alone. It can be physical, conceptual, or a combination ofboth.Key aspects of a system include: Interconnected elements: A system comprises multiple components thatinteract with each other. Purposeful arrangement: The elements are organized in a specific wayto achieve one or more stated purposes. Emergent properties: The system exhibits behaviors orcharacteristics that cannot be attributed to any single component. Boundaries: Systems have defined boundaries that separate them fromtheir surroundings. Inputs and outputs: Systems typically receive inputs, process them,and produce outputs. System vs Product: A product is a system that has been specificallydesigned, developed, and packaged to meet user needs and be offered inthe market. It is typically a refined and polished version of a system.Common Characteristics of Man-Made Systems: Complexity: Man-made systems are typically complex, consisting ofmultiple interconnected components that work together to achieve aspecific purpose. Interdisciplinary Nature: These systems often require expertise fromvarious fields of engineering and other disciplines to design,develop, and maintain. Evolutionary Nature: Man-made systems tend to evolve over time, withupdates, improvements, and adaptations to meet changing requirementsor technological advancements. Team-Based Development: They are usually created and managed byteams of professionals with diverse specialties, reflecting theinterdisciplinary nature of the systems. Formal Systems Engineering Approach: The development and managementof these systems typically follow structured systems engineeringprocesses and methodologies. Meta System: Multiple systems can work together to form a \"Systemof Systems\", whose results cannot be obtained by a single system.What do Systems Entail? Hierarchy: Systems often have layered structures with subsystems andcomponents. Attributes: Characteristics of system elements, including functionaland non-functional properties. Relationships: Interconnections and interactions between systemelements. State: Condition of an element or the entire system at a specificpoint in time. Behaviors: Describes how the sequences of element states and systemresponses change over time. Processes: Sequence and timing of behaviors. Significance of Top-Down Representation of a System:Top-down representation involves starting with a high-level view of thesystem and progressively breaking it down into its constituent parts.Its importance: Comprehensive Understanding: It provides a holistic view of how allcomponents interact within the system, facilitating bettercomprehension of the overall functionality. Effective Problem-Solving: By identifying the system‚Äôs structure,potential issues can be localized more efficiently, allowing fortargeted interventions. Enhanced Planning: A top-down approach aids in organizing tasks andallocating resources effectively by establishing clear prioritiesand dependencies among components. Improved Communication: This method simplifies the explanation ofcomplex systems to stakeholders by presenting information in alogical and structured manner. Design Flexibility: Changes can be implemented at lower levelswithout necessitating a complete redesign of the entire system, thuspromoting adaptability. Functional vs Non-Functional AttributesFunctional Attributes: Define what the system or software shall do - specificfunctionalities and features. Specified by users/customers. Describe system behavior and functions that it should perform duringoperation. Captured in use cases. Mandatory to implement. It refers to the system as a whole. Non-Functional Attributes: Define how the system should perform or behave - quality attributes. Specified by technical teams (architects, developers). Describe system properties and constraints. Captured as quality attributes. Not always mandatory, but impact overall quality. Some non-functional attributes can include: Physical parameters - Size, weight, speed, range, accuracy, flowrate, throughput, etc. Deployment and distribution - Geographic deployment locations;Quantity of personnel, equipment, facilities, etc. at each location Life cycle horizon - Who will operate the system? For how long? Whatinventory of parts do I need to maintain? Utilization - How many hours/day, days/week, weeks/year will be thesystem be used? How many operational cycles/period? How will usersinteract with the system? Effectiveness - Availability, mean time between failures,maintenance downtime, operator skill level Environmental - Temperature, humidity, altitude, Airborne, ground,underwater Interoperability with other systems Key differences: Functional requirements specify \"what\" the system does, whilenon-functional requirements specify \"how\" it does it. Functional requirements are usually visible to users, whilenon-functional requirements are often behind-the-scenes qualities.Systems EngineeringIt focuses on defining customer needs and required functionality earlyin the development cycle, documenting requirements, and then proceedingwith design synthesis and system validation while considering thecomplete problem: operations, cost and schedule, performance, trainingand support, test, manufacturing, and disposal.Distinctions of systems engineering: Top-down, holistic approach: Views the entire system beforeexamining individual parts, considers system-wide goals andrequirements first, and ensures cohesive integration of allcomponents. Focused on the what, not the how: Defines system requirements andcapabilities, and avoids prescribing specific implementationmethods. Life-cycle oriented: Considers all stages from conception toretirement, addresses long-term sustainability and adaptability,plans for maintenance, upgrades, and eventual disposal. Interdisciplinary: Integrates knowledge from multiple fields,facilitates collaboration between different specialists, ensurescomprehensive consideration of all system aspects. Waterfall Model All major phases of design and engineering happen sequentially Each phase is distinct There is no iteration between phases Requires careful planning and execution Each phase must be carried out in great detail and address potentialrisks Effective if requirements are well defined, budget allows forcomplete development cycles, and prior similar developments exist Not suitable for large projects and brand new systems subject toevolving requirements REQUIREMENTS CAPTURE $‚Äì&gt;$ DESIGN SYSTEM $‚Äì&gt;$ BUILD SYSTEM $‚Äì&gt;$DEVELOPMENTAL TESTING $‚Äì&gt;$ ACCEPTANCE TESTING $‚Äì&gt;$ SYSTEM FIELDED ANDIN USESpiral Model Four major phases Iterative process Multiple waterfalls with feedback System matures with each iteration Various prototypes Prototyping follows risk identification More suitable for large projects and large budgets, handles changingrequirements, customers can use interim versions of system Complex to plan and execute, high cost, risks must be identifiedcorrectly V Model Left side of V: Requirements, architectures, design; fromhigher-level to detailed Bottom of V: Build, fabrication, assembly, prototyping, initialintegration Right of V: Full integration, testing, validation, acceptance; fromsmall unit testing to full-system testing and acceptance Testing matches level of system hierarchy, e.g. element designrequirements are validated at the unit testing level, subsystemdesign requirements are validated at the subsystem level, etc. Encourages careful definition of requirements and architecturesupfront. Detailed execution of the early stages is critical to thesuccess of the system and project Facilitates deliberate testing, debugging, and validation of smallersystem elements before escalating to larger elements Any change in requirements must be handled by a thorough review ofarchitecture and design and, if necessary, re-architecting andre-designing Since no early system prototypes are built, one must follow processin completing the testing and validation steps thoroughly to ensurecompliance to requirements For larger, more complex systems and projects we apply the V method inapplied R$\\&amp;$D.Agile Model Incorporates both incremental and iterative methods Emphasizes flexibility and adapting to changing priorities Uses short iterative cycles with frequent demonstrations and userfeedback Focuses on continuous delivery of value and customer satisfaction Better suited for projects with high uncertainty or changingrequirements For retrofits-for-automation and subsystem R$\\&amp;$D we apply agilemethods.\\What is the difference between Spiral model and Agile model? The spiral model is more structured, relies heavily on riskanalysis, and is best suited for larger, more complex projects. Itbalances planning and prototyping with iterative development. The agile model is more flexible, emphasizes on frequent delivery ofworking software, and continuously involves customer interaction. Itis best suited for projects that need to quickly adapt to changingrequirements. Key Considerations when Choosing a Systems Engineering Model Treatment of Requirements: Decide between defining all requirementsupfront or using an iterative approach. Attributes of System: Assess if the system is new, modified, orretrofitted, and its size (small, medium, large). Budget and Time Constraints: Evaluate available funding and projecttimelines for system delivery. Technical Expertise and Organizational Capabilities: Consider theskills of the team and the maturity of organizational processes. Treatment of Risk: Determine if the design will focus on riskmitigation and how to handle uncertainties. Concept Development PhaseIt is the first phase of the system life cycle. Its key commitments are: Function: Defined in the Functional Architecture What the system needs to do Capabilities and behaviors Form: Captured in the Cyberphysical Architecture How the system will be physically realized Hardware and software components Its main purposes are to: Develop detailed conceptual designs Conduct trade studies to evaluate alternatives Make important subsystem-level decisions Formulate approach for system development Outline project management strategy It sets the foundation for the entire project and bridges the gapbetween high-level needs and concrete system concepts.Needs AnalysisStudy, analyze, and assess: Study Market and Technology Data gathering: Collect relevant information Market analysis: Understand current market conditions and trends Techno-economic feasibility: Assess technical and economicviability Analyze Need Socio-economic objectives: Identify broader societal andeconomic goals Functional/operational objectives: Define specific systemfunctions and operations Key performance measures: Establish metrics to evaluate systemsuccess Assess Technology New or improvement: Determine if developing new technology orimproving existing State-of-art: Evaluate current technological landscape, identifyareas for improvement or innovation, set standards forcomparison Patents: Review existing patents to avoid infringement andidentify opportunities Concept Exploration - Requirements AnalysisRequirements translate stakeholder needs into specific, actionablecriteria for system design and development. Systems must comply tospecific requirements. They should describe the \"what\", not the\"how\", and must be unambiguous.Types and classification of Requirements: User requirements (needs, musts, wishes, wants...) System requirements (some user, some derived) -Functional/Performance requirements, Non-functional requirements Mandatory (threshold) and desirable (stretch, objective) Hardware and software subsystem requirements Interface requirements The life cycle of Requirements are: Elicitation (seek, extract) Development (formulate, write, edit) system,functional/non-functional Analysis (allocate, prioritize) subsystem, threshold/objective Tracking Validation Revision Performance Requirements: They define how well the system shouldperform its functional requirements and meet its capabilities. Keyfeatures include: Quantitative nature: They are measurable and expressed in specific,numerical terms. Minimal quantitative baselines are set, and can also include\"stretch\" goals for exceptional performance. They specify the level of performance for each functionalrequirement. They are reviewed and refined throughout concept development andpossibly engineering development phases. Functional requirements start with ShallPerformance requirements start with WillNon-functional requirements start with WillElicit RequirementsWhere do the requirements come from? These are some of the ways by whichrequirements can be elicited: Learn from users, customer, sponsor Learn from the application Study operational documents Study design documents Examine state-of-art and relevant designs Review market studies Review technology studies etc.While eliciting requirements, it is important to keep in mind the typeof system, such as new, upgrade or retrofit, and whether the system isbuilt for a user or stakeholder.User vs Stakeholder:Users directly interact with the system on a daily basis and havefirsthand experience with the system functionality and usability. Theyare responsible for operating, maintaining, and managing the system.Stakeholders are a broader group with interest or influence in thesystem. They can include users, but can also be non-users with someinterests in the system. They may not directly use the system but areaffected by its outcomes.Key difference between User and Stakeholder: All Users are Stakeholders, but not all Stakeholders are Users. Stakeholders typically impose design constraints: Cost and schedulelimitations; Standards, policies, and procedures to follow; Safetyand security requirements; Performance specifications; Systemqualities (e.g., reliability, maintainability). While Users provide valuable insights into day-to-day systemoperation, Stakeholders shape the broader context and constraintswithin which the system must function. Elicitation for a Novel SystemWhen we‚Äôre trying to figure out what a brand new system should do, weface both good things and tricky parts:Good Things: Big Picture Thinking: It‚Äôs easier to talk about what the systemshould do in general terms. We‚Äôre not stuck with old ideas. Blank Canvas: We can design the system however we want. There‚Äôs noold system holding us back. New Ideas Welcome: We can use the latest tech and come up with freshsolutions. Tricky Parts: No Users Yet: We might not have actual users to ask about what theyneed. Unfamiliar Territory: People aren‚Äôt used to this kind of system, sothey might struggle to tell us what they want. Seeing the Whole Picture: It‚Äôs hard to think of everything thesystem needs to do when it‚Äôs all new. Unclear Connections: We might not know how different parts of thesystem should work together at first. No Instructions: There are no old manuals or documents to look atfor guidance. What to Keep in Mind: Focus on what the system needs to achieve, not specific featuresyet. Use models or demos to help people imagine the system. Look at what similar products do and what people like or dislikeabout them. Brainstorm a lot and think about different situations where thesystem might be used. Be ready to change your ideas as you learn more. Note: When working on a completely new system, it‚Äôs important to becreative and flexible. Keep talking to people who might use or beaffected by the system, and don‚Äôt be afraid to change your plans as yougo along.Elicitation for Retrofitting/Upgrading a SystemGood Things: Experienced Users: People have used the system and know it well.They can tell us what‚Äôs good and what needs fixing. Real System to Look At: We can see and work with the actual system.This helps us understand how it works and what needs changing. Existing Documentation: There are usually manuals or guides aboutthe system. Tricky Parts: Limited by Old Design: The current system might restrict what we canchange. We might have to work around existing parts or features. Outdated Tech: The system might use old technology. This can make ithard to add new, modern features. Too Many Rules: The existing system might have lots of requirementsthat we have to follow. This can limit how much we can changethings. User Bias: People used to the old system might resist big changes.They might prefer familiar features, even if they‚Äôre not the best. What to Keep in Mind: Listen to users, but also think about new possibilities they mightnot have considered. Look for ways to improve the system while keeping what works well. Check if old documents are still accurate before relying on them. Balance keeping familiar features with introducing helpful new ones. Consider if some parts of the system need a complete overhaul ratherthan just an upgrade. Note: Upgrading an existing system means working with what‚Äôs alreadythere. It‚Äôs about finding the right balance between keeping what worksand making important improvements.Methods and Techniques to Elicit Requirements: Objectives Tree Use Cases Surveys/Questionnaires Focus Groups Interviews Prototypingetc.Objectives TreeAn Objectives Tree is like a family tree, but for our project goals. Ithelps us organize what we want to achieve, from big ideas down tospecific tasks. It‚Äôs a great way to make sure everyone understands thebig picture and their part in it. Key features of objectives trees are: Top-Down Structure: Starts with our main goal at the top, thenbranches out into smaller objectives. The objectives become morespecific as we go down the tree. Breaks Down Big Goals: Takes our big idea and splits it intosmaller, manageable pieces. Shows Relationships: We can see how different objectives connect toeach other. This helps us understand which tasks support whichgoals. Use CasesUse Cases are like stories that explain how people will use a system toget things done. They‚Äôre really helpful when designing new systems orimproving old ones. They‚Äôre a great way to make sure everyoneunderstands what the system needs to do, without getting lost intechnical details too early. Key features of use cases are: Collection of Scenarios: They‚Äôre a bunch of examples showing how thesystem will be used. Think of them as \"day in the life\" stories ofsystem users. Goal-Focused: Each Use Case shows how the system helps achieve aspecific goal or task. Multiple Cases Needed: Complex systems need several Use Cases tocover everything. Not About How, But What: Use Cases describe what the system does,not how it does it. Communication Tool: They help designers and customers understandeach other better. How should the use cases be written? We should focus entirely on the user and what they want to doinstead of the system doing things. The title should be what the goal is or what the user wants toachieve. Clearly define the \"actor\" that desires the goal. This could be aperson or an interfacing system. Write about what should happen when everything goes right. Don‚Äôtworry about the failure cases. Narrate the story in such a way that both technical andnon-technical people can readily understand it. It doesn‚Äôt matter if a step is done by a person, software, ormachine. Concept Definition - System ArchitecturesSystem Architecture is like creating a blueprint for a complex machineor process. It‚Äôs important because it helps us understand how everythingworks together. It shows where you‚Äôre going, how all the parts connect,and helps everyone work together more smoothly. It‚Äôs a tool that makescomplex systems easier to understand, build, and manage.It captures the flow of functions and relationships and capturesbreak-down of systems from top to down. There are two mains types ofsystem architectures: Functional Architecture Cyberphysical Architecture - Cyber (software) + physical (hardware) The cyberphysical architecture is derived from the functionalarchitecture.Difference between Functional Architecture and CyberphysicalArchitecture:Functional architecture describes what the system shall do, i.e., thefunctions and attributes of the components of the system. It focuses onthe logical components and their interactions. They are typicallyrepresented using functional block diagrams.Cyberphysical architecture describes how the system will be physicallyrealised. It focuses on the physical, hardware and software components.They are typically represented by system block diagrams.Functional Decomposition MethodIt comprises of two main steps: Identify the functions carried out by the system Apply different ways to logically connect functions Key points to keep in mind while developing a functional architecture: The functions are identified by needs analysis and elicitingrequirements using objective trees, use cases, etc. It is essential to have formulated the functional requirementsbefore developing the functional architecture. Apply different ways to logically connect the functions. Capture the \"what\", not the \"how\". Think of transformation of material, information and energy. Start synthesizing architectures assuming little or no sharing offunctions among subsystems. Deciding on connectivity and flow for our architectures is criticalto what our system will be. Include only what is needed and makes sense. It can be reiteratedduring the concept development phase. Morphological ChartsMorphological charts aid in concept synthesis by: Breaking down the system into key functional attributes Explore different possible solutions for each function Allowing systematic exploration of different combinations ofsolutions Their significance: Encouraging creative thinking and innovation Providing a structured approach to generating diverse systemconcepts Facilitating the identification of novel combinations that might notbe immediately obvious Supporting a comprehensive exploration of the solution space Trade StudiesIt is a systematic and structured process for comparing and selectingamong different alternatives or options for a system design or solution.It is used to inform decisions by methodically framing the trade spaceand systematically evaluating alternatives. They help balance multiple,often conflicting criteria to find the best overall solution.Weighted Objectives Method:It is a specific technique used within trade studies and involvesassigning numerical values to features or alternatives based onpredefined criteria. Each criterion is given a different weight based onits importance.How to perform? Identify and list all possible options Define relevant criteria Assign numeric weighting values to each criterion (ensure weights total to 100) Score each option against the criteria (e.g., on a scale of 1-5 or1-10) Calculate weighted scores by multiplying the score by the weightingvalue Sum up the total score for each option Compare the results to identify the best alternative What are the evaluation criteria and weighting factors?Evaluation criteria: Performance requirements Non-functional requirements Life-cycle parameters (cost, reliability, sustainability, etc.) Technical specifications (manuals, product reports, etc.) Weighting factors: Customer priorities, market pull, technology push, etc. Team and project specific prioritization Requires a lot of communication with the stakeholders Significance: Provides a structured, objective approach to decision-making Allows for consideration of multiple factors with varying importance Helps justify decisions with quantitative data Useful for complex decisions with multiple stakeholders Can be applied to various fields including systems engineering,product management, and project planning Project ManagementIntroductionProject management is the application of knowledge, skills, andtechniques to execute projects effectively and efficiently. It‚Äôs astrategic competency for organizations, enabling them to tie projectresults to business goals ‚Äî and thus, better compete in their markets.Perspective ModelThe Perspective Model for Project Management, as depicted in the image,outlines a systematic, structured approach to managing projects frominitiation to closure.How does this translate to the V-model of systems engineering? The perspective model starts by defining and initiating the project,and formulating its goals, objectives and scopes. This is translatedto V-model‚Äôs focus on developing requirements. In the Perspective Model, detailed work plans, schedules, budgets,and risk management are created. This aligns with the V-Model‚Äôsdecomposition and detailing phase, where system architecture andsubsystem designs are detailed. The Perspective Model implements the plan, while the V-Modeltransitions to fabrication and assembling elements. The Perspective Model monitors progress, comparable to the V-Model‚Äôsintegration and validation phase, which tests elements, subsystems,and the entire system. This stage involves proper tracking,evaluating, iterating, and replanning if necessary. Both models conclude by ensuring the final output meets the initialobjectives‚ÄîPerspective Model through project evaluation andclosure, and V-Model through acceptance testing and systemvalidation. Systems engineering is concerned with a top-down system-oriented lifecycle. Project management is concerned with a bottom-up project-focusedlife cycle.Initiating a ProjectIn order to obtain formal authorization to start the project, thefollowing major initiation/formulation documents are required: Statement of Work - Discusses the work to be accomplished, inputrequirements, work not to be accomplished, and specific results anddeliverables. Technical Proposal - Details the technical approach andmethodologies Management Proposal - Outlines team structure, schedule, and riskmanagement Cost Proposal - Breaks down project costs and budget allocation Develop a Work PlanA top-down breakdown of the work needs to be performed. This should bedone in levels as: Level 1: Total scope, system, product Level 2: Sub-projects, subsystems, project activities; Technical,management, facilities, etc. Level 3: Functions, activities, major tasks, assemblies/components Work Breakdown StructureThe WBS is a tool to break down the total scope of a project intosmaller, manageable components. It provides a structured view of whatneeds to be done, facilitating planning, assigning tasks, trackingprogress, and managing resources. The WBS helps ensure that no importanttasks are overlooked, and it provides clarity on how each task fits intothe overall project.Product-Oriented WBS: Focus - The breakdown is based on the product or service beingcreated or delivered. This approach organizes work around theproduct‚Äôs features, components, and deliverables. Structure - Focuses on specific product components or sub-systemsthat need to be built or created. Advantages - It makes the product features clear and shows howindividual components fit together. It focuses on product outputs,so progress can be tracked by evaluating the completion of productcomponents. Process-Oriented WBS: Focus - Focuses on the processes or phases required to complete theproject. It breaks down the work based on the sequence of activitiesor workflows involved in delivering the product or service. Structure - Organizes the project into processes, procedures, orphases, detailing the steps or activities required to move throughthe project lifecycle. Advantages - It provides insight into the steps and processesinvolved in delivering the project, helping with resource allocationand scheduling. It works well when the project requires specificworkflows or well-defined phases to be followed in sequence. DictionaryWhen analyzing each work package in a Work Breakdown Structure, it isessential to capture detailed information to ensure proper management,execution, and tracking of the project. A WBS Dictionary is typicallyused to document this information, providing an organized descriptionfor each work package, task, or deliverable. This ensures clarity andconsistency across the project.Key components: WBS Work Package Task: Specific task/activity Estimated Level of Effort: By first order estimation and refinethrough iterations (in hours) Owner: Specify primary owner and secondary owner Resources Needed: Number and specialty of engineers, manager,subcontractors, Design, build, test facilities, hardware/softwaretools, budget Work Products: Quantified output of work, Cyberphysical product,Documents, Deliverables, Recipient of delvierables Description: Precise description of task and breakdown of what eachsubtasks are involved Input: Define connection to other work packages Dependencies Risks: First pass on identifying risks, The lower the level of thework package, the more technical risks dominate, Risks related toscope of tasks, resources, budget, effort allocation Develop a ScheduleThe following factors need to be considered for effective estimation ofa project schedule: Work Effort vs. Duration: Work effort refers to labor hours (e.g.,40 hours); duration accounts for calendar time, factoring inavailability and efficiency. Account for non-project activities such as administrative tasks,approvals, and external dependencies such as delays from vendors. Estimation at the Lowest Level: Break tasks into smaller workpackages or activities to ensure precision, and roll up estimatesfor an overall view. Skill Set and Expertise: Identify and assign specific skills neededfor the task; Leverage experienced resources for faster, moreaccurate task completion; Use lessons learned from similar projectsto improve accuracy. Prior Estimations and Performance: Analyze past estimates to refinecurrent ones; Identify recurring inaccuracies to enhance futureplanning. Contingency Planning: Add time and cost buffers to account for risksand unforeseen challenges; Link contingency to specific risksidentified in the WBS. By addressing these considerations, estimates become more realistic,reducing risks and improving project outcomes.MilestonesTypes of milestones: External: These are mandated by the sponsor Required Internal: Based on project flow such as integration, firstroll-out, etc. Non-mandatory Internal: Used to facilitate management ofactivities/tasks, costs, schedule, and risks Comments on milestones: Milestones are significant zero-duration events in the life-cycle ofa project Changing a milestone impacts all facets of the project If an external milestone is changed, a partial or full projectre-planning is required External milestones are rigidly defined and cannot be changed unlessthere is formal contract modification Internal milestones should be treated as if they were external Risk ManagementIssue vs RiskIssue: An issue is a current problem or challenge that is activelyimpacting the project. It requires immediate attention to resolve ormitigate its impact.Risk: A risk is a potential problem or uncertainty that could occurin the future and might affect the project. It requires planning toprevent or minimize impact if the risk materializes.Components of a Risk Root cause: The underlying reason or source of the risk. Identifyingthe root cause helps in understanding why the risk might occur andprovides insights for mitigation strategies Probability: The likelihood or chance of the risk occurring. This isoften assessed on a scale (e.g., low, medium, high) or as apercentage. Understanding probability helps prioritize risks andfocus on those more likely to happen The potential impact or effect the risk would have if itmaterializes. Consequences can affect scope, schedule, cost, orquality and are often quantified in terms of severity (e.g.,minimal, moderate, critical) ApproachThe following approach should be followed to manage risks in a project:Identify risks (what could go wrong?): Review your WBS work packages Examine what could go wrong Compile a list of potential risk root causes Analyze risks (how big is the risk?): Use likelihood and consequences matrix to rank Be conservative for the initial pass Discuss with stakeholders Formulate action plan: Identify simple, realistic actions you will take - eliminate rootcause if possible, or transfer risk if possible, or assume risk andcontinue State what will be done, when, and by whom Set success metrics to mitigate risk (specific quantitative targets) " }, { "title": "Controller for Autonomous Surface Vehicle", "url": "/posts/asv/", "categories": "Projects, Robotics", "tags": "matlab, underwater, dynamics, simulation, pid, control", "date": "2023-12-04 21:30:00 +0530", "snippet": "" }, { "title": "Development of Python-Based Simulator for Analyzing Autonomous Underwater Glider Motions and Performance", "url": "/posts/aug-simulator/", "categories": "Projects, Robotics", "tags": "python, underwater, dynamics, simulation, pid, control", "date": "2023-12-04 21:30:00 +0530", "snippet": "The following project was done as part of my undergraduate thesis at the Institute for Systems and Robotics, Lisbon under the supervision of Dr. David Cabecinhas and Dr. Pedro BatistaAbstractThe objective of this thesis is to design, develop, and implement a comprehensive Python-based simulator for Autonomous Underwater Gliders (AUGs), enabling the precise simulation and in-depth analysis of various motions exhibited by them. Underwater Gliders are a unique kind of vehicle that fall under the class of Autonomous Underwater Vehicles (AUVs), which maneuver through a water body by varying their buoyancy and attitude using internal mass actuators and a buoyancy engine.The simulator will be used in analyzing the dynamics and behaviors of various glider models, such as the Slocum, Spray, and Rogue, both in a simplified two dimensional setting, as well as in full three dimensional gliding. Marine researchers can configure the simulator to specific mission scenarios and for specific glider designs.The development of such a robust platform will serve as a valuable resource for researchers and practitioners in the field of marine robotics by facilitating the examination and evaluation of different motion patterns. This enhances our understanding of AUG behavior and performance, thereby furthering the development of such vehicles.IntroductionAutonomous Underwater GlidersAutonomous Underwater Gliders (AUGs) are a class of underwater robots that fall under the category of Autonomous Underwater Vehicles (AUVs). Unlike traditional propeller-driven vehicles that use external thrusters to move, AUGs are set apart because of their reliance on buoyancy-driven propulsion systems. This unique feature enables these gliders to achieve sustained vertical movement through the water column by harnessing changes in buoyancy and hydrodynamic forces.DesignThe design of AUGs can vary slightly depending on the models and the applications that they serve. However, most of them have a cylindrical hull, and are roughly shaped like a torpedo. They are typically small and reusable vehicles, with a weight of around 50 kg and 2m length, making them suitable for small missions and easily operated by fewer people. They are capable of operating at low speeds of 20-30 cm/s and can traverse thousands of kilometers over several months.Several AUG models have been developed over the past years, such as the Slocum, Spray, and Seaglider. Each of these AUG models have slightly different design approaches, including battery and thermal powered propulsion, speed and depth capabilities, strategies to reduce drag, wing spans, and communication methods. Despite these differences, all of them have some standard dynamic characteristics, which will be discussed in Chapter 3.ApplicationsSome of the applications of AUGs include: Study of ocean currents - Equipped with various sensors, AUGs can collect data on water temperature, salanity, and other parameters to better understand the patterns of ocean currents. They can also measure the levels of dissolved oxygen, pH, and other nutrients to assess the health of marine ecosystems. Climate change - They can help in predicting the rise in sea level due to the melting of polar ice caps, and can also monitor the levels of carbon dioxide in the water. Defense - AUGs can be used by the military/Navy for monitoring underwater activities, locate sea mines, and carry out secret missions. Disaster management - They can be used to detect changes in the oceans, such as temperature, vibrations in the sea floor that may indicate the occurrence of a tsunami or a hurricane.MotivationThe development of a simulator for AUGs would help researchers to assess glider behavior under various conditions, validate algorithms, and conduct virtual missions.Need for a SimulatorThe use of AUGs has become increasingly important in the field of oceanography due to their exceptional efficiency and adaptability in collecting valuable data. These underwater vehicles are designed to independently navigate the ocean, carrying out various missions ranging from oceanographic data collection to environmental monitoring. This makes them essential assets for understanding and addressing critical challenges in marine science and technology.Despite their growing importance in marine research and exploration, a noticeable shortage of accessible simulators specifically for the analysis and testing of underwater gliders persists. This glaring gap in available simulation tools presents a significant challenge to researchers and engineers in the field. The absence of user-friendly, open-source simulators severely limits our ability to comprehensively study and validate glider behaviors.The analysis of the dynamics of AUGs is a highly sophisticated and challenging task. The core features may differ for different glider models, which can influence the dynamics and behaviors of the AUG, such as different mechanisms for internal mass actuation and ballast systems, use of external rudders, etc. This necessitates the need for a robust simulator that can be easily tailored to different AUG models depending on their application.In the future, the use of a simulator can contribute to the development of methods for improved performance in specific flight regimes, as well as the testing of novel flight trajectories, optimization of control algorithms, and conducting virtual experiments. In addition, the simulator can allow for the comparison of performance of glider models such as the Slocum, Spray, and Seaglider.Thus, this thesis aims to address this pressing need by developing an open-source simulator specifically for AUGs, thereby paving the way for the development of more efficient and advanced glider technologies.Existing Underwater SimulatorsWhile there do exist some underwater simulators, specifically for Autonomous Underwater Vehicles (AUVs), they are either not very suitable for AUGs, not very efficient, or not open-source.Every AUG model, such as the Slocum, and Spray, comes with a ‚ÄúSimulation‚Äù option that can help in planning missions. This however, carries out simulations in real-time, due to which a simulation to plan missions for a week will take a week to run.The following are a few existing open-source simulators:DAVEDAVE Aquatic Virtual Environment (DAVE) is a specialized simulation platform built on ROS and Gazebo, and is designed for the testing of underwater robots. It focuses on autonomous underwater vehicles (AUVs/UUVs) and their ability to carry out missions involving tasks like autonomous manipulation.With ROS being widely used among the robotics community, this simulator offers great functionality and robustness. However, it is based on the kinematic model of an AUG, which is not very reliable. Interfacing the physical glider‚Äôs simulation API with ROS is also a challenging task. Moreover, reading the glider data, such as position of center of mass, variation of internal mass actuators, etc. is not straightforward to do. Additionally, changing the glider model, or its system dynamics is tedious, which necessitates the need for a more simple and efficient simulator.Github: DAVE SimulatorWAVEThe WAVE simulator is a MATLAB based simulator that can be used to evaluate and improve the performance of an AUV by varying the blade geometry and braking strategies of the arms. It can be used in the development and testing of wing profiles and braking strategies.Github: WAVE SimulatorFISH GYMFish Gym is a physics-based simulation platform particularly for bionic underwater robots. It is integrated into the OpenAI Gym interface, thereby enabling the use of reinforcement learning algorithms and control techniques on the underwater agents.Underlying Principles of Glider BehaviorAs mentioned in Fossen‚Äôs ‚ÄúGuidance and Control of Ocean Vehicles‚Äù, the modeling of marine vehicles requires the study of statics and dynamics. Statics deals with the equilibrium of bodies, i.e., bodies at rest or moving with constant velocity. Dynamics is concerned with bodies having accelerated motion. Dynamics, in turn, is divided into Kinematics and Kinetics. Kinematics treats only the geometrical aspects of motion, and Kinetics is the analysis of forces that cause this motion.The overall kinematics and dynamics of AUGs are derived below, and the model presented here follows the glider model presented by Graver.KinematicsDefining two separate frames of reference makes it easier to model an AUG having 6 Degrees of Freedom (DOF). These are:Inertial FrameThe Inertial Frame serves as a global reference frame, as it is fixed to the surface of the Earth. This frame lets us understand the position of the glider relative to the Earth, enabling precise navigation and global planning. The accelerations of a point on the Earth‚Äôs surface can be neglected since the motion of the Earth hardly affects low speed marine vehicles.Let $xyz$ be the fixed inertial frame, such that the $x$ and $y$ axes lie in the horizontal plane, perpendicular to gravity. The $z$ axis points downward in the direction of gravity and denotes depth. The frame can be chosen on the surface of the water body such that $z = 0$.Body-Fixed FrameThe Body-Fixed Frame is attached to the glider‚Äôs body and moves along with it. It allows for measuring the internal dynamics of the glider, and control inputs. This local reference is vital for precise maneuvers and simplifying how we gather information from the on-board sensors.The Origin $O$ of the body-fixed frame usually coincides with the vehicle‚Äôs Center of Buoyancy (CoB). The body axes $X_{0}$, $Y_{0}$, $Z_{0}$ coincide with the principal axes of inertia: $X_{0}$ is the longitudinal axis - from aft to fore $Y_{0}$ is the transverse axis - directed to the starboard $Z_{0}$ is the normal axis - from top to bottomThe general motion of an AUG in 6 DOF can be described by the following vectors:\\[\\begin{align}{\\boldsymbol{\\eta}=[\\boldsymbol{b}^T,\\boldsymbol{\\eta_2}^T]^{T};\\ \\ \\ \\boldsymbol{b}=[x,y,z]^{T};\\ \\ \\ \\boldsymbol{\\eta_2}=[\\phi,\\theta,\\psi]^{T}}\\\\ {\\boldsymbol{\\nu}=[\\boldsymbol{\\nu}^{T},\\boldsymbol{\\Omega}^{T}]^{T};\\ \\ \\ \\boldsymbol{v}=[v_1,v_2,v_3]^{T};\\ \\ \\boldsymbol{\\Omega}=[\\Omega_1,\\Omega_2,\\Omega_3]^{T}}\\\\ {\\boldsymbol{\\tau}=[\\boldsymbol{\\tau_{1}}^{T},\\boldsymbol{\\tau_{2}}^{T}]^{T};\\ \\ \\ \\boldsymbol{\\tau_1}=[X, Y, Z]^{T};\\ \\ \\ \\boldsymbol{\\tau_{2}}=[K,M,N]^{T}}\\end{align}\\]Where $\\boldsymbol{\\eta}$ is the position and orientation in the inertial frame, $\\boldsymbol{\\nu}$ is the linear and angular velocity in the body-fixed frame, and $\\boldsymbol{\\tau}$ is the force and moment in the body-fixed frame.The body-fixed frame can be related to the earth-fixed frame with the linear velocity and angular velocity transformations, using the Euler angles: roll ($\\phi$), pitch ($\\theta$), and yaw ($\\psi)$.The vehicle‚Äôs vectors in the inertial frame is given by the velocity transformation:\\[\\begin{align} \\boldsymbol{\\dot{b}={J}_{1}({\\eta}_{2})v}\\end{align}\\]Where \\({J}_{1}({\\eta}_{2})\\) is the transformation matrix. For simple principal rotations,\\[\\begin{align} \\boldsymbol{J_1\\left(\\eta_2\\right)}=\\left[\\begin{array}{ccc}\\mathrm{c} \\psi \\mathrm{c} \\theta &amp; -\\mathrm{s} \\psi \\mathrm{c} \\phi+\\mathrm{c} \\psi \\mathrm{s} \\theta \\mathrm{s} \\phi &amp; \\mathrm{s} \\psi \\mathrm{s} \\phi+\\mathrm{c} \\psi \\mathrm{c} \\phi \\mathrm{s} \\theta \\\\\\mathrm{s} \\psi \\mathrm{c} \\theta &amp; \\mathrm{c} \\psi \\mathrm{c} \\phi+\\mathrm{s} \\phi \\mathrm{s} \\theta \\mathrm{s} \\psi &amp; -\\mathrm{c} \\psi \\mathrm{s} \\phi+\\mathrm{s} \\theta \\mathrm{s} \\psi \\mathrm{c} \\phi \\\\-\\mathrm{s} \\theta &amp; \\mathrm{c} \\theta \\mathrm{c} \\phi &amp; \\mathrm{c} \\theta \\mathrm{c} \\phi\\end{array}\\right]\\end{align}\\]The angular velocity transformation is given by\\[\\begin{align}\\boldsymbol{\\dot{\\eta}_2=J_2\\left(\\eta_2\\right) \\Omega}\\end{align}\\]\\[\\begin{align}\\boldsymbol{J_2\\left(\\eta_2\\right)}=\\left[\\begin{array}{ccc}1 &amp; s \\phi \\mathrm{t} \\theta &amp; c \\phi \\mathrm{t} \\theta \\\\0 &amp; c \\phi &amp; -s \\phi \\\\0 &amp; s \\phi / c \\theta &amp; c \\phi / c \\theta\\end{array}\\right]\\end{align}\\]Thus, the kinematic equations can be expressed in the vector form as\\[\\begin{align}\\left[\\begin{array}{c}\\boldsymbol{\\dot{b}} \\\\\\boldsymbol{\\dot{\\eta}_2}\\end{array}\\right]=\\left[\\begin{array}{cc}\\boldsymbol{J_1\\left(\\eta_2\\right)} &amp; \\boldsymbol{0_{3 \\times 3}} \\\\\\boldsymbol{0_{3 \\times 3}} &amp; \\boldsymbol{J_2\\left(\\eta_2\\right)}\\end{array}\\right]\\left[\\begin{array}{c}\\boldsymbol{v} \\\\\\boldsymbol{\\Omega}\\end{array}\\right]\\end{align}\\]Since we use \\(\\boldsymbol{J_{1}(\\eta_{2})}\\) very often, we replace this rotation matrix with $\\boldsymbol{R}$ to map vectors expressed in the body frame to the inertial frame.Vehicle ModelThe underwater glider is considered to be a rigid body fitted with hydrofoils, which are underwater wings that allow it to glide forward, and a tail. The glider dynamics are derived by considering the entire frame to be immersed in a fluid.The glider comprises of the following masses: The hull mass $m_{h}$, which is fixed and uniformly distributed throughout the body. The ballast mass $m_{b}$ which varies as water is pumped in and out of the system, and whose position is fixed with respect to the vehicle‚Äôs CoB. The internal moving mass $\\bar{m}$ which moves inside the glider‚Äôs body but has a fixed mass. The static offset mass $m_{w}$ which is a fixed point mass that can be offset from the vehicle‚Äôs CoB.The stationary mass of the glider is given by \\begin{align} m_{s} = m_{h} + m_{b} + m_{w} \\end{align}The total mass of the vehicle is then \\begin{align} m_{t} = m_{h} + m_{b} + m_{w} + \\bar{m} = m_{s} + \\bar{m}\\end{align}The positions of the internal point masses $m_{b}$ and $m_{w}$ inside the glider frame is given by the vectors $\\boldsymbol{r_{b}}$ and $\\boldsymbol{r_{w}}$ with respect to the Center of Buoyancy (CoB). The vector $\\boldsymbol{r_{p}(t)}$ gives the position of the movable mass $\\bar{m}$ in the body-fixed frame.The ballast mass $m_{b}$ changes as water is pumped in or emptied off of the buoyancy compartments in order to vary the vehicle‚Äôs buoyancy. The movable mass $\\bar{m}$ is used to vary the pitch of the glider, which together with the ballast mass $m_{b}$ helps the glider maneuver. The offset static point mass $m_{w}$ can be set to balance the rolling and pitching moment on the glider at equilibrium.We denote $m$ to be the mass of fluid displaced by the glider. As a result, the net buoyancy is given by \\begin{align} m_{0} = m_{t} - m \\end{align}When $m_{0}$ is positive, it means that the vehicle is negatively buoyant (it sinks), and similarly, when $m_{0}$ is negative, it means that the vehicle is positively buoyant (it floats).Ballast System and Controlled MassesThe ballast system is responsible for varying the AUGs depth within the water column. It comprises of volume compartments that can be flooded or emptied with water to change the glider‚Äôs buoyancy. By doing so, the glider can achieve positive, negative, or neutral buoyancy, and can either ascend or descend. Positive buoyancy - glider floats upward Negative buoyancy - glider sinks downward Neutral buoyancy - glider neither floats nor sinksThe type of ballast system used and its position inside the glider is motivated by the geometrical design of the AUG model. For most this thesis we model the Slocum glider, in which the ballast mass $m_{b}$ and internal moving mass $\\bar{m}$ are both well forward of the glider‚Äôs CoB. The Slocum design uses a syringe-type ballast tank to take in or pump out water.An important control input for the ballast mass is the ballast pumping rate $\\dot{m_{b}}$. We assume that when ballast is pumped into the glider, it doesn‚Äôt create any significant thrust or turning forces, similar to how a rocket‚Äôs thrust is generated by ejecting mass. For existing gliders used in oceanography, the ballast masses are very small compared to their total mass. Moreover, the ballast is ejected with very low relative velocity, considering the glider‚Äôs own movement.Moving the internal mass $\\bar{m}$ allows the glider to vary its pitch. When actuated in more than one direction, it can produce other motions such as roll and yaw. Some glider designs can also have two internal moving masses, with one mass actuated in the roll direction and the other in the pitch direction. Making additions like these to the system dynamics is a straightforward task.Specifying control to the internal masses $m_{b}$, $\\bar{m}$, and $m_{w}$ can be done two ways: Vector of forces Vector of accelerationsThe latter method of providing control inputs, that is, as a vector of accelerations of the internal point masses is preferred because of the following reasons: It provides a suspension system for the internal masses by preventing them from moving within the glider in response to its motions. It more closes approximates the internal mass systems of existing glider designs. It can fix some point masses in place by specifying their corresponding velocities and accelerations to zero.Forces and TorquesRestoring ForcesRestoring forces are the forces that tend to bring the glider back to its equilibrium position and counteract any deviations from the desired trajectories. There are two types of restoring forces acting on AUGs: Buoyant Force: This is the upward force acting on the immersed glider, at its Center of Buoyancy (CoB), by the surrounding fluid (water in our case) as per the Archimedes‚Äô Principle. It is dependent on the volume of water displaced by the glider and the difference in densities between the glider and the water. Gravitational Force: This is the downward force acting on the glider, at its Center of Mass (CoG), and counteracts the upward buoyant force.The gravitational and buoyant forces experienced by the glider are given by,\\[\\begin{align} \\boldsymbol{f_{gravity}} = m_{t}g(\\boldsymbol{R}^T\\boldsymbol{k}) \\\\ \\boldsymbol{f_{buoyancy}} = -mg(\\boldsymbol{R}^T\\boldsymbol{k})\\end{align}\\]Hydrodynamic ForcesHydrodynamic forces are the forces acting on the glider by the surrounding fluid as its moves through it, thereby influencing its motion. There are three components of hydrodynamic forces acting on the glider: Drag Force: It is the resistance experienced by the AUG as it moves through the water and acts opposite to the direction of velocity. It is dependent on the properties of the fluid, as well as the glider speed and surface area. Side Force: It acts horizontally, perpendicular to the glider‚Äôs longitudinal axis, and influences the yaw or turning motion of the glider. The amount of side force can be adjusted by controlling the angles of external surfaces like rudders that contribute to its generation. Lift Force: It acts perpendicular to the direction of the vehicle‚Äôs motion and is generated as the water flows over and around the glider‚Äôs surface. Although typically smaller in AUGs compared to airplanes, they can affect glider stability and influence its motion.The drag, lift and side forces act at the Center of Pressure (CoP) of the vehicle. The expressions for these forces experienced by the glider are given by,\\[\\begin{align} D &amp; =\\frac{1}{2} \\rho C_D(\\alpha) A V^2 \\approx\\left(K_{D_0}+K_D \\alpha^2\\right)\\left(V^2\\right)\\\\ SF &amp; =\\frac{1}{2} \\rho C_{SF}(\\beta) A V^2 \\approx\\left(K_{\\beta}\\beta\\right)\\left(V^2\\right) \\\\ L &amp; =\\frac{1}{2} \\rho C_L(\\alpha) A V^2 \\approx\\left(K_{L_0}+K_L \\alpha\\right)\\left(V^2\\right) \\end{align}\\]where $\\alpha$ is the angle of attack, $\\beta$ is the side-slip angle, $C_{D}$, $C_{SF}$ and $C_{L}$ are standard aerodynamic drag, side force and lift coefficients that are a function of $\\alpha$ or $\\beta$, $A$ is the maximum cross sectional area of the glider, and $\\rho$ is the density of the fluid. $K_{D}$, $K_{D_{0}}$ are the drag coefficients, $K_{\\beta}$ is the side force coefficient, and $K_{L}$, $K_{L_{0}}$ are the lift coefficients. $V$ is the velocity of the glider.Torques and MomentsThe restoring and hydrodynamic forces can generate torques and moments that affects the glider‚Äôs rotational motion.TorquesA torque is generated when there is an offset between the glider CoG and CoB. Ideally, the CoB of the glider should be directly above its CoG.The CoG is the weighted centroid of the vehicle and is given by,\\[\\begin{align} \\boldsymbol{r_{C G}}\\ =\\ \\frac{\\sum_{i}m_{i}r_{i}}{\\sum m_{i}}\\ =\\ \\frac{m_{h}r_{h}+m_{w}r_{w}+m_{b}r_{b}+\\bar{m}r_{p}}{m_{h}+m_{w}+m_{b}+\\bar{m}}\\end{align}\\]Since the CoG of the uniformly distributed hull mass always coincides with the CoB of the glider (which is also the origin of the body-fixed coordinate frame), $r_{h} = 0.$The torque on the glider due to the restoring forces is given by,\\[\\begin{align} \\boldsymbol{\\tau_{gravity}} = \\boldsymbol{r_{CG}} \\times m_tg(\\boldsymbol{R}^T\\boldsymbol{k})\\end{align}\\]Since the origin of the body-fixed frame coincides with the CoB and the torque due to buoyancy acts at the vehicle CoB, it is equal to zero.\\[\\begin{align} \\boldsymbol{\\tau_{buoyancy}} = 0\\end{align}\\]MomentsA moment is generated when there is an offset between the glider CoG and Center of Pressure (CoP). A glider whose CoP is forward of its CoG will have a positive hydrodynamic pitching moment at equilibrium.The expression for hydrodynamic moments experienced by a glider is given by,\\[\\begin{align}M_{D L_1} &amp; =K_{M R} \\beta V^2+K_{q 1} \\Omega_1 V^2 \\\\M_{D L_2} &amp; =\\left(K_{M 0}+K_M \\alpha+K_{q 2} \\Omega_2\\right) V^2 \\\\M_{D L_3} &amp; =K_{M Y} \\beta V^2+K_{q 3} \\Omega_3 V^2\\end{align}\\]where $\\beta$ is the side-slip angle, $K_{MR}$, $K_{M}$, $K_{M_{0}}$, $K_{MY}$ are the moment coefficients, $K_{q1}$, $K_{q2}$, $K_{q3}$ are the rotational damping coefficients, and $\\Omega_{1}$, $\\Omega_{2}$, $\\Omega_{3}$ are the components of the glider‚Äôs angular velocity.Ultimately, the viscous forces and moments are written as,\\[\\begin{align}\\boldsymbol{F}_{\\boldsymbol{e x t}}=\\left(\\begin{array}{c}-D \\\\S F \\\\-L\\end{array}\\right) \\text { and } \\boldsymbol{T}_{\\boldsymbol{e x t}}=\\left(\\begin{array}{c}M_{D L_1} \\\\M_{D L_2} \\\\M_{D L_3}\\end{array}\\right)\\end{align}\\]DynamicsEquations of MotionThe complete equations of motion of an AUG in three dimensions is given by\\[\\begin{align}\\left(\\begin{array}{c}\\dot{R} \\\\\\dot{b} \\\\\\dot{\\Omega} \\\\\\dot{v} \\\\\\dot{r}_p \\\\\\dot{r}_b \\\\\\dot{r}_{\\boldsymbol{w}} \\\\\\dot{P}_{\\boldsymbol{p}} \\\\\\dot{P}_b \\\\\\dot{P}_{\\boldsymbol{w}} \\\\\\dot{m}_b\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{R} \\hat{\\boldsymbol{\\Omega}} \\\\\\boldsymbol{R} \\boldsymbol{v} \\\\\\boldsymbol{J}^{-1} \\overline{\\boldsymbol{T}} \\\\\\boldsymbol{M}^{-1} \\overline{\\boldsymbol{F}} \\\\\\frac{1}{\\bar{m}} \\boldsymbol{P}_{\\boldsymbol{p}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}} \\\\\\frac{1}{m_b} \\boldsymbol{P}_{\\boldsymbol{b}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}} \\\\\\frac{1}{m_w} \\boldsymbol{P}_{\\boldsymbol{w}}-\\boldsymbol{v}-\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}} \\\\\\boldsymbol{\\bar{u}} \\\\\\boldsymbol{u}_{\\boldsymbol{b}} \\\\\\boldsymbol{u}_{\\boldsymbol{w}} \\\\u_{ballastrate}\\end{array}\\right)\\end{align}\\]where\\[\\begin{align}\\overline{\\boldsymbol{T}}= &amp; \\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_p \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_b \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_w \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})\\nonumber \\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}}+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\nonumber \\\\&amp; +\\boldsymbol{T}_{\\boldsymbol{e x t}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\overline{\\boldsymbol{u}}-\\left(\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{u}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{u}_{\\boldsymbol{w}}\\right) \\\\\\overline{\\boldsymbol{F}}= &amp; \\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{ext}}-\\overline{\\boldsymbol{u}}-\\left(\\boldsymbol{u}_{\\boldsymbol{b}}+\\boldsymbol{u}_{\\boldsymbol{w}}\\right) .\\end{align}\\]and\\[\\begin{align}\\boldsymbol{F_{e x t}} &amp; =\\boldsymbol{R}^{T} \\sum \\boldsymbol{f_{ext_i}} \\\\ \\boldsymbol{T_{e x t}} &amp; =\\boldsymbol{R}^{T} \\sum\\left(\\boldsymbol{(x_i-b)})\\right (\\times \\boldsymbol{f_{e x t_i}})+\\boldsymbol{R}^{T} \\sum \\boldsymbol{\\tau_{e x t_j}}\\end{align}\\]where $\\boldsymbol{x_i}$ is the point in the inertial frame where $\\boldsymbol{f_{ext_i}}$ acts and represents the external forces and moments experienced by the glider w.r.t body-fixed frame. In the simulator code, these are expressed as a vector of the drag, lift, and hydrodynamic moments, as per the equations (2.14) - (2.16) and (2.20) - (2.23).Control TransformationIn equation (3.1), the control inputs $\\boldsymbol{\\bar{u}}$, $\\boldsymbol{u_b}$, and $\\boldsymbol{u_w}$ are equivalent to the forces acting on the point masses $\\bar{m}$, $m_b$ and $m_w$. However, as mentioned in Section 2.3, it is preferred to provide a vector of accelerations of the internal point masses instead.As a result, the controls need to be transformed and the state vector needs to be changed from $z = (R, b, \\Omega, v, r_p, r_b, P_p, P_b, P_w, \\dot m_b)^T$ to $x = (R, b, \\Omega, v, r_p, r_b, \\dot r_p, \\dot r_b, \\dot r_w, \\dot m_b)^T$.This changes the control vector from\\[\\begin{align} \\boldsymbol{u}=\\left(\\begin{array}{c}\\boldsymbol{\\bar{u}} \\\\\\boldsymbol{u_b} \\\\\\boldsymbol{u_w}\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{\\dot{P}_p} \\\\\\boldsymbol{\\dot{P}_b} \\\\\\boldsymbol{\\dot{P}_w}\\end{array}\\right)\\end{align}\\]corresponding to the forces on the internal point masses into\\[\\begin{align} \\boldsymbol{w}=\\left(\\begin{array}{c}\\boldsymbol{w_p} \\\\\\boldsymbol{w_b} \\\\\\boldsymbol{w_w}\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{\\ddot{r}_p} \\\\\\boldsymbol{\\ddot{r}_b} \\\\\\boldsymbol{\\ddot{r}_w}\\end{array}\\right)\\end{align}\\]In this control transformation, we specify $\\boldsymbol{Z = (Z_p, Z_b, Z_w)}^T$ as the drift vector field and $\\boldsymbol{F}$ as the control vector field. $\\boldsymbol{F}$ is computed to be\\[\\begin{align} \\boldsymbol{F} &amp; =\\left(\\begin{array}{ccc}\\boldsymbol{M}^{-1}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+\\frac{1}{\\bar{m}} \\mathcal{I} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\\\ \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+\\frac{1}{m_b} \\mathcal{I} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\\\ \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} &amp; \\boldsymbol{M}^{-\\mathbf{1}}-\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{J}^{-\\mathbf{1}} \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}+\\frac{1}{m_w} \\mathcal{I}\\end{array}\\right) \\end{align}\\]The determinant of $\\boldsymbol{F}$ is always greater than zero, hence $\\boldsymbol{H} = \\boldsymbol{F}^{-1}$.\\[\\begin{align}\\boldsymbol{Z_{p}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\text {ext }}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{p}} \\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right. \\nonumber\\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{p}} \\\\\\boldsymbol{Z}_{\\boldsymbol{b}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{e x t}}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{b}}\\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right.\\nonumber \\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{b}} \\\\\\boldsymbol{Z}_{\\boldsymbol{w}}= &amp; -\\boldsymbol{M}^{-1}\\left[\\left(\\boldsymbol{M} \\boldsymbol{v}+\\boldsymbol{P}_{\\boldsymbol{p}}+\\boldsymbol{P}_{\\boldsymbol{b}}+\\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\Omega+m_0 g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}+\\boldsymbol{F}_{\\boldsymbol{e x t}}\\right]-\\boldsymbol{\\Omega} \\times \\dot{\\boldsymbol{r}}_{\\boldsymbol{w}}\\nonumber \\\\&amp; -\\boldsymbol{J}^{-\\mathbf{1}}\\left[\\left(\\boldsymbol{J} \\boldsymbol{\\Omega}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{p}} \\boldsymbol{P}_{\\boldsymbol{p}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{b}} \\boldsymbol{P}_{\\boldsymbol{b}}+\\hat{\\boldsymbol{r}}_{\\boldsymbol{w}} \\boldsymbol{P}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{\\Omega}+(\\boldsymbol{M} \\boldsymbol{v} \\times \\boldsymbol{v})+\\boldsymbol{T}_{\\boldsymbol{e x t}}\\right. \\nonumber\\\\&amp; +\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{p}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{p}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{b}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{b}}+\\left(\\boldsymbol{\\Omega} \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\right) \\times \\boldsymbol{P}_{\\boldsymbol{w}} \\nonumber\\\\&amp; \\left.+\\left(\\bar{m} \\hat{\\boldsymbol{r}}_{\\boldsymbol{p}}+m_b \\hat{\\boldsymbol{r}}_{\\boldsymbol{b}}+m_w \\hat{\\boldsymbol{r}}_{\\boldsymbol{w}}\\right) g \\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{k}\\right] \\times \\boldsymbol{r}_{\\boldsymbol{w}}\\end{align}\\]We can then transform the control inputs as\\[\\begin{align} \\left(\\begin{array}{c}\\bar{\\boldsymbol{u}} \\\\\\boldsymbol{u}_{\\boldsymbol{b}} \\\\\\boldsymbol{u}_{\\boldsymbol{w}}\\end{array}\\right)=\\boldsymbol{H}\\left(\\begin{array}{c}-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}} \\\\-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}} \\\\-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\end{array}\\right)=\\left(\\begin{array}{c}H_{11}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{12}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{13}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right) \\\\H_{21}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{22}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{23}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right) \\\\H_{31}\\left(-\\boldsymbol{Z}_{\\boldsymbol{p}}+\\boldsymbol{w}_{\\boldsymbol{p}}\\right)+H_{32}\\left(-\\boldsymbol{Z}_{\\boldsymbol{b}}+\\boldsymbol{w}_{\\boldsymbol{b}}\\right)+H_{33}\\left(-\\boldsymbol{Z}_{\\boldsymbol{w}}+\\boldsymbol{w}_{\\boldsymbol{w}}\\right)\\end{array}\\right) \\end{align}\\]Motion in Vertical PlaneThe equations of motion are derived by restricting the model to the vertical plane. This means that the Euler angles $\\phi = 0$ and $\\psi = 0$. It follows a sawtooth trajectory, which is basically a repetitive cycle of ascending and descending. The offset static point mass is eliminated $m_w=0$, and the ballast mass is fixed to the glider‚Äôs CoB, $\\boldsymbol{r_b=0}$. We also restrict the internal movable mass $\\bar{m}$ to move only along the $X_0$ axis. Thereby,\\[\\begin{aligned}\\boldsymbol{R}=\\left(\\begin{array}{ccc}\\cos \\theta &amp; 0 &amp; \\sin \\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin \\theta &amp; 0 &amp; \\cos \\theta\\end{array}\\right), \\boldsymbol{b}=\\left(\\begin{array}{c}x \\\\ 0 \\\\ z\\end{array}\\right), \\boldsymbol{v}=\\left(\\begin{array}{c}v_1 \\\\ 0 \\\\ v_3\\end{array}\\right), \\boldsymbol{\\Omega}=\\left(\\begin{array}{c}0 \\\\ \\Omega_2 \\\\ 0\\end{array}\\right), \\\\ \\boldsymbol{r}_{\\boldsymbol{p}}=\\left(\\begin{array}{c}r_{P 1} \\\\ 0 \\\\ r_{P 3}\\end{array}\\right), \\quad \\boldsymbol{P}_{\\boldsymbol{p}}=\\left(\\begin{array}{c}P_{P 1} \\\\ 0 \\\\ P_{P 3}\\end{array}\\right), \\quad \\overline{\\boldsymbol{u}}=\\left(\\begin{array}{c}u_1 \\\\ 0 \\\\ u_3\\end{array}\\right),\\end{aligned}\\]The glider speed $V$ is given by $V = \\sqrt{v_1^2 + v_3^2}$.The glide path angle $\\xi$ is given by $\\xi = \\theta - \\alpha$.Glide EquilibriumThe conditions for glide equilibrium are calculated at every extreme of the sawtooth trajectory. The desired trajectory is computed by specifying the desired glide path angle $\\xi_d$ and desired glide speed $V_d$.\\[\\begin{align} \\theta_{d} = \\xi_{d} + \\alpha_{d}, v_{1_d} = V_dcos\\alpha_{d}, v_{3_d} = V_{d}sin\\alpha_{d} \\end{align}\\]\\[\\begin{align} P_{P1_d} = \\bar{m}v_{1_d}, P_{P3_d} = \\bar{m}v_{3_d}\\end{align}\\]The admissible values of $\\xi_d$ should lie in the range,\\[\\begin{align} \\xi_d \\in\\left(\\tan ^{-1}\\left(2 \\frac{K_D}{K_L}\\left(\\frac{K_{L_0}}{K_L}+\\sqrt{\\left(\\frac{K_{L_0}}{K_L}\\right)^2+\\frac{K_{D_0}}{K_D}}\\right)\\right), \\frac{\\pi}{2}\\right)\\end{align}\\]or\\[\\begin{align} \\xi_d \\in\\left(-\\frac{\\pi}{2}, \\tan ^{-1}\\left(2 \\frac{K_D}{K_L}\\left(\\frac{K_{L_0}}{K_L}-\\sqrt{\\left(\\frac{K_{L_0}}{K_L}\\right)^2+\\frac{K_{D_0}}{K_D}}\\right)\\right)\\right)\\end{align}\\]The desired angle of attack,\\[\\begin{align} \\alpha_d=\\frac{1}{2} \\frac{K_L}{K_D} \\tan \\xi_d \\times\\left(-1+\\sqrt{1-4 \\frac{K_D}{K_L^2} \\cot \\xi_d\\left(K_{D_0} \\cot \\xi_d+K_{L_0}\\right)}\\right)\\end{align}\\]The desired ballast mass,\\[\\begin{align} m_{b_d}=\\left(m-m_h-\\bar{m}\\right)+\\frac{1}{g}\\left(-\\sin \\xi_d\\left(K_{D_0}+K_D \\alpha_d^2\\right)+\\cos \\xi_d\\left(K_{L_0}+K_L \\alpha_d\\right)\\right) V_d^2\\end{align}\\]The desired excess mass is given by,\\[\\begin{align} m_{0_d} = m_{b_d} + m_h + \\bar{m} - m\\end{align}\\]If we assume $r_{P3}$ to be fixed, that is, the mass $\\bar{m}$ has only 1 Degree of Freedom (DoF) and moves only along the axis $X_0$, we have\\[\\begin{align} r_{P 1_d}=-r_{P 3_d} \\tan \\theta_d+\\frac{1}{\\bar{m} g \\cos \\theta_d}\\left(\\left(m_{f 3}-m_{f 1}\\right) v_{1_d} v_{3_d}+\\left(K_{M_0}+K_M \\alpha_d\\right) V_d^2\\right)\\end{align}\\]Motion in Three DimensionsIn three dimensions, we also consider the yaw, sideslip and roll dynamics of the vehicle. The sideslip angle $\\beta$ is the lateral angle between the glider velocity and the longitudinal $X_0$ axis.\\[\\begin{align} \\beta = \\sin ^ {-1} (\\frac{v_2}{\\lVert \\boldsymbol{V} \\rVert})\\end{align}\\]The Euler angles $\\phi$ and $\\psi$ are no longer zero. The offset static point mass is still eliminated $m_w = 0$, and the ballast mass is fixed at the glider‚Äôs CoB, $\\boldsymbol{r_b = 0}$. According to the mode of turning, which is discussed in the subsequent section, the internal movable mass $\\bar{m}$ can have 1 Degree of Freedom (only along $X_0$) or 2 Degrees of Freedom (along $X_0$ and $Y_0$).TurningThere are two ways in which a glider can actuate its yaw: Using a Rudder: An external hydrodynamic surface such as a rudder can produce a turning moment on the glider. Rolling to cause a turn: The internal mass actuator has a degree of freedom along the lateral $Y_0$ axis and produces a roll which, in turn, induces a yaw and causes a banked turn.When considering the design aspects of a glider‚Äôs yaw control, it is important to consider the frequency of inflections during the glider‚Äôs operations. If roll is used to control the glider‚Äôs yaw, it becomes necessary to change the roll direction each time the glider transitions from an upward to a downward glide. This may be unfavourable in operations involving multiple inflections. On the other hand, in using a rudder, the relationship between the rudder angle and yaw rate remains independent of the glide direction.While designing a glider with roll actuation for yaw control, the roll/yaw relationship also plays a crucial role. Aircrafts typically have a positive roll/yaw relationship, where a roll to the right produces a yaw to the right when gliding downwards. In case of underwater gliders, we see examples of both positive and negative roll/yaw relations. In this thesis, we consider the positive roll/yaw relation of the glider while designing the simulator.Glide EquilibriumThe gliding equilibria are solutions of the three dimensional dynamic equations with \\(\\boldsymbol{\\dot{\\Omega}} = \\boldsymbol{\\dot{v}} = \\boldsymbol{\\dot{r}_p} = \\boldsymbol{\\ddot{r}_p} = \\dot{m}_b = 0\\). The same vertical plane equations for \\(\\theta_d, \\xi_d, \\alpha_d, m_{b_d}, m_{0_d}\\), and \\(r_{P1_d}\\) from Section 3.2.1 are used.To note the characteristics of spiral glides of AUGs, the thesis \\cite{bhatta2006nonlinear} provides proofs on observations about spiral motion equilibrium, which can be summarized as: The AUG moves with constant velocity along the circular helix. The pitch and roll angles are constant, while the yaw changes at a constant rate. The axis of the circular helix is aligned with the direction of gravity.The same can also be seen from the plots obtained from the simulator, which are displayed in Section 4.4.The radius of the spiral traversed by the glider is given by,\\[\\begin{align} R=\\frac{V \\cos (\\theta-\\alpha)}{\\Omega_3}\\end{align}\\]Simulation ResultsThe complete code for the AUG Simulator is made open-source and can be found at: AUG-Simulator.The code is written in the Python programming language.Simulator InterfaceThe following are the various arguments that can be given to the simulator:usage: main.py [-h] [-i] [-m MODE] [-c CYCLE] [-g GLIDER] [-a ANGLE] [-s SPEED] [-pid PID] [-r RUDDER] [-sr SETRUDDER] [-p [PLOT ‚Ä¶]] -h ‚Äìhelp show this help message and exit -i ‚Äìinfo give full information in each cycle -m MODE ‚Äìmode MODE set mode as 2D, 3D, or waypoint -c CYCLE ‚Äìcycle CYCLE number of desired cycles in sawtooth trajectory -g GLIDER ‚Äìglider GLIDER [‚Äòslocum‚Äô] -a ANGLE ‚Äìangle ANGLE desired glider angle -s SPEED ‚Äìspeed SPEED desired glider speed -pid PID ‚Äìpid PID enable or disable PID control -r RUDDER ‚Äìrudder RUDDER enable or disable rudder -sr SETRUDDER ‚Äìsetrudder SETRUDDER desired rudder angle. Defaults to 10 degrees -p [PLOT ‚Ä¶] ‚Äìplot [PLOT ‚Ä¶] variables to be plotted [3D, all, x, y, z, omega1, omega2, omega3, vel, v1, v2, v3, rp1, rp2, rp3, mb, phi, theta, psi] Parameter IdentificationThe values of parameters for the Slocum glider are shown in Table 4.2 and Table 4.3. Obtaining these values requires us to perform CFD analysis, and are hence picked up from research papers that have previously done the same. Body length 1.5 m Radius 0.11 m Hull mass $m_h$ 40 kg Ballast mass $m_b$ 1 kg Internal movable mass $\\bar{m}$ 9 kg Offset static point mass $m_w$ 0 kg Fluid displacement mass $m$ 50 kg $m_{f_{1,2,3}}$ 5, 60, 70 kg $J_{1,2,3}$ 4, 12, 11 kg.m^2 $K_{L_0}$ 0 $K_L$ 132.5 $K_{\\beta}$ 20 $K_{D_0}$ 2.15 $K_D$ 25 $K_{M_0}$ 0 $K_M$ -100 $K_{MY}$ 100 $K_{MR}$ -60 $K_{q1}$ -20 $K_{q2}$ -60 $K_{q3}$ -20 We specify the desired glide path angle to be $\\xi_d = \\deg{25}$ and desired glider speed as $V_d = 0.3 m/s$. These same values are used throughout to obtain plots for sawtooth as well as spiral motions.Upon calculating the values at glide equilibrium for the downward dive at $t=0$, we get the values shown in Table 4.4. $\\xi_d$ -25 deg $\\theta_d$ -23 deg $\\alpha_d$ 2 deg $v_{1_d}$ 0.299 m/s $v_{3_d}$ 0.0106 m/s $r_{p1_d}$ 0.0198 m $r_{p3_d}$ 0.05 m $m_{b_d}$ 1.047 kg Sawtooth MotionThe slide-slip angle $\\beta = 0$ deg since there is no component of velocity along the $Y_0$ axis, due to which the viscous side force $SF$ and moments $M_{DL_1}$, $M_{DL_3}$ are all equal to $0$.The images below show the sawtooth trajectory and the relevant plots for 4 cycles (1 cycle is considered to be 1 upward or downward motion).Notice in the above figure, $y$ remains zero at all times since the motion is restricted to the $xz$ plane. The roll $\\phi$ and yaw $\\psi$ are also zero, while the pitch $\\theta$ changes at each inflection.In the above figure, $\\Omega_1, \\Omega_3$ are zero since there is no rotation about the $X_0$ and $Z_0$ axes. $v_2$ is also zero since there is no component of velocity in the lateral direction.Here, we see that the component of \\(\\boldsymbol{r_p}\\) along $X_0$ axis, i.e., $r_{p1}$ reaches the desired position \\(r_{p1_d}\\) at each inflection. \\(r_{p2}\\) is zero while \\(r_{p3}\\) is fixed at \\(5 \\space cm\\). The image below shows the change in values of acceleration given to \\(\\bar{m}\\) to change its position \\(r_{p1}\\) along the \\(X_0\\) axis. A value of \\(0.02 \\space m/s^2\\) is given at the beginning of each inflection and remains zero at every other instant.The simulator also allows us to simulate the sawtooth motions for \\textit{n} number of cycles with the -c CYCLE or ‚Äìcycle CYCLE argument. The images below show the sawtooth trajectory and the relevant plots for 6 cycles. The plots are similar to the ones shown above.Spiral MotionThe presence of a side-slip angle, given by equation (3.22), results in a side force and hydrodynamic moments \\(M_{DL_1}, M_{DL_3}\\), which stimulates the glider‚Äôs lateral dynamics.As mentioned previously, gliders can actuate their heading in two ways - using a rudder or by rolling, and the subsequent plots obtained from the simulator are displayed below.Using a RudderThe presence of a rudder changes the hydrodynamic forces and moments, particularly the drag $D$, side force $SF$ and moment \\(M_{DL_3}\\). The hydrodynamic rudder coefficients are set as \\(K_{D_\\delta} = 2.0, K_{SF_\\delta} = 5.0, K_{MY_\\delta} = 1.0\\), and a rudder angle $\\delta$ is chosen.As a result, the viscous forces and moments equations change as follows,\\[\\begin{align}D &amp; =\\left(K_{D_0}+K_D \\alpha^2 + K_{D_\\delta} \\delta^2\\right)\\left(V^2\\right)\\\\ SF &amp; =\\left(K_{\\beta}\\beta + K_{SF_\\delta}\\delta\\right)\\left(V^2\\right) \\\\M_{D L_3} &amp; =\\left(K_{M Y} \\beta +K_{q 3} \\Omega_3 + K_{MY_\\delta}\\delta\\right)\\left(V^2\\right)\\end{align}\\]When rudder is set at $\\delta = 25$ deg:Equilibrium roll angle of glider: -0.006355428997639093 degEquilibrium pitch angle of glider: -23.18914824153443 degSideslip angle of glider: -0.14978467830958261 degEquilibrium glide speed: 0.26533944912351476 m/sRadius : 14.659688296097228 mFrom the above plots, we see that the roll and pitch reach stable values, while the yaw changes at a constant rate. $r_{p2}, r_{p3}$ are zero and the ballast mass remains constant throughout the glide. When the rudder is set at a high angle, it forces the glider to spiral down in a smaller circle, and hence makes more spirals in the given simulation time (2000s).When rudder is set at a smaller angle, the glider moves along a circle of larger radius, and hence makes lesser spirals in the given time. The plots when the rudder is set at $\\delta = 15$ deg are shown below:Equilibrium roll angle of glider: -0.061902809708920835 degEquilibrium pitch angle of glider: -23.41541331190702 degSideslip angle of glider: -0.04893320699757756 degEquilibrium glide speed: 0.28140084245628366 m/sRadius : 24.39382252404712 mRolling to TurnWhile in most AUGs, such as the Slocum glider, the internal movable mass $\\bar{m}$ has only 1 DOF and uses a rudder to turn, this simulator just offers an additional functionality to move $\\bar{m}$ even along the lateral $Y_0$ axis to produce a roll, and thereby induce a yaw. Here, $r_{p2_d}$ is set as 0.02 m and is the amount by which $\\bar{m}$ moves along $Y_0$.Equilibrium roll angle of glider: 17.338401948861858 degEquilibrium pitch angle of glider: -22.924819423213446 degSideslip angle of glider: 3.2097452502678503 degEquilibrium glide speed: 0.3110046819320383 m/sRadius : 13.07315069665681 mThe image above shows the plots of $r_{p1}$ and $r_{p2}$ until they reach their desired values. $r_{p3}$ is fixed at \\(0.05 \\space m\\) throughout the glide. The figure below shows the acceleration $w_{p2}$ given to $\\bar{m}$ to change $r_{p2}$.PID ControlTo maximize the benefits of underwater gliders in ocean sampling and other applications, a reliable control system is crucial. Efficient gliding, whether precise elevation or heading, depends on precise control. Improvements in control enhance gliders‚Äô utility in scientific missions, with feedback systems ensuring robustness against disturbances during prolonged missions.In this thesis, two types of PID (Proportional Integral Derivative) controllers are presented - one for pitch control and the other for yaw control.Sawtooth Pitch ControlEssentially a PD controller, the gain values are set as $K_p = 0.05, K_i = 0.0, K_d = 0.0005$. The controller takes in the current value of the pitch $\\theta$ and the desired reference value of pitch $\\theta_d$ as the inputs and outputs the acceleration of the internal movable mass $\\bar{m}$ to control its position $\\boldsymbol{r_p}$.The plots below are similar to the ones presented previously in the sawtooth motion results but with much lesser fluctuations of the glider pitch $\\theta$.Since the model is again restricted to the $xz$ plane, the values of $y, \\phi, \\psi, \\Omega_1, \\Omega_3, v_2$ are always zero.In the figure above, we see that $r_{p1}$ varies to obtain a more controlled pitch, as opposed to what we see in Figure 4.15. Figure 5.6 shows the acceleration $w_{p1}$ applied to the internal movable mass $\\bar{m}$ along $X_0$ axis, which is the output of the PID controller, and causes $r_{p1}$ to change.Heading Control with RudderA PD controller, the gain values are set as $K_p = 0.05, K_i = 0.0, K_d = 0.05$. The controller takes in current value of the yaw $\\psi$ and the desired reference value of yaw $\\psi_d$ as the inputs and outputs the desired rudder angle $\\delta$, which in turn, changes the yaw of the glider.In this example, the desired yaw to be achieved $\\psi_d$ is set as 40 deg, and the glide angle is the same as before. In the figure below, the blue line makes an angle of 40 deg with the x-axis and marks the desired direction of motion of the AUG. The red line shows the actual trajectory of the glider, which is fitted with the yaw controller.The image above shows the variation in the rudder angle, which is the output of the PID controller. Initially starting off at a high value, it varies until the yaw $\\psi$ matches with the desired value $\\psi_d$ and then stabilizes at zero, which means there is no further change in the yaw angle.The image above shows the variation of the yaw as it starts at 0 deg and reaches the desired value $\\psi_d = 40$ deg.The image below shows the variation of $r_{p1}$ to reach the desired pitch $\\theta_d$, as done previously." }, { "title": "Multi-Agent Traffic Signal Control using Reinforcement Learning", "url": "/posts/phase-duration-control-rl/", "categories": "Projects, RL", "tags": "rl", "date": "2023-11-11 21:30:00 +0530", "snippet": "The following work was carried out during my summer internship at the Multi-Agent Robotic Motion Laboratory, National University of Singapore under the supervision of Dr. Guillaume Sartoretti.Hybrid PPOHybrid Proximal Policy Optimization (Hybrid PPO) is an innovative actor-critic algorithm designed for learning in a parameterized action space. The key feature of H-PPO lies in its utilization of parallel actors, where discrete and continuous action selection are performed independently. This architecture decomposes the complex action space into simpler components, enhancing the learning efficiency of the model. The algorithm employs a global critic network to update the policy parameters of both the discrete and continuous actors.H-PPO operates in a parameterized action space, allowing the agent not only to select discrete actions but also to choose continuous parameters associated with those actions. This flexibility enables the model to make nuanced decisions, crucial for tasks with intricate action requirements.ArchitectureParallel Actors: Discrete Actor Network (œÄŒ∏d): Learns a stochastic policy for discrete actions. It outputs k values fa1 , fa2 , . . . , fak for the k discrete actions, and the discrete action ‚Äòa‚Äô is randomly sampled from the softmax(f) distribution. Continuous Actor Network (œÄŒ∏c): Learns a stochastic policy for continuous parameters associated with discrete actions xa1 , xa2 , . . . , xak. It generates the mean and variance of a Gaussian distribution for each parameter.The complete action to execute is the selected action a paired with the chosen parameter xa corresponding to action a. The two actor networks shares the first few layers to encode the state information.H-PPO incorporates a single critic network (V(s)) that estimates the state-value function. Using the state-value function instead of the action-value function mitigates over-parameterization issues in the parameterized action space.The discrete policy œÄŒ∏d and the continuous policy œÄŒ∏c are updated separately by minimizing their respective clipped surrogate objective.The probability ratio rtd (Œ∏d) only considers the discrete policy and rtc (Œ∏c) only considers the continuous policy. Even though the two policies work with each other to decide the complete action, their objectives are not explicitly conditioned on each other. œÄŒ∏d and œÄŒ∏c are viewed as two separate distributions instead of a joint distribution.RewardsIn traffic signal control, the choice of reward functions plays a pivotal role in shaping the behavior of reinforcement learning-based approaches. We investigate two prominent reward strategies: Max Pressure (MP) and Intensity-based Proximal Dynamic Adaptive Light (IPDALight). While MP prioritizes throughput and congestion minimization, IPDALight introduces a nuanced perspective by considering detailed vehicle dynamics, such as speed and position, to enhance traffic signal control policies.Max Pressure (MP) ControlThe objective is to minimize intersection pressure defined as the difference between the number of vehicles on incoming and outgoing lanes.Strengths: Throughput Maximization: MP aims to maximize the flow of vehicles through intersections, optimizing for high throughput. Simple and Effective: Known for its simplicity, MP often yields effective solutions for reducing congestion and delays.Limitations: Locally Optimal Solutions: MP can be greedy, leading to locally optimal solutions that may not be globally efficient. Lack of Vehicle Dynamics: The method does not consider vehicle dynamics, ignoring factors such as speed and position.IPDALight: Intensity-based Proximal Dynamic Adaptive LightThe objective is to minimize the intensity of intersections, considering detailed vehicle dynamics, including speed and position.Features: Intensity Calculation: Considers the speed, position, and distance of vehicles approaching an intersection to compute intensity. Logarithmic Scale: Mitigates the impact of large product values, offering a more balanced intensity metric. Dynamic Phase Duration: Proposes a heuristic to fine-tune phase duration based on dynamic traffic situations.\\[\\mathcal{T}_{v e h}=\\log \\left(\\frac{L-x}{L} \\times \\frac{\\delta \\times\\left(v_{\\max }-v\\right)}{v+1}+1\\right)\\]Intensity of vehicle increases when it approaches an intersection or the speed decreases due to traffic congestion.$(L-x)/L$ shows the impact of distance on intensity: closer the vehicle is to the intersection, greater is the intensity.The other term indicates the impact of slower vehicles on intensity. So by definition, vehicles crossing an intersection with higher speed will pose lower intensity on the intersection.Intensity of Lanes: sum of intensity of all vehicles on the lane.\\[\\mathcal{T}_{\\text {lane }}=\\sum_{\\text {veh }_i \\in \\text { lane }} \\mathcal{T}_{\\text {veh }_i}\\]Intensity of Intersections: It is the intensity difference between the incoming lanes and the outgoinglanes. It evaluates the overall traffic pressure suffered by the intersection.\\[\\mathcal{T}_{\\mathcal{I}}=\\sum_{\\text {lane }_i \\in \\text { lane }_{\\text {in }}} \\mathcal{T}_{\\text {lane }_i}-\\sum_{\\text {lane }_j \\in \\text { lane }_{\\text {out }}} \\mathcal{T}_{\\text {lane }_j}\\]Reward definition - The intensity of intersections reflects the average travel time of vehicles crossing the intersection more accurately than the MP-model because it considers more vehicle dynamics. So their reward is $r$ = $-$(Intensity of $I$)Advantages: Accurate Representation: Reflects average travel time more accurately by considering detailed vehicle dynamics. Adaptive Phase Duration: Provides a heuristic to adjust phase duration, adapting to changing traffic conditions.Challenges: Complexity: The inclusion of detailed vehicle dynamics adds computational complexity compared to MP. Implementation Challenges: Fine-tuning phase duration requires careful implementation and real-time adaptation mechanisms.Results" }, { "title": "Swarm Robot Coordination", "url": "/posts/swarm-robot-coordination/", "categories": "Projects, Robotics", "tags": "swarm, python, simulation", "date": "2023-05-21 21:30:00 +0530", "snippet": "AbstractSwarm robotics is an emerging field that enables a group of robots to collaborate and achieve complex tasks that are difficult for a single robot to accomplish. One of the key challenges in swarm robotics is the coordination of multiple robots‚Äô movements towards a common goal. This report provides a comprehensive overview of swarm robot coordination, with a specific focus on four common tasks: aggregation, dispersion, line formation, and shape formation.We discuss the different algorithms used for swarm robot coordination, including Potential Fields Algorithm, Centroid based Algorithm, and Line Marching Algorithm, and their respective advantages for each task. Furthermore, we present the simulation results for each task, demonstrating the effectiveness of these algorithms in achieving coordinated movements among the various swarm robots.AggregationSwarm aggregation refers to the process where a group of agents, often called a swarm, come together to form a single, cohesive group or cluster. This behavior is commonly observed in nature, such as with swarming insects, schooling fish, or flocking birds. In the context of robotics and artificial intelligence, swarm aggregation algorithms enable a group of robots or agents to converge to a single location or gather around a point of interest. The goal of these algorithms is to achieve robust, adaptive, and scalable aggregation, while maintaining simplicity and minimizing communication.Fields Algorithm Define constants: NUM_ROBOTS (number of robots), ROBOT_RADIUS (radius of each robot), SWARM_RADIUS (radius of the swarm area), and AGGREGATION_THRESHOLD (a distance threshold below which robots don‚Äôt move closer to the center of mass). Create the swarm of robots. For each robot, generate random initial positions within the swarm area. Main a. Calculate the center of mass of the swarm by summing the positions of all robots and dividing by the number of robots. b. Move each robot towards the center of mass: calculate the direction vector from the robot‚Äôs position to the center of mass. Update the positions of the robots in the scatter plot.In summary, this code demonstrates a simple swarm robot aggregation behavior by simulating the motion of robots towards the center of mass of the swarm.ResultsDispersionDispersion in swarm robotics refers to a group of robots‚Äô capacity to disperse over a wide region while still interacting and coordinating with one another. In other words, it requires evenly distributing robots around a place but still enabling them to cooperate as a group to achieve a shared objective.Dispersion is a crucial capability in swarm robotics, as it enables the robots to accomplish tasks such as environmental monitoring, search and rescue operations, and surveillance, among others. The robots can cover a broader area more rapidly and effectively when they are dispersed than when they are grouped together in one location. This can also lessen the chance of crashes or congestion, which may happen if the robots are too closely grouped together.Several algorithms have been developed to perform dispersion, with the Potential Fields Algorithm being one of the most commonly used.Potential Fields AlgorithmThe Potential Fields Algorithm is a popular method for controlling dispersion of swarm robots. In this method, each robot is represented as a point in a potential field, that is created by a combination of attractive and repulsive forces. While the repulsive forces assist the robots in avoiding objects or other robots, the attractive forces motivate them to go in the direction of a desired place.The following steps can be taken to implement the algorithm: Define the target dispersion area: First, the target dispersion area needs to be defined. This can be done by defining a virtual boundary around the area where the swarm robots need to be dispersed. Define the attractive force: Next, an attractive force needs to be defined to encourage the robots to move towards the target dispersion area. This can be done by setting up a virtual potential field with a low magnitude force that pulls the robots towards the center of the target area. Define the repulsive force: A repulsive force needs to be defined to prevent the robots from clustering together. This can be done by setting up a virtual potential field with a high magnitude force that pushes the robots away from each other. Update the forces: The attractive and repulsive forces need to be updated continuously based on the position of the robots and the obstacles in the environment. This can be done by using sensors or cameras to detect the position of the robots and obstacles and adjusting the virtual potential field accordingly. Move the robots: Finally, the robots can be moved based on the resultant force vector calculated from the attractive and repulsive forces. This can be done by using differential drive motors or other types of actuators.AdvantagesThe Potential Fields Algorithm has several advantages that make it a popular method for controlling the movement of swarm robots. Some of the key advantages of this algorithm are: Scalability: The algorithm is highly scalable, meaning that it can be used to control the movement of large numbers of robots in a swarm. This makes it ideal for applications where multiple robots need to work together to accomplish a task. Robustness: The algorithm is robust to changes in the environment, as it continuously adapts to changes in the position of obstacles or other robots. This makes it ideal for use in dynamic and unpredictable environments, such as disaster zones or construction sites. It is effective in complex environments with many obstacles and goals, as it provides a smooth and continuous way to move the robots. It can ensure that the robots converge to their goal location and avoid getting stuck in local minima.ResultsLine FormationLine formation in swarm robotics is a task in which a group of robots organizes themselves into a linear structure, following a predefined path or creating a path as they move. The goal of line formation is to achieve a linear arrangement of robots with a certain level of spacing between them. This task is useful in various scenarios, such as for search and rescue missions, inspection of long pipelines or cables, and monitoring of large areas.In line formation, robots must be able to move in a coordinated way to avoid collisions and maintain the desired distance between each other. They need to communicate with each other to share information about their positions, orientations, and velocities. Several algorithms have been developed to achieve line formation in swarm robotics, and one of the popular algorithms used for this task is the Line Marching Algorithm.Line Marching AlgorithmThe Line Marching Algorithm (LMA) is a swarm robotics algorithm used for line formation. It involves coordinating a group of robots to form a line that follows a predefined path. The algorithm involves dividing the line formation task into two sub-tasks: Moving forward along the line while maintaining the desired distance between robots. Correcting the deviations from the line caused by disturbances or errors in the robot‚Äôs motion.To use the Line Marching Algorithm, the following steps can be taken: Define the line: The first step is to define the line that the robots will form. The line can be defined using a series of points, with each point assigned a unique ID. The distance between each point should be small enough to ensure that the robots can move smoothly along the line. Initialize the swarm: Next, the swarm of robots is initialized. Each robot is given a unique ID and placed in a random position on the field. Calculate distance to line points: Each robot calculates its distance to all points on the line and determines the closest point. The robot then moves towards that point. Move towards the line: The robots move towards the closest point on the line using a simple proportional control system. The robot‚Äôs speed and direction are adjusted based on its distance to the line and the desired speed of the swarm. Maintain distance from neighbors: As the robots move towards the line, they must also maintain a minimum distance from their neighbors. This is done using a repulsion force, which pushes the robot away from any neighbors that are too close. The repulsion force is calculated based on the distance between the robots and the desired minimum separation. Adjust movement: If a robot encounters an obstacle or another robot, it must adjust its movement to avoid a collision. This is done using a simple obstacle avoidance algorithm. Continue to adjust: The swarm continues to adjust its movement until it forms a straight line that follows the path defined by the points. Maintain the line: Once the line is formed, the swarm must maintain it. This is done using a combination of attraction towards the line and repulsion from other robots. Each robot calculates its position on the line and moves towards the point that corresponds to that position. At the same time, it must also maintain a minimum distance from its neighbors.Advantages Decentralization: The Line Marching algorithm is a decentralized approach to formation control that does not require a centralized control unit. Each robot only needs to communicate with its nearest neighbors, which reduces the communication overhead and improves the scalability of the system. Robustness to communication delays: The Line Marching algorithm is robust to communication delays, which are common in wireless networks. This is because the algorithm uses local information to compute the desired positions of each robot in the formation, rather than relying on global information. Stability: The Line Marching algorithm is designed to maintain formation stability and avoid collisions between robots. This is achieved by imposing separation constraints between neighboring robots, which ensures that the robots maintain a safe distance from each other.Code StructureResultsShape FormationSwarm shape formation is a process in which a group of agents, often referred to as a swarm, organizes themselves into specific shapes or patterns. This is often observed in nature, such as with flocking birds or schooling fish, and has been extensively studied in the context of robotics and artificial intelligence. The goal of swarm shape formation algorithms is to enable the agents to coordinate and maintain the desired shape or pattern while maintaining a high degree of robustness, adaptability, and scalability.There are several algorithms and approaches to swarm shape formation, some of which are outlined below:Reynolds‚Äô Boids algorithm: It is based on three simple rules: separation (avoid collisions with neighbors), alignment (align velocity with the average velocity of neighbors), and cohesion (move toward the average position of neighbors). By adjusting the weights of these rules, a variety of swarm behaviors, including shape formation, can be achieved.Artificial potential fields: In this approach, agents in the swarm are influenced by potential fields that push or pull them toward specific positions. These fields can be generated based on the desired shape, and each agent calculates its movement based on the sum of forces from these fields. This method enables swarms to form shapes without explicit communication between agents.Graph-based methods: In these methods, the swarm is represented as a graph where nodes correspond to agents and edges represent connections or relationships between them. Shape formation can be achieved through graph manipulation techniques such as graph transformations, graph matching, or graph drawing. Agents can then follow specific rules to move toward their target positions, guided by the graph structure.We used Artificial Potential Fields AlgorithmThe algorithm works as below: Initialize the parameters, such as the number of robots (n_robots), size of the shape (size), number of iterations (iterations), step size (step_size), shape, and collision threshold (collision_threshold). Randomly initialize the positions of the robots in a 2D space. Define a function (generate_target_positions) to generate target positions for the robots based on the chosen shape. Calculate the target positions for the chosen shape. Define a function (avoid_collision) that computes the direction a robot should move to avoid collisions with other robots, based on a given threshold. Create an update function that executes the following steps for each robot: a. Find the closest target position. b. Calculate the direction towards the target position. c. Normalize the direction vector and scale it by the step size. d. Add the collision avoidance direction scaled by the step size. e. Update the robot‚Äôs position by adding the calculated direction. Create a matplotlib animation that iteratively calls the update function, which updates the robots‚Äô positions, and plots the updated positions on a 2D scatter plot.The forces of attraction and repulsion.In short, this algorithm moves robots towards their target positions while avoiding collisions with each other. The robots form the desired shape over a series of iterations while ensuring that they maintain a safe distance from one another.ResultsStart EndStart End" }, { "title": "Swarm Robot Tasks and Algorithms", "url": "/posts/swarm-algorithms/", "categories": "Blog, Robotics", "tags": "swarm, python, simulation", "date": "2023-05-21 21:30:00 +0530", "snippet": "The following are some of the algorithms that can be used to perform different tasks with swarm robots using Python:1. AggregationA Geometry Based Algorithm for Swarm AggregationsIn Geometry Based Algorithm (GBA), the robots rely on the relative positions of their neighboring robots to determine their movements towards a common goal. In general, the GBA approach involves defining a geometric shape that encloses the swarm robots, such as a circle or a polygon. The robots then use information about their relative positions within this shape to determine their movements towards the center or another predetermined location.One example of a GBA algorithm for swarm robot aggregation is the ‚Äúcentroid algorithm,‚Äù which involves calculating the centroid of the swarm robot positions and then moving each robot towards it at a predetermined speed. The speed of each robot can be proportional to the distance between the robot‚Äôs position and the centroid, such that robots that are farther away move faster than those that are closer. Another example is the ‚ÄúVoronoi-based algorithm,‚Äù which involves dividing the space around the swarm robots into Voronoi cells and then moving each robot towards the centroid of its cell.GBA algorithms have the advantage of being relatively simple and easy to implement, requiring only local communication between neighboring robots.2. DispersionThe Randomized Mobility Algorithm (RMA) is a popular algorithm used for dispersion in swarm robots. The goal of RMA is to spread the robots out evenly over a given area, while avoiding collisions with other robots and obstacles.In RMA, each robot moves randomly within the given area, with its movement direction chosen randomly at each time step. However, the movement of each robot is biased towards areas with lower robot density. Specifically, each robot maintains a virtual density map of the area, which is updated based on the positions of nearby robots. The robot then moves in a direction that is biased towards areas with lower density, which encourages it to move away from areas with high robot density.The degree of bias towards lower density areas can be adjusted based on the desired level of dispersion. For example, a higher bias towards lower density areas will result in a more dispersed swarm, while a lower bias will result in a more clustered swarm.One advantage of RMA is its simplicity and ease of implementation. It requires only local communication between neighboring robots and does not rely on complex algorithms or calculations.3. Line FormationLine Marching Algorithm For Planar Kinematic Swarm Robots: ADynamic Leader-Follower ApproachThe Line Marching Algorithm is a simple algorithm used for line formation in swarm robotics. In this algorithm, each robot in the swarm follows a simple set of rules to maintain a constant distance from its neighbors and to align its movement with the desired direction of the line.The Line Marching Algorithm can be implemented in a decentralized way, where each robot is only aware of its local neighbors, or in a centralized way, where a master robot controls the movements of the swarm.The algorithm is based on three main rules: Separation: each robot maintains a minimum distance from its neighbors to avoid collisions. Alignment: each robot aligns its movement with its neighbors to maintain the direction of the line. Cohesion: each robot moves towards the average position of its neighbors to maintain the formation. To implement these rules, each robot measures the distance and orientation to its neighbors and adjusts its speed and direction accordingly. The algorithm can be extended to include obstacles avoidance and other complex behaviors, such as rotation and splitting of the line.The Virtual Structure Algorithm is a decentralized algorithm for line formation in swarm robotics. In this algorithm, the swarm is divided into two groups: leaders and followers. The leaders are responsible for defining the virtual structure that the swarm should follow, while the followers adjust their movements to maintain the desired formation.The Virtual Structure Algorithm can be implemented in different ways, depending on the desired formation and the constraints of the system. One common approach is to use a line as the virtual structure, where the leaders are positioned at the ends of the line and the followers adjust their positions to maintain a constant distance from their neighbors and to align their movements with the line.The leaders can be controlled manually or by a centralized controller, or they can be selected from the followers based on certain criteria, such as connectivity or centrality. The leaders can communicate with their neighbors to coordinate their movements and to adjust the position and orientation of the virtual structure, if needed.The Virtual Structure Algorithm has some advantages over other line formation algorithms, such as scalability, flexibility, and adaptability to changes in the environment. It can also be extended to other formations, such as circles, squares, and polygons.4. Shape FormationThe Distributed Gradient Descent Algorithm is a centralized algorithm that can be used to form various shapes in a swarm of robots, including circles and other polygons. The algorithm works by iteratively adjusting the positions of the robots to minimize a cost function that captures the deviation from the desired shape.In the case of circle formation, the desired shape is a circle with a given radius and center. The cost function is defined as the sum of the squared distances between each robot and its desired position on the circle. The gradient of the cost function is then computed, which indicates the direction of steepest descent. Each robot is assigned a target position on the circle based on its current position and the gradient, and moves towards it at a certain speed. The process is repeated until the robots converge to the desired formation.In the case of polygon formation, the desired shape is a regular polygon with a given number of sides, radius, and center. The cost function is defined as the sum of the squared distances between each robot and its desired position on the polygon. The gradient is computed using a similar approach as in the case of circle formation, but with some modifications to account for the polygon geometry. Each robot is assigned a target position on the polygon based on its current position and the gradient, and moves towards it at a certain speed. The process is repeated until the robots converge to the desired formation.The Distributed Gradient Descent Algorithm can be extended to handle more complex shapes, such as irregular polygons and non-convex shapes.5. FlockingReynolds‚Äô Boids Algorithm is a decentralized algorithm that models the behavior of birds flocking. The algorithm is based on three simple rules that govern the movement of each robot or ‚Äúboid‚Äù in the flock: Alignment: The boid aligns its velocity with the average velocity of its neighbors within a certain radius. This means that the boid tends to move in the same direction as its neighbors. Cohesion: The boid moves towards the center of mass of its neighbors within a certain radius. This means that the boid tends to stay close to its neighbors. Separation: The boid avoids collisions with its neighbors by moving away from any neighbor that is too close within a certain radius. This means that the boid tends to avoid crowding or overlapping with its neighbors.These rules are implemented as simple mathematical equations, and the boid moves based on the net force acting on it. The net force is the sum of the forces generated by the alignment, cohesion, and separation rules. The algorithm can be implemented in a decentralized manner, where each boid only has access to local information about its neighbors.6. Particle Swarm OptimizationIn PSO, a group of particles or agents move in the search space to find the optimal solution to a given problem. Each particle has a position and a velocity, and its movement is governed by its own experience and the experience of the swarm. The algorithm iteratively updates the position and velocity of each particle based on a set of mathematical equations, which are derived from the social behavior of animals.PSO can be used in swarm robotics to optimize the behavior of a swarm of robots for various applications, including warehouse management, such as programming a swarm of robots with one-on-one communication for warehouse management tasks." }, { "title": "Path Planning", "url": "/posts/path-planning/", "categories": "Blog, Robotics", "tags": "planning, astar, dijkstra, search", "date": "2023-03-28 21:30:00 +0530", "snippet": "ResourcesProgrammingPath PlanningPath planning, also known as motion planning, is the process of finding a feasible path from a starting point to a desired goal point while avoiding obstacles in between. This is a fundamental problem in robotics, automation, and computer graphics.There are several types of algorithms used for path planning: Visibility graph algorithm: This algorithm constructs a graph of visibility between the starting point, the goal point, and all obstacles. The nodes of the graph are the starting point, goal point, and the vertices of the obstacles. The edges of the graph connect nodes that have a clear line of sight. This graph is then used to find the shortest path between the starting point and the goal point. Random-exploring algorithms: These algorithms randomly sample the configuration space (the space in which the robot can move) to find a path from the starting point to the goal point. Examples of random-exploring algorithms include Rapidly-exploring Random Trees (RRT) and Probabilistic Roadmaps (PRM). Optimal search algorithms: These algorithms use a search strategy to find the optimal path from the starting point to the goal point. Examples of optimal search algorithms include A* (A-star), Dijkstra‚Äôs algorithm, and Breadth-First Search (BFS). These algorithms use heuristics or cost functions to guide the search towards the goal point while minimizing the path length.Autonomy requires that the robot is able to plan a collision-free motion from an initial to a final posture on the basis of geometric information.Information about the workspace geometry can be entirely known in advance, called off-line planning gradually discovered by the robot, called on-line planningDiscrete PlanningThey are the simplest to describe because the state space will be finite (or countably infinite) in most cases. No forms of uncertainty will be considered.Formulation: A nonempty state space X, with finite or countably infinite set of states. For each state $x \\in X$, a finite action space $U(x).$ A state transition function $f$ that produces a state $f(x,u) \\in X$ for every $x \\in X$ and $u \\in U(x)$. The state transition equation is given by $x‚Äô=f(x,u)$. An initial state $x_I \\in X.$ A goal set $X_G \\subset X.$General Forward SearchPseudo code:The set of alive states is stored in a priority queue, Q, for which a priority function must be specified. The only significant difference between various search algorithms is the particular function used to sort Q. Therefore, assume for now that Q is a common FIFO (First-In First-Out) queue; whichever state has been waiting the longest will be chosen when $Q.GetFirst()$ is called. Initially, Q contains the initial state $x_I$. A while loop is then executed, which terminates only when Q is empty. This will only occur when the entire graph has been explored without finding any goal states, which results in a FAILURE (unless the reachable portion of X is infinite, in which case the algorithm should never terminate). In each while iteration, the highest ranked element, x, of Q is removed. If $x$ lies in $X_G$, then it reports SUCCESS and terminates; otherwise, the algorithm tries applying every possible action, $u \\in U(x)$. For each next state, x‚Äô = f(x,u), it must determine whether x‚Äô is being encountered for the first time. If it is unvisited, then it is inserted into Q; otherwise, there is no need to consider it because it must be either dead or already in Q.Breadth FirstBreadth-First Forward Search (BFS) is a graph traversal algorithm that systematically explores all the vertices (or nodes) of a graph in a breadth-first order, i.e., visiting all nodes at the same level before moving on to the next level.The algorithm starts by visiting the root node (or starting node) and then visits all the nodes at the next level before moving on to the nodes at the next level, and so on until all nodes have been visited. It uses a FIFO queue data structure to keep track of the nodes that have been visited but whose neighbors have not yet been visited. The queue is initialized with the root node, and the algorithm iteratively dequeues a node, visits all its neighbors, and enqueues them if they have not been visited before.The time complexity of BFS is $O(V+E)$, where V is the number of vertices (nodes) in the graph, and E is the number of edges. This is because the algorithm visits each node and each edge at most once. Also, $V=X, E=UX$ if the same actions U are available from every state. It is systematic. The worst-case performance of BFS is worse than that of A* and dynamic programming.Pseudo code: Initialise a queue data structure with the root node. Mark the root node as visited. Dequeue a node from the queue and visit it. Enqueue all the unvisited neighbors of the node. Mark all the enqueued neighbours as visited. Repeat steps 3-5 until the queue is empty.function BFS(start_node): Q = Queue() // Initialize a queue Q.enqueue(start_node) // Enqueue the start node visited = set() // Initialize a set of visited nodes while not Q.empty(): // While the queue is not empty node = Q.dequeue() // Dequeue the next node if node not in visited: // If the node has not been visited visited.add(node) // Mark the node as visited // Process the node (e.g., print its value) print(node) // Enqueue all unvisited neighbors of the node for neighbor in get_neighbors(node): if neighbor not in visited: Q.enqueue(neighbor)Depth FirstDepth First Forward Search (DFS) is a graph traversal algorithm that explores the vertices of a graph in a depth-first manner, starting from a given source vertex, and visiting all the vertices reachable from it.The DFS algorithm can be implemented using a LIFO stack data structure to keep track of the vertices being explored. When a vertex is visited, it is pushed onto the stack, and its unvisited neighbours are added to the stack in the order in which they are encountered. When there are no more unvisited neighbours, the vertex at the top of the stack is popped off and the algorithm backtracks to the previous vertex.The time complexity of DFS algorithm is $O(V+E)$, where V is the number of vertices in the graph and E is the number of edges. The space complexity of the algorithm is O(V) due to the stack used to keep track of the visited vertices. It is systematic. The search could easily focus on one direction and completely miss large portions of the search space as the number of iterations tends to infinity.Pseudo code: Start at the source vertex. Mark the source vertex as visited. Explore each unvisited neighbor of the source vertex in depth-first order. When there are no more unvisited neighbors, backtrack to the previous vertex and repeat step 3 for its unvisited neighbors. Continue this process until all reachable vertices have been visited.function depthFirstForwardSearch(graph, source): // Initialize a stack for DFS traversal and a set to keep track of visited vertices stack = new Stack() visited = new Set() // Push the source vertex onto the stack and mark it as visited stack.push(source) visited.add(source) while not stack.isEmpty(): // Pop the top vertex from the stack and print it currentVertex = stack.pop() print currentVertex // Explore the neighbors of the current vertex for neighbor in graph.getNeighbors(currentVertex): if neighbor not in visited: // Push the neighbor onto the stack and mark it as visited stack.push(neighbor) visited.add(neighbor)Best FirstBest First search is a heuristic search algorithm that is used to find the optimal path between two points in a graph. The algorithm expands the node with the lowest heuristic value first, i.e., the node that appears to be the closest to the goal. The search starts from the initial state and moves towards the goal state. It uses a heuristic function h(n) to evaluate the distance between a node n and the goal state. The algorithm maintains a priority queue (or a heap) of the nodes to be expanded, with the node with the lowest heuristic value at the front of the queue. It is not systematic.Pseudo code: Initialise the open list with the start node as the only element. Initialise the closed list as an empty set. While the open list is not empty, do the following: Remove the node with the lowest heuristic value from the open list. If the removed node is the goal node, return the path. Otherwise, expand the node by generating all its neighbouring nodes. For each neighbouring node, calculate its heuristic value h(n) and add it to the open list if it is not already in the closed list or the open list. Add the expanded node to the closed list. If the open list becomes empty and the goal node is not found, return failure.Iterative DeepeningIterative Deepening Algorithm (IDA) is a general search algorithm based on depth-first search (DFS), but unlike DFS, IDA performs multiple iterations, each with increasing depth limits. The algorithm starts with a depth limit of one and iteratively increases it until the goal is found.The iterative deepening algorithm uses a breadth-first search-like strategy to incrementally search the space by exploring each level of the search tree. The algorithm starts by searching the tree at a depth limit of one. If the goal is not found, the algorithm increases the depth limit to two and searches the tree again. This process is repeated until the goal is found.The iterative deepening algorithm is often used in cases where the search space is too large for a complete search. By limiting the search depth, the algorithm can focus on the most promising parts of the search space, while still being able to find the optimal solution. The time complexity of IDA is O(b^d), where b is the branching factor and d is the depth of the goal node. The space complexity of the algorithm is O(d). It has better worst-case performance than BFS. Furthermore, the space requirements are reduced because the queue in BFS is usually much larger than for DFS.Combinatorial Motion PlanningRoadmap Based Path Planning: Visibility Graph and Generalised Voronoi Diagrams as roadmapsCovers visibility graphs and voronoi diagrams.Combinatorial motion planning is a subfield of robotics and artificial intelligence that focuses on developing algorithms and techniques for planning the motion of robots in complex environments. The goal is to find feasible paths that enable robots to reach their desired goals while avoiding obstacles and adhering to various constraints.Unlike continuous motion planning, which deals with continuous trajectories, combinatorial motion planning involves searching through a discrete set of possible motions and selecting the optimal one. This involves exploring the space of all possible robot configurations and determining which configurations can be reached without colliding with obstacles or violating constraints.Some common techniques used in combinatorial motion planning include graph-based search algorithms, sampling-based planners, and cell decomposition methods. These techniques leverage computational tools and mathematical models to efficiently plan the motion of robots and ensure they operate safely and effectively.Continuous Motion Planning MethodsContinuous motion planning methods are algorithms and techniques used to plan the continuous motion of robots in complex environments. Unlike combinatorial motion planning, which deals with a discrete set of possible motions, continuous motion planning deals with continuous trajectories.Continuous motion planning methods use mathematical models and algorithms to determine optimal trajectories that enable robots to reach their desired goals while avoiding obstacles and adhering to various constraints. These methods often involve the use of calculus and optimization techniques to find the optimal path.They are often more computationally intensive than combinatorial motion planning methods. This is because they involve solving complex mathematical equations and optimizing continuous functions. However, they offer greater flexibility and precision in planning robot motion, enabling robots to operate more efficiently and effectively in complex environments.Examples of continuous motion planning techniques include: Trajectory optimization: This method involves finding the optimal trajectory for a robot by solving an optimization problem that minimizes a cost function. The cost function can be customized to include constraints on the robot‚Äôs motion, such as collision avoidance or energy consumption. Trajectory optimization can be solved using numerical optimization techniques, such as gradient descent or nonlinear programming. Control-based planning: This method involves using a control law to steer the robot along a desired trajectory. The control law can be designed using various techniques, such as feedback linearization or model predictive control. Control-based planning can be used to plan the motion of robots in dynamic environments, where the robot‚Äôs motion must be continuously updated in response to changing conditions. Model predictive control (MPC): MPC is a control-based planning method that involves predicting the future behavior of the robot and optimizing its motion accordingly. MPC uses a dynamic model of the robot and the environment to predict how the robot‚Äôs motion will affect its surroundings, and it optimizes the robot‚Äôs motion to achieve a desired goal while satisfying various constraints. Sampling-based planning: This method involves randomly sampling the robot‚Äôs configuration space to generate a set of feasible trajectories. The set of trajectories is then pruned to remove those that collide with obstacles or violate constraints. Sampling-based planning is computationally efficient and can be used to plan the motion of robots in high-dimensional spaces, but it may not always find the optimal trajectory.RoadmapsRoadmaps in combinatorial motion planning refer to a set of interconnected nodes and edges that are constructed in the configuration space of a robot or a group of robots. The configuration space represents all possible configurations of the robot(s) in the environment, including their position, orientation, and other relevant parameters.The roadmap construction process involves sampling the configuration space and identifying collision-free configurations that can be used as nodes in the roadmap. These nodes are then connected by edges that represent feasible paths between them. The resulting roadmap provides a discrete representation of the configuration space and enables efficient path planning for the robot(s).There are several types of roadmaps that can be used in combinatorial motion planning, including probabilistic roadmaps (PRMs), visibility graphs, and cell decomposition-based roadmaps. Each type has its strengths and weaknesses, and the choice of roadmap depends on the specific application and requirements.There are many roadmap-based approaches, and they primarily fall into 3 categories: Topological Model - Every location is represented by a node, which are connected by edges. Geometrical Model - The geometry of the environment, containing obstacles, is considered and a free space, which is a collection of all free configurations that do not collide with the obstacles, is created. Grid-based Model - The entire environment is divided in cells or grids and based on the connectivity between these grids, roadmaps are created.Formal definition: A union of one dimensional curves R is a roadmap if for all starting positions q and goal positions g, that can be connected by a path, the following properties hold - Accessibility - There exists a path from q to some point q‚Äô on R Departability - There exists a path from a point g‚Äô on R to g Connectivity - There exists a path in R from q‚Äô to g‚ÄôVisibility GraphThe visibility graph is a method used to represent the connectivity of a configuration space. A configuration space is the space that represents all possible positions and orientations of a robot or object.The basic idea is that if we stretched a rubber band from start to target, weaving in and out of the obstacles, the band would form a straight line between the obstacles, and would wrap around the obstacles themselves.The visibility graph is constructed by first placing a node at each obstacle boundary and at the starting and ending configurations of the robot. Then, an edge is added between two nodes if the line segment connecting them does not intersect any obstacle boundaries. This process creates a graph that represents all possible paths that the robot can take without colliding with any obstacles. Its edges are the edges of the obstacles and edges joining all pairs of vertices that can see each other.It is based on the principle that given a start point and a goal point, we look for all vertices of obstacles present in the environment which are visible from the given start location. We then traverse through those vertices to reach the destination in the shortest path.Start, goal, vertices of obstacles are graph nodes. Edges are ‚Äúvisible‚Äù connections between nodes, including obstacle edges.Reduced Visibility GraphA reduced visibility graph consists of very few edges to reduce its complexity. It makes use of supporting and separating lines. Supporting lines are edges in the visibility graph that connect two vertices on the boundary of the same obstacle. The obstacles lie on the same side of the supporting lines. These edges are important because they represent the possibility of the robot following a path along the boundary of an obstacle without colliding with it. Supporting lines can be used to construct a continuous path for the robot along the boundary of an obstacle, which is useful in situations where the robot cannot pass through the obstacle. Separating lines, on the other hand, are edges in the visibility graph that connect two vertices on the boundaries of different obstacles. The obstacles lie on opposite sides of the separating lines. These edges represent the possibility of the robot moving between two obstacles without colliding with either one. Separating lines are important because they define the regions of the configuration space that are accessible to the robot, and can be used to divide the configuration space into different regions, each of which can be explored separately.Neither supporting nor separating since they cut through the obstacles. They are avoided in constructing visibility graphs.Rotational Plane Sweep AlgorithmIt is used for constructing a visibility graph for a set of obstacles in a configuration space. The algorithm works by sweeping a ray around the configuration space and adding edges to the visibility graph whenever the ray intersects an obstacle boundary. Input: A set of vertices ${v_i}$(whose edges do not intersect) and a vertex v. Output: A subset of vertices from ${v_i}$ that are within the line of sight of v.Pseudo code:FOR each vertex v(i), calculate alpha(i)NEW vertexSet = set of alpha(i) which are increasing orderNEW activeSet = set of edge which intersect the horizontal half line emanating from vFOR all alpha(i)\tIF v(i) is visible v\t\tTHEN visibleSet.PUSH(edge(v,v(i))\tENDIF\tIF v(i) is the beginning of an edge, vertexSet, not in activeSet\t\tTHEN activeSet.PUSH(vertexSet)\tENDIF\tIF v(i) is the end of an edge in activeSet\t\tTHEN activeSet.DELETE(edge(v(i), v))\tENDIFENDFOROnce the visibility graph is built, then robot starts to find the path among visibility graph. Since the visibility map posses the properties of accessibility and departability by definition, if it is connected, then several kinds of searching algrithm can be implemented such as potential function and various star algorithm. Cost between nodes can be set to distance of nodes and heuristic cost function can be set to estimated cost of shortest path from node to goal.The shortest path being computed tries to stay as close as possible to the obstacles. Any execution error will lead to a collision. It also becomes more complex in higher dimensions.Cell DecompositionCell decomposition is a technique used in combinatorial motion planning to divide the workspace into smaller regions, called cells, that can be easily navigated by a robot. Each cell is defined as a region that is either free of obstacles or contains a single obstacle.The process of cell decomposition involves dividing the workspace into a grid of cells, where each cell is either classified as free or occupied by an obstacle. This grid can then be used to represent the workspace as a graph, where the cells are the nodes and the edges connect adjacent cells.One advantage of cell decomposition is that it simplifies the motion planning problem by reducing it to a discrete search problem. It also allows for the use of graph-based algorithms, such as Dijkstra‚Äôs algorithm or A*, to find a path from the starting cell to the goal cell.There are several types of cell decomposition techniques, including: Voronoi Diagrams: These are diagrams that divide the workspace into regions based on the distance to the obstacles. The cells in a Voronoi diagram represent the regions of the workspace that are closest to a particular obstacle. Binary Space Partitioning (BSP): This technique divides the workspace into two subspaces recursively, with each subspace divided by a line or plane. This creates a tree structure where each node represents a subspace and each leaf node represents a single cell. Quadtree Decomposition: This technique recursively divides the workspace into four quadrants, with each quadrant further subdivided until each cell contains at most one obstacle.Voronoi DiagramVoronoi diagrams are a geometric structure that partition a space into a set of regions based on the distance to a given set of points or objects. In the context of motion planning, Voronoi diagrams can be used to represent the free space and obstacles in a configuration space, and to compute paths or trajectories for a robot navigating through that space.The Voronoi diagram for a set of points in a space consists of a set of cells, where each cell represents the region of space that is closest to a particular point or object. The cells are defined by a set of lines or curves, called the Voronoi edges, which are equidistant from the neighboring points or objects. The Voronoi diagram is often visualized as a set of polygonal cells that cover the entire space, with the edges of the cells corresponding to the Voronoi edges.Generalized Voronoi DiagramsRobot Path Planning Using Generalized Voronoi Diagrams.The obstacles in the real world are never points but they‚Äôre large objects. Generalized Voronoi Diagrams are just like regular VDs, but instead of the regions around points, we have the regions around objects.Generalized Voronoi GraphCreate Voronoi regions with PythonVertical Strip Cell Decomposition The cells joining the start and target are shaded. The resolution of the decomposition is chosen to get a collision-free path dependent on the sensitivity of controlling robot‚Äôs motion.Planning Find the point \\(q*_{start}\\) of the Voronoi diagram closest to \\(q_{start}\\). Find the point \\(q*_{goal}\\) of the Voronoi diagram closest to \\(q_{goal}\\). Compute the shortest path from \\(q*_{start}\\) to \\(q*_{goal}\\) on the Voronoi diagram.Sampling based Motion PlanningThe state space for motion planning, C, is uncountably infinite. Yet a sampling based planning algorithm can consider at most a countable number of samples.Single Query and Multi Query PlanningMulti-query path planning and single-query path planning are two different types of path planning algorithms used in robotics and computer science.Single-query path planning refers to finding a path between two given points in a given environment, where the start and end points are fixed and do not change. In other words, it is the process of finding a path from a single source to a single destination in a given environment. This is a common problem in robotics, where robots need to navigate from one point to another without colliding with obstacles. It is only concerned about a portion of free C-space needed for a query.On the other hand, multi-query path planning refers to finding paths between multiple pairs of points in a given environment. Unlike single-query path planning, in multi-query path planning, the start and end points can vary from one query to another. For example, a robot may need to navigate to different locations in a warehouse to pick up items, and the start and end points will change with each new task. Th goal is to efficiently model the entire C-space so as to answer any query in that space. Example - Probabilistic Roadmap (PRM)Multi-query path planning is more complex than single-query path planning, as it requires the algorithm to efficiently handle multiple queries and avoid recalculating the entire path for each new query. However, it can be more efficient in situations where there are multiple tasks to be completed in the same environment.Probabilistic Roadmap of Tree (PRT) combines both ideas.Probabilistic Roadmap (PRM)It is a randomized algorithm that is designed to efficiently find collision-free paths for robots in complex environments. PRM algorithm involves the following components: Roadmap construction: The first step of PRM is to construct a roadmap of the environment. This is done by randomly sampling configurations of the robot in the environment and checking whether they are collision-free. Collision-free configurations are added to the roadmap as nodes, and connections are made between nodes that are within a certain distance of each other. Milestones: Milestones are the nodes in the roadmap that represent the important configurations of the robot in the environment. These configurations are sampled randomly from the configuration space of the robot, and they are checked for collision with the obstacles in the environment. If a configuration is collision-free, it is added as a milestone to the roadmap. Local paths: Local paths are the edges that connect the milestones in the roadmap. They represent the feasible paths that the robot can take between the milestones. The local paths are computed by planning a short path between each pair of milestones, while avoiding the obstacles in the environment. Roadmap search: The next step is to search the roadmap for a path between the start and goal configurations. The local paths are used to guide the robot along the roadmap, and the milestones serve as guideposts to ensure that the robot stays on a collision-free path. This is done using a standard graph search algorithm, such as A* search or Dijkstra‚Äôs algorithm. The algorithm tries to find the shortest path between the start and goal configurations while avoiding collisions with obstacles. Path refinement: The final step is to refine the path found by the roadmap search. This is done by iteratively attempting to move along the path and checking for collisions. If a collision is detected, the path is locally modified to avoid the obstacle. This process continues until a collision-free path is found.Features: PRMs don‚Äôt represent the entire free configuration space, but rather a roadmap through it. Roadmap is an undirected acyclic graph R = (N,E). Nodes N are robot configurations in free C-space, called milestones. Edges E represent local paths between configurations. Learning Phase Construction: reasonably connected graph covering C-space Expansion: improve connectivity Local paths not memorized (cheap to re-compute) Construction overview: R = (N,E) begins empty A random free configuration c is generated and added to N Candidate neighbours to c are partitioned from N Edges are created between these neighbours and c, such that acyclicity is preserved Repeat 2-4 until doneGeneral Local Planner: Connect the two configurations is C-space with a straight line segment Check the joint limits Discretize the line segment into a sequence of configurations c1, c2, ‚Ä¶, cm such that for every $(c_i, c_{i+1})$, no point on the robot at ci lies further than $\\epsilon$ away from its position at ci+1 For each ci, grow robot by $\\epsilon$, check for collisionsEdges are created between these neighbours and c, such that acyclicity is preserved.Random Bounce Walk: Pick a random direction of motion in C-space, move in this direction from c. If collision occurs, pick a new direction and continue. The final configuration n and the edge (c,n) are inserted into the graph. Attempt to connect n to other nodes using the construction step technique. Path between c and n must be stored, since process is non-deterministic.Select c such that P(c is selected) = w(c)Query Phase: Connect start and goal configurations to roadmap (say $\\hat{s}$ and $\\hat{g}$) Find path between $\\hat{s}$ and $\\hat{g}$ in roadmapPros: Once learning is done, queries can be executed quickly Complexity reduction over full C-space representation Adaptive - can incrementally build on roadmap Probabilisically complete, which is usually good enoughCons: $\\hat{s}$ and $\\hat{g}$ should be in same connected component, else failure Paths are not optimal. They can be long and indirect, depending on how the graph was created. Smoothing can be applied.Rapidly-Exploring Random Tree (RRT) [Path Planning with A* and RRT Autonomous Navigation, Part 4](https://www.youtube.com/watch?v=QR3U1dgc5RE&amp;t=45s) RRT, RRT* &amp; Random TreesRRT is a probabilistic algorithm that builds a tree of feasible paths in the search space.The RRT algorithm starts with an initial configuration of the robot or system being planned for, and then iteratively grows a tree by randomly selecting a new configuration, and connecting it to the closest node in the tree. The algorithm continues to grow the tree until a path is found from the initial configuration to the goal configuration, or until a certain maximum number of iterations is reached.The random sampling and connection of nodes in RRT allows it to quickly explore large areas of the search space, making it well-suited for solving complex path planning problems in high-dimensional spaces. Additionally, RRT can handle non-holonomic constraints, as well as dynamic obstacles and changes in the environment.RRT has several variants, including RRT* and RRT-Connect, which incorporate optimization and connectivity constraints to improve the quality of the resulting paths.RRTs have the advantage that they expand very fast and bias towards unexplored regions of the configuration space and so covering a wide area in a reduced time.When one of the leaves of the tree reaches a goal region, the algorithm stops and a path can directly be found by following the predecessors of the last added node.Features: It is implemented using tree data structure Tree is special case of directed graph Edges are directed from child node to parent Every node has one parent, except root Nodes represent physical states or configurations Edges represent feasible paths between states Each edge has cost associated traversing feasible pathPseudo code:1. Initialize tree T with a single node at the start state S2. while not goal_found and iterations &lt; max_iterations do3. randomly sample a new configuration Q4. find the node N in T that is closest to Q5. extend the tree from N towards Q by a maximum distance delta_q6. if Q is within delta_q of the goal state G, add a node at Q and goal_found = true7. iterations = iterations + 18. return the path from S to G if goal_found, otherwise return failureIn the above pseudocode, T represents the tree of feasible paths, S is the initial state, G is the goal state, and delta_q is a parameter that controls the maximum distance the algorithm can extend from the current node. The algorithm iteratively adds nodes to the tree T until either a path from S to G is found, or the maximum number of iterations is reached.A* AlgorithmA* Pathfinding (E01: algorithm explanation)Dijkstra‚Äôs AlgorithmDijkstra‚Äôs Algorithm - ComputerphileHow Dijkstra‚Äôs Algorithm Works" }, { "title": "Chess Engine using Reinforcement Learning", "url": "/posts/chess-engine-rl/", "categories": "Projects, RL", "tags": "rl, games", "date": "2023-03-28 21:30:00 +0530", "snippet": "AbstractMachine learning based chess engines have always proven themselves to outperform human capabilities in the strategic field of chess. The number of possibilities after every move someone makes increases exponentially. For example, after just two moves (two turns for white and two for black), there are 197,742 possible board positions. Certain grandmasters of chess are able to see 5 to 10 moves in the future in order to decide the best move to play, but that is in exchange for the lifetime of training they devote to this world of 64 squares.This is where computers come into play. While chess engines learn much faster and can calculate to much further depths than a human can, a perfect chess engine has still always been the biggest challenge to the world of Artificial intelligence. While the Deep Blue started making headlines in 1996, the top-level chess engines started beating human beings around the 1970s. However, needless to say, the chess engines have come a long way since then, with stockfish being the highest-rated chess engine to date, implementing supervised and unsupervised learning techniques with some pre-fed information contributed by top world grandmasters, and DeepMind‚Äôs Deep Reinforcement learning based AlphaZero, which learned to play excellent chess, even beating stockfish, entirely from self-play over a few hours of training.The aim of our project is to create a similar chess engine, which can at least beat average-level players and is more or less self-trained, using deep reinforcement learning techniques and neural network-based algorithms. The aim is to build an AlphaZero-based chess engine implementing deep neural network architectures paired with Monte Carlo Tree Search, but at the same time diverge from AlphaZero in that we‚Äôll avoid a full board representation and use a handcrafted feature vector instead, inspired by the Giraffe paper. We will also use Stockfish and a few other methods to accelerate training.Literature ReviewGiraffe: Using Deep Reinforcement Learning to Play Chess by Matthew LaiNEURAL NETWORK BASED EVALUATIONFeature Representation Side to Move Castling Rights Material Configuration Piece Lists Sliding Pieces Mobility Attack and Defend MapsTotal Features - 363Network ArchitectureTraining Set Generation Approach5 million positions randomly selected from databases and a random legal move to each position, introducing imbalances and more unusual positions.Network InitializationEvaluative function with only basic material knowledge used.TD-LeafImplementation of TD Leaf using following update rule:ResultPROBABILISTIC SEARCHLimit the search based on the probability of a move being a part of the main line.NEURAL NETWORK BASED PROBABILITY ESTIMATIONTrain a neural network to give $ P(child_i|parent)$Feature Representation Piece Type From Square To Square Promotion Type RankNetwork ArchitectureSimilar to the one used earlier.Training Positions GenerationTime Limited Search on the root positions from the training set for the evaluation network training and randomly sample from the positions encountered as internal nodes in those searches. 5 million positions.Network TrainingStochastic Gradient Descent with AdaDelta update rules using the cross-entropy loss function.RESULTS AND EVALUATIONDifference is 48+-12 Elo points.CONCLUSIONThe learned system performed at least comparably to the best expert-designed counterparts in existence at the time.Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess by Timo Bertram, Johannes Furnkranz and Martin MullerThe paper explores how to teach an artificial intelligence (AI) agent to play the imperfect information game of reconnaissance blind chess (RBC) using the AlphaGo methodology. RBC is an imperfect information game, unlike traditional board games like chess and go, where players have little knowledge of where the opponent‚Äôs pieces are.The research suggests a modification of the AlphaGo training method for RBC, which entails training a supervised agent on publicly accessible game records and then improving its performance through self-play using the Proximal Policy Optimisation reinforcement learning algorithm. Instead of a detailed description of the game state, the agent is trained to produce moves based on observations of the current state of the game. According to the article, using only observations is required to prevent issues brought on by the partial observability of game states in RBC.Network ArchitectureResultsThe suggested strategy led to an ELO of 1330 on the RBC leaderboard, moving the agent up to position 27 at the time of writing. The outcomes show that self-play considerably enhances the agent‚Äôs performance and that the agent is capable of playing respectably without search and without assuming the genuine game state.Reinforcement Learning in an Adaptable Chess Environment for Detecting Human-understandable ConceptsThis paper aims to make the engine master the game of chess via self play. The agent is a deep neural network with randomly initialised weights, which get better as it trains by playing against itself and learns to recognise board patterns. The environment consists of a board with user defined board size, with a simple interface for extracting useful information regarding the board state. The environment is also coupled with a standard variant of Monte Carlo Tree Search (MCTS). TensorFlow Lite is utilised to provide fast neural network guidance without having to rely on batching predictions for the GPU.Concepts are detected by learning a set of logistic probes for a data set representing the concept of interest, using the activation outputs for each intermediary layer in the neural network. The aim is to find the best-fitting logistic regressor with weights w and bias b so that$||œÉ (w ¬∑ Oi + b) ‚àí Pi||^2$ is minimised, where Oi are the activation outputs and Pi are the binary labels {0, 1}. Then a random sample of 10% of the data is tested for validation of the modelThe probed concepts for the 4x5 and 6x6 agents are:We can see that soon after in the training process, proceeding approximately 30 iterations, the 4x5-agent learns to represent whether it is threatening the opponent‚Äôs queen. . Later, after approximately 50 iterations, the agent learns to represent whether it has a potential mating attack. Surprisingly, the agent represents the state of being in check only weakly throughout the entire training process, despite learning to play optimally. All the probed concepts flatten out after about 100 training iterations, regardless of whether training continues. This indicates that this agent is highly unlikely to allocate more resources to represent these concepts provided more experience.For the 6x6-agent, many of the same trends as the 4x5 agent can be seen, the main difference being that material advantage as well as has mate threat are detectable already at the beginning of the training loop.In an RL context, most of the early learning takes place because the agent accidentally succeeds, i.e. delivers a checkmate. As a consequence, concepts connected to actions that are statistically more likely to win the game for a given agent, appear during the earliest stages of training.The implementation is limited to probing for linearly represented concepts, although there are no guarantees that the network represents its knowledge in a linearly separable manner. Also, it cannot highlight the differences between being able to detect concepts and knowing how the represented concepts are utilised within the given model. For example: it is easy for a human to understand that the player holding less material is more likely to lose. However, whether or not the model has made the same connection cannot be guaranteed.Network ArchitectureMastering Chess and Shogi by Self-Play with a General Reinforcement Learning AlgorithmThe paper introduces the AlphaZero reinforcement learning method, which trains a neural network to play board games. By teaching the algorithm to play chess and shogi (Japanese chess) without prior knowledge of the game‚Äôs rules or strategies, the authors demonstrated the algorithm‚Äôs effectiveness. The system eventually attained a superhuman level of performance, outperforming the top computer and human players in each game.The introduction to reinforcement learning, a subset of machine learning involving an agent interacting with its surroundings to learn how to complete a task, is the paper‚Äôs first section. The authors describe the AlphaZero approach, which combines a deep neural network with a Monte Carlo tree search to learn how to play a game through self-play.The algorithm for the proposed AlphaZero was - The parameters of the neural network are initialized randomly Games are played by selecting moves for both players by MCTS At the end of the game, the terminal position is scored according to the rules as +1 for a win, 0 for neutral, and -1 for a loss Neural network parameters are updated with 2 aims To minimize the error between predicted and game outcome To maximize the similarity of the policy vector to the search probabilities The author also discussed how their algorithm was different from the AlphaGo Zero algorithm AlphaGo Zero estimates and optimizes the probability of winning, assuming binary win/loss outcomes. AlphaZero estimates and optimizes the expected outcome, taking account of draws or potentially other outcomes In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. AlphaZero simply maintains a single neural network that is updated continually rather than waiting for an iteration to complete AlphaGo Zero tuned the hyper-parameters of its search by Bayesian optimization. In alphaZero, they reuse the same hyper-parameters for all games without game-specific tuningThe writers used chess, Shogi, and Go as test cases for AlphaZero. After only a few hours of practice, the algorithm was able to master all three games at a superhuman level. The results for chess and shogi are the main subject of the paper.The authors compared AlphaZero‚Äôs chess performance to that of Stockfish and Elmo, two more chess programs. Elmo was a forerunner of AlphaZero and was trained on a smaller sample of data than Stockfish, one of the greatest chess engines in the world. In a 100-game contest against Stockfish and Elmo, AlphaZero prevailed by winning 28 games and drawing the remaining 72. The unusual playing style of AlphaZero was also recognized, with a propensity for sacrifice attacks.The authors evaluated AlphaZero‚Äôs performance in shogi against the top human players and the YSS algorithm, the most powerful shogi program. In a contest of 100 games, AlphaZero defeated the YSS algorithm by winning 90 of them and drawing the remaining ten. The top human players were also used to compare AlphaZero‚Äôs performance, and it was discovered that AlphaZero‚Äôs style of play was distinct from both the human and YSS styles.Overall, the study offers a substantial advance in artificial intelligence and shows how well a broad reinforcement learning algorithm works for teaching players how to play challenging games. According to the authors, the technique might be used for other games and fields, such as robotics and optimization issues.MethodologyDeep Pepper SummaryThe Deep Pepper paper adapts methods from AlphaZero, deep networks and monte carlo tree search algorithms. It implements a neural network (value network) to predict state values and the optimal move (policy network) at any state using custom feature representation. This policy network is used together with an MCTS algorithm, and the optimal move is decided by an upper confidence tree selection process. Training is done by embedding some prior knowledge of chess, and using stockfish to early stop the games for an accelerated training.Deep Pepper uses MCTS for both policy evaluation and policy improvement.There are three main phases of the algorithm (each board state is stored as a node: Selection: The most promising child node for each root node is considered until we come across an unvisited node. Expansion and evaluation: This leaf node is expanded using the neural network, giving the probabilistic values for all possible board moves, and the value of the leaf, ‚Äòv‚Äô is found out using the value network. Back up: The statistics (action values and the number of times the move has been taken) of each explored move is updatedEach node contains N(s, a) (number of times each of its children were visited), W(s, a) (The total value of all descendent leaf nodes as per the value given by the neural network), Q(s, a) (the average value for each node =W(s, a)/N(s, a) ) and P(s, a) (the initial move probabilities that the neural network provides), with Dirichlet noise added to favour exploration. The states and actions are explored using a variant of Polynomial Upper Confidence Tree‚Äôs (PUCT), based on estimates on the upper bound of selection probabilities U(s, a) having an exploration factor Cpuct, and the child node is selected as the one that maximises the sum of Q(s, a) and U(s, a).At the end of each iteration (800 simulations of MCTS), MCTS provides a policy to be used in the current game state. The policy is given by: œÄ(s, a) = N(s, a)/Œ£a N(s, a)STATE REPRESENTATION: Complete board representation in the form of 8x8x73 poses certain issues, like having too many layers makes it harder and more time consuming to train,and CNNs can‚Äôt be trained to predict sudden changes in output to considerably small changes in input. In light of this, we take inspiration from the Giraffe paper, having a custom 353-dimensional feature vector, which also gives favorable results during pre-training. We also use stockfish embedded knowledge to accelerate training.NEURAL NETWORK ARCHITECTURE: Multiple architectures are experimented: PolicyValNetwork-parts: The global features are connected to h1a neurons, the piece centric features to h1b neurons and square centric features to h1c neurons in the first layer. All the concatenated outputs from the first layer are then connected to the h2p and h2v neurons in the second layer which give us the policy output and the value function respectively. PolicyValNetwork-Full: Instead of having distinct connections in the first layer, we have a fully connected layer allowing the network to learn more implicit connections between each of the features. An insignificant improvement in performance as compared to the increased training time is observed. PolicyNetwork-parts and Critic-parts: A minor improvement in performance is observed if we also train the first layer independently for policy and value networks (separate). PolicyNetwork-full and Critic-full: very slight improvement in performanceTRAINING ALGORITHM PIPELINE: The training procedure can be viewed as a CAPI. MCTS is used for game generation and improvement testing. Game generation phase: During this phase, deep pepper plays many games against itself. The current board state is used as a root node and the optimal policy is determined. After k half moves,in order to accelerate training, stockfish evaluation is used to indicate the probability for the player to win or lose, resulting in the player/opponent to resign Network training phase: The data is created during the game generation phase as a triplet of: {s, œÄ(s, a), ŒΩ}, where s is the board state, œÄ(s, a) is the policy calculated by MCTS, and v is the end game value determined by a draw, win or resignation. The network is trained via back propagation with gradient updates calculated using an Adam optimizer on mini-batches. Improvement Testing phase: an optional check to ensure that the new network has improved over the last iteration. Should the updated network defeat the older iteration by more than half the time, the newly trained network is kept and used in the next round of game generation.PRETRAINING: An optional phase added in order to offer early guidance in network training.Stockfish assigns values to the board positions taken from professional chess games, helping us to generate an approximate policy label. This is because training of the policy network doesn‚Äôt work directly on expert games. Then, supervised learning is used to train the networks using the generated labels.The Best Chess EnginesBackgroundStockfish chess engine was released in 2008, which was made by open-source developers. It is derived from Glaurang, an open-source engine which was developed in 2004. This chess engine was the strongest chess engine and had won many chess matches.In 2017, DeepMind, which is a subsidiary of Google, developed a chess engine, named AlphaZero, which defeated Stockfish multiple times and shocked the chess community. In Stockfish, the approach is to use sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades.But in AlphaZero, the only input given to it was rules of chess. It uses a deep neural network for evaluation which is trained by a reinforcement learning algorithm. This engine has also worked successfully on games like Shogi and Go.StockfishStockfish uses handcrafted features which consists of : Mobility or Trapped Pieces, Pawn Structures, King‚Äôs Safety, number of pieces left, outposts etc. Each of these features has a specific weight assigned to it and evaluation of a position is resulted from the sum of features multiplied to its respective weights. For evaluating a position, stockfish spans a tree of possible ways to move and evaluates using the evaluation function. The problem is that the size of complete chess tree could go to near 10123, and evaluating that would be difficult. So we limit the breadth and depth of search by using Alpha-Beta algorithm. This algorithm tries to go through the search which drops the worst options for the other player, instead of finding the highest value of search evaluation. This approach works best if we consider the best moves as early as possible. For example, if we conduct a search of all moves in ascending order, we would have to go through all possible positions. But if we consider strongest move first, then the algorithm would reduce the node that has to be evaluated to its root (if x is number of node, it will reduce it to ). Stockfish uses an opening book to choose moves in the first phase of the game as well an endgame tablebase that includes the best moves for all positions with six or less pieces left on the boardAlphaZeroIn the case of AlphaZero, instead of using handcrafted features for evaluation function, it uses a deep neural network which is trained by reinforcement learning.In this algorithm, the first step is to choose the initial position from where it should start and then to decide what moves it should make. The input to the neural network is all 64 squares in the chess board and all the move possibilities each piece would have on each of these squares. As each square is treated in the same way, it leads to certain illegal moves, which is then rectified by the chess rule which is given input to the engine by reducing their probability to zero. The output from this neural network is a vector containing 2 values. One signifies the percentage of search time and another is the win probability that the neural network assigns to this move. The training of this neural network is done by making it play with itself. The neural network is trained by Monte-Carlo Tree Search algorithm.Results and ImprovementsMonte Carlo Tree Search Accuracy:The accuracy of Monte Carlo Tree Search (MCTS) refers to the algorithm‚Äôs capacity to accurately predict the outcome of future moves in a game of chess. It depends on several factors such as the accuracy of the evaluation function used to estimate the value of a game position and the number of simulations performed by the algorithm. It is crucial in determining the effectiveness of the moves made by the AI agent. Poor accuracy can result in the agent performing poorly and making bad decisions. As a result, one of the key objectives of this project is to increase MCTS accuracy.Cyclic Learning Rates:Cyclic Learning Rates (CLR) is a technique used in reinforcement learning to tune the learning rate hyperparameter. The learning rate, in a conventional approach, is manually established and remains constant over the course of the training. However, this fixed learning rate may not be appropriate for different phases of the training process, which can result in suboptimal results.Cyclic Learning Rates vary the learning rate over multiple cycles, by initially starting with a low learning rate and gradually increasing it to a maximum value before lowering it once more. Throughout the training phase, this cycle is repeated numerous times.The main advantage of doing this is that the learning rates adapt to the dynamics of the training process and avoid getting stuck in local optima. They encourage the model to move out of the local minimum and instead identify the global one by periodically raising the learning rate.This figure shows how the learning rate and momentum is varied every iteration. It can also be noticed that the change in learning rate is minimal towards the end of training to accommodate the fact that the policy shouldn‚Äôt change too much towards the end.This figure shows the MCTS accuracy with and without cyclic learning rates. It can be seen that initially the model using cyclic learning rates has a higher accuracy but towards the end the difference is negligible.Squeeze and Excite:Squeeze and Excitation (SE) is a technique that can be used to improve the performance of convolutional neural networks (CNNs). It aims to capture the interdependence between channels by learning a channel-dependent weighting mechanism, which dynamically adjusts the channel activations based on their importance.The squeeze operation and the excitation operation are the SE module‚Äôs two main parts. In order to capture the global information for each channel, the squeeze operation decreases the spatial dimension (H x W) of feature maps to a single dimension. The excitation operation is in charge of learning a channel-wise weighting mechanism, which determines the significance of each channel at each point.The chess engine‚Äôs performance can be enhanced by including the SE module in its CNN architecture. It can be used to extract high-level features from the input data following a convolutional layer. The data from all channels are then combined into a vector via the SE module‚Äôs squeezing of the feature maps along the channel dimension.The excitation operation follows, which learns a set of parameters to model the dependencies between various channels in the feature maps. A weighted vector is created as a result, capturing the significance of each channel. Finally, the feature maps are obtained by multiplying the weighted vector with the original feature maps.This figure shows the MCTS accuracy trends with and without Squeeze and Excite. It can be seen that there are some early gains early during the training but there some losses later on. Overall, it does not affect the performance much and intuitively it should help.C-PUCT:C-PUCT extends the traditional MCTS algorithm by adding continuous progress estimates, which allow the algorithm to better allocate computational resources and improve search efficiency. To determine the optimum moves, it uses an Upper Confidence Bound (UCB) method with the additional benefit of updating the confidence bounds in real time while the search goes on.The C-PUCT algorithm can be used in a chess engine to find the best move by searching through the possible move sequences from the current position on the chessboard. The algorithm can reduce the search space and quickly determine the optimum move by using a search tree and a combination of deterministic and probabilistic heuristics.The quality of the input features used and the precise algorithmic parameters chosen will determine the move that the C-PUCT algorithm generates. To get the chess engine to function at its best, these parameters typically need to be adjusted.ConclusionThe observation of a limited application due to the significant hardware demands in this research spurred the need for effective and scalable improvements of Deep Pepper. Cyclic Learning Rates showed great potential while other methods like Squeeze and Excite and Hyperparameter Tuning showed decent increase in performance too. Overall, the hardware demands were reduced and the performance of the model was increased with these improvements.Bibliography Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting https://arxiv.org/pdf/1806.00683.pdf Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm https://arxiv.org/abs/1712.01815 Giraffe: Using Deep Reinforcement Learning to Play Chess https://doi.org/10.48550/arXiv.1509.01549 Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess https://doi.org/10.48550/arXiv.2208.02029 Reinforcement Learning in an Adaptable Chess Environment for Detecting Human-understandable Concepts https://doi.org/10.48550/arXiv.2211.05500" }, { "title": "Probabilistic Robotics", "url": "/posts/aifr/", "categories": "Blog, Robotics", "tags": "slam", "date": "2023-03-28 21:30:00 +0530", "snippet": "In progressResourcesProgrammingBasic Concepts of ProbabilityTerminologyThe environment, or world, is a dynamical system that possesses internal state. The robot can acquire information about its environment using its sensors. However, sensors are noisy, and there are usually many things that cannot be sensed directly. As a consequence, the robot maintains an internal belief with regards to the state of its environment.State State - It is a collection of all aspects of the robot and its environment that can impact the future. Dynamic states are state variables that tend to change over time, while static states are non-changing. The state at time $t$ is denoted as $x_t.$ Pose - The robot pose comprises of its location and orientation relative to a global reference frame. In the context of mobile robots, the pose is usually given by three variables, two location coordinates in the plane and one orientation w.r.t the vertical axis normal to the plane. The location and features of the surrounding objects in the environment are also state variables. Based on the quality of the state, robot environments can have even hundreds to billions of state variables. Some objects in the environment can be recognized reliably and are referred to as landmarks. Other state variables can include whether or not a sensor is broken, the level of battery charge, etc. These take discrete values. Markov chains - A state $x_t$ is said to be complete if the knowledge of past states, measurements or controls carry no additional information to help predict the future more accurately. The future may be stochastic but no variables prior to $x_t$ may influence the future states, unless this dependence is mediated through the state $x_t.$Environment Interaction Sensor Measurements - Perception is the process by which the robot uses its sensors to obtain information about the state of its environment. Typically, sensor measurements have some delay and provide information about the state a few moments ago. However, we will assume that measurements corresponds to a specific point in time. Measurement data at point $t$ will be denoted by $z_t.$\\[z_{t1:t2} = z_{t1},z_{t1+1},z_{t1+2},....,z_{t2}\\] Control Actions - They carry information about the change of state of the environments and they change the state by actively asserting forces on the robot‚Äôs environment. It is denoted by $u_t.$\\[u_{t1:t2} = u_{t1},u_{t1+1},u_{t1+2},....,u_{t2}\\] Hidden Markov ModelThe emergence of state $x_t$ might be conditioned on all past states, measurements and controls. This gives us a probability distribution of the following form:\\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t})\\]We assume that the robot executes the control action $u_1$ first and then takes the measurement $z_1$.If the state $x$ is complete, then\\[p(x_t | x_{0:t-1},z_{1:t-1},u_{1:t}) = p(x_t|x_{t-1},u_t) \\implies State \\space transition \\space probability\\]Similarly, if $x_t$ is complete, we have\\[p(z_t | x_{0:t},z_{1:t-1},u_{1:t}) = p(z_t|x_{t}) \\implies Measurement \\space probability\\]The state at time $t$ is stochastically dependent on the state at time $t-1$ and the control $u_t.$ The measurement $z_t$ depends stochastically on the state at time $t.$Belief DistributionsBelief represents the robot‚Äôs internal knowledge about the state of the environment. We denote the belief over a state variable $x_t$ by $bel(x_t),$\\[bel(x_t) = p(x_t|z_{1:t},u_{1:t})\\]We assume that the belief is taken after incorporating the measurement $z_t$. Sometimes it is also useful to calculate the belief before incorporating $z_t$, just after executing $u_t.$\\[\\bar{bel(x_t)} = p(x_t|z_{1:t-1},u_{1:t})\\]Calculating $bel(x_t)$ from $\\bar{bel(x_t)}$ is called correction or measurement update.Bayes FilterPseudo-codeAlgorithm Bayes_filter ($bel(x_{t-1}),u_t,z_t):$for all $x_t$ do:\\[\\bar{bel(x_t)} = \\int{p(x_t|u_t,x_{t-1}) bel(x_{t-1})dx_{t-1}}\\]\\[bel(x_t) = \\eta p(z_t|x_t)\\bar{bel(x_t)}\\]endforreturn $bel(x_t)$Markov AssumptionAlso called the complete state assumption, it postulates that the past and future data are independent if one knows the current state $x_t$. However, the following factors may have a systematic effect on sensor readings and induce violations to the Markov assumption: Unmodeled dynamics in the environment are not included in $x_t$ Inaccuracies in probabilistic models - state transition probability and measurement probability Approximation errors when using approximate representations of belief functions Software variables in the robot control software that influence multiple controls Estimation is after the occurrence of the event i.e. posterior probability. Prediction is a kind of estimation before the occurrence of the event i.e. apriori probability.What is the difference between estimation and prediction?The $\\alpha-\\beta-\\gamma$ FilterOnline Kalman Filter TutorialNotation$x$ is the true value of the weight$z_n$ is the measured value of the weight at time¬†n$\\hat{x}_{n,n}$ is the estimate of¬†$x$¬†at time¬†n¬†(the estimate is made after taking the measurement¬†$z_n$)\\(\\hat{x}_{n+1,n}\\) is the estimate of the future state ( n+1¬†) of¬†$x$. The estimate is made at the time¬†n. In other words,¬†$\\hat{x}_{n+1,n}$¬†is a predicted state or extrapolated state\\(\\hat{x}_{n-1,n-1}\\) is the estimate of¬†xx¬†at time¬†n‚àí1¬†(the estimate is made after taking the measurement¬†$z_{n-1}$)$\\hat{x}_{n,n-1}$ is a prior prediction - the estimate of the state at time¬†n. The estimate is made at the time¬†n‚àí1\\[For \\space constant \\space dynamics: \\newline \\hat{x}_{n+1,n}= \\hat{x}_{n,n}\\]\\[State \\space Update \\space Equation: \\newline \\hat{x}_{n,n} = \\hat{x}_{n,n-1} + \\alpha_n \\left( z_{n} - \\hat{x}_{n,n-1} \\right)\\]$\\alpha_n$ is called the Kalman Gain$(z_n - \\hat{x}_{n,n-1})$ ¬†is the ‚Äúmeasurement residual‚Äù, also called¬†innovation. The innovation contains new information.$\\alpha - \\beta$ Filter\\[State \\space Extrapolation \\space Equations: \\newline\\]\\[x_{n+1}= x_{n}+ \\Delta t\\dot{x}_{n} \\newline \\dot{x}_{n+1}= \\dot{x}_{n}\\]The above system of equations is also called Transition Equation or¬†Prediction Equation, and is true for constant velocity dynamics.$\\alpha-\\beta$¬†track update equations¬†or $\\alpha-\\beta$ track filtering equations -\\[State \\space Update \\space Equation \\space for \\space position: \\newline\\]\\[\\hat{x}_{n,n} = \\hat{x}_{n,n-1}+ \\alpha \\left( z_{n}- \\hat{x}_{n,n-1} \\right) \\newline\\]\\[State \\space Update \\space Equation \\space for \\space velocity: \\newline\\]\\[\\hat{\\dot{x}}_{n,n} = \\hat{\\dot{x}}_{n,n-1}+ \\beta \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{ \\Delta t} \\right)\\]$\\alpha-\\beta-\\gamma$ Filter\\[State \\space Extrapolation \\space Equations: \\newline\\]\\[\\hat{x}_{n+1,n}= \\hat{x}_{n,n}+ \\hat{\\dot{x}}_{n,n} \\Delta t+ \\hat{\\ddot{x}}_{n,n}\\frac{ \\Delta t^{2}}{2} \\newline\\]\\[\\hat{\\dot{x}}_{n+1,n}= \\hat{\\dot{x}}_{n,n}+ \\hat{\\ddot{x}}_{n,n} \\Delta t \\newline\\]\\[\\hat{\\ddot{x}}_{n+1,n}= \\hat{\\ddot{x}}_{n,n}\\]\\[State \\space Update \\space Equations: \\newline\\]\\[\\hat{x}_{n,n}= \\hat{x}_{n,n-1}+ \\alpha \\left( z_{n}- \\hat{x}_{n,n-1} \\right) \\newline\\]\\[\\hat{\\dot{x}}_{n,n}= \\hat{\\dot{x}}_{n,n-1}+ \\beta \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{ \\Delta t} \\right) \\newline\\]\\[\\hat{\\ddot{x}}_{n,n}= \\hat{\\ddot{x}}_{n,n-1}+ \\gamma \\left( \\frac{z_{n}-\\hat{x}_{n,n-1}}{0.5 \\Delta t^{2}} \\right)\\]The main difference between these filters is the selection of weighting coefficients¬†$\\alpha-\\beta-(\\gamma)$. Some filter types use constant weighting coefficients; others compute weighting coefficients for every filter iteration (cycle).The choice of the¬†$\\alpha,\\beta,\\gamma$¬†is crucial for proper functionality of the estimation algorithm.Kalman FilterOnline Kalman Filter TutorialThe Kalman Filter is an¬†optimal filter. It combines the prior state estimate with the measurement in a way that minimizes the uncertainty of the current state estimate.\\[For \\space constant \\space dynamics: \\newline p_{n+1,n}= p_{n,n}\\]\\[Covariance \\space Extrapolation \\space Equation: \\newline p_{n+1,n}^{x}= p_{n,n}^{x} + \\Delta t^{2} \\cdot p_{n,n}^{v} \\newline p_{n+1,n}^{v}= p_{n,n}^{v}\\]$p^x$ is the position estimate uncertainty, $p^v$ is the velocity estimate uncertainty, and the above is true for constant velocity dynamics.Note that for any normally distributed random variable¬†$x$¬†with variance ${\\sigma}^2$,¬†$kx$¬†is distributed normally with variance¬†$k^2{\\sigma}^2$, therefore the time term in the uncertainty extrapolation equation is squared.\\[\\hat{x}_{n,n} = \\hat{x}_{n,n-1} + \\frac{p_{n,n-1}}{p_{n,n-1} + r_{n}}\\left( z_{n} - \\hat{x}_{n,n-1} \\right)\\]Kalman Gain:\\[K_{n}= \\frac{Uncertainty \\quad in \\quad Estimate}{Uncertainty \\quad in \\quad Estimate \\quad + \\quad Uncertainty \\quad in \\quad Measurement}= \\frac{p_{n,n-1}}{p_{n,n-1}+r_{n}}\\]\\[0 \\leq K_{n} \\leq 1\\]\\[\\left( 1 - K_{n} \\right) = \\left( 1 - \\frac{p_{n,n-1}}{p_{n,n-1} + r_{n}} \\right) = \\left( \\frac{p_{n,n-1} + r_{n} - p_{n,n-1}}{p_{n,n-1} + r_{n}} \\right) = \\left( \\frac{r_{n}}{p_{n,n-1} + r_{n}} \\right)\\]\\[Covariance \\space Update \\space Equation: \\newline p_{n,n} = \\left( 1 - K_{n} \\right)p_{n,n-1}\\]This equation updates the estimate uncertainty of the current state. It is clear from the equation that the estimate uncertainty is constantly decreasing with each filter iteration, since¬†$\\left( 1-K_{n} \\right) \\leq 1$. When the measurement uncertainty is high, the Kalman gain is low. Therefore, the convergence of the estimate uncertainty would be slow. However, the Kalman gain is high when the measurement uncertainty is low. Therefore, the estimate uncertainty would quickly converge toward zero.The Kalman Gain is close to zero when the measurement uncertainty is high and the estimate uncertainty is low. Hence we give significant weight to the estimate and a small weight to the measurement.On the other hand, when the measurement uncertainty is low, and the estimate uncertainty is high, the Kalman Gain is close to one. Hence we give a low weight to the estimate and a significant weight to the measurement.If the measurement uncertainty equals the estimate uncertainty, then the Kalman gain equals 0.5.The Kalman Gain Defines the measurement‚Äôs weight and the prior estimate‚Äôs weight when forming a new estimate. It tells us how much the measurement changes the estimate.Summary of all 5 Kalman EquationsThe State Extrapolation Equation and the Covariance Extrapolation Equation depend on the system dynamics.SLAMLocalizationNavigation by odometry is prone to errors and can only give an estimate of the real pose of the robot. Further the robot moves, the larger is the error in the estimation of the pose. It can be compared to walking with eyes closed and counting our steps until we reach the destination. The further we walk with our eyes closed, the more uncertain we are about our location.For moving short distances odometry is good enough, but when moving longer distances the robot must determine its position relative to an external reference called a landmark. This process is called localization.LandmarksLandmarks, such as lines on the ground or doors in a corridor can be detected and identified by the robot and used for localization.Determining Position from Objects Whose Position Is KnownThe following are two methods by which a robot can determine its position by measuring angles and distances to an object whose position is known.From an Angle and a DistanceSuppose an object is placed at the origin $(x_0,y_0)$ of the coordinate system. The azimuth of the robot $\\theta$ is the angle between the north and the forward direction of the robot. A laser scanner is used to measure the distance s from the object and the angle $\\phi$ between the forward direction of the robot and the object.\\[\\Delta x = s \\sin(\\theta-\\phi) \\newline \\Delta y = s \\cos(\\theta-\\phi)\\]TriangulationTriangulation is based on the principle that from two angles of a triangle and the length of the included side, the lengths of the other sides can be computed.The robot measures the angles $\\alpha$ and $\\beta$ to the object from two positions separated by a distance $c$. This distance can be measured by odometry as the robot moves from one position to another, although this may be less accurate.Global Positioning SystemGPS navigation is based upon orbiting satellites. Each satellite knows its precise position in space and its local time. The position is sent to the satellite by ground stations and the time is measured by a highly accurate atomic clock on the satellite.A GPS receiver must be able to receive data from 4 satellites. For this reason, a large number of satellites (24-32) is needed so that there is always a line-of-sight between any location and at least four satellites. From the time signals sent by a satellite, the distances from the satellites to the receiver can be computed by multiplying the times of travel by the speed of light. These distances and the known locations of the satellites enable the computation of the three-dimensional position of the receiver: latitude, longitude and elevation.Advantages: Accurate Easily available - require only an electronic component which is small and inexpensiveDisadvantages: Positional error is roughly 10m. Not sufficient to perform tasks that need higher accuracy. GPS signals are not strong enough for indoor navigation and are subject to interference in dense urban environments.Probabilistic Localization Consider a robot that is navigating within a known environment for which it has a map. Suppose the map shows 5 doors (dark gray) and 3 areas where there is no door (light gray). The task of the robot is to enter a specific door. By odometry, the robot can determine its current position given a known starting position.MappingA robot can use its capability to detect objects to localize itself, and this information is usually provided by a map. However, building a map requires the robot to localize itself, but at the same time, solving the localization problem requires a map. Hence, we are now presented with a chicken-and-egg problem. This problem is overcome by using SLAM algorithms.Discrete and Continuous MapsA robot requires a non-visual representation of a map that it can store in its memory. There are 2 techniques for storing maps: discrete maps (or grid maps) and continuous maps.Sonar Sensor ModelFrontier AlgorithmMapping using Knowledge of the EnvironmentEven with bad odometry, the robot can construct a better map if it has some information on the structure of the environment. Suppose that the robot tries to construct the plan of a room by following its walls. If the robot knows in advance that the walls are straight and perpendicular to each other, the robot can correctly construct the map.There will also be an error when measuring the lengths of the walls and this can lead to the gap shown in the figure between the first and the last walls. If the robot is mapping a large area, the problem of closing a loop in a map is hard to solve because the robot has only a local view of the environment.Map correction can be improved by using sensor data that can give information on regular features in the environment. The regular features can be lines on the ground, a global orientation, or the detection of features that overlap with other measurements.Large area measurements facilitate identifying overlaps between the local maps that are constructed at each location as the robot moves through the environment. By comparing local maps, the localization can be corrected and the map can be updated accurately.SLAM PerceptionDue to odometry and perception error, there is a mismatch between the current map and the sensor data which should correspond to the known part of the map.How is this mismatch corrected? We assume that the odometry does give a reasonable estimation of the pose of the robot. For each relatively small possible error in the pose, we compute what the perception of the current map would be and compare it with the actual perception computed from the sensor data. The pose that gives the best match is chosen as the actual pose of the robot and the current map is updated accordingly.The similarity matrix is computed, and once we have this result, we correct the pose of the robot and use data from the perception map to update the current map stored in the robot‚Äôs memory.Probabilistic SLAMThe SLAM problem asks if it is possible for a mobile robot to be placed at an unknown location in an unknown environment and for the robot to incrementally build a consistent map of this environment while simultaneously determining its location within this map.Both the trajectory of the platform and the location of all landmarks are estimated on-line without the need for any a priori knowledge of location.SLAM problems possess a continuous and a discrete component: Continuous estimation problem: Deals with location of the objects in the map and the robot‚Äôs own pose variables. Discrete estimation problem: Deals with correspondence, i.e., the relation of the object to previously detected objects. The reasoning is discrete - whether the object is same as a previously detected object or not.Online SLAM: Involves estimating the posterior over the momentary pose along with the map.\\[p(x_t,m | z_{1:t}, u_{1:t})\\]Full SLAM: Involves calculating the posterior over the entire path along with the map, instead of just the current pose.\\[p(x_{1:t},m | z_{1:t}, u_{1:t})\\]The online SLAM problem is the result of integrating out past poses from the full SLAM problem:\\[p\\left(x_t, m \\mid z_{1: t}, u_{1: t}\\right)=\\iint \\cdots \\int p\\left(x_{1: t}, m \\mid z_{1: t}, u_{1: t}\\right) d x_1 d x_2 \\ldots d x_{t-1}\\]Solving the SLAM problem requires that a state transition model and an observation model be defined describing the effect of the control input and observation respectively. The observation model describes the probability of making an observation \\(z_k\\) when the vehicle location and landmark locations are known $$ P(z_k x_k,m) $$. The motion model for the vehicle described in terms of a probability distribution on state transitions $$ P(x_k x_{k-1},u_k) $$. The SLAM algorithm is now implemented in a recursive (sequential) prediction (time-update) correction (measurement-update) form.Time update:\\[\\begin{aligned}&amp; P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right) \\\\&amp; =\\int P\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\mathbf{u}_k\\right) \\times P\\left(\\mathbf{x}_{k-1}, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k-1}, \\mathbf{x}_0\\right) \\mathrm{d} \\mathbf{x}_{k-1}\\end{aligned}\\]Measurement update:\\[\\begin{aligned}&amp; P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right) \\\\&amp; =\\frac{P\\left(\\mathbf{z}_k \\mid \\mathbf{x}_k, \\mathbf{m}\\right) P\\left(\\mathbf{x}_k, \\mathbf{m} \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}, \\mathbf{x}_0\\right)}{P\\left(\\mathbf{z}_k \\mid \\mathbf{Z}_{0: k-1}, \\mathbf{U}_{0: k}\\right)}\\end{aligned}\\]EKF SLAMEKF-SLAM (Cyrill Stachniss)It takes in observed landmarks from the environment and compares them with the known landmarks to find associations and new landmarks. It then uses the association to correct the state and state covariance matricies.Maps in EKF SLAM are feature-based, which means they are composed of point landmarks. It tends to work well the less ambiguous the landmarks are, and hence requires significant engineering of feature detectors, sometimes using artificial beacons as features.EKF SLAM makes a Gaussian noise assumption for robot motion and perception.SLAM with Known CorrespondenceIt addresses only the continuous portion of the SLAM problem. In addition to estimating the robot pose, the algorithm also estimates the coordinates of all landmarks encountered along the way.Let the combined state vector comprising the robot pose and the map be denoted by $y_t$.\\[\\begin{aligned}&amp; y_t=\\left(\\begin{array}{l}x_t \\\\m\\end{array}\\right) \\\\&amp; =\\left(\\begin{array}{llllllllllll}x &amp; y &amp; \\theta &amp; m_{1, x} &amp; m_{1, y} &amp; s_1 &amp; m_{2, x} &amp; m_{2, y} &amp; s_2 &amp; \\ldots &amp; m_{N, x} &amp; m_{N, y} &amp; s_N\\end{array}\\right)^T \\\\&amp;\\end{aligned}\\]where $x,y,\\theta$ denote the robot‚Äôs coordinates at time t.$m_{i,x},m_{i,y}$ are the coordinates of the i-th landmark for i=1,2,‚Ä¶,N and $s_i$ is its signature. The dimension of this state vector is 3N+3, where N is the number of landmarks.Extended Kalman Filter AlgorithmInputs to the above algorithm are - the corrected estimate from the previous iteration, covariance or uncertainties, control input and measurements.Line 2 represents the motion model. Line 4 is calculating the Kalman Gain. Lines 5 and 6 are the update equations.Disadvantages of EKF SLAM: Linearization errors: The EKF SLAM algorithm relies on linearizing the nonlinear motion and measurement models, which can lead to approximation errors. These errors can accumulate over time, leading to inaccurate state estimates. Computational complexity: EKF SLAM is computationally intensive, and its complexity grows with the number of landmarks and the size of the state vector. This can make it difficult to use in real-time applications or on resource-limited devices. Limited observability: EKF SLAM assumes that all landmarks are observable, but in practice, this may not be the case. Landmarks that are not observed cannot be included in the state estimate, which can lead to inaccuracies. Sensitivity to initialization: The EKF SLAM algorithm is sensitive to the initial estimates of the robot‚Äôs pose and the landmark locations. Poor initial estimates can result in the algorithm converging to the wrong solution or getting stuck in a local minimum. Difficulty in handling loop closures: EKF SLAM assumes that the robot‚Äôs path is acyclic, which means it cannot handle loop closures. Loop closures occur when the robot revisits a previously visited location, and they are essential for accurate mapping. Handling loop closures requires more complex algorithms such as GraphSLAM.Loop ClosingIn loop closing, the SLAM algorithm attempts to identify and correct errors in the estimated robot trajectory by detecting and closing loops in the generated map. A loop is created when the robot revisits a previously mapped location after having moved through some other parts of the environment. When a loop is detected, the SLAM algorithm tries to reconcile the previously mapped location with the current robot pose estimate, usually by optimizing the robot‚Äôs trajectory using bundle adjustment or similar techniques.Loop closing is important in SLAM because it enables the creation of consistent and accurate maps of the environment. Without loop closing, SLAM algorithms can suffer from drift, in which errors accumulate over time, causing the estimated robot pose to diverge from the true pose. Loop closing helps to correct such errors and ensure that the generated map is globally consistent. Loop closing means revisiting and recognizing an already mapped area. It reduces uncertainty in robot and landmark estimates. Uncertainties collapse after a loop closure, whether the closure was correct or not. This can be exploited when exploring an environment for the sake of better and more accurate maps. However, wrong loop closures lead to filter divergence.GraphSLAMGraph-based SLAM using Pose Graphs (Cyrill Stachniss)GraphSLAM extracts from the data a set of soft constraints represented by a sparse graph. It obtains the map and the robot path by resolving these constraints into a globally consistent estimate.The constraints are generally nonlinear, but in the process of resolving them they are linearized and transformed into an information matrix.Each edge in the graph corresponds to an event: a motion event generates an edge between two robot poses, and a measurement event creates a link between a pose and a feature in the map.For a linear system, these constraints are equivalent to entries in an information matrix and an information vector of a large system of equations.Adding Confidence Measures Linear Least Squares allows us to include a weighting of each linear constraint. If we know something about how confident a measure is, we can include that in the computation. We weight each constraint by a diagonal matrix where the weights are 1/(variance for each constraint). Highly confident constraints have low variance; 1/variance is large weight, and vice-versa.Cyclic Dependence Features that are observed multiple times, with large time delays in between. This might be the case because the robot goes back and forth through a corridor, or because the world possesses cycles. In either situation, there will exist features that are seen at drastically different time steps." }, { "title": "Third-order Tchebyshev Low-pass Filter", "url": "/posts/chebyshev/", "categories": "Projects, Electronics", "tags": "emfme, microstrip, filter", "date": "2022-12-11 21:30:00 +0530", "snippet": "AbstractGiven the significance of microwave filters in real-world systems and the wide variety of potential applications, the domain of microwave filter design has been overgrowing. Modern computer-aided design (CAD) software based on the insertion loss approach is used for the majority of microwave filter designs. Microwave filter design is still a topic of active research due to ongoing developments in distributed element network synthesis, the use of low-temperature superconductors and other novel materials, and the integration of active devices in filter circuits.This report discusses the design and analysis of a third-order Tchebyshev low-pass microstrip filter. The prototype filter and the lumped element filter with a cut-off of 2.5 GHz and characteristic impedance of 50 Œ© are designed using standard formulas. The design and simulation of the low-pass filter are performed on CST Studio software. After the simulation, the final component is fabricated using Fr 4 substrate with er = 4.3 and thickness h = 0.8 mm and then tested using Vector Network Analyzer. The practical simulation and the measured results are reported and show a good agreement together.IntroductionThe filter is a device used to control the frequency response. Typical frequency responses include low-pass, high-pass, bandpass, and band-reject characteristics. A filter that allows low-frequency signals and rejects high-frequency signals, and transmits low-frequency signals from the input to the output port with minimum attenuation is called a low-pass filter. The amount of attenuation depends on the type of filter. The Low pass Chebyshev filter, made with LC combinations (Inductor and Capacitor), provides a faster transition from passband to stopband and has more ripples in the pass band.The Microstrip line is used for designing the filter as it is easy to fabricate and has low insertion loss. The microstrip line sections are equivalent to Inductors and Capacitors, and to obtain a filter with the desired cutoff frequency, length and width parameters are calculated. After getting the required specifications, the filter structure has been designed with the help of CST Studio. This is software used for designing, analyzing, and optimizing electromagnetic Components and Systems. The dxf files obtained from the design were imported into the Design Pro softwareAnd used for fabrication.Literature ReviewA number of research papers have been published on the design, simulation, and fabrication of low-pass filters using different software and design techniques. This review provides a brief overview of research papers relevant to the project title. Saxena, Shefali &amp; Porwal, Shikha &amp; Soni, Komal &amp; Chhawchharia, Pradeep. (2009). Design and Analysis of Chebyshev Low Pass Filter using Insertion Loss Method. This paper discusses the use of the insertion loss method for the design and analysis of a 5th Chebyshev low-pass filter with a cutoff frequency of 2.5 GHz, characteristic impedance of 50 Œ©, and passband ripple of 0.01 dB. PUFF software is used to simulate the lumped element filter, and the results are examined. The lumped element filter is then converted into a planar filter, and Micro-stripes software is used for its simulation and analysis. The microstrip is fabricated using GML100 substrate with er =3.2 and thickness h = 0.762 mm. The optimized low pass filter with end correction shows a measured cutoff frequency of 2.68 GHz and an impedance bandwidth of 2.82 GHz. Due to lead inductance and stray capacitance, the cutoff frequency of distributed and lumped elements exhibits a 700 MHz variance during analysis. R. Chaurasia, Mukesh &amp; Raval, Falguni. (2016). Third Order Low Pass Chebyshev Filter Characteristic Realization Using the Ansoft Designer.. Inventi Impact - Antennas &amp; Propagation. 2016. 1-4. This paper reports the design of a third-order low-pass Chebyshev filter with a cutoff frequency of 1 GHz and having a 1dB passband ripple. The lumped parameter filter is simulated in ANSOFT DESIGNER. The stepped impedance method is used to obtain the Microstrip filter, and the microstrip design and analysis are carried out using Fr 4 epoxy as substrate in ANSOFT HFSS. An insertion loss of -3.14 dB is obtained at 1.09 GHz for lumped network low pass filter design, and an insertion loss of -3.19 dB at 1.05 GHz for microstrip low pass filter design. Sheetal (2015). Stepped Impedance Microstrip Low-Pass Filter Implementation for S-band Application. This paper discusses the design of an S-band low pass filter using microstrip technology operating at 2.5 GHz for permittivity 4.1 with a substrate of thickness 1.6 mm and order n=6. The filter is designed using AWR microwave office simulation software, and the design and optimization of the lowpass filter is done using microstrip lines. The lowpass filter is then fabricated using photolithographic process and tested using vector network analyzer. The cutoff frequency obtained is lower than the desired value of 2.5 GHz, which may be due to imperfect fabrication and connection of SMA connector.MethodologyIn order to create practical filters, the lumped component filters must be transformed into distributed elements. In the design of microwave filters, we are mainly faced with two problems. The first is that only lumped parts function correctly within the constrained frequency range. The second is that while operating at microwave frequencies, there is no insignificant space between components. In order to turn lumped parts into transmission line sections, Richard‚Äôs transforms are utilized to solve the first issue. Using transmission elements to separate filter elements, Kuroda‚Äôs identities can be utilized to solve the second issue.The design of low-pass filters involves two steps: The first step is to choose the appropriate type of response and order of the filter based on the desired specifications. The lowpass prototype filters‚Äô element values are normalized to have a source impedance of gO = 1 and a cutoff frequency of wc = 1. They are then transformed to obtain the L-C elements for the required cutoff frequency and the source impedance. The next step is to find an appropriate microstrip realization that approximates the lumped element filter. The filter is then fabricated for a normalized cutoff frequency of wc on an Fr4 substrate with permittivity er and thickness h mm.The filter design steps are as follows:1) Filter specifications:Passband: lowpassRelative Dielectric Constant $\\epsilon_r$ = 4.3Height of substrate $h$ = 0.8 mmCutoff frequency = 2.5 GHzThe filter impedance $Z_o$ = 50 ‚Ñ¶The highest line impedance $Z_H$ = $Z_{OL}$ = 120 $\\Omega$The lowest line impedance $Z_L$ = $Z_{OC}$ = 20 $\\Omega$2) Order of the filter:For normalized LPF design with source impedance $g_0 = 1$ and cut-off frequency $w_c = 1$, the elemental values for equal-ripple low-pass filter prototypes are tabulated in table 1 for N=1 to N=10.Table 1: Element values for Equal-Ripple low-pass filter prototypes ( $g_0=1$ , $w_c = 1$, N=1 to N=10, 3.0 dB ripple)Figure 1: Attenuation characteristics for various N values versus normalized frequency.Since the order of the filter needed is N = 3, we obtain the following filter element coefficients - $g_0 = R_s$ $g_1 = L_1$ $g_2=C_2$ $g_3 = L_3$ $g_4 = R_L$ 1.000 3.3487 0.7117 3.3487 1.000 The actual values of the inductance Li and capacitance Ci can be calculated from the equations:\\[L_i = Z_0 g_i/2\\pi f_c \\newline C_i = g_i/Z_02\\pi f_c\\]The electrical length of the inductor\\[\\beta l = LZ_0/Z_H\\]The electrical length of the capacitor\\[\\beta l = CZ_L/Z_0\\]The L and C values are the normalized element values of the low pass prototype.The width of the high impedance line assuming $w/h \\leq 2$ and $w/h \\geq 2$ can be calculated using the formulas -\\[\\frac{W}{d}= \\begin{cases}\\frac{8 e^A}{e^{2 A}-2} &amp; \\text { for } W / d&lt;2 \\\\ \\frac{2}{\\pi}\\left[B-1-\\ln (2 B-1)+\\frac{\\epsilon_r-1}{2 \\epsilon_r}\\left\\{\\ln (B-1)+0.39-\\frac{0.61}{\\epsilon_r}\\right\\}\\right] &amp; \\text { for } W / d&gt;2\\end{cases}\\]where\\(\\begin{aligned}&amp; A=\\frac{Z_0}{60} \\sqrt{\\frac{\\epsilon_r+1}{2}}+\\frac{\\epsilon_r-1}{\\epsilon_r+1}\\left(0.23+\\frac{0.11}{\\epsilon_r}\\right) \\\\&amp; B=\\frac{377 \\pi}{2 Z_0 \\sqrt{\\epsilon_r}} . \\\\&amp; \\sqrt{\\varepsilon_e} k_0 l=\\beta l=\\frac{L R_0}{Z_n}=\\frac{C z_L}{R_0}\\end{aligned}\\)where $k_0=2 \\frac{\\pi f}{c}, c=3 \\times 10^8$ and\\(\\varepsilon_e=\\frac{\\varepsilon_R+1}{2}+\\frac{\\varepsilon_R-1}{2} \\frac{1}{\\sqrt{1+\\frac{12 d}{w}}}\\)3) Dimensions of the filter:The following are the dimensions of the filter, calculated using the formulas mentioned above - l0 5 mm W0 1.5559 mm l1 9.35 mm W1 0.5 mm l2 3.21235 mm W2 5.62716 mm l3 9.35 mm W3 0.5 mm l4 5 mm W4 1.5559 mm The values of $L_0$ and $L_4$ were chosen to be 5 mm so that the total length of the feedline, which is equal to 31.91235 could be close to $\\lambda / 4$, which is equal to 30 mm for 2.5 GHz frequency.4) Design of the filter:CST Studio software has been used for the design and simulation of the low-pass filter. The following picture shows the schematic -Figure 2: Schematic of microstrip filter with dimensions.5) Fabrication:The dxf files obtained from designing the low-pass filter prototype on CST Studio were used for the fabrication process. The dxf files were imported into the Design Pro software which is used for generating milling outlines, rubout, and contour routing. The Design View software is then used to fabricate boards with the desired design on a plate comprised of the substrate material and a copper layer.Figure 3: MITS PCB Machine6) Analysis:After obtaining the microstrip of the desired specifications, ports are soldered on each side, and the response of the fabricated low-pass filter is measured on a Vector Network Analyzer (VNA).Design and Simulation1) DesignThe software used for the design and simulation of the filter was CST (Computer Simulation Technology) Studio which is an electromagnetic wave analysis software. The software offers multiple templates across the EM spectrum, the ‚ÄòMicrowave and RF/Optical‚Äô template was used for designing the planar microstrip line filter in the frequency domain.Since the goal was to study the losses/ reflection, E-filed, H-field, Power loss were the parameters chosen for monitoring. Since the cut-off frequency was 2.5 GHz, the frequency range selected for modeling and analysis was 1.5-3GHz. The design procedure was as follows: First the bottom layer of the filter, that is, the ground plane was inserted [material from library: copper (lossy)] and dimensions were specified according to the calculations shown in the Methodology section. This was followed by the substrate layer made of FR4 substrate with ∆êr = 4.3. These two were added as separate elements. Next, the feedline layer was modeled, using copper(lossy). The 4 feedline parts were added in accordance to dimensions specified in the Methodology section. During the simulation, they were added as different elements. While fabrication, it was learnt that a better design strategy is to model the entire feedline as a single component. To simulate and study the s-parameters, port definition was required. The ports were created by using the face pick tool and the port extension coefficient calculator under macros solver. After this the simulation was run.The following is the model designed in CST studio:Figure 5: Microstrip filter modelFigure 6: Microstrip filter schematic with dimensions2) Simulation and s-parameter analysisOnce the simulation is run, the circuit can be analyzed through s-parameters (scattering parameters). S-parameters are network analysis parameters used for microwave circuits since it is not feasible to measure z-, abcd, h- parameters for RF circuits. Since the circuit under consideration is a low pass filter, which is a 2-port device, its s-parameters can be represented through a 2x2 reciprocal matrix.Figure 7: s-parameter results from CST StudioThe S11 parameter represents the return loss/ reflection coefficient of the transmission line. The ideal value of this parameter for a good transmission should be -15 to -20dB. The designed filter has low reflection loss for lower frequencies (below the cut off frequency 2.5GHz, the S11 value is below -10dB, which implies that at low frequencies, most of the power is absorbed by the load and there are very less reflections). At higher frequencies, especially beyond point 1 on the graph, the reflection loss is high, that is, the signal for higher frequencies is rejected by the filter, thereby showing the characteristics of a low pass filter.Figure 8:¬† S11 parameterThe S21 parameter represents the insertion loss, that is, the power loss in transmission from port 1 to port 2. The loss is close to 0dB for lower frequencies and cut-off occurs at 2.5 Ghz beyond which the losses are higher. Thus the designed filter shows low pass response.Figure 9:¬† S21 parameterMeasurement ResultsFigure 10:¬† s-parameter analysis obtained from VNAThe S11 and S21 Parameters crossover point was obtained nearly at -3db Point.In the results obtained from the VNA, the S11 parameter is indicated by the Yellow Line. The reflection/return loss value is well below -10dB for frequencies lower than 2.5 GHz and increases for higher frequencies. This implies that the cut-off for the filter is occurring close to 2.5GHz. This is in sync with the simulated values for reflection loss. The fabricated filter has more ripples in the passband as compared to the simulated one. These results are also reflected in the following table:The S21 Parameter (Indicated by Green Line) represents insertion loss, for lower frequencies loss is very low, and cut-off occurs nearly at 2.5 GHz beyond which the losses are higher. But the transition from passband to stopband is slower compared to simulation results.ApplicationsChebyshev filters (type I) exhibit a fast transition between pass-band and stop-band but this is at the expense of an in-band ripple. The Chebyshev filter is still essential to many RF applications even though the in-band ripple may render some applications unfeasible.¬† In order to significantly reduce undesirable out-of-band spurious transmissions like overtones or intermodulation, the steep roll-off is advantageously used. The optimal attenuation of undesirable signals can be obtained due to the quick transition between the pass-band and the stop-band.Chebyshev filters (type II) do not have a ripple in the passband, that is, the passband is flat which makes them very useful for low frequency and DC measurement systems, such as bridge sensors, loadcells, etc.Summary and ConclusionIn this report, we have designed a third-order Tchebyshev lowpass microstrip filter with a cut-off frequency of 2.5 GHz using CST Studio. All the calculations were performed using the mentioned standard formulas. After the verification of the design, the fabrication of the filter was done. The filter was then tested using the Vector Network Analyzer. From the return loss values, we observe that the cut-off frequency occurs at around 2.5 GHz, which is close to our desired result. The same can also be inferred from the insertion loss values. Thus, the results obtained for the designed filter match with the simulations in CST Studio." }, { "title": "Underwater Depth Estimation and Localization", "url": "/posts/underwater-depth-estimation/", "categories": "Projects, Robotics", "tags": "perception, underwater, sauvc, depth, camera", "date": "2022-12-11 21:30:00 +0530", "snippet": "AbstractSophisticated robots operating in an underwater environment require vision to perform different tasks. This project involves developing a reliable vision system by employing a depth camera, rather than a conventional binocular-stereo camera, for underwater depth estimation and localization. The project‚Äôs initial phase, until midsemester, consisted of testing the depth camera‚Äôs ability to estimate depth in the air. This report covers the second phase of the project, which involves underwater experimentation with objects for depth measurements.DEPTH ESTIMATION AND LOCALIZATIONCamera CalibrationCamera calibration is the process of estimating the parameters of a lens and camera image sensor. These calibrated camera parameters can be used to rectify the image by correcting lens distortion, estimating the depth of objects, and localization of the camera in the environment.The camera parameters, given by a camera matrix, includes the intrinsic, extrinsic and distortion parameters of the camera. The 3D world coordinates of the objects and their corresponding 2D image points are required to compute the camera matrix.The following are the components of the camera calibration matrix: Intrinsic parameters - The camera internal parameters such as the focal length, optical center, and the skew coefficient. Extrinsic parameters - The pose parameters of the camera given by a rotation and a translation vector. Distortion parameters - Radial distortion - When light rays bend differently near a lens‚Äôs edges and in its optical center, the result is radial distortion. The radial distortion coefficients model this type of distortion. Tangential distortion - Occurs when the lens and the image plane are not parallel. The tangential distortion coefficients model this type of distortion. Radial distortion:\\[\\begin{aligned}&amp; x_{\\text {corrected }}=x\\left(1+k_1 r^2+k_2 r^4+k_3 r^6\\right) \\\\&amp; y_{\\text {corrected }}=y\\left(1+k_1 r^2+k_2 r^4+k_3 r^6\\right)\\end{aligned}\\]Tangential distortion:\\[\\begin{aligned}&amp; x_{\\text {corrected }}=x+\\left[2 p_1 x y+p_2\\left(r^2+2 x^2\\right)\\right] \\\\&amp; y_{\\text {corrected }}=y+\\left[p_1\\left(r^2+2 y^2\\right)+2 p_2 x y\\right]\\end{aligned}\\]where,$x$, $y$ = undistorted pixel locations$r^2=x^2+y^2$$k_1,k_2,k_3 \\space -$ radial distortion coefficients$p_1,p_2 \\space -$ tangential distortion coefficientsThere exist quite a few methods for camera calibration, the most popular one being the Zhang Zhengyou calibration method.Zhang‚Äôs method:The Zhang Zhengyou technique is a calibration method that uses a checkerboard pattern. It runs a corner detector to find the points of interest and their positions on the checkerboard. The world coordinate system is set to the corner of the checkerboard, and all points are assumed to lie on the XY-plane.The method requires test patterns for camera calibration. The following shows the set of all test images used for calibration -The corners of the checkerboard are found using OpenCV, as shown below.Once the calibration is done, we can take an image and undistort it. The camera matrix and the distortion coefficients can then be stored. The following are the results obtained after running the code -Camera Matrix:\\[\\begin{bmatrix} 1.07443280e+03 &amp; 0.00000000 \\mathrm{e}+00 &amp; \\quad 4.32669818 \\mathrm{e}+02 \\\\ 0.00000000 \\mathrm{e}+00 &amp; \\quad 1.01862189 \\mathrm{e}+03 &amp; \\quad 2.63179677 \\mathrm{e}+02\\\\ 0.00000000 \\mathrm{e}+00 &amp; \\quad 0.00000000 \\mathrm{e}+00 &amp; \\quad 1.00000000 \\mathrm{e}+00\\\\\\end{bmatrix}\\]Distortion Parameters:\\[\\begin{bmatrix} -0.09265683 &amp; 0.80628656 &amp; -0.00354457 &amp; -0.00947235 &amp; -1.59645404\\end{bmatrix}\\]Total Error: $0.06509290538498035$Depth camera:The Intel Realsense D415 Depth camera uses stereo vision to calculate depth, and consists of a pair of depth sensors, RGB sensor, and an infrared projector. Image sensors - The set of image sensors enable capturing of disparity between images up to 1280 x 720 resolution. RGB sensor - Dedicated color image signal processor for image adjustments and scaling color data. Infrared sensor - Active infrared projector to illuminate objects to enhance the depth data.The camera data rectified in its Vision Processor D4 hardware component is streamed in most modes. The rectified data is then sent to the computer via the USB cable and made available for us to view. The rectified and unrectified images can be obtained by using the relevant IR stream modes, as follows -1) Y8 IR mode - provides calibrated and rectified images.2) Y16 IR mode - provides unrectified images (closest to what you would get from a true rawstream without calibration)The Intel Realsense SDK, however, does not allow simultaneous RGB stream of the left and right cameras. Thus the process of camera calibration in the depth camera is done by the on-board chip, and the calibrated data can be obtained by looking into the Y8 IR mode stream. The same can also be obtained using OpenCV and the provided librealsense API, as shown below.Depth EstimationUsing calibrated IR data:The calibrated IR image data from the depth camera can be used to perform depth estimation. The StereoSGBM class provided by OpenCV is used for computing the stereo correspondence using the Sum of Squared Differences (SSD) block-matching algorithm, and for disparity calculation by matching the blocks in left and right images as shown below.The image below shows the left and right IR images that are used as input to the stereo-depth algorithm.The following is the depth map, plotted on matplotlib, obtained from the stereo-depth algorithm. The map resembles the image on the right side to some extent, but needs to be improved by playing around with the values of the number of disparities and block size, which are given as input to the StereoSGBM class.Using default calibration data:The entire process of camera calibration, rectification, block matching, disparity calculation, and depth estimation is carried out by the On-board Chip of the Intel Realsense depth camera by default. Thus, the simpler way of performing depth estimation is by listening to the relevant streams and using them to find the depth. The depth stream, Z16 mode, and the color stream, BGR8 mode, are used for this purpose.The image on the left below shows the color frame and the corresponding distance at the point of placement of the cursor, while the image on the right shows the depth frame.OAK-D CameraThe OAK-D depth camera by Luxonis has three on-board cameras which can implement stereo and RGB vision used for depth and AI processing. The camera has a baseline length of 7.5cm, which is the distance between the left and the right stereo cameras. Stereo Cameras - The set of image sensors enable capturing of disparity between images up to 1280 x 800 (1MP) resolution. Color Camera - Dedicated color image signal processor that provides a resolution of up to 4032 x 3040 (12MP).The camera is capable of performing stereo depth perception with filtering, post-processing, RGB-depth alignment, and high configurability.UNDERWATER TESTINGThe depth camera calibrated in the air cannot be used for performing underwater experiments due to differences in disparities caused by the refraction of light. As a result, the depth camera needs to be calibrated underwater and then used for depth estimation.SetupThe setup to perform underwater depth measurement involves placing the OAK-D depth camera in a transparent container and holding it inside water partially submerged, as shown in the images below. The entire setup and all the experiments were performed in the swimming pool.Camera CalibrationThe camera is calibrated using a charuco board, in a process very similar to calibrating using the Zhang Zhengyou technique. The charuco board is printed onto a flat surface, and 13 different images are captured in different orientations.The following shows an image of the charuco board having a square size of 2.45 cm -The following shows the set of all test images (left + right + rgb) used for calibration. The numbers that appear on some images in the middle show the timer while capturing the image -Once the camera calibration is complete, we obtain the distortion coefficients, intrinsic parameters, extrinsic parameters, and the stereorectification data of the camera.\"extrinsics\": { \"rotationMatrix\": [ [ 0.9999324679374695, -0.008666034787893295, 0.007746713235974312 ], [ 0.008631718344986439, 0.9999528527259827, 0.0044522895477712154 ], [ -0.00778493145480752, -0.0043851216323673725, 0.9999600648880005 ] ], \"specTranslation\": { \"x\": -7.5, \"y\": 0.0, \"z\": 0.0 }, \"toCameraSocket\": 2, \"translation\": { \"x\": -7.581012725830078, \"y\": -0.0006467599887400866, \"z\": 0.040638044476509094 } },\"intrinsicMatrix\": [ [ 799.7395629882813, 0.0, 662.93359375 ], [ 0.0, 799.383056640625, 382.2420654296875 ], [ 0.0, 0.0, 1.0 ] ],\"rectifiedRotationLeft\": [ [ 0.9999605417251587, -0.008557096123695374, 0.002386769512668252 ], [ 0.00855177454650402, 0.9999609589576721, 0.0022310472559183836 ], [ -0.002405767561867833, -0.002210548147559166, 0.9999946355819702 ] ], \"rectifiedRotationRight\": [ [ 0.9999856352806091, 8.531191269867122e-05, -0.005360426381230354 ], [ -9.721628157421947e-05, 0.9999975562095642, -0.0022205670829862356 ], [ 0.005360223352909088, 0.0022210562601685524, 0.9999831914901733 ] ],\"distortionCoeff\": [ 9.491045951843262, -102.06877136230469, 0.0008228731458075345, 0.001999291591346264, 401.19476318359375, 9.264853477478027, -100.42776489257813, 394.56182861328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ],Depth EstimationThe StereoDepth node, provided by the OAK-D API, is used to calculate the disparity and depth from the stereo camera pair.The generation of the depth map from the StereoDepth node can be visualized using the following images, which are taken in air -When the object is placed closer to the camera -When the object is placed farther away from the camera -The same experiments are performed underwater, but now using the newly calibrated camera matrix. We obtain the depth values and the disparity map as follows -From the above images, we see that as the bottle is placed farther away from the depth camera setup, the value of the depth increases, and the disparity map changes accordingly. From the experiments performed above, we find that the depth camera can measure a depth of up to 1m (~3ft) underwater.Camera LocalizationAs done earlier, the position of the depth camera can be estimated using the extrinsic parameters of the camera.The following are the rotation and translation vectors obtained after calibration -\"rotationMatrix\": [ [ 0.9999324679374695, -0.008666034787893295, 0.007746713235974312 ], [ 0.008631718344986439, 0.9999528527259827, 0.0044522895477712154 ], [ -0.00778493145480752, -0.0043851216323673725, 0.9999600648880005 ] ], \"translation\": { \"x\": -7.581012725830078, \"y\": -0.0006467599887400866, \"z\": 0.040638044476509094 }ConclusionThe work done so far includes camera calibration, block matching, disparity calculation, and depth estimation in an underwater environment. While performing the experiments, the depth camera could measure the depth of obstacles only up to a distance of 1m. This can be further improved by building a better canister for housing the depth camera and by improving the lighting conditions using subsea LED lights.One major issue that I faced while working with the underwater camera setup was the problem of fogging. Due to the difference in temperatures at the back of the camera, which heats up, and the front of the camera, which is cold, fog begins to form on the left and right stereo cameras. This issue can also be solved by building a properly insulated canister." }, { "title": "The Singapore AUV Challenge!", "url": "/posts/sauvc-intro/", "categories": "Projects, Robotics", "tags": "", "date": "2022-09-01 21:30:00 +0530", "snippet": "AboutThe Singapore Autonomous Underwater Vehicle Challenge (SAUVC) is an annual international competition that aims to promote the development of autonomous underwater vehicles (AUVs) and encourage the exploration of innovative technologies in the field. The competition is open to undergraduate and graduate students from around the world, and provides a platform for teams to showcase their skills and contribute to the advancement of AUV technology.Participating teams are required to design and build an AUV that can successfully complete a series of tasks, including object identification and retrieval, navigation through an obstacle course, and acoustic localization. The tasks are designed to test the capabilities and efficiency of the AUVs, as well as the team‚Äôs understanding of the underlying technologies and principles.One of the main tasks of the SAUVC is object identification and retrieval. Teams are required to locate and retrieve a predetermined object from the competition area using their AUV. The AUV must be able to locate the object using its sensors and then retrieve it using a manipulator arm or other mechanism. This task requires the AUV to have precise navigation capabilities and the ability to identify and distinguish objects based on their characteristics. The object identification and retrieval task is particularly important as it simulates real-world scenarios in which AUVs may be used for search and rescue operations or to retrieve objects from the ocean floor.Another task of the SAUVC is navigation through an obstacle course. Teams are required to program their AUV to navigate through a series of gates and markers placed in the competition area. The AUV must be able to avoid obstacles and accurately follow the predetermined course. This task requires the AUV to have robust and reliable navigation capabilities, as well as the ability to adjust its course in real-time to avoid obstacles. The obstacle course task is designed to test the AUV‚Äôs ability to navigate complex environments and to adapt to changing conditions.Overall, the SAUVC is a valuable opportunity for teams to showcase their skills and contribute to the advancement of AUV technology. The competition provides a platform for students to gain practical experience in the design and development of AUVs, and to engage with experts in the field. The SAUVC is an important event for the AUV community, and continues to be a driving force in the development and promotion of this exciting and innovative technology." }, { "title": "AUV Simulation", "url": "/posts/sauvc-sim/", "categories": "Projects, Robotics", "tags": "simulation, ros, cad, gazebo, python", "date": "2022-08-31 21:30:00 +0530", "snippet": "Simulation is an important aspect of building an Autonomous Underwater Vehicle (AUV). It allows the team to test and validate the robot‚Äôs design and performance in a controlled environment before deploying it in the actual competition. Simulations also enable us to identify and troubleshoot potential issues before they occur in the field, which can save valuable time and resources.It helps us test various aspects of the robot‚Äôs design, such as its hydrodynamics, control algorithms, and sensor systems. By simulating the robot‚Äôs movement and behavior in different scenarios and environments, we can optimize its design for maximum performance.Additionally, simulation can be used to test the robot‚Äôs autonomy and decision-making capabilities. The simulated robot can be programmed to respond to different situations and tasks, such as obstacle avoidance, path planning, and target tracking. This can help the team to build a more robust and capable robot, which can improve its chances of success in the competition.All the simulations were carried out on Gazebo using Robot Operating System (ROS). The following were the components of the simulation/testing process:Environment - The initial environment consisted of an open ocean with objects such as a shipwreck. This environment was not enough to satisfy the simulations needed, so a pool environment called sauvc_pool which contained accurate information about the size of the pool as well as the obstacles and tasks inside the pool was implemented.Bluerov2(ffg) - The bot we chose to simulate was the bluerov2 from Bluerobotics, as it consisted of a 6-thruster design, and its thrusters would give us an accurate idea of our own thrusters.Bluerov in pool - The bot was integrated into the sauvc_pool environment, where it could freely interact with the tasks and obstacles. Since it was operated in ROV (Remotely Operated Vehicle) mode and not AUV mode, the bot had to be controlled with keyboard input.Simulation Video:6 thruster AUV in pool - Bluerov2 has 6 thrusters, but in our tests, we need to simulate the results for an AUV having anywhere from 1 to 6 thrusters and testing its DOF‚Äôs, thrust values, etc. For this, the bluerov2‚Äôs URDF files were tweaked to include/exclude thrusters as well as specify the directions they were pointed in.Simulation Video:ROV in pool After this, we imported the STL file of our own SAUVC entry into Gazebo for simulation in place of the bluerov2. The file followed the same format as that of the bluerov2 so that the thruster locations, thrust values, as well as ROV operation is possible for our bot the same way as the bluerov2." }, { "title": "Object Detection", "url": "/posts/sauvc-object-detection/", "categories": "Projects, Robotics", "tags": "dl, cnn, computer vision, python, yolo", "date": "2022-08-31 21:30:00 +0530", "snippet": "Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class in digital images and videos. One popular algorithm for object detection is YOLO (You Only Look Once). YOLO uses a single neural network to predict bounding boxes and class probabilities directly from full images in one evaluation. It‚Äôs become popular in real-time object detection because of its speed, as well as its ability to detect a large number of object classes.Using YOLO for object detection in an underwater environment, however, is a challenging task. The lighting conditions, turbidity, and low visibility can affect the performance of the algorithm. In addition, underwater images may have different characteristics than images taken in an above-water environment, such as color distortion, refraction, and reflection. To overcome these challenges, image pre-processing and training the model on a dataset of underwater images is crucial. This dataset should include a variety of underwater scenarios, lighting conditions, and different types of objects.In the context of the Singapore Autonomous Underwater Vehicle Challenge (SAUVC), YOLO can be used to detect and classify various objects in the underwater environment. This can help to improve the navigation and control of the autonomous underwater vehicle (AUV). For example, YOLO can be used to detect obstacles and map the environment, which can be used to plan a safe and efficient path for the AUV to follow. YOLO can also be used to identify targets of interest, such as underwater structures or objects of a specific class. Furthermore, object detection information can be used to trigger specific actions or behaviors of the AUV, such as obstacle avoidance, and capturing images or samples of a specific object.In the SAUVC competition, the vehicle is required to navigate in an unknown and unstructured environment. The object detection algorithm can be used for mission planning and localization and can help the vehicle perform more complex tasks, such as search and recovery." }, { "title": "Acoustic Localization", "url": "/posts/sauvc-acoustic/", "categories": "Projects, Robotics", "tags": "acoustics, localization", "date": "2022-08-31 21:30:00 +0530", "snippet": "Localization is required to complete the target acquisition and re-acquisition task in the challenge. The pingers fitted to the drums act as an acoustic source. A 2/3/4 acoustic sensor setup can be used for localization. There are different localization techniques, such as received signal strength indication (RSSI), angle of arrival (AOA), time difference of arrival (TDOA), and time of arrival (TOA), of which different methods are used in various applications.The angle of arrival estimates is found by using TDOA, and the far-field approach is followed for simplicity.Time difference of arrival estimation: We get a hyperbola with two possible locations with two hydrophones, and with four hydrophones, it is easier to find the location of the source. The time difference between the two microphones is computed and used in estimating the position of the acoustic source.Finding the time difference:The following methods are widely used, with each having its pros and cons: Normalized frequency-based TDOA: This method can be used when the source is of a single frequency(and the value is known). We convert the obtained signals into the frequency domain and use the phase difference to find the time difference, and this method is prone to noise. GCC-PHAT: This is based on the GCC algorithm. In order to compute the TDOA between the reference channel and any other channel for any given segment, it is usual to estimate it as the delay that causes the cross-correlation between the two signal segments to be maximum. This method is less prone to noise but is computationally expensive.Hilbert transformation:This method works well for narrow-band signals and can be simplified using FFT to reduce computational costs and gives good results when compared to cross-correlation methods." }, { "title": "Thruster Control of AUV Using LQR", "url": "/posts/lqr-control/", "categories": "Projects, Robotics", "tags": "control, lqr, pid, simulink, matlab", "date": "2022-08-11 21:30:00 +0530", "snippet": "Abstract To make two different control system models in Simulink on PID and LQR controllers respectively and to get the results for positional and velocity parameters of the ROV based on the desired reference inputs. To tune parameters related to both the controllers to achieve better results and finally test the model physically underwater.PID ControllerLiterature ReviewThe basic mathematical model of an ROV is given by:\\[\\dot{\\eta}=J(\\eta)v \\newline M\\dot{v}+C(v)v + D(v)v + g(\\eta)=\\tau\\] where M = Mass matrix (with added mass effect taken into account) C = Coriolis force matrix D = Damping forces matrix g = Restoring forces matrix $\\tau$ = Forces &amp; moments matrix1. Added Mass Effect:The added mass of an object is the effect in which some mass of fluid surrounding the object under observation is accelerated/decelerated along with it.2. Coriolis Effect:The Coriolis force is a fictitious force that comes into play whenever we are trying to explain the forces on an object with respect to a rotating frame.3. Ziegler-Nichols Method for tuning PIDs:This method can be used to tune our PID both in the case where we have a working model for our plant or even when we don‚Äôt. The first step to this method is measuring two parameters: KU which is the gain at which the system becomes marginally stable and TU which is the period of oscillation at marginal system response. These values are found by taking KI and KD values to be zero for that input and changing KP until marginal stability is achieved.After these parameters are evaluated controller gains can simply be calculated from the below table:4. Controller Models:The two controller models that we used are: Linear Model Non-linear ModelLinear ModelThe schematic of the simulink model created using the linear PID control looks like this:It consists of different functionalities in each block of the design:The PID controller uses an error signal, called the tracking error, generated from the difference between the desired position and the current position of the rover.\\[e = \\eta_d - \\eta\\]This error signal, in the world frame, is converted to the error signal in body frame ùëíb using the following equation:\\[e^b = J^T(\\eta)e\\]where the transformation matrix from the vehicle body frame to the world reference frame using Euler angle transformation is given by:\\[J_\\theta(\\eta) = \\begin{bmatrix} R^n_b(\\theta) &amp; 0_{3x3} \\\\ 0_{3x3}&amp; T_\\theta(\\theta) \\end{bmatrix}\\]Using the error signal in the body frame ùëíb, the torque generated by the PID can be calculated using the equation:\\[\\tau_{P I D}=K_P e^b(t)+K_I \\int_0^t e^b\\left(t^{\\prime}\\right) d t^{\\prime}+K_D \\frac{d e^b(t)}{d t}\\]In order to generalise the required control forces, control allocation calculates the control input signal u to apply to the thrusters. The control forces due to the control inputs applied to the thrusters can be expressed as:\\[\\tau = T(\\alpha)F = T(\\alpha)Ku\\]As a result, the control input vector can be derived as:\\[u = K^{-1}T^{-1}\\tau\\]For the linear model, the following values of KP, KI, and KD have been obtained using the Ziegler-Nichols method: 6-DoF PID Surge Sway Heave Roll Pitch Yaw KP 3 3 3 4 4 2 KI 0.2 0.2 0.2 0.3 0.3 0.1 KD 2.5 2.5 0.5 0.5 1 0.5 Using the control input vector, the thruster system generates the control forces in 6 DoFs with the help of the above mentioned equation $\\tau = T(\\alpha)F = T(\\alpha)Ku$Since the 8 thrusters of the ROV produce a maximum thrust of 40N at operating voltage of 16V, the thrust coefficients are approximated to 40. Thus the thrust coefficient matrix ùêæ is taken as$K = diag[40,40, 40,40,40,40,40,40]$After obtaining the torque vector, the kinetics is used to determine the acceleration in the body frame for the given forces using the state equation:\\[M\\dot{v}+C(v)v + D(v)v + g(\\eta)=\\tau\\]The kinematics block is then used to define the vehicle velocity in the world frame ùë£n. The position of the vehicle is then determined in the integrator and, using inverse kinematics, is converted to the body frame before being supplied back into the controller.Non-Linear ModelBecause of disturbances underwater like current speed, there will be non-linearities introduced into the system. This makes the linear-PID controller model inappropriate to use as there will be a lot of deviations from the desired o/p and noise in the system. So, we need to include the system dynamics to get the control input to the thrusters.In the nonlinear model-based PID control system design, the dynamic model of the ROV is utilized to produce a 6-DoF predictive force and the model-based PID is used to provide a corrective force in 6 DoFs to adjust the error in the model. This is advantageous in that the model error and non-linearities tend to be smaller than the dynamics themselves.In the predictive force generation, a virtual reference trajectory strategy is introduced for the design of trajectory tracking. With the use of a scalar measure of tracking in Fossen (Fossen, 1994), a virtual reference $x_r$ can be defined that satisfies:\\[\\dot{x_r}=\\dot{x_d}+\\lambda e^b\\]where $\\lambda$ &gt; 0 is the control bandwidth that describes the amount of tracking error to the overall tracking performance, and $e^b$ is the tracking error in the body frame.Since the velocity $v$ is the time derivative of the position (i.e. $v=\\dot{\\eta}$), for a defined virtual reference position $\\eta_r$, the following is satisfied:\\[v_r=v_d+\\lambda e^b\\]So, $\\lambda$ is used to tune the 6-DoF predictive force.This is shown in the following block:Where ‚ÄòA‚Äô is the new controller output.The PID controller gains(Kp, Ki and Kd) for the non-linear model are found out by Ziegler Nichols method and were found out to be as follows:Finally, the control law for the nonlinear model-based PID controller is computed given by:\\[\\begin{aligned}\\tau= &amp; M\\left(v_d+\\lambda\\left(v_d-v\\right)\\right)+C_{R B}(v) v+C_A\\left(v_w\\right) v_w+D\\left(v_w\\right)\\left(v_d+\\lambda e^b-v_c\\right)+g(\\eta) \\\\&amp; +K_P e^b(t)+K_I \\int_0^t e^b\\left(t^{\\prime}\\right) d t^{\\prime}+K_D \\frac{d e^b(t)}{d t}\\end{aligned}\\]The final model for this system is shown below:Here, we took desired position input as $[1;1;2;0;0;0]$.PID ImplementationWe implemented the models for both the controllers above and got the following results when we give a desired positional input:Linear Controller ModelUsing the above method, we have obtained the following results for a desired position input: $\\eta_d=[3;4;1;1.57;0;0]$.The output of the position and orientation control is obtained as follows:Position X:Desired output: 3 mObtained result:The steady state response final value = 3.Position Y:Desired output: 4 mObtained result:The steady state response final value = 4.Position Z:Desired output: 1 mObtained result:The steady state response final value = 1.Orientation $\\phi$:Desired output: 1.57 radiansObtained result:The steady state response final value = 1.57.Orientation $\\theta$:Desired output: 0 radiansObtained result:The steady state response final value = 0.Orientation $\\psi$:Desired output: 0 radiansObtained result:The steady state response final value = 3.The control inputs to the thrusters for the same are represented in the plots below:As it is evident from the plots, each thruster is instructed to provide the required thrust every instant for a finite period of time after which the control input eventually settles down to a final value.The model of the rover under study somewhat looks like this:Here, the thrusters 1,2,3,4 are used in the position control in the X,Y directions while 5,6,7,8 are used for the Z-directional control. If we see the plots for the thruster control inputs for 1,2,3,4:We can observe the steady state final value to be zero for all the thrusters 1,2,3,4.Reason: The above thrusters are used for controlling the position in X,Y directions and their respective control inputs drive them to work in a synchronised manner so as to reach the respective position. Once the rover has reached the required X,Y, coordinates of the position with the required orientation, there is no need for them to provide any more thrust given the lack of any external force acting in the X,Y directions.The interesting part comes when we observe the control input plots for the thrusters 5,6,7,8:As it is observed, each of the control inputs have a non-zero steady state value; positive for thrusters 6,7 and negative for thrusters 5,8.Reason: Thrusters 5,6,7,8 are used for positional control in the Z direction. When the rover moves to the desired Z coordinate position in the required orientation, the job of the thrusters is not done since the position has to be retained while neutralising an external force. This external force is the buoyancy acting upon the rover since it is designed to be positively buoyant. The values of forcing acting due to weight and buoyancy for the rover under study are:W = 112.8 NB = 114.8 N ; this implies that the net external force acting on the rover is 2N upwards.When we look at thrusters 5,8 ; they provide a thrust vertically upwards when rotated clockwise while thrusters 6,7 produce a thrust vertically downwards for the same. This means that for thrusters 5,8 positive thrust is upwards while negative thrust is downwards. This is opposite in the case of thrusters 6,7 From the plots of control inputs to the above thrusters, the steady state values are as follows:For thrusters 5,8: $-1.25*10^{-2}$ (approx.)For thrusters 6,7: $+1.25*10^{-2}$ (approx.)Thrust produced by thrusters 5,8 is negative, which implies that thrust is produced in the¬† vertically downwards direction.Thrust produced by thrusters 6,7 is positive, which again implies that thrust is produced in the vertically downwards direction.Total thrust produced $\\tau=Ku$where K= 40 for all the thrusters.Total thrust produced in steady state:\\[4*1.25*10^{-2}*40=2N\\]downwards.Since the external force in Z direction has been neutralised, the rover is stabilised once it reaches the desired position.This is how the positional control of the underwater rover has been established using the linear PID control method.Non-Linear Controller ModelFor desired positional input as $[1;1;2;0;0;0]$, we got the following results for positional coordinates in world frame:Position X:Desired output: 1 mPosition Y:Desired output: 1 mPosition Z:Desired output: 2 mOrientation $\\phi$:Desired output: 0 radOrientation $\\theta$:Desired output: 0 radOrientation $\\psi$:Desired output: 0 radThe controller input plots (controller input for each thruster will be as follows):So, we can see that the thrust control input for thrusters 5,6,7 and 8 will be: \\(-1.375*10^{-2}, 1.375*10^{-2}, 1.375*10^{-2}, -1.375*10^{-2}\\) respectively.Now, since propeller pair of 5 and 8 will be opposite to that of the pair 6 and 7, we get the total thrust on the ROV in Z-direction as:\\(-1.375*10^{-2}*4*40 = 2N\\) (approx.), where 40 is the gain.So, the force in the vertical direction is balanced.LQR ControllerLiterature ReviewThe PID controller has been one of the most commonly used controllers for a really long time. There have been numerous PID tuning techniques, such as the Ziegler-Nichols method but are insufficient for high-performance control applications. The Linear Quadratic Regulator (LQR) is an optimal control method based on full state feedback. It aims to minimise the quadratic cost function and is then applied to linear systems, hence the name Linear Quadratic Regulator.Why LQR controller?The use of LQR¬† over PID control comes from the higher robustness of the former in terms of tuning the parameters with varying conditions. PID control uses the error in the input parameter of the closed loop system and tunes the parameters to reduce the error to zero. LQR, on the other hand, uses the state space model of the system and takes complete state feedback: -Kx, to calculate the error. LQR uses the method of cost function to calculate the control input cost vs the importance of achieving desired states.State-Space Model with Full State Feedback Gain\\[\\dot{X}=AX+BU\\]\\[y=CX+DU\\]Cost FunctionThe cost function defined by the system equations is minimised by the LQR controller using an optimal control algorithm. The cost function involves the system‚Äôs state parameters and input (control) parameters, along with the Q and R matrices. For the optimal LQR solution, the overall cost function must be as low as possible. The weights given to the state and control parameters are represented by the Q and R matrices, which act as knobs whose values can be varied to adjust the total value of the cost function.The system must have a linearized state-space model to solve the LQR optimization problem. The cost function to be optimised is given by\\[J=\\int{(X^TQX+U^TRU) dt}\\]Algebraic Riccati Equation and Its Solution ($S$ matrix)The Q and R matrices are used to solve the Algebraic Riccati Equation (ARE) to compute the full state feedback matrix.\\[A^TS+SA-SBR^{-1}B^TS+Q=0\\]On solving the above equation, we obtain the matrix $S$.Feedback Gain (K) and Eigen Values from $S$ matrixThe matrix ùëÜ obtained from the above ARE is used to find the full state feedback gain matrix K using the relation,\\[K=R^{-1}B^TS\\]The control matrix U is then given by\\[U=-KX\\]Linearisation of a state-space modelThe linearization of a state-space model is needed while using the LQR technique since it works on linear systems. In our case, the state space model is in the form: ·∫ã = f(x); where f(x) is a nonlinear function of x. In such cases, to linearise the equation, we use the concept of linearising about a fixed point. The steps for the same are as follows: Find the fixed points $\\bar{x}$; where $f(\\bar{x})=0.$ Linearise about an xÃÑ, by calculating the Jacobian of dynamics at the fixed point xÃÑ; where the latter can be represented as:\\[\\mathbf{J}=\\left[\\begin{array}{ccc}\\frac{\\partial \\mathbf{f}}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial \\mathbf{f}}{\\partial x_n}\\end{array}\\right]=\\left[\\begin{array}{c}\\nabla^{\\mathrm{T}} f_1 \\\\\\vdots \\\\\\nabla^{\\mathrm{T}} f_m\\end{array}\\right]=\\left[\\begin{array}{ccc}\\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\\\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n}\\end{array}\\right]\\]where each term is a partial derivative of the dynamics with respect to a variable.This step is executed since the dynamics of a nonlinear system behave linearly at the fixed point, or, in a small neighbourhood around the fixed point.Changing the frame of reference to one with xÃÑ as the origin:\\[\\dot{x} - \\bar{\\dot{x}} = f(x) \\newline = f(\\bar{x}) + J.(x-\\bar{x})+J^2.{(x-\\bar{x})}^2 + ....\\]where¬†$J$ is calculated at $\\bar{x}$.The higher order terms from the third term $J^2.{(x-\\bar{x})}^2$ are neglected since they are really small, hence the equation reduces to:‚àÜ·∫ã = 0 +J.‚àÜx + 0‚áí ‚àÜ·∫ã = J.‚àÜx\\[\\Delta\\dot{x} = 0+ J.\\Delta x + 0 \\newline = J.\\Delta x\\]This is in the form¬†$\\dot{x} = Ax$.Note: The above method for linearization works only when the fixed point satisfies the condition for linearising a system given by the Hartman Grobman Theorem, which states that:‚Äúthe behaviour of a dynamical system in a domain near a hyperbolic equilibrium point is qualitatively the same as the behaviour of its linearisation near this equilibrium point, where hyperbolicity means that no eigenvalue of the linearisation has real part equal to zero. Therefore, when dealing with such dynamical systems one can use the simpler linearisation of the system to analyse its behaviour around equilibria.[1]‚ÄùHence, linearization works only when the fixed point is hyperbolic or put simply, has a non-zero real part.LQR ImplementationThe following is the Simulink model that we built:To implement this, we need the following: State-space model: A and B matrices: These relate the derivative of the states with the current states and the control input. But, since our model is non-linear in nature, we cannot get a direct relation consisting of A and B matrices. Also, the LQR controller works only for Linear Systems. So, we have to linearise our system around an operating point. For our system, we took the origin in the world frame (initial point of the bot) as the operating point. Also, we used the position of the bot in the world frame and velocity of the bot in its own frame as the two states. Meaning: \\[x = \\begin{bmatrix} \\eta \\newline v \\end{bmatrix}\\] (Here $x$ is the state but not the distance in x direction)\\[\\begin{aligned}\\dot{\\boldsymbol{x}} &amp; =\\left[\\begin{array}{c}\\dot{\\boldsymbol{\\eta}} \\\\\\dot{\\boldsymbol{v}}\\end{array}\\right]=f\\left(\\boldsymbol{x}, \\boldsymbol{u}, \\boldsymbol{\\tau}_d, t\\right) \\\\&amp; =\\left[\\begin{array}{c}J(\\eta) \\boldsymbol{v} \\\\M^{-1}\\left[K p(A \\boldsymbol{u})+\\tau_d-C(\\boldsymbol{v}) \\boldsymbol{v}-D(\\boldsymbol{v}) \\boldsymbol{v}-g(\\boldsymbol{\\eta})\\right]\\end{array}\\right]\\end{aligned}\\]Since,\\[\\dot{\\eta} = J(\\eta)v\\]The above state space model is clearly non-linear. So, we have to linearise to find A and B as follows:\\[A(t) \\equiv\\left[\\begin{array}{cccc}\\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\\\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\&amp; \\vdots &amp; \\\\\\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n}\\end{array}\\right]_0 \\newline \\newline B(t) \\equiv\\left[\\begin{array}{cccc}\\frac{\\partial f_1}{\\partial u_1} &amp; \\frac{\\partial f_1}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial u_m} \\\\ \\frac{\\partial f_2}{\\partial u_1} &amp; \\frac{\\partial f_2}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial u_m} \\\\ &amp; \\vdots &amp; \\\\ \\frac{\\partial f_n}{\\partial u_1} &amp; \\frac{\\partial f_n}{\\partial u_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial u_m}\\end{array}\\right]_0\\]So, we obtain the A and B as follows:But we obtained the above results by taking weight W and buoyancy B as 110 N and 120 N respectively. We take the C matrix in state-space $y = CX+DU$ as $eye(12)$ so as to send back all the states as feedback to the summing point. Q and R matrices: Q stands for the importance of the states to reach their desired values. R stands for the cost of the control input. So, if we have an expensive control input compared to that of our needs to reach the states, we take the Q matrix to be dominating compared to R and vice versa. Then, we calculate the full-state feedback matrix, the Algebraic Riccati Equation and the eigenvalues by using the following command:\\[[K,S,P] = lqr(A,B,Q,R)\\] Then, we put the obtained value of the Gain matrix in the state-space model in Simulink. The following results are obtained for different R matrices (Q matrix being a 12x12 identity matrix) and desired states being $[1;2;1;[0;0;0;0;0;0;0;0;0]]$:\\[R = \\begin{bmatrix}1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;0.1 \\\\ 0.1 &amp; 1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 1 &amp; 0.1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 1 &amp;0.1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;1 &amp;0.1\\\\ 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp;0.1 &amp;1 \\end{bmatrix}\\]For the above, the Poles of the closed loop and open loop transfer function are as follows:Poles:Closed loop (With the feedback):Open loop (Without the feedback):Making ‚ÄòR‚Äô more dominant\\[R = \\begin{bmatrix}3 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;1.1 \\\\ 1.1 &amp; 3 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 3 &amp; 1.1 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 3 &amp;1.1 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;3 &amp;1.1\\\\ 1.1 &amp; 1.1 &amp; 1.1 &amp; 1.1 &amp;1.1 &amp;3 \\end{bmatrix}\\]Making ‚ÄòR‚Äô less dominant\\[R = \\begin{bmatrix}0.11 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.01 \\\\ 0.01 &amp; 0.11 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.11 &amp; 0.01 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.11 &amp;0.01 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.11 &amp;0.01\\\\ 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp;0.01 &amp;0.11 \\end{bmatrix}\\]Results Observed states:More dominant ‚ÄòR‚ÄôLess dominant ‚ÄòR‚ÄôThrust i/p to the thrusters:More dominant ‚ÄòR‚ÄôLess dominant ‚ÄòR‚ÄôBand Limited White NoiseWe planned on modelling a system with White Noise. So, we used the following control system:Where:We included a low pass filter for removing noise with frequency greater than 1Hz. With low values of ‚ÄòQ‚Äô we are getting really unstable results as the importance to the states is reduced. Later, we increased the value of Q to allow the states to reach quickly. This made the response of our system better. Taking the reference input as: [0;0;2;1.57;1;0.8;0;0;0;0;0;0]; $Q = eye(12):$States:Thrust provided by each thruster:Poles of this system:$Q = 1000*eye(12)$:States:Thrust provided by each thruster:Poles of this system:Clearly, the response of the system is faster in the second case.Note: In the above two cases, the open loop and closed loop pole plots are drawn without taking the noise into account. The effect of noise is shown only in the ‚ÄòThrust‚Äô and ‚ÄòStates‚Äô plots.Also, in the paper that we referred to, they considered noise caused due to the ‚Äòcurrent velocity‚Äô which has a direct effect on the velocity of the rover instead of the torques. So, for that, a Random Number generator block was included in¬† the model as follows:The parameters of the noise block are as follows:Here, noise is added considering the velocity of current as: $[0.25,0.25,0.25]$ and taking the variance as 0.01 in all the three directions.Taking R matrix as: \\(R = 1.2*eye(6)+0.8*ones(6)\\)And Q matrix as: $Q = 1000*eye(12)$The results that we obtained for 1 min stop time are:States:Thrust provided by each thruster:Clearly, the thrust output involves too much vibrations.Analysis of EigenvaluesWhen we analyse the change in eigen values while considering the open loop and LQR controlled closed loop systems, for the same values of Q,R we see a clear change in their position in the pole zero plot.Considering the above plot, we can see a clear shift in the eigen values in the direction of the negative real axis, ensuring more stability of the system. As we write down the eigen values or the poles of the two systems, the difference becomes evident.For $Q=eye(12)$ &amp; \\(R = 0.1*ones(12)+0.9*eye(12)\\) Open loop poles Closed loop poles 0.0000 + 0.0000i ¬† ¬† -6.3274 + 0.0000i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-3.2047 + 3.1841i ¬†¬†-0.2740 + 4.3867i ¬†¬†-3.2047 - 3.1841i ¬†¬†-0.2740 - 4.3867i ¬†-3.4503 + 2.9329i ¬†¬†-0.2633 + 0.0000i ¬†¬†-3.4503 - 2.9329i ¬†¬†-0.3003 + 4.3834i ¬†¬†-0.9222 + 0.4604i ¬†¬†-0.3003 - 4.3834i ¬†¬†-0.9222 - 0.4604i ¬†¬†-0.4067 + 0.0000i ¬†¬†-0.8836 + 0.4709i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-0.8836 - 0.4709i ¬†¬†¬†0.0000 + 0.0000i ¬†¬†-1.0104 + 0.0000i ¬†¬†-0.4504 + 0.0000i ¬†¬†-0.2170 + 0.0000i ¬†¬†-0.4375 + 0.0000i ¬†¬†-0.4043 + 0.0000i As it can be observed, in the case of the open loop system, we have four eigen values or poles located at the origin which leads to the fact that the system is marginally stable. Whereas, for the closed loop system, most of¬† the poles have shifted towards the left side of the imaginary axis with all the poles lying on the left half of the s-plane. This gives enough proof to say that after implementing LQR control, the system has become more stable as compared to the open loop system and hence has helped in increasing performance in terms of reaching the end states.ConclusionGiven the results of implementing both PID and LQR control on the ROV system, we can see a better response for an LQR control over the PID when we include disturbances in the environment. Also, the LQR control is far more robust in adapting to the change in buoyancy as compared to the PID control, which broadens the spectrum of usage in the former as compared to the latter. When we have changes in the system, using a PID requires tuning all the three gain values for all the control inputs. Tuning the gain values for multiple parameters is not an easy task and there is a very narrow range of values which give the desired results for a specific situation. On the other hand, when we want to adapt to changes while working on LQR control, our tuning is dependent only on the Q,R cost matrices which can help in changing the¬† relative importance of the states and control inputs for a given circumstance. Here, our goal is achieved by changing the values of the matrices, so as to select the more important factor among the states and inputs,¬† where we achieve the desired results for a wide range of values which only differ in terms of speed of transient response and steady state error. Another advantage of LQR over PID control that we have come across was the ability to easily control velocity components along with the position coordinates, which would lead to much more complexity in the case of a PID based controller. For simulating in a noisy environment, we used nonlinear PID control where a water current velocity was added as a disturbance; its value being constant with time and we attained appreciable results. Whereas for the LQR setup, we have used a gaussian white noise as a source of disturbance, which gives a better look at the real world scenario. Upon increasing the Q matrix or simply, the cost of attaining the states, we have observed better performance in attaining the states and less erratic, more uniform thrust outputs produced by the thrusters; which is a more reliable result since it is realisable in a real world environment.However, one drawback of using the LQR control is its applicability to only linear systems, whereas most of the real world scenarios tend to have non linearity in them. However, this problem can be overcome by linearising the system near the fixed points and applying the control.Hence, we have come to the conclusion that using an LQR control is far more reliable than a PID control for the positional and velocity control of a 6-DOF Autonomous Underwater Vehicle.Future Work The model can be implemented with an Adaptive Controller based system which is expected to give better performance. We wish to implement our work on the actual physical system to know how it works in the real world. And by doing that, we will be able to understand how the sensor noise affects the system and how the controller will compensate for it." }, { "title": "Autonomous Ground Vehicle", "url": "/posts/transnomous/", "categories": "Projects, ML", "tags": "ml, dl, cnn, computer vision, python", "date": "2022-08-11 21:30:00 +0530", "snippet": "To be Updated" }, { "title": "Quadcopter Navigation", "url": "/posts/quadcopter-navigation/", "categories": "Projects, Electronics", "tags": "electronics, arduino, led", "date": "2022-08-11 21:30:00 +0530", "snippet": "Implementing Semi-Direct Visual Odometry (SVO) on drones for autonomous navigation.Code and ROS bag for sample implementation taken from: Github 1Github2" }, { "title": "CubeD", "url": "/posts/cubed/", "categories": "Projects, Electronics", "tags": "electronics, arduino, led", "date": "2022-08-11 21:30:00 +0530", "snippet": "CubeD is an interactive touch-based display that lets users engage and play games.Mechanical DesignCubeD consists of 200 squares hinged onto a 4 segment steel back panel. Its current dimensions are 3 meters x 2 meters. Each steel back panel measures 1meter x 1.5 meters and houses 50 squares. Each square has 12 - Neopixels arranged around the inner perimeter of the square, a diffuser ensures that that the light is uniformly projected to the user.The back panels are screwed onto a two-piece wooden frame with triangular support structures. Each square has a microswitch behind it on the panel, which is used as an input device to detect touch from the user.ElectronicsThe CubeD has been designed such that it can be assembled and dismantled without any wastage of resources, and hence the electronics and electrical wiring of the entire project is divided into modules that are easy to work on. The Power Supply for CubeD consists of the main power source and two junctions that are branched from the main source.NeopixelsThe WS2812 Integrated Light Source ‚Äî or NeoPixel in Adafruit parlance ‚Äî is the latest advance in the quest for a simple, scalable, and affordable full-colour LED. Red, green, and blue LEDs are integrated alongside a driver chip into a tiny surface-mount package controlled through a single wire. Neoxpixels use a Single-Wire Protocol to transfer data.Writing out WS2812 data requires some pretty tight timing. Tight enough that FastLED disables interrupts while it is writing out led data. This means that while the led data is being written out, any interrupts that happen will be delayed until all the led data is written out.NeoPixels receive data from a fixed-frequency 800 kHz datastream. One bit, therefore, requires 1/800,000 sec ‚Äî 1.25 microseconds. One pixel requires 24 bits (8 bits each for red, green blue) ‚Äî 30 microseconds. After the last pixel‚Äôs worth of data is issued, the stream must stop for at least 50 microseconds for the new colours to ‚Äúlatch.‚ÄùFor a strip of 2400 Pixels, that‚Äôs 2400*30 + 50 which is 72,050 microseconds, therefore 13.8 updates per second.Since the latching is done internally inside the Neopixel strip, the interrupts are only disabled for data transfer.For a strip of 2400 Pixels, that‚Äôs 2400*30 which is 72,000 microseconds. During this time interrupts are disabled on most boards.Shift RegistersThis sequential device loads the data present on its inputs and then move or ‚Äúshifts‚Äù it to its output once every clock cycle, hence the name Shift Register.A shift register consists of several single bit ‚ÄúD-Type Data Latches‚Äù, one for each data bit, either a logic ‚Äú0‚Äù or a ‚Äú1‚Äù, connected together in a serial type daisy-chain arrangement so that the output from one data latch becomes the input of the next latch and so on.PISO Shift RegistersThe parallel data is loaded into the register simultaneously and is shifted out of the register serially one bit at a time under clock control.The 8 inputs are translated into a series of HIGH and LOW pulses on the serial-out pin of the shift register. This pin should be connected to an input pin on your Arduino Board, referred to as the data pin. The transfer of information on the data pin is called ‚ÄúSynchronous Serial Output‚Äù because the shift register waits to deliver a linear sequence of data to the Arduino until the Arduino asks for it. Synchronous Serial communication, either input or output, is heavily reliant on what is referred to as a clock pin. The clock pin is the metronome of the conversation between the shift register and the Arduino, it is what keeps the two systems synchronous. Every time the Arduino changes the clock pin from LOW to HIGH the shift register changes the state of the Serial Output pin, indicating the value of the next switch.The third pin attached to the Arduino is a ‚ÄúParallel to Serial Control‚Äù pin. It is referred to as a latch pin. When the latch pin is HIGH the shift register is listening to its 8 parallel inputs. When the latch pin is LOW, it listens to the clock pin and passes information serially. That means every time the latch pin transitions from HIGH to LOW the shift register will start passing its most current switch information.In the current design, we have 4 sets of 7 PISO Registers daisy-chained with a UNO as a sub-unit to process the states of 50 microswitches.Micro SwitchesA Microswitch has three terminals NC, NO, and C (or COM). C is the signal to be switched. NC is normally closed and NO is normally open.A 5V signal is given to the Common terminal and the output signal from NO is monitored using a digitalRead. Each microswitch is placed behind a square box on the back panel, in such a way that the switch would be triggered when the box is touched. Each NO terminal is connected to the Parallel Input Pin of a PISO Shift Register with the help of a terminal box to maintain the modular structure.Power SupplySwitching Mode Power Supply units are used as the main power source for CubeD. Multiple SMPS units are used along with Junctions at appropriate places to distribute loads to keep the circuit stable. A key feature in the design is the fact that all the connections made to individual modules are in parallel which ensures the working of other modules in case a single module is down for maintenance.Each Neopixel grid is powered using one SMPS from both ends, this is to avoid reverse feed from one SMPS to another due to minute changes in the output voltage. All the grounds are interconnected.NeoPixels don‚Äôt care what end they receive power from. Though data moves in only one direction, electricity can go either way. You can connect power at the head, the tail, in the middle, or ideally distribute it to several points. Think of power distribution as branches of a tree rather than one continuous line.Power RequirementsEach individual NeoPixel draws up to 60 milliamps at maximum brightness white (red + green + blue). In actual use though, it‚Äôs rare for all pixels to be turned on that way. When mixing colours and displaying animations, the current draw will be much less.For a strip of 2400 Pixels at 5V, the current consumption is 144 Amps. Which is around 720 Watts. Therefore, each SMPS should be capable of giving out around 40 Amps and a rated power output of more than 200W.Terminal BoxEach of the 4 PISO Registers PCB contains around 56 PCB Terminals Soldered onto the PCB for easy connections between the microswitches and the registers.Junction BoxJunction Boxes are used to distribute power without directly tapping into the main power source and this also helps us keep all the connections modular.It is also used for interconnecting grounds across the circuits.Complete Frame" }, { "title": "Smart Agricultural Seeding Robot", "url": "/posts/agricultural-bot/", "categories": "Projects, Robotics", "tags": "arduino, electronics", "date": "2022-08-11 21:30:00 +0530", "snippet": "ElectronicsArduino Code#include &lt;Servo.h&gt;Servo myservo;int botrightpin = 5;int botleftpin = 6;int toprightpin = 9;int topleftpin = 10;int servopin = 11;int val=0;int buzzPin = 3;void setup(){ pinMode(botleftpin, OUTPUT); pinMode(botrightpin, OUTPUT); pinMode(topleftpin, OUTPUT); pinMode(toprightpin, OUTPUT); pinMode(buzzPin, OUTPUT); myservo.attach(servopin); Serial.begin(9600);}void loop(){ digitalWrite(botleftpin, HIGH); digitalWrite(botrightpin, HIGH); digitalWrite(topleftpin, HIGH); digitalWrite(toprightpin, HIGH); delay(7000); digitalWrite(botleftpin, LOW); digitalWrite(botrightpin, LOW); digitalWrite(topleftpin, LOW); digitalWrite(toprightpin, LOW); myservo.write(90); digitalWrite(buzzPin, HIGH); delay(200); myservo.write(0); digitalWrite(buzzPin, LOW); delay(2000);}FunnelChassis and Bot" }, { "title": "Recognizing Traffic Signs using CNNs", "url": "/posts/traffic-signs-recognition/", "categories": "Projects, ML", "tags": "ml, dl, cnn, object detection, computer vision", "date": "2022-08-11 21:30:00 +0530", "snippet": "IntroductionThe following project shows the implementation of a simple convolutional neural network (CNN). The model will be able to identify which signal it is when presented with a colour image of a traffic sign. Being my first project in deep learning, I gained extensive knowledge about how the dataset is composed, how to pre-process images, which deep network to use, and how to efficiently choose the number of layers and units.DatasetThe dataset used for this project can be obtained fromPublic Archive: daaeac0d7ce1152aea9b61d9f1e19370Data PreprocessingThe entire dataset contains images of different sizes. Since the very first operation of the model involves reading and standardizing the images, it is important to resize all the images to a predefined size, 32x32 in this case. The colorspace of the images is also converted from RGB to grayscale.Another important data preprocessing step is one-hot encoding. One-hot encoding refers to the process of converting categorical data variables to a numerical form, and this is done with a 43-dimensional array.Image Resizingimport numpy as npimport matplotlib.pyplot as pltfrom skimage.color import rgb2labfrom skimage.transform import resizeimport globfrom collections import namedtuple%matplotlib inlineN_CLASSES = 43RESIZED_IMAGE = (32,32)Dataset = namedtuple('Dataset',['X','y'])def read_dataset_ppm(rootpath, n_labels, resize_to): images = [] labels = [] for c in range(n_labels): full_path = rootpath + '/' + format(c,'05d') + '/' for img_name in glob.glob(full_path + '*.ppm'): img = plt.imread(img_name).astype(np.float32) img = rgb2lab(img/255.0)[:,:,0] img = resize(img, resize_to, mode='reflect').astype(np.float32) label = np.zeros(shape=(n_labels,), dtype=np.float32) label[c] = 1.0 labels.append(label) images.append(img) return Dataset(X = img_stack(/assets/images/L3D/a3/images), y = np.array(labels))def img_stack(imgs): return np.stack([img[:,:,np.newaxis] for img in imgs], axis=0).astype(np.float32)dataset = read_dataset_ppm('GTSRB_Final_Training_Images/GTSRB/Final_Training/Images', N_CLASSES, RESIZED_IMAGE)print(dataset.X.shape)print(dataset.y.shape)Train test splitfrom sklearn.model_selection import train_test_splitidx_train, idx_test = train_test_split(range(dataset.X.shape[0]), test_size=0.25)X_train = dataset.X[idx_train,:,:]X_test = dataset.X[idx_test,:,:]y_train = dataset.y[idx_train,:]y_test = dataset.y[idx_test,:]Creating minibatches of dataEvery training iteration would require the addition of a minibatch of randomly chosen samples taken from the practise set. Different minibatches of data in every generator will compel the model to learn the in-out connection rather than memorizing the sequence.n_samples = X_train.shape[0]def minibatcher(X, y, batch_size): i = np.random.permutation(n_samples) for j in range(int(np.ceil(n_samples/batch_size))): fr = j * batch_size to = (j+1) * batch_size yield X[i[fr:to],:,:,:], y[i[fr:to],:]Layersimport tensorflow.compat.v1 as tftf.disable_v2_behavior()def fc_no_activation(in_tensors, n_units): w = tf.get_variable(name=\"fc_W\", shape=[in_tensors.get_shape()[1], n_units], dtype=tf.float32, initializer=tf.keras.initializers.glorot_normal()) b = tf.get_variable(name=\"fc_B\", shape=[n_units,], dtype=tf.float32, initializer=tf.constant_initializer(0.0)) return tf.matmul(in_tensors,w) + b# Fully connected layerdef fc_layer(in_tensors, n_units): return tf.nn.leaky_relu(fc_no_activation(in_tensors, n_units))# COnvolution layerdef conv_layer(in_tensors, kernel_size, n_units): w = tf.get_variable(name=\"conv_W\", shape=[kernel_size, kernel_size, in_tensors.get_shape()[3], n_units], dtype=tf.float32, initializer=tf.keras.initializers.glorot_normal()) b = tf.get_variable(name=\"conv_B\", shape=[n_units,], dtype=tf.float32, initializer=tf.constant_initializer(0.0)) return tf.nn.leaky_relu(tf.nn.conv2d(input=in_tensors, filters=w, strides=[1,1,1,1], padding='SAME') + b)# Maxpool layerdef maxpool_layer(in_tensors, sampling): return tf.nn.max_pool(in_tensors, [1, sampling, sampling, 1], [1, sampling, sampling, 1], 'SAME')# Dropout layerdef dropout(in_tensors, keep_proba, is_training): return tf.cond(is_training, lambda: tf.nn.dropout(in_tensors, keep_proba), lambda: in_tensors)StructureThe model will be composed of the following layers: 2D convolution, 5x5, 32 filters 2D convolution, 5x5, 64 filters Flattenizer Fully connected later, 1,024 units Dropout 40% Fully connected layer, no activation Softmax outputdef model(in_tensors, is_training): # First layer: 5x5 2d-conv, 32 filters, 2x maxpool, 20% drouput with tf.variable_scope('l1', reuse=tf.AUTO_REUSE): l1_conv = conv_layer(in_tensors, 5, 32) l1_maxpool = maxpool_layer(l1_conv, 2) l1_drop = dropout(l1_maxpool, 0.8, is_training) # Second layer: 5x5 2d-conv, 64 filters, 2x maxpool, 20% drouput with tf.variable_scope('l2', reuse=tf.AUTO_REUSE): l2_conv = conv_layer(l1_drop, 5, 64) l2_maxpool = maxpool_layer(l2_conv, 2) l2_drop = dropout(l2_maxpool, 0.8, is_training) with tf.variable_scope('flatten', reuse=tf.AUTO_REUSE): l2_flatten = tf.layers.flatten(l2_drop) # Fully collected layer, 1024 neurons, 40% dropout with tf.variable_scope('l3', reuse=tf.AUTO_REUSE): l3 = fc_layer(l2_flatten, 1024) l3_drop = dropout(l3, 0.6, is_training) # OUTPUT with tf.variable_scope('output', reuse=tf.AUTO_REUSE): output_tensors = fc_no_activation(l3_drop, N_CLASSES) return output_tensorsTrainingfrom sklearn.metrics import classification_report, confusion_matrixdef train_model(X_train, y_train, X_test, y_test, learning_rate, max_epochs, batch_size): in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1)) in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES)) is_training = tf.placeholder(tf.bool) logits = model(in_X_tensors_batch, is_training) out_y_pred = tf.nn.softmax(logits) loss_score = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=in_y_tensors_batch) loss = tf.reduce_mean(loss_score) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) with tf.compat.v1.Session() as session: session.run(tf.global_variables_initializer()) for epoch in range(max_epochs): print(\"Epoch=\", epoch) tf_score = [] for mb in minibatcher(X_train, y_train, batch_size): tf_output = session.run([optimizer, loss], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True}) tf_score.append(tf_output[1]) print(\"train_loss_score=\",np.mean(tf_score)) print(\"TEST SET PERFORMANCE\") y_test_pred, test_loss = session.run([out_y_pred, loss], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False}) print(\" test_loss_score=\", test_loss) # CLASSIFICATION REPORT y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32) y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32) print(classification_report(y_test_true_classified, y_test_pred_classified)) \t\t# CONFUSION MATRIX\t\tcm = confusion_matrix(y_test_true_classified, y_test_pred_classified) plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) plt.colorbar() plt.tight_layout() plt.show() # And the log2 version, to enphasize the misclassifications plt.imshow(np.log2(cm + 1), interpolation='nearest', cmap=plt.get_cmap(\"tab20\")) plt.colorbar() plt.tight_layout() plt.show()tf.reset_default_graph()train_model(X_train, y_train, X_test, y_test, 0.001, 10, 256)ResultsAccuracy = 98%Performance ReportEpoch= 0train_loss_score= 4.8711586Epoch= 1train_loss_score= 0.83951813Epoch= 2train_loss_score= 0.3692537Epoch= 3train_loss_score= 0.2198893Epoch= 4train_loss_score= 0.1685863Epoch= 5train_loss_score= 0.11884567Epoch= 6train_loss_score= 0.10304754Epoch= 7train_loss_score= 0.07655481Epoch= 8train_loss_score= 0.072380714Epoch= 9train_loss_score= 0.059970096TEST SET PERFORMANCE test_loss_score= 0.0674421 precision recall f1-score support 0 1.00 1.00 1.00 45 1 0.99 0.98 0.98 545 2 0.92 1.00 0.96 561 3 0.99 0.98 0.99 357 4 0.98 0.98 0.98 484 5 0.99 0.94 0.97 472 6 1.00 1.00 1.00 104 7 1.00 0.97 0.98 347 8 1.00 0.97 0.98 372 9 0.99 0.99 0.99 370 10 0.99 1.00 1.00 500 11 1.00 0.97 0.99 352 12 0.99 1.00 0.99 538 13 1.00 0.99 1.00 549 14 0.99 1.00 0.99 193 15 0.99 0.99 0.99 166 16 0.99 0.99 0.99 96 17 1.00 0.99 1.00 293 18 0.97 1.00 0.99 276 19 1.00 0.98 0.99 44 20 0.95 0.99 0.97 93 21 0.96 0.99 0.97 77 22 0.99 1.00 1.00 119 23 0.97 0.98 0.98 127 24 0.96 0.99 0.97 72 25 0.98 0.98 0.98 396 26 0.97 0.97 0.97 155 27 1.00 1.00 1.00 68 28 0.96 0.99 0.98 133 29 0.98 0.89 0.93 64 30 1.00 0.91 0.95 113 31 1.00 0.98 0.99 195 32 1.00 1.00 1.00 53 33 1.00 0.96 0.98 170 34 1.00 0.98 0.99 98 35 1.00 1.00 1.00 263 36 0.99 0.98 0.98 98 37 1.00 1.00 1.00 48 38 0.99 1.00 0.99 519 39 1.00 1.00 1.00 64 40 0.95 0.99 0.97 89 41 1.00 0.98 0.99 61 42 1.00 0.97 0.98 64 accuracy 0.98 9803 macro avg 0.99 0.98 0.98 9803weighted avg 0.99 0.98 0.98 9803Confusion Matrix" }, { "title": "Persistence of Vision Wand", "url": "/posts/pov-wand/", "categories": "Projects, Electronics", "tags": "arduino, electronics, led", "date": "2022-08-11 21:30:00 +0530", "snippet": "IntroductionPOV (Persistence of Vision) is a kind of optical illusion in which a visual image seems to persist even when the light from it ceases to enter our eyes. This can be used to make POV displays where we can display text, images, gifs, etc.Hardware Components Quantity Arduino Uno 1 LED (Red) 19 Resistors 220 ohm 19 Perf board 1 Header pins 20 Toggle switch 1 Tools: Soldering ironConstruction Take a perf board and cut it to the required size for 20 LEDs. Solder the LEDs in a single line with all Anode on the same side. Attach 220 resistors beside each LED and solder them. Add toggle switch to turn on the display whenever the the switch is turned on. Solder wires to anode of each LED and common Gnd. Pin connections:Top of wand1¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 132¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 123¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 114¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 105¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 96¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 87¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 78¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 69¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 510¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 411¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 312¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 213¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 114¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Digital Pin 015¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 516¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 417¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 318¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 219¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Analog Pin 1Bottom of wandSoftwareThe code for the same can be found below.Each letter of the alphabet is generated by a collection of binary arrays that are stored on the Arduino. The Arduino matches each letter to one of its stored letters as it reads the message you want to display, and then it produces the stored array column by column.Results" }, { "title": "Disaster Management System for Floods using IoT and ML", "url": "/posts/iot-disaster-management-system/", "categories": "Projects, ML", "tags": "ml, iot", "date": "2022-08-11 21:30:00 +0530", "snippet": "IntroductionFloods are the most catastrophic and cataclysmic events of all-natural disasters. The World Meteorological Organization has stated that out of all the disasters in the world, floods are the most severe. Specifically, in India, about 12% of the land is vulnerable to flood conditions. Heavy unprecedented rainfall results in floods bringing everyday life to a standstill. Most floods occur during monsoons; however, floods can also occur due to dams &amp; levees breaking, which can be triggered by thunderstorms, cyclones, and low-pressure regions.With substantial technological innovations in sensing systems, communication networks, cloud computing, machine learning, and data analytics, it is readily possible to develop an integrated flood disaster management system that can effectively alert the flood affecting regions. Internet of Things (IoT) is one such technology that can not only help predict the occurrence of a flood but can also help provide an emergency floor plan using an AI approach.This project uses an IoT framework with AI to develop flood monitoring, prediction, and post-flood management systems.Basic OutlineOccurrencesPredictionA model will be designed to monitor the environmental parameters used for flood disaster prediction. The environmental parameters like temperature, relative humidity, atmospheric pressure, rainfall, etc., are sensed by an array of sensors, and the measured data is sent to the microcontroller via a suitable communication standard. Further, the relationships between the input data received and the output rainfall are modeled using ANN (Artificial Neural Network) techniques. Continuous monitoring of changes in the environment will be done by updating the old values with new ones after a specified time interval. A flood event is thus predicted using the ANN model, which will alert people to upcoming disasters according to the increase of rainfall and corresponding rising water levels in low-lying areas near river flow.Emergency Floor PlanPeople routing in an emergency is an exciting and complex problem due to the several challenges that affect the result of the provided solution. We would have to consider minute-by-minute changes in the emergency environment, which would require dynamic generation and management of the evacuation routes. Some possible hazards might include structural damages and collapses that block the route or limit route capacity while evacuating, low visibility, too much water logging, etc. Taking into consideration all the possible challenges, clear objectives have to be decided for the same.CommunicationUse of Intermediate NodesESP32 nodes can be used for effective communication of the sensor information and traversing floor plan information in times of an emergency. The range of a typical ESP32 board is limited to around 240 meters but can be extended up to 10km using a directional antenna, with a 4-kilometer range of efficient communication. When implemented over an extensive area, intermediate nodes can be utilized to deliver the information to the destination nodes and ensure data syncing.DeploymentThe smart buoy being built can be deployed in multiple ways depending on the city and the area where it is being placed. The purpose is flood detection. In coastal areas where floods can be caused by the water in the sea i.e. tsunamis and cyclones , the smart buoy can be placed near the sea region , where it can predict the chances of a flood judging the sea levels. Smart buoys can be deployed using drones , in case the water enters the cities.These buoys will communicate the info from various areas and can be used to analyze the situation in the entire city at once.End goal : To achieve efficient flood managementRescue Teams/Police StationRescue teams and police stations can act faster in case floods are predicted and an overall information is presented to them. The smart buoy can aid in better application of emergency procedures and can decrease the loss caused due to these disasters by a large scale.News MediaThe media can alert the citizens in case a flood is approaching and warn the areas which are at a high risk. The timely updates can help in efficient evacuation.Internet and SMSThe Internet and SMS play a major role in any disaster management situation. Notifications regarding the routes to be taken during evacuation, roads blocked, structural damage in various areas can be made available to the public.Requests for help so that the rescue teams arrive can also be made , by providing an emergency number to which sms sent will alert the authorities about the problem there.Protocols and ArchitectureIEEE 802.11IEEE 802.11 is a set of LAN standards, and specifies a set of MAC and PHY protocols for implementing Wireless Local Area Networks (WLAN) communication. It supports wireless connectivity for portable, moving devices (Wi-Fi devices) within a local area. Compared to theIEEE 802.16 standard, IEEE 802.11 standard supports shorter distance and higher data rate for the Wi-Fi devices. The data rate supported by IEEE 802.11 ranges from 11 Mbps to more than 1 Gbps.802.11b, 802.11g, and 802.11n-2.4 utilize the 2.400‚Äì2.500 GHz spectrum, one of the ISM bands. 802.11a, 802.11n, and 802.11ac use the more heavily regulated 4.915‚Äì5.825 GHz band. Each spectrum is subdivided into channels with a center frequency and bandwidth.The 2.4 GHz band is divided into 14 channels spaced 5 MHz apart, beginning with channel 1, which is centered on 2.412 GHz. The latter channels have additional restrictions or are unavailable for use in some regulatory domains.IEEE 802.15.4Unlike the 802.11 based networks, the IEEE 802.15 family of standards support short-range, low-power, low-rate communications.The IEEE 802.15.4 standard is designed to support low-data rate wireless connectivity withfixed, portable, and mobile devices with very limited battery consumption and relaxed throughput requirements. The standard through its energy-efficient link-layer technologies sup-ports networking of the constrained IoT devices. It supports longer network lifetime with periodic sleep cycles, low-rate, and low-power communications. Hence, it is one of the most widely adopted link-layer technologies for building IP-based IoT networks.Standards like ZigBee, WirelessHart and ISA100.11a define higher-layers on top of the IEEE 802.15.4 standard.IEEE 802.15.4 specifies three frequency bands of operation: 868 MHz, 915 MHz and the 2.4GHz unlicensed industrial, scientific and medical (ISM) band.IEEE 802.11ahIEEE 802.11ah is a wireless networking standard that uses 900 MHz license-exempt bands to provide extended-range Wi-Fi networks, compared to conventional Wi-Fi networks operating in the 2.4 GHz and 5 GHz bands. It also benefits from lower energy consumption, allowing the creation of large groups of stations or sensors that cooperate to share signals, supporting the concept of the Internet of things (IoT). The protocol‚Äôs low power consumption competes with Bluetooth and has the added benefit of higher data rates and wider coverage range.The sensor network standards (such as ZigBee, RFID, or Bluetooth) work over relatively short distances (i.e., tens of meters), with low data rates and low energy consumptions. On the other hand, standards like GPRS, LTE, WiMAX, etc., work over long distances and provide high throughput; however, they consume more energy, and demand an expensive and fixed infrastructure of base stations with proper line of sight. Owing to its low power consumption, the IEEE 802.15.4 is a suitable standard for many IoT applications. However, it is not suited for facilitating communication among a large number of IoT devices or for covering large areas.The 802.11ah standard enables single-hop communication over distances up to 1000 m. Relay Access Points can be used to extend the connectivity as well. The 802.15.4 standard, with a maximum range of 100m, alone cannot provide a communication framework for a larger coverage range.The 802.15.4 standard usually operates in the unlicensed 2.4 GHz band which can accommodate data rates up to 250 Kbps. On the other hand, the 802.11ah utilizes the sub-1 GHz license-exempt bands to provide an extended range to Wi-Fi networks.You can refer to sections 2.3, 2.5, 3.2, 3.3, 3.4 from here for more information.DevicesUtilitySensorsReference - https://flood.network/ Smart Cameras - Intelligent image processing and pattern recognition algorithms for coastal zone management, forecasting tidal waves and detecting overtopping waves, detecting smoke and fire caused by a flood, etc. Smart Buoys - Can be used to detect water level, flow velocity, tidal waves, etc. The floating object can carry a GPS and an acceleration sensor. Any sudden rise or gradual change in water level can be instantaneously used to alert nearby people through the web. Water Level Sensors - Used to measure water level in real-time near dams, rivers, reservoirs, etc. Weather Sensors - They measure humidity, wind speed, amount of rainfall, air pressure and temperature, the data from which can be used to predict the occurrence of a flood. Navigation Platforms - Real-time navigation platforms used by drivers/common people can provide hazard reports, making it possible to categorize the hazard into subcategories, such as a small flood, etc.Creating the Mesh Network and TestingThe initial setup consists of 9 ESP8266 boards, each loaded with the code and path-planning algorithm.Code can be found on GithubThe operating voltage range on an ESP8266 board is 3V-3.6V. The board comes with a low-dropout (LDO) voltage regulator to keep the voltage steady at 3.3V. It can reliably supply up to 600mA of current and has an operating current of 80mA. Power to the NodeMCU can be supplied via its USB connector, which is also used to load code into the board. Alternatively, we can use a 5V power supply by connecting it to the Vin pin in order to power up the board and its peripherals.The NodeMCU will not be connected to any other external electronic components but will just act as nodes in the mesh.Once all the boards are powered up, they will connect to a mesh of the following specifications (as directed by the code): MESH_SSID ‚ÄúdisasterManagement‚Äù MESH_PASSWORD ‚Äúpassword‚Äù MESH_PORT 5555Power connections and Hardware SetupFree Space Path LossFree Space Path Loss (FSPL) is the loss in signal strength of an electromagnetic wave (WiFi Signal in the present case). Free-space path loss is proportional to the square of the distance between the transmitter and receiver, and also proportional to the square of the frequency of the signal.\\[\\begin{align*}FSPL(dB)=10\\log_{10}((\\frac{4{\\pi}df}{c})^2)\\\\=20\\log_{10}(\\frac{4{\\pi}df}{c})\\end{align*}\\\\\\begin{align*}=20\\log_{10}(d)+20\\log_{10}(f)+20\\log_{10}(\\frac{4{\\pi}}{c})\\\\=20\\log_{10}(d)+20\\log_{10}(f)-147.55\\end{align*}\\]d- distance between emitter and receiverf- frequency of WiFi Signal (lies between 2400-2500 Hz)c- speed of light" }, { "title": "ROS Theory", "url": "/posts/ros-theory/", "categories": "Blog, Robotics", "tags": "ros", "date": "2022-07-28 21:30:00 +0530", "snippet": "SynopsisROS is an open-source project that provides a framework to your robot. It has become an integral part of robots today, and has massively impacted the Robotics Arena. ROS is extremely fascinating to study, but is not easy or beginner friendly.ROS Theory is an attempt to document the basics of ROS. This documentation will take you through a number of examples to better understand the concepts, and is neatly demonstarted by code and pictures of my terminal.Table of contents Introduction Tools Topics Messages Services Actions TF2Introduction1.1 What is ROS?ROS stands for Robot Operating System. Although the name implies that it is an OS, it is not. Rather, it is a framework that helps integrate the various parts of a robot by setting up communication between hardware and software of your robot, and between different processes going on within the software.Let us consider the following example in order to understand it better. Suppose you are building a simple robot for a ‚Äòfetch an item‚Äô task, in which your robot needs to navigate a given environment, find the required item and bring it back to a specified location. The robot has various parts that perform different functions such as locomotion and navigation (by the wheels of the robot), computer vision (by the camera on the robot), etc. However, these parts cannot talk to each other directly, due to which the robot will not know when it should stop/resume a certain task assigned to it. This is where ROS comes in. ROS helps integrate these various parts, thereby facilitating easy communication. The different parts now receive messages from other parts, thereby performing their functions more efficiently.1.2 ROS GraphROS Graph is a convenient way of representing the various programs, messages and message streams of a ROS system as a graph. The various ROS programs, called nodes, communicate with each other by sending/receiving messages. The nodes in a ROS Graph are connected by ‚Äútopics‚Äù, which represents a stream of messages that nodes use to communicate with each other.1.3 roscoreroscore is a service that provides connection information to nodes so that they can find and transmit/receive messages with other nodes. Every node connects to roscore at startup to register details of the message streams it publishes and the streams to which it wishes to subscribe. When a new node appears, roscore provides it with the information that it needs to form a direct connection with other nodes publishing and subscribing to the same message topics.1.4 catkincatkin is the ROS build system, which is a set of tools that ROS uses to generate executable programs, scripts, libraries, etc. catkin comprises a set of CMake macros and custom Python scripts. Every catkin directory will contain two files - CMakeLists.txt and package.xml, that you need to make changes in order for things to work properly.1.5 WorkspacesA workspace is a set of files and directories that contain ROS code. You can have multiple workspaces but you can work on only one workspace at a time.A catkin workspace is a directory where you build, modify, and install catkin packages.Running catkin_make will create two new directories - devel and build, along with the previously existing src directory. Build is where catkin will store the results of some of its work such as libraries, etc. Devel contains a number of files and directories, including setup files. Running these setup files configures the system to use the workspace and run the necessary code. Hence it is necessary to source your workspace every time you open a new shell.1.6 ROS PackagesDocumentation - http://wiki.ros.org/ROS/Tutorials/CreatingPackagePackages are projects that contain your ROS work. A workspace can have several packages, and all of them are located inside the src directory.For a package to be considered a catkin package, it must meet the following requirements- Must contain package.xml file - provides meta information about the package Must contain CMakeLists.txt file - Describes how to build the code and where to install it Each package must have its own folder All packages must be created inside ~/catkin_ws/src1.6.1 Creating a catkin packageTo create a package the following syntax can be used:catkin_create_pkg &lt;package_name&gt; [depend1] [depend2] [depend3]On creating a new package, the CMakeLists.txt file, package.xml file and the src directory come built-in automatically.1.6.2 Building the catkin workspaceOnce a package has been created, it needs to be compiled in order for it to work. catkin_make - Will compile the entire src directory and needs to be issued only in the catkin_ws directory. catkin_make --only-pkg-with-deps &lt;package_name&gt; - Will only compile the selected package. To add your workspace to the ROS environment, you need to source the generated setup file.Once the package has been created, the Python nodes can be saved in the src directory of the package.1.6.3 Package dependenciesrospack depends1 beginner_tutorials returns a list of the first-order dependenciesrospack depends beginner_tutorials returns a list of all dependencies, direct and indirect.These dependencies are stored in the package.xml file.1.7 package.xmlDocumentation: http://wiki.ros.org/catkin/package.xmlThe package.xml file provides meta information about the package such as package name, version number, authors, etc.What does a package.xml file contain? &lt;name&gt;¬†- The name of the package &lt;version&gt; - The version number of the package (required to be 3 dot-separated integers) &lt;description&gt; - A description of the package contents &lt;maintainer&gt; - The name of the person(s) that is/are maintaining the package &lt;license&gt; - The software license(s) under which the code is released.1.7.1 DependenciesThese four types of dependencies are specified using the following respective tags: &lt;buildtool_depend&gt;: Build Tool Dependencies specify build system tools which this package needs to build itself. Typically the only build tool needed is catkin. &lt;build_depend&gt;: Build Dependencies specify which dependencies are needed to build this package. &lt;run_depend&gt;: Run Dependencies specify which dependencies are needed to run code in this package, or build libraries against this package. &lt;test_depend&gt;: Test Dependencies specify only additional dependencies for unit tests. They should never duplicate any dependencies already mentioned as build or run dependencies.ROS Tools2.1 rosbashDocumentation- http://wiki.ros.org/rosbashrosbash is a package that contains some useful bash functions and adds tab-completion to a large number of the basic ROS utilities. When you source your setup file, you will implicitly get all bash-specific commands.rosbash includes the following command-line utilities: roscd - change directory starting with package, stack, or location name rospd - pushd equivalent of roscd rosd - lists directories in the directory-stack rosls - list files of a ros package rosed - edit a file in a package roscp - copy a file from a package rosrun - run executables of a ros package rospd:rospd is the pushd equivalent of roscd, that allows you to navigate between different ros directories by keeping the multiple locations in a directory-stack, and allowing you to jump back to a ros directory that you were previously working on. rosed:rosed allows you to edit files in a ROS package by typing the package name and the name of the file that you want to edit. roscp:roscp allows you to copy a file from a ROS package by specifying the package name and the name of the file that you want to copy. 2.2 Common File system tools rospack: Documentation- https://docs.ros.org/en/independent/api/rospkg/html/rospack.html rospack is a command-line tool that is used to get information about ROS packages available on the filesystem. Below are listed some common rospack options- rospack find - returns the absolute path to the package rospack depends - returns a list of all the package‚Äôs dependencies, direct and indirect rospack depends1 - returns a list of the package‚Äôs primary dependencies rospack depends-on - returns a list of all the packages that depend on the given package rospack export - returns flags necessary for building and linking against a package rospack list - returns a list of all ROS packages on the filesystem roscd: roscd allows us to change directory or subdirectory using a package name, stack name, or special location. You can only move to the packages installed into your ROS system. roscd log will take you to the ROS directory that contains log files. If no ROS program has been run yet, it will yield an error saying that it does not exist. rosls: rosls allows you to view the contents of a package, stack or location. rosparam: 2.3 rosrunrosrun is a ROS command-line utility that searches for a package for the requested program and runs it.Syntax-rosrun &lt;package&gt; &lt;executable&gt;rosrun displays a sequence of timestamps, which in the above picture, prints the string ‚ÄòHello World‚Äô 10 times per second. This reduced frequency helps in recognizing any changes in the messages.2.4 roslaunchAlthough rosrun is great for starting single ROS nodes, most robot systems end up running tens or hundreds of nodes simultaneously. Since it would not be practical to call rosrun on each of these, ROS includes a tool for starting collections of nodes, called roslaunch.roslaunch is a command line tool that helps run several nodes at a time, instead of using rosrun for each individual node.Syntax-roslaunch PACKAGE LAUNCH_FILEroslaunch operates on launch files instead of nodes. Launch files are XML files that are a collection of nodes along with their topic remappings and other parameters. These files have the suffix .launch.roslaunch includes several other features such as the ability to launch programs on other computers in the network via ssh, to automatically respawn nodes that crash, etc.roslaunch will also automatically run roscore, if it doesn‚Äôt already exist. However, pressing Ctrl+C will exit roscore as well, along with roslaunch.2.4.1 Creating a launch file&lt;launch&gt; &lt;group ns=\"turtlesim1\"&gt; &lt;node pkg=\"turtlesim\" name=\"sim\" type=\"turtlesim_node\"/&gt; &lt;/group&gt; &lt;group ns=\"turtlesim2\"&gt; &lt;node pkg=\"turtlesim\" name=\"sim\" type=\"turtlesim_node\"/&gt; &lt;/group&gt; &lt;node pkg=\"turtlesim\" name=\"mimic\" type=\"mimic\"&gt; &lt;remap from=\"input\" to=\"turtlesim1/turtle1\"/&gt; &lt;remap from=\"output\" to=\"turtlesim2/turtle1\"/&gt; &lt;/node&gt;&lt;/launch&gt;The launch tag &lt;launch&gt; is used to identify the file as a launch file.Lines 3-9: starts two instances of the same nodeLines 11-14: start the mimic node with the topics input and output renamed to turtlesim1 and turtlesim2. This renaming will cause turtlesim2 to mimic turtlesim1.2.5 ROS NodesA node is a ROS program that performs a certain task. Differnent nodes communicate with each other by sending messages through topics.2.5.1 Launching a nodeSyntax-rosrun &lt;package_name&gt; &lt;node_name&gt; __name:=new_node_name __ns:=name_space topic:=new_topic2.5.2 Controlling 2 turtles using different keyboards NOTE - We cannot have 2 turtlesims in the same namespace. Take a look at the below image to see what happens if we initialize 2 turtlebots in the same namespace.2.5.3 Using rosnodeThe rosnode command is used to display information about ROS nodes that are currently running. rosout documentation - http://wiki.ros.org/rosoutROS TopicsROS Topics represent a stream of messages that connect two or more nodes. The nodes act as publishers or subscribers of messages - publisher nodes send messages through topics and subscriber nodes receive the messages transmitted through a particular topic. A publisher node would have to register the topic name and the type of messages, and only then it can publish to a topic. A subscriber node would make a request to roscore to get details about a topic in order to receive the transmitted messages sent through it. In ROS, all messages on the same topic must have the same data type.Any number of nodes can publish to a topic as long as they have the same message type. A ROS Topic cannot be published without initializing a ROS Node.3.1 Using rostopicThe rostopic command is used to display information about ROS topics that are currently running.rostopic hz reports the rate at which data is being published. The below image tells us that turtlesim is publishing data about our turtle at the rate of 60 Hz.ROS Messages4.1 ROS msgThe message definition file has an extension of .msg and all such files need to be located inside the msg directory of a package.4.2 Creating a ROS msgNow we need to make sure that the msg files are turned into source code for Python, C++, and other languages.Open package.xml and make sure the following lines are uncommented.&lt;build_depend&gt;message_generation&lt;/build_depend&gt;&lt;build_export_depend&gt;message_runtime&lt;/build_export_depend&gt;&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;Now we need to make changes to the CMakeLists.txt file.# Modify the existing textfind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation)catkin_package( ... CATKIN_DEPENDS message_runtime ... ...)add_message_files( FILES Message1.msg Message2.msg)generate_messages( DEPENDENCIES std_msgs)Now since we have made a few changes to our package and created new files/directories, we need to compile out workspace by running catkin_make or catkin_make --only-pkg-with-deps &lt;package_name&gt;.4.3 Using rosmsgDocumentation - http://wiki.ros.org/rosmsg4.4 Publishing to a Topic#! /usr/bin/python2import rospyfrom std_msgs.msg import Stringimport timerospy.init_node('Node_name',anonymous=True)pub = rospy.Publisher('/orchis_dark', String, latch=True, queue_size=10)r = rospy.Rate(10)while True:\tpub.publish('Current time: {}'.format(time.time()))\tr.sleep()4.4.1 Code explanation:Line 1: #! /usr/bin/python2 is known as the shebang. It lets the kernel know that this is a Python file and that it should be passed into the Python interpreter.Line 3: import rospy appears in every ROS node and imports some basic functionalities, classes and functions.Line 4: this line allows us to reuse the std_msgs/String message type for publishing.Line 8: This is used to initialize a ROS node and the node takes the name that you give it. anonymous = True ensures that you have a unique name by adding random numbers to the end of name.Line 9: pub here is an object of the class Publisher, and allows us to publish to any topic of any message type that you give it. queue_size limits the amount of queued messages if any subscriber is not receiving them fast enough.Line 11: This line creates an object r of the class Rate. When r.sleep() is called, it sleeps just long enough for the loop to run at the desired rate. When the argument 10 is passed, it goes through the loop 10 times per second.Line 14: This is used to publish the desired message to the topic.rostopic hz tells us the rate at which messages are being published. Here we see that it is being published at a rate specified by the argument in rospy.Rate(arg).4.5 Subscribing to a Topic#! /usr/bin/python2import rospyfrom std_msgs.msg import Stringrospy.init_node('simple_subscriber', anonymous=True)def function(my_string):\tprint(my_string.data)\trospy.Subscriber('/force', String, function)rospy.spin()4.5.1 Code explanation:Line 1: #! /usr/bin/python2 is known as the shebang. It lets the kernel know that this is a Python file and that it should be passed into the Python interpreter.Line 3: import rospy appears in every ROS node and imports some basic functionalities, classes and functions.Line 4: this line allows us to reuse the std_msgs/String message type for publishing.Line 6: This is used to initialize a ROS node and the node takes the name that you give it. anonymous = True ensures that you have a unique name by adding random numbers to the end of name.Line 8-9: This is a callback function. Once a node has subscribed to a topic, everytime a message arrives on it, the associated callback function is called with the message as it‚Äôs parameter.Line 11: In this line, we subscribe to the topic, giving it the name of the topic, message type and callback function as its arguments.Line 13: This basically instructs ROS to loop over again, once a subscription has been made.Behind the scenes, the subscriber passes this information on to roscore and tries to make a direct connection with the publishers of this topic. If the topic does not exist, or if the type is wrong, there are no error messages: the node will simply wait until messages start being published on the topic. A subscriber callback function is not executed continuously, that is, it is not processing all the time. It will only process when new data is published.On VS Code - Output:ROS Services5.1 What are Services?Services are synchronous calls, which when called by one node executes a function in another node. They are used only when a function/task needs to be executed occassionally, say when a robot needs to perform a very discrete task such as taking a high resolution picture using a camera, etc. Synchronous refers to an intereference with time, and means the that functions are performed one after the other. Messages, on the other hand, are asynchronous which means that they branch out into dfferent functions that execute simultaneously.A Server provides a service by responding to a service call, and a Client requests for a service and accesses the service response. What is the difference between Services and Messges? ROS messages, which are transported using publishers and subscribers, are used whenever we need data to flow constantly and when we want to act on this data asynchronously.ROS services, on the other hand, are used only when we require data at specific time or when we want a task to be executed only at particular instances. To better understand this, take a look at the following example. We have a robot that simulates it‚Äôs environment in real-time. In such cases, we would use publishers/subscribers to send messages as that data flow needs to be constant. Also, we would want the robot to do other tasks as well, apart from just reading real-time data. If we use services, then the server/client would have to wait for a response/request and blocks the other code in the node, preventing other tasks from being executed, We have a robot that detects people in front of it. We would use services here as the node will wait for a person to come in front of it, then sends a request to the server and blocks the code while waiting for a response. Using messages here is pointless as we don‚Äôt want to continuously check for people in front of the robot (it‚Äôs just a one-time task). Reference: https://stackoverflow.com/questions/29458467/ros-service-and-message#:~:text=It%20is%20a%20similar%20example,the%20rest%20of%20its%20job.5.2 Service filesService files have an input and output call, and have the extension .srv. These files are present in the srv directory of a package. Here‚Äôs how a typical service file is definedint 64 aint 64 b---int64 sumIn order for the service files to run, a few changes have to be made in the package.xml file and CMakeLists.txt file.In package.xml# The following lines need to be added at the end of the file &lt;build_depend&gt;message_generation&lt;/build_depend&gt; &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;In CMakeLists.txtModify the file to make the following changes, if not done earlier# Modify the existing linefind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation) message_generation was added in both the files and is done while creating messages as well. It works for both msg and srv.add_service_files( FILES Service1.srv Service2.srv)After the necessary changes are made, we need to run catkin_make or catkin_make --only_pkg_with_deps &lt;package_name&gt; to compile the workspace or a particular package.Running catkin_make will create 2 additional classes for the service file as well. For example, if a service file is named add_two_int.srv, then the two classes add_two_intResponse and add_two_intRequest are created along with the previously existing add_two_int class. These classes allow us to interact with the service by calling for a Response or a Request.5.3 Using rossrvDocumentation - http://wiki.ros.org/ROS/Tutorials/CreatingMsgAndSrv5.4 Using rosservice Difference between rossrv and rosservice rossrv is a tool that provides us with information about all files ending in .srv rosservice is a tool that allows us to interact with Servers and Clients that are currently active. Reference - https://answers.ros.org/question/349148/rossrv-vs-rosservice/5.5 Creating a simple ServerBefore we get into creating a server, let us take a look at out service definition file.The same info can be gathered using the rossrv command, as shown below.Let us now create a server file, which would act as a python node for providing the service.Here‚Äôs the code for creating a simple server.Code:#! /usr/bin/python2import rospyfrom beginner_tutorials.srv import add_two_int,add_two_intResponseresponse = add_two_intResponse()def add_ints(req): print('Adding {} + {} '.format(req.a,req.b)) response.sum = req.a + req.b return response.sumrospy.init_node('add_two_int_server')srv = rospy.Service('add_two_int',add_two_int,add_ints)print('Server is ready')rospy.spin()5.5.1 Code ExplanationWe import the add_two_intResponse class as we are writing code for a Server, which provides a response for the service that is called.response is an object of the class add_two_intResponse().srv = rospy.Service('add_two_int',add_two_int,add_ints) This line creates an object named srv and declares/starts a service. The new service is given the name ‚Äòadd_two_int‚Äô and has the service type add_two_int. add_ints is a callback function that performs a desired task.The function add_ints takes a variable as a request and returns the sum of ‚Äòa‚Äô and ‚Äòb‚Äô.Execution:5.6 Creating a simple ClientLike all python nodes, the client service file should also be created under the src directory of a package.Code:#! /usr/bin/python2import rospyfrom beginner_tutorials.srv import add_two_int, add_two_intRequestreq = add_two_intRequest()req.a = 4req.b = 5rospy.wait_for_service('add_two_int')add_two_ints = rospy.ServiceProxy('add_two_int',add_two_int)response = add_two_ints(req)print(response.sum)5.6.1 Code ExplanationThe class add_two_intRequest is imported in a client node as we need to send a Request to the Server.req = add_two_intRequest() : req is an object of the class add_two_intRequest(). The inputs for the object are given in the next two lines, which are to be sent to the server.rospy.wait_for_service('add_two_int'): This is a method that blocks until the service named ‚Äòadd_two_int‚Äô is available.add_two_ints = rospy.ServiceProxy('add_two_int',add_two_int): This is how we call a service of the service name add_two_int and service type add_two_int. What is ServiceProxy? Before we get into that, what is a Proxy ?A proxy is a gateway between a client and a server (usually a website) that takes a request and performs a function. In a similar way, a ServiceProxy acts as an intermediary between a Client and a Server. It takes in the request from a Client and sends it to the Sever where it is passed into a function.response = add_two_ints(req): The request is passed into the service call line, and is sent to the server which returns the desired output.print(response.sum): The sum is then printed on the screen. NOTE - The server node needs to be running while a client node is being executed.Execution:Refer to the server code above to better understand how the service is being provided and why it returns the above outputs.5.7 A deeper dive into ServicesLet us take a look at a few other interesting things that we can do using services.5.7.1 Spawning 2 turtles in the same nodeHere‚Äôs a list of the services associated with turtlesim.The one which we are particularly interested in, to spawn 2 turtles in the same node, is the service named ‚Äò/spawn‚Äô. Here‚Äôs some info about the service ‚Äò/spawn‚Äô.In order to spawn two turtles, we call a service and pass in the necessary parameters, which is demonstrated below5.7.2 Getting laser scan data using gazeboSpawn your turtlebot3 on gazebo by running the following commandroslaunch turtlebot3_gazebo turtlebot3_empty_world.launchPlace a block in front of your turtlebot3, as shown belowLet us take a look at the various topics associated with gazeboHere, we find a topic named /scan which is pretty interesting. To learn more about this topic, let us do a rostopic info, as shown below.Let us now take a look at what this message sensors_msgs/LaserScan has to offer.Thus, this message provides us with several things related to the sensors output, and more importantly, gives us an array of all ranges of the laser scan. This is something that can be incredibly useful. However, subscribing to this would provide us with a list of 360 values (for 360 degrees), which is kinda unnecessary. Using services, we can have a synchronous callback function that provides us with data only for discrete inputs, such as giving us the range for a particular angle.Let us now call for a service, by creating a server and a client. Before we do this, we would obviously need a service definition file present in the srv directory.Create a file named gazebo_server.srv under the srv directory, and have the following lines.int64 direction---float32 distanceint64 direction would serve as an input while float32 distance serves as the output.We need to make the necessary changes to package.xml file and CMakeList.txt file. This has been discussed above.Creating a ServerCode#! /usr/bin/python2import rospyfrom sensor_msgs.msg import LaserScanfrom beginner_tutorials.srv import gazebo_server, gazebo_serverResponserospy.init_node('My_Node', anonymous=True)ls_obj = LaserScan()response = gazebo_serverResponse()def callback(val): global ls_obj ls_obj = val def func_serv(request): global ls_obj response.distance = ls_obj.ranges[request.direction] return response.distancesub = rospy.Subscriber('/scan', LaserScan, callback)serv = rospy.Service('my_service', gazebo_server, func_serv)rospy.spin()Explanationls_obj = LaserScan(), response = gazebo_serverResponse() : Objects are created for the respective classessub = rospy.Subscriber('/scan', LaserScan, callback) : Subscribing to the topic /scan of the message type LaserScan, and the data from this is passed into the function named callback which stores the data in the object ls_obj.serv = rospy.Service('my_service', gazebo_server, func_serv) : A service is started with the name my_service, service type gazebo_server, and a callback function named func_serv which returns the desired output based on the request.ExecutionCreating a ClientCode#! /usr/bin/python2import rospyfrom sensor_msgs.msg import LaserScanfrom beginner_tutorials.srv import gazebo_server, gazebo_serverRequestrospy.init_node('client_node')rospy.wait_for_service('my_service')req = gazebo_serverRequest()req.direction = 1srv = rospy.ServiceProxy('my_service', gazebo_server)r = rospy.Rate(5)while not rospy.is_shutdown(): result = srv(req) print(result.distance) r.sleep()Explanationrospy.wait_for_service('my_service') : Waits for the service to start, which in our case is initialized by the server.req = gazebo_serverRequest() : An object is created and the desired request is passedsrv = rospy.ServiceProxy('my_service', gazebo_server) : Calling for a service of the service name my_service and type gazebo_serverresult = srv(req) : The request is passed into the proxy, which is sent to the server and executes the desired function.print(result.distance) : The distance is printed on the screen. NOTE - The server node needs to be running while a client node is being executed.ExecutionROS Actions6.1 IntroductionActions are asynchronous calls in ROS, which means that you do not have to wait for a particular task to be finished and you can also do other tasks simultaneously.Reference: http://wiki.ros.org/actionlib What is the difference between Actions and Services? Actions are asynchronous calls which would perform almost the same functions as Services (kinda), but additionally allows you to work on other tasks in the node (i.e. no blocking code). It also allows you to get feedback, status and even cancel a call. You can better understand this with an analogy, as explained in this link. In some cases, if a service takes a long time to execute, the user might want the ability to cancel the request during execution or get periodic feedback about how the request is progressing. The actionlib package provides tools to create servers that execute long-running goals that can be preempted. It also provides a client interface in order to send requests to the server.A more detaied description: http://wiki.ros.org/actionlib/DetailedDescriptionThe ActionClient and ActionServer communicate via a ‚ÄúROS Action Protocol‚Äù, which is built on top of ROS messages. The client and server then provide a simple API for users to request goals (on the client side) or to execute goals (on the server side) via function calls and callbacks.There are 5 Topics provided by an Action Server: goal: User (or Client) sends a goal to the Server to initiate the action. cancel: User sends a signal under cancel topic to interrupt or stop an action from being executed. status: Tells the current status of the server. There are 10 status states - PENDING, ACTIVE, PREEMPTED, SUCCEEDED, ABORTED, REJECTED, PREEMPTING, RECALLING, RECALLED and LOST. result: Provides the final output after executing the action. feedback: Provides us with intermediate results about the action while it is being executed. NOTE - We cannot execute two actions at the same time. Doing so will cancel the previous action from being executed. If a new goal is sent to an action server that is already busy processing a goal, then the currently active goal will be pre-empted. However, this is not a hard limitation as we can have an action server that can process multiple goals.6.2 Action filesThe Action definition files have an extension of .action and is used to specify the format of goal, result, feedback message. These files are present in the action directory of a package. Here‚Äôs how a typical action file is definedint32 goal---int32 result---int32 feedbackIn order for the service files to run, a few changes have to be made in the package.xml file and CMakeLists.txt file.In package.xmlModify the file to make the following changes, if not done earlier&lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt; &lt;build_depend&gt;rospy&lt;/build_depend&gt; &lt;build_depend&gt;std_msgs&lt;/build_depend&gt; &lt;build_depend&gt;actionlib_msgs&lt;/build_depend&gt; &lt;build_depend&gt;actionlib&lt;/build_depend&gt; &lt;build_export_depend&gt;rospy&lt;/build_export_depend&gt; &lt;build_export_depend&gt;std_msgs&lt;/build_export_depend&gt; &lt;build_export_depend&gt;actionlib_msgs&lt;/build_export_depend&gt; &lt;build_export_depend&gt;actionlib&lt;/build_export_depend&gt; &lt;exec_depend&gt;rospy&lt;/exec_depend&gt; &lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;actionlib_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;actionlib&lt;/exec_depend&gt; &lt;build_depend&gt;message_generation&lt;/build_depend&gt; &lt;build_export_depend&gt;message_runtime&lt;/build_export_depend&gt; &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;In CMakeLists.txtModify the file to make the following changes, if not done earlier# Modify the existing linefind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation actionlib_msgs)# Generate actions in the 'action' folderadd_action_files( FILES my_action.action)## Generate added messages and services with any dependencies listed heregenerate_messages( DEPENDENCIES std_msgs actionlib_msgs)# Add actionlib_msgs as a catkin dependencycatkin_package( CATKIN_DEPENDS rospy std_msgs message_runtime actionlib_msgs)After the necessary changes are made, we need to run catkin_make or catkin_make --only_pkg_with_deps &lt;package_name&gt; to compile the workspace or a particular package.After compiling a package, ROS wil create additional messages for you. For example, if your action definition file is named robot.action, then the following messages will be created: robotAction.msg robotActionGoal.msg robotActionResult.msg robotActionFeedback.msg robotGoal.msg robotResult.msg robotFeedback.msgThese messages are used by actionlib to facilitate communication between ActionServer and ActionClient.The topics used in actions (mentioned above) will use one of these messages, whichever suits its function. In general, if you‚Äôre using the libraries in the actionlib package, you should not need to access the autogenerated messages with Action in their type name. The bare Goal, Result, and Feedback messages should suffice. The others are used internally by ROS, i.e., by the actionlib package. Let‚Äôs take a closer look at the counterAction message type.Once the goal is registered by the action server, the request is first given a time stamp and header information by actionlib indicating when and which action client requested this goal. Then the actionlib package also assigns a unique goal identifier to this goal along with a time stamp. Finally we have the goal message that we sent from our action client. Now the same goes with the action result.Inside the actionlib package, there is a state machine that is started to process the goal request that we sent. This state machine can lead to the requested goal being in different states and provides us with the status message.The same goes with the action feedback again, but now with a different state machine, and it also has the content of the feedback message.Let us create a custom action file and call it counter.action.# Goalint32 num_counts---# Resultstring result_message---# Feedbackint32 counts_elapsedCreating a simple ServerCode:#! /usr/bin/python3import rospy# import actionlib library used for calling actionsimport actionlib# import custom action file. Since actions are based on messages, notice how we import actions from the msg directory of a package.from beginner_tutorials.msg import counterAction, counterResult, counterFeedbackclass Server(): def __init__(self): # create a simple ActionServer by passing the name, action type, and callback function as its parameters. # auto_start has to be declared explicitly and always has to be set to False to prevent autostarting the server (can break your code otherwise). self.server = actionlib.SimpleActionServer('my_action_server', counterAction, self.counter, auto_start=False) # start server self.server.start() rospy.loginfo(\"Action server started\") def counter(self,goal): self.res = counterResult() self.feedback = counterFeedback() # initializing the feedback variable to 0 self.feedback.counts_elapsed = 0 # for 1s delay r = rospy.Rate(1) self.res.result_message = \"Counting complete!\" # start counting till the goal for i in range(0, goal.num_counts): success = True # check that preempt has not been requested by the user if self.server.is_preempt_requested(): rospy.loginfo(\"my_action_server: Preempted\") self.server.set_preempted() success = False break # publish the feedback self.feedback.counts_elapsed = i self.server.publish_feedback(self.feedback) # wait for 1s before incrementing the counter r.sleep() if success == True: # Once the necessary function is executed, the server notifies the client that the goal is complete by calling set_succeeded. self.server.set_succeeded(self.res)rospy.init_node(\"counter_action_server\")# initialize object called server to call the Server() class.server = Server()rospy.spin()Execution:rosrun &lt;package_name&gt; &lt;file_name&gt;Send goals via terminal:rostopic pub /action_server_name/goal /package_name/action_message_type parametersCreating a simple ClientCode:#! /usr/bin/python3import rospy# import actionlib libraryimport actionlib# import custom action filefrom beginner_tutorials.msg import counterAction, counterGoaldef Client(): # create client and specify the name of server and action message type client = actionlib.SimpleActionClient('my_action_server', counterAction) rospy.loginfo(\"Waiting for server\")\t# wait for server (name specified above) client.wait_for_server() goal = counterGoal() goal.num_counts = 20 \t# send goal to server client.send_goal(goal) # client.send_goal(goal, feedback_cb=feedback_func) rospy.loginfo(\"Goal has been sent to the action server\") # can perform other tasks here as well # wait for result\tclient.wait_for_result() return client.get_result()while not rospy.is_shutdown(): # initialize node rospy.init_node(\"counter_action_client\") r = rospy.Rate(1) \t# call the function and print result res = Client() print(res) r.sleep()Explanationclient.wait_for_result(): This will wait until the action is complete and blocks the remaining code. This won‚Äôt allow you to continue to work on your thread.client.get_state(): It returns an integer that specifies the state of the action. There are 10 possible states, a few of which are 0 implies PENDING 1 implies ACTIVE 2 implies PREEMPTED 3 implies SUCCEEDED 4 implies ABORTEDIf your get_state() is less than 2, it indicates that your action is still not complete.client.cancel_goal(): Used to cancel or preempt a goal.ExecutionTurtlesim exampleAs you can see below, running the following commands instructs turtlesim to move along a pentagon.On running rostopic list we see a list of all the current topics. /turtle_shape/goal is one such topic, which allows you to publish messages to the server through the terminal. It allows us to enter the number of edges and define the radius as well.GUIThe actionlib offers a graphical way to send goal to action server.Syntax-rosrun actionlib axclient.py /name_of_the_actionThe GUI has areas for Goal, Feedback and Result. You can press the SEND GOAL button to send goals with the relevant parameters, and you can also cancel or preempt a goal anytime with the CANCEL GOAL button. After the action finished successfully, the GUI shows the Result.ROS TF27.1 Introduction to TF2TF2 is the second generation of the transform library, which lets the user keep track of multiple coordinate frames over time. TF2 maintains the relationship between coordinate frames over time, and lets the user transform points, vectors, etc. between any two coordinate frames at any desired point in time.TF2 is not a centralized service, but is a distributed protocol with which many different robots will communicate about the state of the robot.In TF2, Publishers are known as Broadcasters, and Subscribers are known as Listeners. Listeners listen to /tf and cache all data heard up to the cache limit. Broadcasters publish transforms between coordinate frames on /tf.7.1.1 Applications of transformations between frames:The following are some of the applications where the tf2 library can be used: Compute inverse and forward kinematics of multi-joint robots Carry out obstacle avoidance Convey location of robot or parts of a robot Convert sensor data from one reference to another Control robot about a particular point in space Analyze multiple robot data in world frame Reference external objects w.r.t robot frame7.2 Demo using turtlesimEnter the following command to install the necessary dependencies and compile the demo package.sudo apt-get install ros-$ROS_DISTRO-turtle-tf2 ros-$ROS_DISTRO-tf2-tools ros-$ROS_DISTRO-tfAfter compiling the turtle_tf2 tutorial package, enter the following the run the demo:roslaunch turtle_tf2 turtle_tf2_demo.launchOnce the turtlesim is started you can drive the center turtle around in the turtlesim using the keyboard arrow keys. On moving the center turtle, you will see that the other turtle also moves continuously to follow the turtle that you are driving around.On entering the above command, you are essentially executing two main things: A TF2 broadcaster that is publishing the coordinate frames of both the turtles w.r.t the world frame. A TF2 listener that reads the transformations and uses it to calculate the direction in which turtle2 has to move to follow turtle1.7.2.1 TF2 toolsview_frames:view_frames is one of the TF2 tools that generates a diagram of the frames being broadcast by TF2 over ROS with the current TF2 tree. This diagram is stored as a pdf.Syntax to run view_frames:rosrun tf2_tools view_frames.pyThis is what you will see:Listening to tf data during 5 seconds...Generating graph in frames.pdf file...To view the tree diagram, we just need to open the pdf. If evince is your default document viewer, then run the command evince frames.pdfOn opening the pdf, we see a very simple tree diagram that depicts three frames - world frame, turtle1 frame, and turtle2 frame. The world frame is transformed to the turtle1 and to turtle2, seperately. We can see the Average rate, which is the publishing rate We can also see the most recent transform number, which should more or less coincide with the recording time (otherwise it is not publishing correctly)You can also listen to the TF being published, using the echo. There is a topic named /tf where ALL the TF are published. In simple systems like this one there is no problem, but as the system becomes more sophisticated, the quantity of data can be overwhelming. To tackle this, tf2 library provides you with tools that filters which tranformation you are interested in and just shows you that one.Running the following commands shows you the TF of the respective turtle only w.r.t the world frame.tf_echo:If you want to see the transform change as the two turtles move relative to each other, you can use tf_echo. tf_echo reports the transform between any two frames broadcast over ROS.Syntax:rosrun tf tf_echo [reference_frame] [target_frame]On running rosrun tf tf_echo turtle1 turtle2 you get the following information: Relative translation between the two turtles Relative rotation between the two turtles in terms of Quaternions, RPY (Roll Pitch Yaw)tf_monitor:7.2.2 Better visualization - rvizrviz is a visualization tool that is useful for examining tf2 frames.To open the turtle_tf2 file on rviz, enter the command: rosrun rviz rviz and open the relevant file. This is how your rviz environment will typically look like. Reference - https://youtu.be/Ra-nXIfPWdg7.3 Quaternion and Roll-Pitch-Yaw Conversion#! /usr/bin/python2import rospyimport tf2_rosimport tffrom geometry_msgs.msg import Twistfrom nav_msgs.msg import Odometryimport mathroll = math.radians(30)pitch = math.radians(50)yaw = math.radians(75)print('Roll: {}'.format(math.degrees(roll)))print('Pitch: {}'.format(math.degrees(pitch)))print('Yaw: {}'.format(math.degrees(yaw)))quaternion = tf.transformations.quaternion_from_euler(roll, pitch, yaw)print(\"\\nResulting quaternions:\")for i in range(0, 4): print(quaternion[i])ori = tf.transformations.euler_from_quaternion(quaternion)print('\\nEuler from quaternion:')for i in range(0, 3): print(math.degrees(ori[i]))\tExecution:7.4 TF2 Publisher/BroadcasterWe shall first create a new catkin package called learning_tf2 to better understand and demonstrate writing code for publishers and subscribers.Syntax to create the new package:catkin_create_pkg learning_tf2 tf2 tf2_ros roscpp rospy turtlesimCreate a new directory called nodes inside the learning_tf2 package. We will be storing all our python nodes here.To create a broadcaster, make a new file called tf2_broadcaster.py under the nodes directory.Code:#! /usr/bin/python2# import rospyimport rospy# import tf2 moduleimport tf2_ros# import tfimport tfimport geometry_msgs.msgimport turtlesim.msgdef turtle_func(msg, turtlename): # TransformBroadcaster makes publishing of transforms easy. To use the TransformBroadcaster, we need to import the tf2_ros module. br = tf2_ros.TransformBroadcaster() # We create a TransformStamped object which will contain the message to be published. t = geometry_msgs.msg.TransformStamped() # Before stuffing the actual transform values we need to give the TransformStamped object the appropriate metadata. t.header.stamp = rospy.Time.now() # We need to give the transform being published a timestamp, which in our case will be the current time t.header.frame_id = \"world\" # name of parent frame t.child_frame_id = turtlename # name of child frame t.transform.translation.x = msg.x t.transform.translation.y = msg.y t.transform.translation.z = 0.0 # convert angles from euler (radians/degrees) to quaternion q = tf.transformations.quaternion_from_euler(0, 0, msg.theta) t.transform.rotation.x = q[0] t.transform.rotation.y = q[1] t.transform.rotation.z = q[2] t.transform.rotation.w = q[3] br.sendTransform(t) # publish the transformif __name__=='__main__': rospy.init_node('tf2_broadcaster') # create node turtlename = rospy.get_param(\"~turtle\") rospy.Subscriber('%s/pose' % turtlename, turtlesim.msg.Pose, turtle_func, turtlename) rospy.spin()Execution:To run the broadcaster, we first need to create a launch file. Inside your package, create a folder called launch, create a file called start_demo.launch.&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt;&lt;/launch&gt;To execute, just run the command roslaunch learning_tf2 start_demo.launch.Rviz:7.5 TF2 Subscriber/Listener#! /usr/bin/python2# import rospyimport rospy# import tf2 moduleimport tf2_ros# import math moduleimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawn# initialize listener noderospy.init_node(\"tf2_listener\")# A listener object is created. Once the listener is created, it starts receiving tf2 transformations, and buffers them for up to 10 seconds.tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)# Spawn another turtle in the same turtlesim node (Refer to https://github.com/Bhaswanth-A/ROS-Theory/blob/main/Services.md#571-spawning-2-turtles-in-the-same-node )rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try: # Gets the transformation from source frame to target frame (change turtle1 to carrot1 for frames example) trans = tfBuffer.lookup_transform(turtle_name, 'turtle1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue # some math angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular # publish the pose pub.publish(cmd) r.sleep()Execution:Change the launch file start_demo.launch to this (only 1 line is added, nothing else is changed from before).&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt;&lt;/launch&gt;To execute, just run the command roslaunch learning_tf2 start_demo.launch.If you drive around turtle1 using your keyboard, you‚Äôll find the second turtle following the first one.Rviz:7.6 Adding framesCreate a new file called add_frame_1.py under the nodes directory.7.6.1 Example 1Broadcasterimport rospyimport tf2_rosfrom geometry_msgs.msg import TransformStampeddef frames(): br = tf2_ros.TransformBroadcaster() t = TransformStamped() t.header.stamp = rospy.Time.now() t.header.frame_id = \"turtle1\" t.child_frame_id = \"carrot1\" rate = rospy.Rate(10.0) while not rospy.is_shutdown(): t.header.stamp = rospy.Time.now() t.transform.translation.x = -2.0 t.transform.translation.y = 0.0 t.transform.translation.z = 0.0 t.transform.rotation.x = 0.0 t.transform.rotation.y = 0.0 t.transform.rotation.z = 0.0 t.transform.rotation.w = 1.0 br.sendTransform(t) rate.sleep()if __name__ == \"__main__\": rospy.init_node('frames') frames()We create a new transform, from the parent ‚Äúturtle1‚Äù to the new child ‚Äúcarrot1‚Äù. The carrot1 frame is 2 meters offset from the turtle1 frame.Listener#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawnrospy.init_node(\"tf2_listener\")tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try:\t\t# parent frame changed to carrot1 trans = tfBuffer.lookup_transform(turtle_name, 'carrot1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular pub.publish(cmd) r.sleep()Launch file&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt; &lt;node pkg=\"learning_tf2\" type=\"add_frame_1.py\" name=\"broadcaster_frames\" output=\"screen\" /&gt;&lt;/launch&gt;7.6.2 Example 2Broadcaster#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import TransformStampeddef frames(): br = tf2_ros.TransformBroadcaster() t = TransformStamped() t.header.stamp = rospy.Time.now() t.header.frame_id = \"turtle1\" t.child_frame_id = \"carrot1\" rate = rospy.Rate(10.0) while not rospy.is_shutdown(): x = rospy.Time.now().to_sec() * math.pi t.header.stamp = rospy.Time.now() t.transform.translation.x = 10 * math.sin(x) t.transform.translation.y = 10 * math.cos(x) t.transform.translation.z = 0.0 t.transform.rotation.x = 0.0 t.transform.rotation.y = 0.0 t.transform.rotation.z = 0.0 t.transform.rotation.w = 1.0 br.sendTransform(t) rate.sleep()if __name__ == \"__main__\": rospy.init_node('frames') frames()Instead of a fixed definition of our x and y offsets, we are using the sin and cos functions on the current time so that the offset of carrot1 is constantly changing.Listener#! /usr/bin/python2import rospyimport tf2_rosimport mathfrom geometry_msgs.msg import Twistfrom turtlesim.srv import Spawnrospy.init_node(\"tf2_listener\")tfBuffer = tf2_ros.Buffer()listener = tf2_ros.TransformListener(tfBuffer)rospy.wait_for_service('spawn')spawner = rospy.ServiceProxy('spawn', Spawn)turtle_name = rospy.get_param('turtle', 'turtle2')spawner(4.0, 2.0, 0.0, turtle_name)pub = rospy.Publisher('%s/cmd_vel' % turtle_name, Twist, latch=True, queue_size=1)r = rospy.Rate(10)while not rospy.is_shutdown(): try:\t\t# parent frame changed to carrot1 trans = tfBuffer.lookup_transform(turtle_name, 'carrot1', rospy.Time()) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continue angular = 4 * math.atan2(trans.transform.translation.y, trans.transform.translation.x) linear = 0.5 * math.sqrt(trans.transform.translation.x ** 2 + trans.transform.translation.y ** 2) cmd = Twist() cmd.linear.x = linear cmd.angular.z = angular pub.publish(cmd) r.sleep()Launch file&lt;launch&gt; &lt;node pkg=\"turtlesim\" type=\"turtlesim_node\" name=\"sim\" /&gt; &lt;node pkg=\"turtlesim\" type=\"turtle_teleop_key\" name=\"teleop\" output=\"screen\" /&gt; &lt;node name=\"turtle1_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle1\" /&gt; &lt;/node&gt; &lt;node name=\"turtle2_tf2_broadcaster\" pkg=\"learning_tf2\" type=\"tf2_broadcaster.py\" respawn=\"false\" output=\"screen\"&gt; &lt;param name=\"turtle\" type=\"string\" value=\"turtle2\" /&gt; &lt;/node&gt; &lt;node pkg=\"learning_tf2\" type=\"tf2_listener.py\" name=\"listener\" output=\"screen\" /&gt; &lt;node pkg=\"learning_tf2\" type=\"add_frame_1.py\" name=\"broadcaster_frames\" output=\"screen\" /&gt;&lt;/launch&gt;7.7 Time travelLet us see how we can get transformations from the past.7.7.1 Example 1Make the following changes in the listener file.while not rospy.is_shutdown(): try: past = rospy.Time.now() - rospy.Duration(5) trans = tfBuffer.lookup_transform(turtle_name,'turtle1',past) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continueThe above code asks for the pose of turtle1 5 seconds ago relative to turtle2 5 seconds ago. But this is not what we want. We want the pose of turtle1 5 seconds ago relative to the current pose of turtle2.7.7.2 Example 2while not rospy.is_shutdown(): try: past = rospy.Time.now() - rospy.Duration(5) trans = tfBuffer.lookup_transform_full(turtle_name, rospy.Time.now(), 'turtle1', past, 'world', rospy.Duration(1)) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException): continueThe above code computes the pose of turtle1 5 seconds ago relative to the current pose of turtle2." }, { "title": "ROS Navigation", "url": "/posts/ros-nav/", "categories": "Blog, Robotics", "tags": "ros, navigation", "date": "2022-07-28 21:30:00 +0530", "snippet": "ROS Navigation1.1 IntroductionThe ROS Navigation stack takes in information from odometry, sensor streams, and a goal pose and outputs safe velocity commands that are sent to a mobile base.The entire process of robot navigation can be divided into 3 major parts: Mapping Localization Path Planning1.2 MappingIn order to perform autonomous navigation, the robot must have a map of the environment. The robot will use this map for many things such as planning trajectories, avoiding obstacles, etc.Rviz is a very important tool that will be used extensively in the mapping process. For Mapping, you will basically need to use 2 displays of Rviz: LaserScan Display Map Display1.2.1 SLAMSimultaneous Localization and Mapping refers to building a map of an unknown environment while simultaneously keeping track of the robot‚Äôs location on the map that is being built.This scenario in Robotics is solved using the gmapping package on ROS.1.2.2 gmapping packageThe gmapping package implements a special SLAM algorithm called gmapping. We don‚Äôt need to know how to code the alogrithm ourselves but just need to learn how to configure the package for our robot to suit our needs.The gmapping package contains a ROS node called slam_gmapping that allows us to create a 2D map using the laser and odom data provided by the robot while navigating the given environment. This node basically reads data from the laser and the transforms of the robot, and turns it into an Occupancy Grid Map (OGM). Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch Launch the gmapping package with the robot (turtlebot3 in our case) by using a previously created launch file turtlebot3_gmapping.launch. To do so, run the command: roslaunch turtlebot3_slam turtlebot3_gmapping.launch This launch file starts the turtlebot3_slam_gmapping node from the gmapping package. The turtlebot3_slam_gmapping node subscribes to the Laser topic (/scan) and Transform topic(/tf) to get laser and odom data from the robot, which it needs to build the map. The generated map is published during the whole process into the /map topic, which reflects on Rviz. In order to visualize this map on Rviz, enter to following command: roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch Enter the following command to move the turtlebot3 and to produce a map of the complete environment. roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch The /map topic uses the message type nav_msgs/OccupancyGrid since it is an OGM. The occupancy is represented as an integer in the range {0,100}. 0 implies completely free 100 implies completely occupied -1 implies completely unknown.Rviz:On entering roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch, you will see something like this on the Rviz window:This however, does not have the map display and hence we cannot see the map of the environment produced by the robot. To get the map display, click on ‚ÄúAdd‚Äù and select ‚ÄúMap‚Äù.Now in the Map display, select the topic name /map to visualize the map being made in real-time by your robot.You can now navigate your robot the obtain a map of the complete environment.The finished map should look something like this.1.2.3 map_servermap_server is a package belonging to the ROS Navigation Stack.Saving a mapIt provides a map_saver node which allows us to access the map data from a ROS Service and save it into a file.When you request the map_saver to save the current map, the map data is saved into two files: YAML file - contains the image name and the metadata of the map Image_name.pgm - this contains the image itself, which has the encoded data of the OGM. pgm stands for Portable Gray MapTo save the built map, enter the following command: rosrun map_server map_saver -f &lt;map_name&gt;To open the map image, enter xdg-open &lt;map_name&gt;.pgmContents of YAML file: image: Name of the file containing the image of the generated map resolution: Resolution of the map (in meters/pixel) origin: Coordinates of the lower-left pixel in the map. The first two values indicate position and the third value indicates rotation. negate: Inverts the colors of the map. 0 by default occupied_thresh: Pixels which have a value greater than this value will be considered as completely occupied free_thresh: Pixels which have a value smaller than this value will be considered as completely freeProviding a mapThe map_server package also provides the map_server node that reads a map file and provides it to any other node that requests it via a ROS Service. This request is done, for example, by the move_base node to get data from a map and use it for Path Planning, or by the localization node in order to find the position of the robot in the map.The service to call to get the map is /static_map (nav_msgs/GetMap).Apart from requesting the map through the GetMap service, there are two topics that you can connect to in order to obtain ROS Messages about the map. /map_metadata (nav_msgs/MapMetaData): Provides map metadata /map (nav_msgs/OccupancyGrid): Provides the map occupancy dataEnter the following command to launch the map_server node in order to provide the map information:rosrun map_server map_server &lt;map_name&gt;.yaml The map that is created is a static map which means that the details of the map will not change or update with time. If the environment changes in the future, then these changes will not reflect in the map. The map that is created is a 2D map, which means that the obstacles that appear in the map don‚Äôt have any height. So these maps will be invalid in projects where you would want to navigate a drone or an underwater rover. For such cases, you would have to use 3D mappings.Creating a Service ClientThis Service Client will call the /static_map service in order to get the map data, and will print the dimensions and resolution of the map.#! /usr/bin/python2import rospyfrom nav_msgs.srv import GetMap, GetMapRequestrospy.init_node('call_map')rospy.wait_for_service('/static_map')map_data = GetMapRequest()srv = rospy.ServiceProxy('/static_map',GetMap)response = srv(map_data)print(response.map.info.resolution)print(response.map.info.width)print(response.map.info.height)rospy.spin()1.2.4 Creating a SLAM launch fileTill now we have used a previously created gmapping node to obtain a map of the environment. We shall now see how to create our own launch file. The main task to create this launch file is to correctly set the parameters for the turtlebot3_slam_gmapping node. This node is highly configurable and has lots of parameters that can be changed in order to improve the mapping performance. These parameters are read from the ROS Parameter Server, which can be set either in the launch file itself or in a separate YAML file.ParametersYou can refer to http://wiki.ros.org/gmapping for the list of various parameters available for the gmapping package.1.3 LocalizationIn the previous section, we have seen how to obtain a map of the given environment using the robot‚Äôs sensor data. When the robot uses this map to navigate, it is necessary for it to know its position withing the map, and its orientation as well. Determining the location and pose of a robot by using its sensor readings is known as Localization.1.3.1 Monte Carlo LocalizationMonte Carlo Localization (MCL) or particle filter localization is an algorithm used by robots to localize themselves in an environment. As the robot navigates the environment, the algorithm generates random guesses (called particles) about the next possible position of the robot. As the robot gathers more sensor data, the algo discards particles that don‚Äôt match with the readings and generates more particles closer to the probable sensor readings. So the more the robot moves, the more data we get from the sensors and the more precise is the localization. Read more https://en.wikipedia.org/wiki/Monte_Carlo_localization https://in.mathworks.com/help/nav/ug/monte-carlo-localization-algorithm.html1.3.2 amcl packageThe AMCL (Adaptive Monte Carlo Localization) package provides the amcl node which uses the MCL algorithm to track the localization of the robot in 2D space. The node subscribes to the laser data and transformations of the robot, and publishes the estimated position of the robot in the map. On startup, the amcl node initializes its particle filter according to the parameters provided. Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch We have previously saved the map of the gazebo-house environment in catkin_ws/src. In order to use a previously existing amcl node with turtlebot3 on our map, we need to copy the saved map into the directory in which the amcl node is located. Open the relevant launch file using the command code ~/catkin_ws/src/turtlebot3/turtlebot3_navigation/launch/turtlebot3_navigation.launch and change the map name in line 4 from map.yaml to my_map.yaml. Save the launch file, do catkin_make and source your workspace. Now to launch the amcl node with gazebo running, use the command roslaunch turtlebot3_navigation turtlebot3_navigation.launch Rviz:On entering the above commands, you will see something like this.For now, you can remove a few of the displays and keep only the ones shown in the image below.In order to visualize localization, we use the PoseArray display. Using the 2D Pose Estimate Tool, set an approximate initial position and orientation for the robot on Rviz (need not be accurate).On moving your robot around using your keyboard, you will notice that the particle cloud shrinks in size due to the scan data allowing amcl to refine its estimate of the robot‚Äôs pose.Topics: The initial pose set up using the 2D Pose Estimate tool on Rviz is published into the /initialpose topic. The amcl node read data published into the /scan topic (laser readings), /map topic and /tf topic, and published the estimate pose of the robot into the /amcl_pose and /particlecloud topics.1.3.3 ServicesThe amcl node provides the following services: /global_localization (std_srvs/Empty): amcl node provides this service wherein all particles are dispersed randomly throughout the free space in the map. /static_map (nav_msgs/GetMap): when called, this service is used to retreive map info for localizationWhen you call the /global_localization service, you see something like this on Rviz.Syntax: rosservice call /global_localization \"{}\"Creating a Service ServerCode for a service server that returns the position and orientation of the robot at the moment when the service is called.#! /usr/bin/python2import rospyfrom geometry_msgs.msg import PoseWithCovarianceStampedfrom std_srvs.srv import Empty, EmptyResponsebot_pose = PoseWithCovarianceStamped()def serv_func(req): print(\"Pose:\") print(bot_pose) return EmptyResponse()def sub_func(msg): global bot_pose bot_pose = msg.pose.poserospy.init_node('service_server')srv = rospy.Service('my_pose_service',Empty,serv_func)sub = rospy.Subscriber('/amcl_pose',PoseWithCovarianceStamped,sub_func)rospy.spin()Creating a Service ClientCode for a service client which performs a call to the /global_localization service in order to disperse the particles.#! /usr/bin/python2import rospyfrom std_srvs.srv import Empty, EmptyRequestrospy.init_node('client_global_localization')rospy.wait_for_service('global_localization')req = EmptyRequest()srv = rospy.ServiceProxy('global_localization',Empty)result = srv(req)rospy.spin()1.3.4 Creating a launch fileTill now we have used a previously created amcl node (slightly modified) to obtain a map of the environment. We shall now see how to create our own launch file. As mentioned before, the main task to create a launch file is to correctly set the parameters for the amcl node.The basic structure of the launch files will be the same as shown above. All you have to do is change the parameters to obtain different outputs.ParametersYou can refer to http://wiki.ros.org/amcl for the list of various parameters available for the amcl package.1.4 Path Planning1.4.1 move_base packageThe move_base package provides the move_base node, which is one of the most important elements of the ROS Navigation Stack that links all processes that take place in the navigation process. The main function of the move_base node is to move the robot from the current position to the target position. It is an implementation of a Simple Action server. Launch gazebo - roslaunch turtlebot3_gazebo turtlebot3_house.launch We now need to make a few small changes to a previously existing launch file. To do so, first go to the ROS directory turtlebot3_navigation, and open the launch file turtlebot3_navigation.launch present under the launch folder. After that, uncomment the following lines in the code Save the file, go back to your ROS Workspace, do a catkin_make and source your workspace. Now with gazebo running, launch the file containing the required move_base node using the command roslaunch turtlebot3_navigation turtlebot3_navigation.launch. Rviz:Using the 2D Nav Goal tool you can goals to your robot, instructing it to move to a particular location in the map.Topics:The goals are sent to the robot using the 2D Nav Tool on Rviz through the /move_base/goal topic.On doing a rostopic echo on the above mentioned topic, we can listen to the messages being published through it. Initially, when no messages are being published, we do not see anything. But if we send a target location to the robot using the 2D Nav Tool on Rviz, we get a message of the following typeWe can now send goals to the robot directly, without using Rviz, using the command rostopic pub /move_base/goal move_base_msgs/MoveBaseActionGoal &lt;parameters&gt;.Other relevant topics:When the move_base node receives a goal pose, it links other components such as the global planner, local planner, recovery behaviours, and costmaps, and generates an output which is a velocity command with the message type geometry_msgs/Twist, and sends it to the /cmd_vel topic in order to move the robot.1.4.2 ActionsAs mentioned before, the move_base node is an implementation of an Action Server. This makes it possible for us to send goals, receive feedback and results, and even cancel a goal if necessary. The action server takes a goal pose with the message type geometry_msgs/PoseStamped. This allows us to send goals to the server using a simple Action Client.The Action server provides the topic move_base/goal, which is the input of the navigation stack, and uses this topic to provide the goal pose.Action ClientEx 4.41.4.3 Creating a launch fileWe shall now see how to create our own move_base launch file. This node also has a lot of parameters associated with it that can be configured.ParametersYou can refer to http://wiki.ros.org/move_base for the list of various parameters available for the move_base package.1.4.4 The Global PlannerWhenever a new goal is received by the move_base node, the goal messages are also sent to the global planner. The lobal planner is responsible for calculating a safe path in order to arrive at the goal pose. This path is calculated before the robot starts moving, so it will not take into account the sensor data of the robot while moving.There are different types of global planners that different robots use to navigate an environment. Some of the important global planners are:NavfnThe Navfn planner is one of the most commonly used global planners in the ROS Navigation Stack. It uses the Djikstra‚Äôs algorithm to calculate the shortest path between the initial and target pose of the robot.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/navfnGlobal PlannerThe global planner is a more flexible replacement to the Navfn planner. It allows you to change the algorithm used for path planning (Djikstra‚Äôs in case of Navfn). These options include support for A*, toggling quadratic approximation, and toggling grid path.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/global_plannerCarrot PlannerThe Carrot planner takes the goal pose and checks if this goal is an obstacle. If it is an obstacle, it walks back along the vector between the goal and the robot until a goal point where the obstacle is not found. This planner is useful if you require your robot to move close to the given goal, even if the goal is unreachable.Reference: https://github.com/ros-planning/navigation/tree/melodic-devel/carrot_planner1.4.5 The Local PlannerOnce the global planner has calculated the path to follow, this path is sent to the local planner. The local planner will then execute each segment of the global plan. With the help of the plan provided by the global planner and a map, the local planner will send velocity commands in order to move the robot.The local planner monitors the odometry and laser data from the robot and chooses a collision-free local plan. It can recompute the robot‚Äôs path while it is moving in order to keep the robot from colliding with obstacles.base_local_plannerThe base_local_planner provides an implementation of the Trajectory Rollout and Dynamic Window Approach (DWA). The basic idea of how the algorithm works is as follows: Samples the robot space. For each sampled velocity, it performs simulations to predict the outcomes when the sampled velocity is applied. It evaluates the trajectory resulting from the simulation and discards the unwanted trajectories. It picks the best trajectory and sends the associated velocity commands for the robot to moveTrajectory Rollout samples are from a set of achievable velocities ove the entire simulation, while DWA samples are from the set of achievable velocities for just one simulation step.dwa_local_plannerThe dwa_local_planner provides an implementation of the Dynamic Window Approach (DWA) algorithm. It is basically a cleaner version of the base local planner‚Äôs DWA option.You can find the list of parameters associated with the DWA planner here.1.4.6 CostmapsA costmap is a map that represents place that are safe for the robot to be in a grid of cells. Unusally the values in the costmap are binary, representing either free space orplaces where the robot would be in collision. Each cell in a costmap has an integer value in the range {0,255}. 255 (NO_INFORMATION): Cells that do not contain enough information 254 (LETHAL_OBSTACLE): Indicates presence of obstacle in a cell 253 (INSCRIBED_INFLATED_OBSTACLE): Indicates no obstacle but moving the robot to this location will result in a collision 0 (FREE_SPACE): No obstacle in the cellThere are 2 types of costmaps: Global Costmap - created from static map Local Costmap - created from sensor readingsCostmap parameters are defined in 3 different files: A YAML file that sets the parameters for the global costmap - global_costmap_params.yaml A YAML file that sets the parameters for the local costmap - local_costmap_params.yaml A YAML file that sets the parameters for both the global and local costmaps - costmap_common_params.yamlGlobal CostmapThe global costmap is created from a user-generated static map and is used by the global planner. It is initialized to match the width, height, and obstacle information provided by the static map.The global costmap has its own set of parameters defined in a YAML file.See Parameters section of http://wiki.ros.org/costmap_2dLocal CostmapThe local costmap is created from the sensor readings taken from the robot and is used by the local planner.The local costmap too has its own set of parameters defined in a YAML file." }, { "title": "Robotic Arm on ROS", "url": "/posts/ros-manipulator/", "categories": "Projects, Robotics", "tags": "ros, urdf", "date": "2022-07-28 21:30:00 +0530", "snippet": "IntroductionThe aim of this project is to build a robot manipulator on ROS (Robot Operating System). A robot manipulator is a basically a set of links and joints that together forms an arm. By controlling the movement of the joints and links, the robotic arm can perform tasks such as picking up objects. I made use of URDF (Unified Robot Description Format) in order to represent the various components of the manipulator and simulated the same on Rviz and Gazebo.In order to go about designing the arm, my work is broadly divided into 2 parts: Exploring URDF and Xacro to make a simple arm Making a seven degree of freedom robotic armBasic manipulatorThis model of the manipulator comprises of 6 links and 5 joints, as follows:Code&lt;?xml version=\"1.0\"?&gt;&lt;robot name=\"arm\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\"&gt;&lt;!-- Materials --&gt;&lt;material name=\"Black\"&gt; &lt;color rgba=\"0.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"Red\"&gt; &lt;color rgba=\"1.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"White\"&gt; &lt;color rgba=\"1.0 1.0 1.0 1.0\"/&gt;&lt;/material&gt;&lt;link name=\"base_link\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"1 1 1\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt;&lt;/link&gt;&lt;joint name=\"base_link_link_01\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.5\"/&gt; &lt;parent link=\"base_link\"/&gt; &lt;child link=\"link_01\"/&gt; &lt;axis xyz=\"0 0 1\"/&gt; &lt;limit effort=\"300\" lower=\"-3.14\" upper=\"3.14\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_01\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.2\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.35\" length=\"0.4\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.2\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.35\" length=\"0.4\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_01_link_02\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;parent link=\"link_01\"/&gt; &lt;child link=\"link_02\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-0.67\" upper=\"0.67\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_02\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt;&lt;/link&gt;&lt;joint name=\"link_02_link_03\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_02\"/&gt; &lt;child link=\"link_03\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_03\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_03_link_04\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_03\"/&gt; &lt;child link=\"link_04\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_04\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;joint name=\"link_04_link_05\" type=\"revolute\"&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.8\"/&gt; &lt;parent link=\"link_04\"/&gt; &lt;child link=\"link_05\"/&gt; &lt;axis xyz=\"0 1 0\"/&gt; &lt;limit effort=\"300\" lower=\"-1.5\" upper=\"1.5\" velocity=\"0.5\"/&gt;&lt;/joint&gt;&lt;link name=\"link_05\"&gt; &lt;visual&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.8\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.4\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.15\" length=\"0.25\"/&gt; &lt;/geometry&gt; &lt;/collision&gt;&lt;/link&gt;&lt;/robot&gt;SimulationThe image below shows the simulation of the robotic arm on Rviz, which also provides a GUI that allows me to control the movements of the joints.The following image shows the different transformation frames and axes of the links, with the X-axis indicated in red, Y-axis in green and Z-axis in blue.Seven DOF ArmThe seven DOF robotic arm is a serial link manipulator having multiple serial links. It is kinematically redundant, which means it has more joints and DOF than required to achieve its goal position and orientation. The advantage of redundant manipulators are, we can have more joint configuration for a particular goal position and orientation. It will improve the flexibility and versatility of the robot movement and can implement effective collision free motion in a robotic workspace. Joint number Joint name Joint type Limits (rad) 0 fix_world FIxed - - - 1 shoulder_pan_joint Revolute -3.14 to 3.14 2 shoulder_pitch_joint Revolute 0 to 1.899 3 elbow_roll_joint Revolute -3.14 to 3.14 4 elbow_pitch_joint Revolute 0 to 1.5 5 wrist_roll_joint Revolute -1.57 to 1.57 6 wrist_pitch_joint Revolute -1.5 to 1.5 7 gripper_roll_joint Revolute -2.6 to 2.6 8 finger_joint1 Prismatic 0 to 3cm 9 finger_joint_2 Prismatic 0 to 3cm Code&lt;?xml version=\"1.0\"?&gt;&lt;robot name=\"seven_dof_arm\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\"&gt;&lt;!-- Materials --&gt;&lt;material name=\"Black\"&gt; &lt;color rgba=\"0.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"Red\"&gt; &lt;color rgba=\"1.0 0.0 0.0 1.0\"/&gt;&lt;/material&gt;&lt;material name=\"White\"&gt; &lt;color rgba=\"1.0 1.0 1.0 1.0\"/&gt;&lt;/material&gt;&lt;xacro:macro name=\"inertial_matrix\" params=\"mass\"&gt; &lt;inertial&gt; &lt;mass value=\"${mass}\"/&gt; &lt;inertia ixx=\"1.0\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.5\" iyz=\"0.0\" izz=\"1.0\"/&gt; &lt;/inertial&gt;&lt;/xacro:macro&gt;&lt;link name=\"world\"/&gt;&lt;joint name=\"fix_world\" type=\"fixed\"&gt; &lt;parent link=\"world\"/&gt; &lt;child link=\"base_link\"/&gt; &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;&lt;/joint&gt;&lt;link name=\"base_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.1 0.1 0.1\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.05\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.1 0.1 0.1\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"shoulder_pan_joint\" type=\"revolute\"&gt; &lt;origin xyz=\"0 0 0.05\" rpy=\"0 0 0\"/&gt; &lt;axis xyz=\"0 0 1\"/&gt; &lt;parent link=\"base_link\"/&gt; &lt;child link=\"shoulder_pan_link\"/&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-3.14\" upper=\"3.14\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"shoulder_pan_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0.02\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.02\" rpy=\"0 0 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"shoulder_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"shoulder_pan_link\"/&gt; &lt;child link=\"shoulder_pitch_link\"/&gt; &lt;origin xyz=\"-0.04 0.0 0.04\" rpy=\"0 0 -${3.14/2}\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"0\" upper=\"1.89994105047\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"shoulder_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"-0.002 0 0.07\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.14 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.002 0 0.07\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.14 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"elbow_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"shoulder_pitch_link\"/&gt; &lt;child link=\"elbow_roll_link\"/&gt; &lt;origin xyz=\"0.0 0 0.14\" rpy=\"${3.14} ${3.14/2} 0\" /&gt; &lt;axis xyz=\"-1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-3.14\" upper=\"3.14\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"elbow_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"-0.03 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.06\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.03 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.06\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"elbow_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"elbow_roll_link\"/&gt; &lt;child link=\"elbow_pitch_link\"/&gt; &lt;origin xyz=\"-0.06 0 0\" rpy=\"0 ${3.14/2} 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"0\" upper=\"1.5\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"elbow_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0 -0.11\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.22 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0.0 0 -0.11\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.22 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"wrist_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"elbow_pitch_link\"/&gt; &lt;child link=\"wrist_roll_link\"/&gt; &lt;origin xyz=\"0.0 0.0 -0.22\" rpy=\"0 0 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-1.57\" upper=\"1.57\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"wrist_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0.0 -0.02\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"Black\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"-0.02 0 0.00\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.02\" length=\"0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"wrist_pitch_joint\" type=\"revolute\"&gt; &lt;parent link=\"wrist_roll_link\"/&gt; &lt;child link=\"wrist_pitch_link\"/&gt; &lt;origin xyz=\"0.0 0.0 -0.04\" rpy=\"0 0 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-1.5\" upper=\"1.5\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"wrist_pitch_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.0 0.0 -0.03\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.06 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 -0.03\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.06 0.04 0.04\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"gripper_roll_joint\" type=\"revolute\"&gt; &lt;parent link=\"wrist_pitch_link\"/&gt; &lt;child link=\"gripper_roll_link\"/&gt; &lt;origin xyz=\"0 0 -0.06\" rpy=\"-${1.5*3.14} ${.5*3.14} 0\" /&gt; &lt;axis xyz=\"1 0 0\" /&gt; &lt;limit effort=\"300\" velocity=\"0.5\" lower=\"-2.61799387799\" upper=\"2.6128806087\" /&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"gripper_roll_link\"&gt; &lt;visual&gt; &lt;origin xyz=\"0 0 0.0\" rpy=\"0 -${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.02\"/&gt; &lt;/geometry&gt; &lt;material name=\"Red\"/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;origin xyz=\"0 0 0.0\" rpy=\"0 ${3.14/2} 0\"/&gt; &lt;geometry&gt; &lt;cylinder radius=\"0.04\" length=\"0.02\"/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"finger_joint1\" type=\"prismatic\"&gt; &lt;parent link=\"gripper_roll_link\"/&gt; &lt;child link=\"gripper_finger_link1\"/&gt; &lt;origin xyz=\"0.0 0 0\" /&gt; &lt;axis xyz=\"0 1 0\" /&gt; &lt;limit effort=\"100\" lower=\"0\" upper=\"0.03\" velocity=\"1.0\"/&gt; &lt;safety_controller k_position=\"20\" k_velocity=\"20\" soft_lower_limit=\"${-0.15 }\" soft_upper_limit=\"${ 0.0 }\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt;&lt;link name=\"gripper_finger_link1\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.04 -0.03 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.08 0.01 0.01\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;joint name=\"finger_joint2\" type=\"prismatic\"&gt; &lt;parent link=\"gripper_roll_link\"/&gt; &lt;child link=\"gripper_finger_link2\"/&gt; &lt;origin xyz=\"0.0 0 0\" /&gt; &lt;axis xyz=\"0 1 0\" /&gt; &lt;limit effort=\"1\" lower=\"-0.03\" upper=\"0\" velocity=\"1.0\"/&gt; &lt;dynamics damping=\"50\" friction=\"1\"/&gt;&lt;/joint&gt; &lt;link name=\"gripper_finger_link2\"&gt; &lt;visual&gt; &lt;origin xyz=\"0.04 0.03 0\"/&gt; &lt;geometry&gt; &lt;box size=\"0.08 0.01 0.01\"/&gt; &lt;/geometry&gt; &lt;material name=\"White\"/&gt; &lt;/visual&gt; &lt;xacro:inertial_matrix mass=\"1\"/&gt;&lt;/link&gt;&lt;!-- Transmission block --&gt;&lt;xacro:macro name=\"transmission_block\" params=\"joint_name\"&gt;\t&lt;transmission name=\"tran1\"&gt;\t &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;\t &lt;joint name=\"${joint_name}\"&gt;\t &lt;hardwareInterface&gt;hardware_interface/PositionJointInterface&lt;/hardwareInterface&gt;\t &lt;/joint&gt;\t &lt;actuator name=\"motor1\"&gt;\t &lt;hardwareInterface&gt;hardware_interface/PositionJointInterface&lt;/hardwareInterface&gt;\t &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;\t &lt;/actuator&gt;\t&lt;/transmission&gt;&lt;/xacro:macro&gt;&lt;!-- Transmissions for ROS Control --&gt;&lt;xacro:transmission_block joint_name=\"shoulder_pan_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"shoulder_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"elbow_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"elbow_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"wrist_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"wrist_pitch_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"gripper_roll_joint\"/&gt;&lt;xacro:transmission_block joint_name=\"finger_joint1\"/&gt;&lt;xacro:transmission_block joint_name=\"finger_joint2\"/&gt;&lt;!-- Colors in gazebo --&gt;&lt;gazebo reference=\"base_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"shoulder_pan_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"shoulder_pitch_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"elbow_roll_link\"&gt; &lt;material&gt;Gazebo/Black&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"elbow_pitch_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"wrist_roll_link\"&gt; &lt;material&gt;Gazebo/Black&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"wrist_pitch_link\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_roll_link\"&gt; &lt;material&gt;Gazebo/Red&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_finger_link1\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;gazebo reference=\"gripper_finger_link1\"&gt; &lt;material&gt;Gazebo/White&lt;/material&gt;&lt;/gazebo&gt;&lt;!-- ros_control plugin --&gt;&lt;gazebo&gt; &lt;plugin name=\"gazebo_ros_control\" filename=\"libgazebo_ros_control.so\"&gt; &lt;robotNamespace&gt;/seven_dof_arm&lt;/robotNamespace&gt; &lt;/plugin&gt;&lt;/gazebo&gt;&lt;/robot&gt;Configuring the joint parametersIn a separate configuration file, we add all the joint state and joint position parameters.seven_dof_arm: # Joint State Controllers joint_state_controller: type: joint_state_controller/JointStateController publish_rate: 50 # 50Hz # Position Controllers assigned to 7 joints and define PID gains joint1_position_controller: type: position_controllers/JointPositionController joint: shoulder_pan_joint pid: { p: 100, i: 0.01, d: 10 } joint2_position_controller: type: position_controllers/JointPositionController joint: shoulder_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint3_position_controller: type: position_controllers/JointPositionController joint: elbow_roll_joint pid: { p: 100, i: 0.01, d: 10 } joint4_position_controller: type: position_controllers/JointPositionController joint: elbow_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint5_position_controller: type: position_controllers/JointPositionController joint: wrist_roll_joint pid: { p: 100, i: 0.01, d: 10 } joint6_position_controller: type: position_controllers/JointPositionController joint: wrist_pitch_joint pid: { p: 100, i: 0.01, d: 10 } joint7_position_controller: type: position_controllers/JointPositionController joint: gripper_roll_joint pid: { p: 100, i: 0.01, d: 10 }Launch file&lt;launch&gt; &lt;!-- these are the arguments you can pass this launch file, for example paused:=true --&gt; &lt;arg name=\"paused\" default=\"false\"/&gt; &lt;arg name=\"use_sim_time\" default=\"true\"/&gt; &lt;arg name=\"gui\" default=\"true\"/&gt; &lt;arg name=\"headless\" default=\"false\"/&gt; &lt;arg name=\"debug\" default=\"false\"/&gt; &lt;!-- We resume the logic in empty_world.launch --&gt; &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt; &lt;arg name=\"debug\" value=\"$(arg debug)\" /&gt; &lt;arg name=\"gui\" value=\"$(arg gui)\" /&gt; &lt;arg name=\"paused\" value=\"$(arg paused)\"/&gt; &lt;arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/&gt; &lt;arg name=\"headless\" value=\"$(arg headless)\"/&gt; &lt;/include&gt; &lt;!-- Load the URDF into the ROS Parameter Server --&gt; &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro '$(find robot_manipulator)/urdf/seven_dof_arm.xacro'\" /&gt; &lt;!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --&gt; &lt;node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model seven_dof_arm -param robot_description\"/&gt; &lt;!-- Load joint controller configurations from yaml file --&gt; &lt;rosparam file=\"$(find robot_manipulator)/config/seven_dof_arm_control.yaml\" command=\"load\"/&gt; &lt;!-- Load controllers --&gt; &lt;node name=\"controller_spawner\" pkg=\"controller_manager\" type=\"spawner\" respawn=\"false\" output=\"screen\" ns=\"/seven_dof_arm\" args=\"joint_state_controller joint1_position_controller joint2_position_controller joint3_position_controller joint4_position_controller joint5_position_controller joint6_position_controller joint7_position_controller\"/&gt; &lt;!-- convert joint states to TF transforms for rviz --&gt; &lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" output=\"screen\" respawn=\"false\" &gt; &lt;remap from=\"/joint_states\" to=\"/seven_dof_arm/joint_states\"/&gt; &lt;/node&gt;&lt;/launch&gt;List of available topics:SimulationLaunching the robot: roslaunch robot_manipulator gazebo_seven_dof_arm.launchControlling the joints using the terminal:rostopic pub /seven_dof_arm/joint4_position_controller/command std_msgs/Float64 1.0rostopic pub /seven_dof_arm/joint1_position_controller/command std_msgs/Float64 1.57ROS ControllersA ROS Controller mainly consists of a feedback mechanism, usually a PID, that lets us control manipulator joints using feedback from the actuators.ROS controllers are provided by the ros_control package. The ros_control packages have the implementation of robot controllers, controller managers, hardware interface, different transmission interface, and control toolboxes.Some standard ROS controllers: joint_position_controller: implementation of the joint position controller joint_state_controller: publishes joint states joint_effort_controller: implementation of the joint effort (force) controllerCommon hardware interfaces in ROS: Joint Command Interfaces: sends commands to the hardware EffortJointInterface: sends the effort command VelocityJointInterface: sends the velocity command PositionJointInterface: sends the position command Joint State Interfaces: retrieves join states from the actuators encoderWriting ROS Controllers" }, { "title": "Robot Kinematics", "url": "/posts/robot-kinematics/", "categories": "Blog, Robotics", "tags": "kinematics", "date": "2022-07-28 21:30:00 +0530", "snippet": "JointsRobot manipulators consist of links connected by joints.Higher pair joint: The two connecting elements are in contact along a line or at a point.Lower pair joint: Constrains contact between a surface in a moving body to a corresponding surface in the fixed body.There are 6 types of lower pair joint designs possible. Revolute joint (R): This joint is like a hinge and allows for relative rotation between the two links. DOF = 1 Prismatic joint (P): This joint allows for relative linear motion between the two links. DOF = 1 Spherical joint (S): This joint allows for motion along all the three axes. DOF = 3 Cylindrical joint (C): This joint allows for both rotation and linear motion between the two links. DOF = 2 Helical joint (H): This joint can be thought of as a screw-nut and allows for motion only along one axis. The linear and rotatory motions are not independent of each other. DOF = 1. Planar joint (E): This joint allows for motion between two planes. DOF = 3Gruebler‚Äôs equation\\[M = 3L - 2J-3G\\] M = degree of freedom or Mobility L = number of links J = number of full joints (1 DOF) G = number of grounded links (G is always 1)TransformationsA linear transformation is one that preserves the linear and scalar multiplication rules of vectors.A projective transformation shows how an object is perceived as the observer‚Äôs viewpoint changes.An affine transformation is one that preserves collinearity (i.e., all points lying on a line continue to lie on the line after the transformation) and the ratio of distances between any 2 points on it remains the same. They are used for scaling, skewing and rotation.Source: https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htmRigid transformations, also called Euclidean transformation, is one that preserves the Euclidean distance between every pair of points. It includes rotations, translations, reflections, or their combination. Scaling and shear are excluded from such a type of transformation. Sometimes reflections are also excluded by imposing that the transformation also preserves the handedness of figures.Rotational TransformationsA transformation matrix called the Rotation Matrix is used to specify the coordinate vectors for the axes of a frame $o_1x_1y_1z_1$ *with respect to coordinate frame $o_0x_0y_0z_0$.* Thus, $R_1^0$ is a rotation matrix whose column vectors are the coordinates of the axes of frame $o_1x_1y_1z_1$ *expressed relative to coordinate frame $o_0x_0y_0z_0$.*\\[R_1^0 = \\begin{bmatrix}x_1.x_0 &amp; y_1.x_0 &amp; z_1.x_0\\\\x_1.y_0 &amp; y_1.y_0 &amp; z_1.y_0\\\\x_1.z_0 &amp; y_1.z_0 &amp; z_1.z_0\\end{bmatrix}\\]For a positive rotation about the z-axis by an angle Œ∏, we have\\[x_1.x_0=cosŒ∏\\]\\[x_1.y_0=sinŒ∏\\]\\[y_1.x_0=-sinŒ∏\\]\\[y_1.y_0=cosŒ∏\\]\\[z_1.z_0=1\\]Thus the rotation matrix equals\\[R_1^0=\\begin{bmatrix} cosŒ∏ &amp; -sinŒ∏ &amp; 0\\\\sinŒ∏ &amp; cosŒ∏ &amp; 0\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\]If we wish to describe the orientation of frame $o_0x_0y_0z_0$ with respect to frame $o_1x_1y_1z_1$, we would have $R_0^1$ such that\\[R_0^1 = (R_1^0)^T = (R_1^0)^{-1}\\]The rotation matrix $R_1^0$ can not only be used to represent the orientation of the coordinate frame $o_1x_1y_1z_1$ with respect to frame $o_0x_0y_0z_0$, but also to transform the coordinates of a point from one frame to the other. Thus if a given point is expressed with respect to the frame $o_1x_1y_1z_1$ as $p^1$then $R_1^0p^1$ represents the same point with respect to frame $o_0x_0y_0z_0$.\\[p^0=R_1^0p^1\\]Instead of relating the coordinates of a fixed vector with respect to two different coordinate frames, the rotation matrix is also used to represent the coordinates of a vector $v_1$ obtained from a vector $v$ by a given rotation in the same coordinate frame.Composition of RotationsRotation about current frameIn order to transform the coordinates of a point $p$ from its representation $p^2$ in the frame $o_2x_2y_2z_2$ to its representation $p^0$ in the frame $o_0x_0y_0z_0$ , we may first transform to its coordinates $p^1$ in the frame $o_1x_1y_1z_1$ using $R_2^1$ and then transform $p^1$ to $p^0$ using $R_1^0$. Thus,\\[R_2^0=R_1^0R_2^1\\]Rotation about fixed frameIf we wish to perform successive rotations with respect to a fixed frame $o_0x_0y_0z_0$ instead of the current frame, then the above equation does not hold true anymore. Rather we need to multiply the rotation matrices in the reverse order.Let $o_0x_0y_0z_0$ be the reference frame. Let the frame $o_1x_1y_1z_1$ be obtained by performing a rotation with respect to the reference frame, and let this rotation be denoted by $R_1^0$. Now let $o_2x_2y_2z_2$ be obtained by performing a rotation of frame $o_1x_1y_1z_1$ with respect to the reference frame (not with respect to frame $o_1x_1y_1z_1$ itself). Let this rotation be denoted by the matrix $R$. Let $R_2^0$ be the matrix that denotes the orientation of frame $o_2x_2y_2z_2$ w.r.t frame $o_0x_0y_0z_0$. Now\\[R_2^0 ‚â† R_1^0R\\]Instead,\\[R_2^0=RR_1^0\\]Derivation:We will convert the rotation about the fixed frame into one about the current frame. First we will rotate the frame $o_1x_1y_1z_1$ w.r.t the frame $o_0x_0y_0z_0$. This can be done by postmultiplying with the inverse of $R_1^0$. Now since the current frame is aligned with the reference frame, we can simply postmultiply with $R$ to get the frame $o_2x_2y_2z_2$. We now need to undo the initial rotation by postmultiplying with $R_1^0$. Therefore, we have\\[R_2^0=R_1^0R_2^1\\\\R_2^0=R_1^0[(R_1^0)^{-1}RR_1^0]\\\\R_2^0=RR_1^0\\]Euler AnglesRoll, Pitch and Yaw AnglesA rotation by $\\phi$ about $z_0$ is the roll angle, a rotation by $\\theta$ about $y_0$ is the pitch angle, and a rotation by $\\psi$ about $x_0$ is the yaw angle.For the rotations yaw-pitch-roll w.r.t a fixed frame or for the rotations roll-pitch-yaw w.r.t current frame, we get the following transformation matrix\\[R_1^0=R_{z,\\phi}R_{y,\\theta}R_{x,\\psi}\\\\=\\begin{bmatrix}c_\\phi &amp; -s_\\phi &amp; 0\\\\s_\\phi &amp; c_\\phi &amp; 0\\\\0&amp;0&amp;1 \\end{bmatrix}\\begin{bmatrix}c_\\theta&amp; 0&amp;s_\\theta\\\\0&amp;1&amp;0\\\\-s_\\theta&amp;0&amp;c_\\theta\\end{bmatrix}\\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;c_\\psi&amp;-s_\\psi\\\\0&amp;s_\\psi&amp;c_\\psi\\end{bmatrix}\\\\\\]\\[=\\begin{bmatrix}c_\\phi c_\\theta&amp;-s_\\phi c_\\psi+c_\\phi s_\\theta s_\\psi&amp;s_\\phi s_\\psi+c_\\phi s_\\theta c_\\psi\\\\s_\\phi c_\\theta&amp;c_\\phi c_\\psi+s_\\phi s_\\theta s_\\psi &amp; -c_\\phi s_\\psi + s_\\phi s_\\theta c_\\psi \\\\ -s_\\theta&amp;c_\\theta s_\\psi&amp;c_\\theta c_\\psi\\end{bmatrix}\\]Homogeneous TransformationsIf frame $o_1x_1y_1z_1$ is obtained from frame $o_0x_0y_0z_0$ by first applying a rotation specified by$R_1^0$ followed by a translation given (with respect to $o_0x_0y_0z_0$) by $d_1^0$ , then the coordinates $p^0$ are given by\\[p^0=R_1^0p^1+d_1^0\\]In general,\\[r_p^G=R_B^Gr_p^B+d^G\\]where $d^G$ is the displacement or translation of B w.r.t G.If we have 2 rigid motions,\\[First \\space motion: p^0=R_1^0p^1+d_1^0\\]\\[Second \\space motion: p^1=R_2^1p^2+d_2^1\\]On combining the two, we get\\[p^0=R_1^0(R_2^1p^2+d_2^1)+d_1^0 \\\\ p^0=R_1^0R_2^1p^2+d_1^0+R_1^0d_2^1\\]This can also be expressed as\\[p^0=R_2^0p^2+d_2^0\\]where\\[R_2^0=R_1^0R_2^1\\\\d_2^0=d_1^0+R_1^0d_2^1\\]\\[\\begin{bmatrix}R_1^0&amp;d_1^0\\\\0&amp;1\\end{bmatrix}\\begin{bmatrix}R_2^1&amp;d_2^1\\\\0&amp;1\\end{bmatrix}=\\begin{bmatrix}R_1^0R_2^1&amp;d_1^0+R_1^0d_2^1\\\\0&amp;1\\end{bmatrix}\\]\\[T_B^G=\\begin{bmatrix}R_B^G&amp;d^G\\\\0&amp;1\\end{bmatrix}\\]\\[T_G^B=\\begin{bmatrix}R_B^G&amp;d^G\\\\0&amp;1\\end{bmatrix}^{-1}=\\begin{bmatrix}{R_B^G}^T&amp;-{R_B^G}^Td^G\\\\0&amp;1\\end{bmatrix}\\]Forward KinematicsIn Forward Kinematics, we are to determine the position and orientation of the end-effector given the values for the joint variables of the robot. The joint variables are the angles between the links in the case of revolute or rotational joints, and the link extension in the case of prismatic or sliding joints.The following conventions are defined to perform kinematic analysis: A robot manipulator with n joints will have n+1 links, since each joint connects to two links. Joints are numbered from 1 to n. Links are numbered from 0 to n, starting from the base. Joint i connects link i-1 to link i. When joint i is actuated, link i moves. A coordinate from $o_ix_iy_iz_i$ is rigidly attached to link i. So when a joint i moves, link i and its attached coordinate frame also moves.Let $A_i$ be a homogeneous transformation matrix that expresses the position and orientation of frame $o_ix_iy_iz_i$ w.r.t frame $o_{i-1}x_{i-1}y_{i-1}z_{i-1}$.$T_j^i$ is the transformation matrix that expresses the position and orientation of frame $o_jx_jy_jz_j$ w.r.t frame $o_ix_iy_iz_i$.\\[T_j^i=A_{i+1}A_{i+2}...A_{j-1}A_j \\space \\space if \\space i&lt;j\\]\\[T_j^i=I \\space \\space if\\space i=j \\\\ T_j^i = (T_i^j)^{-1} \\space if \\space j&gt;i\\]Denavit-Hartenberg ConventionThe following steps need to be followed to derive the forward kinematics for a manipulator:Step 1: Locate and label the joint axes $z_0,z_1,‚Ä¶,z_{n-1}$. We assign $z_i$ to be the axis of actuation for joint $i+1$.Step 2: Choose the base coordinate frame. Set the origin anywhere on the $z_0$ axis.Step 3: Locate the origin $o_i$ where the common normal to $z_i$ and $z_{i-1}$ intersects $z_i$. If $z_i$ intersects $z_{i-1}$, $o_i$ is located at the point of intersection. If $z_i$ and $z_{i-1}$ are parallel, locate $o_i$ in any point along $z_i$.Step 4: The x-axis is defined along the common normal between $z_{i-1}$ and $z_i$ axes through $o_i$, pointing from $z_{i-1}$ to $z_i$ axis. When the two axes are intersecting, $x_i$ is in the direction of $z_{i-1}$x $z_i$.Step 5: Define $y_i$ to complete the right hand coordinate system.Step 6: Choose $o_nx_ny_nz_n$ to be the end-effector frame.Step 7: Create a table of the following parameters $a_i,d_i,\\alpha_i,\\theta_i$. Link length $a_i$: distance between $z_i$ and $z_{i-1}$axes along the $x_i$ axis. Joint distance $d_i$: distance between $x_{i-1}$ and $x_i$ axes along the $z_{i-1}$ axis. Link twist $\\alpha_i$: required rotation of $z_{i-1}$ axis about the $x_i$ axis to become parallel to $z_i$ axis. Joint angle $\\theta_i$: required rotation of $x_{i-1}$ axis about the $z_{i-1}$ axis to become parallel to $x_i$ axis.Step 8: Form the homogeneous transformation matrix $A_i$ by substituting the above parameters.\\[A_i=Rot_{z,\\theta_i}Trans_{z,d_i}Trans_{x,a_i}Rot_{x,\\alpha_i} \\\\ = \\begin{bmatrix} c_{\\theta_i} &amp; -s_{\\theta_i}c_{\\alpha_i} &amp; s_{\\theta_i}s_{\\alpha_i} &amp; a_ic_{\\theta_i} \\\\ s_{\\theta_i} &amp; c_{\\theta_i}c_{\\alpha_i} &amp; -c_{\\theta_i}s_{\\alpha_i} &amp; a_is_{\\theta_i}\\\\0&amp;s_{\\alpha_i}&amp;c_{\\alpha_i}&amp;d_i\\\\0&amp;0&amp;0&amp;1 \\end{bmatrix}\\]Step 9: Form the transformation matrix $T_n^0=A_1A_2‚Ä¶A_n$. This gives the position and orientation of the end-effector w.r.t the base frame.Stanford Manipulator:DH-parameters: Link ‚Åç ‚Åç ‚Åç ‚Åç 1 0 0 -90 * 2 ‚Åç 0 90 * 3 * 0 0 0 4 0 0 -90 * 5 0 0 90 * 6 ‚Åç 0 0 * We can plug in the above values into the $A_i$ matrix to get $A_1,A_2,A_3,A_4,A_5,A_6$. This gives us $T_6^0 = A_1A_2‚Ä¶A_6.$SCARA ManipulatorHas 4 DOF - 3R+1PDH-parameters: Link ‚Åç ‚Åç ‚Åç ‚Åç 1 0 0 0 * 2 0 ‚Åç 0 * 3 ‚Åç ‚Åç 0 0 4 0 0 0 * Reference: https://www.youtube.com/watch?v=O8nzaDcjTiYInverse Kinematics" }, { "title": "Numpy", "url": "/posts/numpy/", "categories": "Blog, Python", "tags": "numpy, python, programming", "date": "2022-07-28 21:30:00 +0530", "snippet": "What is Numpy?Numpy is the python package that allows us to work with multidimensional array objects. It is extremely useful for scientific computing using Python, and is widely used in various fields such as Machine Learning for Data Analysis etc.ArrayAn Array in Numpy is a matrix of elements having the same datatype. An array class in Numpy is known as ndarray (N-dimensional array).Creating a Numpy Array# Using array() methodimport numpy as nparr = np.array([[1, 4, 7], [7, 2, 9], [5, 0, 1]])print('Created Array:\\n {}'.format(arr))Created Array: [[1 4 7] [7 2 9] [5 0 1]]# Using empty() method# numpy.empty(shape, dtype = float, order = ‚ÄòC‚Äô) : Return a new array of given shape and type, with random values.import numpy as nparr = np.empty((3, 3), dtype=int)print('Empty Array:\\n {}'.format(arr))Empty Array: [[1 4 7] [7 2 9] [5 0 1]]# Using eye() method# numpy.eye(R, C = None, k = 0, dtype = type &lt;‚Äòfloat‚Äô&gt;) : The eye tool returns a 2-D array with 1‚Äôs as the diagonal and 0‚Äôs elsewhere.# The diagonal can be main, upper, lower depending on the optional parameter k.# A positive k is for the upper diagonal, a negative k is for the lower, and 0 k (default) is for the main diagonal.import numpy as nparr1 = np.eye(4, 4, k=0, dtype=int)print('Matrix: \\n {}'.format(arr1))arr2 = np.eye(4, 4, k=1, dtype=int)print('Matrix: \\n {}'.format(arr2))arr3 = np.eye(4, 4, k=-1, dtype=int)print('Matrix: \\n {}'.format(arr3))Matrix: [[1 0 0 0] [0 1 0 0] [0 0 1 0] [0 0 0 1]]Matrix: [[0 1 0 0] [0 0 1 0] [0 0 0 1] [0 0 0 0]]Matrix: [[0 0 0 0] [1 0 0 0] [0 1 0 0] [0 0 1 0]]# Using identity() method# numpy.identity(n, dtype = None) : Return a identity matrix i.e. a square matrix with 1's on the main diagonal.import numpy as nparr = np.identity(4)print('Identity Matrix: \\n {}'.format(arr))Identity Matrix: [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]# Using ones() method# numpy.ones(shape, dtype = None, order = 'C') Returns a new array of given shape and type, with ones.import numpy as nparr_one = np.ones([3, 3])print('Matrix with 1 as elements: \\n {}'.format(arr_one))# Using zeros() method# numpy.zeros(shape, dtype = None, order = 'C') Returns a new array of# given shape and type, with zeros.arr_zero = np.zeros([3, 3])print('Matrix with 0 as elements: \\n {}'.format(arr_zero))Matrix with 1 as elements: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]Matrix with 0 as elements: [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]# Using asarray() method# numpy.asarray()function is used to convert input to an array. # Input can be lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays.import numpy as nparr_tuple1 = ([1,6,5],[2,7,6])print('Input: {}'.format(arr_tuple1))arr1 = np.asarray(arr_tuple1)print('Array: \\n {}'.format(arr1))# Uncomment these lines to see what happens if you try to convert inputs having different lengths into an array# arr_tuple2 = ([1,6,5],[2,7])# print('Input: {}'.format(arr_tuple2))# arr2 = np.asarray(arr_tuple2)# print('Array: \\n {}'.format(arr2))Input: ([1, 6, 5], [2, 7, 6])Array: [[1 6 5] [2 7 6]]# Using arange() method# arange([start,] stop[, step,][, dtype]) : Returns an array with evenly spaced elements as per the interval.import numpy as nparr_1 = np.arange(0, 20)print('Array 1: {}'.format(arr_1))arr_2 = np.arange(2, 20, 3)print('Array 2: {}'.format(arr_2))Array 1: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19]Array 2: [ 2 5 8 11 14 17]# Using linspace() method# Returns number spaces evenly w.r.t interval. Similar to numpy.arange() function but instead of step it uses sample number.import numpy as nparr_1 = np.linspace(start=0, stop=10, num=5, retstep=True, dtype=int)print('Arr 1:\\n {}'.format(arr_1))arr_2 = np.linspace(start=0, stop=10, num=5, retstep=False, dtype=int)print('Arr 2:\\n {}'.format(arr_2))Arr 1: (array([ 0, 2, 5, 7, 10]), 2.5)Arr 2: [ 0 2 5 7 10]# Using diag() method# Extracts and construct a diagonal array# k=0 denotes main diagonal. k&gt;0 denotes diagonal above main diagonal. k&lt;0 denotes diagonal below main diagonalimport numpy as nparr = np.array([[1, 2, 6], [4, 8, 1], [6, 9, 0]])print('Array:\\n {}'.format(arr))diag_1 = np.diag(arr, k=0)print('Main diag: {}'.format(diag_1))diag_2 = np.diag(arr, k=1)print('Upper diag: {}'.format(diag_2))diag_3 = np.diag(arr, k=-1)print('Lower diag: {}'.format(diag_3))Array: [[1 2 6] [4 8 1] [6 9 0]]Main diag: [1 8 0]Upper diag: [2 1]Lower diag: [4 9]# Using diagflat() method# Create a two-dimensional array with the array_like input as a diagonal to the new output array.import numpy as npprint(\"diagflat use on main diagonal : \\n\", np.diagflat([1, 7, 6], k=0))print(\"\\ndiagflat above main diagonal : \\n\", np.diagflat([1, 7, 6], k=1))print(\"\\ndiagflat below main diagonal : \\n\", np.diagflat([1, 7, 6], k=-1))diagflat use on main diagonal : [[1 0 0] [0 7 0] [0 0 6]]diagflat above main diagonal : [[0 1 0 0] [0 0 7 0] [0 0 0 6] [0 0 0 0]]diagflat below main diagonal : [[0 0 0 0] [1 0 0 0] [0 7 0 0] [0 0 6 0]]# RANDOM NUMBERS# randint(low, high=None, size=None, dtype=) Uniformly distributes integers in a given rangeimport numpy as npnp.random.seed(101) # enter an arbitrary number (called seed number)arr1 = np.random.randint(0,100,10) # Returns random numbers from 'low'(inclusive) to 'high'(exclusive)print(arr1)[95 11 81 70 63 87 75 9 77 40]Indexing:Idexing is done in order to obtain slices of the main array. This can be done in the following ways- Basic slicing and indexing: using the form [start: stop: step] Advanced indexing: done by pure integer indexing or boolean indexing # Basic slicing and indexingarr_1 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])print('Array 1:\\n {}\\n'.format(arr_1))print('Indexing 1: \\n {}\\n'.format(arr_1[:, :, 2]))print('Indexing 2: \\n {}\\n'.format(arr_1[:, :, 0]))arr_2 = np.arange(0, 10)print('Array 2: {}\\n'.format(arr_2))print('Indexing: \\n {}\\n'.format(arr_2[2:9:2]))arr_3 = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])print('Array 3:\\n {}\\n'.format(arr_3))index_1 = arr_3[1:4, 0:2]print('Indexing: {}'.format(index_1))index_2 = arr_3[1:4, [2, 0]]print('Indexing:\\n {}\\n'.format(index_2))Array 1: [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]]Indexing 1: [[ 3 6] [ 9 12]]Indexing 2: [[ 1 4] [ 7 10]]Array 2: [0 1 2 3 4 5 6 7 8 9]Indexing: [2 4 6 8]Array 3: [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]Indexing: [[ 3 4] [ 6 7] [ 9 10]]Indexing: [[ 5 3] [ 8 6] [11 9]]# Advanced slicingimport numpy as nparr_1 = np.array([[3, 4, 7], [5, 1, 8], [2, 4, 7]])print('Array 1:\\n {}\\n'.format(arr_1))ind_1 = arr_1[[1, 2, 1], [2, 0, 1]] # index inputs are column-wiseprint('Index 1:\\n {}\\n'.format(ind_1))# Type of rearrangement (See carefully)imp = arr_1[np.array([[1, 2, 1], [2, 0, 1]])]print('Imp:\\n {}'.format(imp))# Boolean indexingarr_2 = np.array([[2, 5, 8], [1, 9, 0], [3, 5, 4]])print('Array 2:\\n {}\\n'.format(arr_2))ind_2 = arr_2[arr_2 &gt;= 3]print('Indexing 2:\\n {}\\n'.format(ind_2))arr_3 = np.array([10, 40, 80, 50, 100])print('Indexing 3:\\n {}'.format((arr_3[arr_3 % 40 == 0])**2))Array 1: [[3 4 7] [5 1 8] [2 4 7]]Index 1: [8 2 1]Imp: [[[5 1 8] [2 4 7] [5 1 8]] [[2 4 7] [3 4 7] [5 1 8]]]Array 2: [[2 5 8] [1 9 0] [3 5 4]]Indexing 2: [5 8 9 3 5 4]Indexing 3: [1600 6400]Using .reshape(rows,columns) methodThis method allows us to convert a given list into an array with required number of rows and columns.# Using reshape() methodimport numpy as nparr = np.arange(0, 20)print('Array 1: {}\\n'.format(arr))arr_2 = arr.reshape(4, 5)print('Array 2:\\n {}'.format(arr_2))Array 1: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19]Array 2: [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]]Using numpy.nditer()This method allows us to iterate over multi-dimensional arrays.# Using numpy.nditer() method# https://youtu.be/XawR6CjAYV4import numpy as nparr = np.arange(0, 12).reshape(3, 4)print('Reshaped Array 1:\\n {}\\n'.format(arr))# Using a for loop# for row in arr:# for cell in row:# print(cell)# Both the codes are equivalent# for row in arr.flatten():# print(row)# Using nditer row-wisefor x in np.nditer(arr, order='C'): print(x)print()# Using nditer column-wisefor x in np.nditer(arr, order='F'): print(x)Reshaped Array 1: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]0123456789101104815926103711# Using flags with nditer()import numpy as npa = np.arange(0, 12).reshape(3, 4)print('Array:\\n {}\\n'.format(a))# Doing this will allow you to print your entire column at oncefor x in np.nditer(a, order='F', flags=['external_loop']): print(x)print()# Doing this will allow you to print your entire row at oncefor x in np.nditer(a, order='C', flags=['external_loop']): print(x)print()# Using optional flags with op_flags option# readwrite option is used to modify the elements of the array, and in this case, it gives us the square of all elementsfor x in np.nditer(a, op_flags=['readwrite']): x[...] = x*x # [...] is ellipsis.print(a)Array: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]][0 4 8][1 5 9][ 2 6 10][ 3 7 11][ 0 1 2 3 4 5 6 7 8 9 10 11][[ 0 1 4 9] [ 16 25 36 49] [ 64 81 100 121]]# Iterating through two arrays using nditer()# To do this, either both arrays should have the same shape, or one of the# dimensions in one of the arrays should be 1a = np.arange(0, 12).reshape(3, 4)print('Array 1:\\n {}\\n'.format(a))b = np.arange(3, 15, 4).reshape(3, 1)print('Array 2:\\n {}\\n'.format(b))for x, y in np.nditer([a, b]): print(x, y)Array 1: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]Array 2: [[ 3] [ 7] [11]]0 31 32 33 34 75 76 77 78 119 1110 1111 11Binary Operations in Numpy numpy.bitwise_and(arr1,arr2): Computes the bitwise AND of two arrays, element wise numpy.bitwise_or(arr1,arr2): Computes the bitwise OR of two arrays, element wise numpy.bitwise_xor(arr1,arr2): Computes the bitwise XOR of two arrays, element wise numpy.invert(arr): Computes the bitwise NOT of an array, element wise numpy.left_shift(arr,bit_shift): Shifts the bits of an integer to the left and returns the corresponding decimal representation. numpy.right_shift(arr,bit_shift): Shifts the bits of an integer to the right and returns the corresponding decimal representation. numpy.unpackbits(array,axis): Unpacks elements of a uint8 array into a binary-valued output array. numpy.packbits(array,axis): Packs the elements of a binary-valued array into bits in a uint8 array. numpy.binary_repr(number,width): Gives the binary represntation of the array element. For negative numbers, if width is not given, a minus sign is added to the front. If width is given, the two‚Äôs complement of the number is returned, with respect to that width. # BINARY OPERATIONSimport numpy as nparr_1 = np.array([2, 8, 125])arr_2 = np.array([3, 3, 115])print('Array 1: {}\\n'.format(arr_1))print('Array 2: {}\\n'.format(arr_2))# Bitswise AND Operationarr_and = np.bitwise_and(arr_1, arr_2)print('Bitwise AND: {}\\n'.format(arr_and))# Bitwise OR Operationarr_or = np.bitwise_or(arr_1, arr_2)print('Bitwise OR: {}\\n'.format(arr_or))# Bitwise XOR Operationarr_xor = np.bitwise_xor(arr_1, arr_2)print('Bitwise XOR: {}\\n'.format(arr_xor))Array 1: [ 2 8 125]Array 2: [ 3 3 115]Bitwise AND: [ 2 0 113]Bitwise OR: [ 3 11 127]Bitwise XOR: [ 1 11 14]# BINARY OPERATIONSimport numpy as nparr_1 = np.array([2, 8, 15])print('Array 1: {}\\n'.format(arr_1))# Bitwise NOT Operationarr_not = np.invert(arr_1)print('Bitwise NOT: {}\\n'.format(arr_not))# Bit shift Operationsbit_shift_1 = np.array([3, 4, 5])print('Bit shift array 1: {}\\n'.format(bit_shift_1))# Left shiftarr_left_shift = np.left_shift(arr_1, bit_shift_1)print('Left shift: {}\\n'.format(arr_left_shift))print()arr_2 = np.array([24, 48, 16])print('Array 2: {}\\n'.format(arr_2))bit_shift_2 = np.array([3, 4, 2])print('Bit shift array 2: {}\\n'.format(bit_shift_2))# Right shiftarr_right_shift = np.right_shift(arr_2, bit_shift_2)print('Right shift: {}\\n'.format(arr_right_shift))Array 1: [ 2 8 15]Bitwise NOT: [ -3 -9 -16]Bit shift array 1: [3 4 5]Left shift: [ 16 128 480]Array 2: [24 48 16]Bit shift array 2: [3 4 2]Right shift: [3 3 4]# BINARY OPERATIONSimport numpy as npin_arr = [5, -8]print()print(\"Input array : \", in_arr)# Without using width parameterout_num = np.binary_repr(in_arr[0])print(\"Binary representation of 5\")print(\"Without using width parameter : \", out_num)# Using width parameterout_num = np.binary_repr(in_arr[0], width=5)print(\"Using width parameter: \", out_num)print(\"\\nBinary representation of -8\")# Without using width parameterout_num = np.binary_repr(in_arr[1])print(\"Without using width parameter : \", out_num)# Using width parameterout_num = np.binary_repr(in_arr[1], width=5)print(\"Using width parameter : \", out_num)Input array : [5, -8]Binary representation of 5Without using width parameter : 101Using width parameter: 00101Binary representation of -8Without using width parameter : -1000Using width parameter : 11000Linear Algebra using NumpyThe Linear Algebra module of NumPy offers various methods to apply linear algebra on any numpy array.One can find: rank, determinant, trace, etc. of an array. eigen values of matrices matrix and vector products (dot, inner, outer,etc. product), matrix exponentiation solve linear or tensor equations# LINEAR ALGEBRAimport numpy as npA = np.array([[6, 1, 1], [4, -2, 5], [2, 8, 7]])# Rank of a matrixprint(\"Rank of A:\", np.linalg.matrix_rank(A))# Trace of matrix Aprint(\"\\nTrace of A:\", np.trace(A))# Determinant of a matrixprint(\"\\nDeterminant of A:\", np.linalg.det(A))# Inverse of matrix Aprint(\"\\nInverse of A:\\n\", np.linalg.inv(A))print(\"\\nMatrix A raised to power 3:\\n\", np.linalg.matrix_power(A, 3))Rank of A: 3Trace of A: 11Determinant of A: -306.0Inverse of A: [[ 0.17647059 -0.00326797 -0.02287582] [ 0.05882353 -0.13071895 0.08496732] [-0.11764706 0.1503268 0.05228758]]Matrix A raised to power 3: [[336 162 228] [406 162 469] [698 702 905]]# LINEAR ALGEBRA# Matrix and Vector productsimport numpy as np# Using numpy.dot(): returns the dot product of vectors a and b# Scalarsproduct = np.dot(5, 4)print(\"Dot Product of scalar values : \", product)# 1D arrayvector_a = 2 + 3jvector_b = 4 + 5jproduct = np.dot(vector_a, vector_b)print(\"Dot Product : \", product)# Using numpy.vdot(): Returns the dot product of vectors a and b. If first# argument is complex the complex conjugate of the first argument is used# for the calculation of the dot productvector_a = 2 + 3jvector_b = 4 + 5jproduct = np.vdot(vector_a, vector_b)print(\"VDot Product : \", product)Dot Product of scalar values : 20Dot Product : (-7+22j)VDot Product : (23-2j)# LINEAR ALGEBRA# Solving equations# numpy.linalg.solve(): Solves a system of linear scalar equations.# Computes the ‚Äúexact‚Äù solution, x, of the linear matrix equation ax = b.import numpy as npa = np.array([[1, 2], [3, 4]])b = np.array([8, 18])print(\"Solution of linear equations:\", np.linalg.solve(a, b))Solution of linear equations: [2. 3.]Using numpy.sum():numpy.sum(arr, axis, dtype, out) : This function returns the sum of array elements over the specified axis.out : Different array in which we want to place the result. The array must have same dimensions as expected output. Default is None.Using numpy.add():numpy.add(arr1, arr2, /, out=None) : This function is used when we want to compute the addition of two array. It adds arguments element-wise.If shape of two arrays are not same, they must be broadcastable to a common shape.Broadcastable arrays - should have same dimensions or the dimension of one of the arrays should be 1.# Using numpy.sum()import numpy as np# 2D arrayarr = [[14, 17, 12, 33, 44], [15, 6, 27, 8, 19], [23, 2, 54, 1, 4, ]]print(\"\\nSum of arr : \", np.sum(arr))print(\"\\nSum of arr : \", np.sum(arr, axis=0))print(\"\\nSum of arr : \", np.sum(arr, axis=1))Sum of arr : 279Sum of arr : [52 25 93 42 67]Sum of arr : [120 75 84]# Using numpy.add()import numpy as npin_arr1 = np.array([[2, -7, 5], [-6, 2, 0]])in_arr2 = np.array([[5, 8, -5], [3, 6, 9]])print(\"1st Input array : \\n\", in_arr1)print(\"2nd Input array : \\n\", in_arr2)out_arr = np.add(in_arr1, in_arr2)print(\"output added array : \\n\", out_arr)1st Input array : [[ 2 -7 5] [-6 2 0]]2nd Input array : [[ 5 8 -5] [ 3 6 9]]output added array : [[ 7 1 0] [-3 8 9]]numpy.concatenate()numpy.concatenate() function concatenate a sequence of arrays along an existing axis.It is used to join two or more arrays# Concatenateimport numpy as nparr1 = np.array([[2, 4], [6, 8]])arr2 = np.array([[3, 5], [7, 9]])print('Array 1: \\n', arr1)print('Array 2: \\n', arr2)gfg_1 = np.concatenate((arr1, arr2), axis=1)print('Horizontally: \\n', gfg_1)gfg_2 = np.concatenate((arr1, arr2), axis=0)print('Vertically: \\n', gfg_2)Array 1: [[2 4] [6 8]]Array 2: [[3 5] [7 9]]Horizontally: [[2 4 3 5] [6 8 7 9]]Vertically: [[2 4] [6 8] [3 5] [7 9]]numpy.append()numpy.append(array, values, axis = None) : appends values along the mentioned axis at the end of the array.It is used to add items/elements/arrays.# numpy.append()import numpy as nparr = np.array([[2, 3, 1], [4, 8, 1]])print('Original Array: \\n', arr)mylist = [[3, 1, 0], [4, 5, 1]] # Dimensions should be the same.print('List: ', mylist)new_arr = np.append(arr, mylist, axis=1)print('\\nAppended array: \\n', new_arr)print()arr_2 = np.append(arr, [[3, 4, 1], [1, 4, 0]], axis=0)print(arr_2)# Error when only one list is given.Original Array: [[2 3 1] [4 8 1]]List: [[3, 1, 0], [4, 5, 1]]Appended array: [[2 3 1 3 1 0] [4 8 1 4 5 1]][[2 3 1] [4 8 1] [3 4 1] [1 4 0]]SortingArranging data in a specific order. We can sort a numpy array in the following ways: numpy.sort(): Returns sorted copy of the array numpy.argsort(): Returns indices that would sort an array numpy.sort_complex(): Sorts an array with real part first, then the imaginary part # SORTING# Using numpy.sort()import numpy as npa = np.array([[12, 15], [10, 1]])print('Array 1: \\n{}\\n'.format(a))arr1 = np.sort(a, axis=0)print(\"Column wise (axis=0) : \\n\", arr1)a = np.array([[10, 15], [12, 1]])print('\\nArray 2: \\n{}\\n'.format(a))arr2 = np.sort(a, axis=1) # Axis=1 and Axis=-1 are the sameprint(\"Row wise (axis=1/-1) : \\n\", arr2)arr1 = np.sort(a, axis=None)print(\"\\nAlong none axis : \\n\", arr1)Array 1: [[12 15] [10 1]]Column wise (axis=0) : [[10 1] [12 15]]Array 2: [[10 15] [12 1]]Row wise (axis=1/-1) : [[10 15] [ 1 12]]Along none axis : [ 1 10 12 15]# SORTING# Using numpy.argsort()import numpy as npa = np.array([9, 3, 1, 7, 4, 3, 6])print('Original array:\\n', a)b = np.argsort(a)print('Sorted indices of original array-&gt;', b)Original array: [9 3 1 7 4 3 6]Sorted indices of original array-&gt; [2 1 5 4 6 3 0]# SORTINGimport numpy as nparr_1 = np.array([4, 8, 1, 2, 9])names = np.array(['A', 'B', 'C', 'D', 'E'])# Low to Highsort_1 = np.sort(arr_1)print('Sorted Array 1: {}\\n'.format(sort_1))# High to Lowsort_2 = np.sort(arr_1)[::-1]print('Sorted Array 2: {}\\n'.format(sort_2))# Sorting namessort_names = names[np.argsort(arr_1)[::-1]]print('Sorted Names: {}'.format(sort_names))Sorted Array 1: [1 2 4 8 9]Sorted Array 2: [9 8 4 2 1]Sorted Names: ['E' 'B' 'A' 'D' 'C']SearchingSearching is an operation or a technique that helps finds the place of a given element or value in the list. This can be done in some of the following ways: numpy.argmax(): Returns indices of the max element of the array in a particular axis. numpy.nanargmax(): Returns indices of the max element of the array in a particular axis ignoring NaNs.The results cannot be trusted if a slice contains only NaNs and Infs. numpy.argmin(): Returns the indices of the minimum values along an axis. numpy.where(): Returns the indices of elements in an input array where the given condition is satisfied. # SEARCHINGimport numpy as nparray = np.arange(12).reshape(3, 4)print(\"INPUT ARRAY : \\n\", array)print(\"\\nMax element : \", np.argmax(array))print(\"Indices of Max element: \", np.argmax(array, axis=0))print(\"Indices of Max element: \", np.argmax(array, axis=1))print(\"Indices of min element : \", np.argmin(array, axis=0))print()# numpy.where()a = np.array([[1, 2, 3], [4, 5, 6]])print('Array 2: \\n', a)print('Indices of elements &lt;4')b = np.where(a &lt; 4)print(b)print(\"Elements which are &lt;4\")print(a[b])INPUT ARRAY : [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]Max element : 11Indices of Max element: [2 2 2 2]Indices of Max element: [3 3 3]Indices of min element : [0 0 0 0]Array 2: [[1 2 3] [4 5 6]]Indices of elements &lt;4(array([0, 0, 0]), array([0, 1, 2]))Elements which are &lt;4[1 2 3]Countingnumpy.count_nonzero() : Counts the number of non-zero values in the array .import numpy as np# Counting a number of non-zero valuesa = np.count_nonzero([[0, 1, 7, 0, 0], [3, 0, 0, 2, 19]])b = np.count_nonzero([[0, 1, 7, 0, 0], [3, 0, 0, 2, 19]], axis=0)print(\"Number of nonzero values is :\", a)print(\"Number of nonzero values is :\", b)Number of nonzero values is : 5Number of nonzero values is : [1 1 1 1 1]" }, { "title": "Analysis of Battery in Low Power Electric Vehicles using Machine Learning", "url": "/posts/ev/", "categories": "Projects, ML", "tags": "ml, ev, matlab", "date": "2022-07-28 21:30:00 +0530", "snippet": "The following project has been done as part of my Practice School-1 cirriculum under CSIR - Central Electronics Engineering Research Institute (CEERI), Pilani. It enabled me to work on real-world issues and provided me with an opportunity to obtain industrial experience under the guidance of Dr. Bhausaheb Ashok Botre, Principal Scientist.AbstractDue to its numerous favorable effects on the environment and society, electric vehicles (EVs) are growing in popularity around the world. This Project Report discusses the use of Machine Learning techniques for the analysis of batteries in low power Electric-tricycle. The report talks about the use of machine learning techniques such as Random Forest (RF) and Gradient Boosting Regression Tree (GBRT) for State of Charge (SOC) estimation and load forecasting. MATLAB simulations for speed control of a BLDC motor using PID controller and a Lithium-ion temperature dependent battery model are used to predict the SOC and the power of the battery. The data generated from these simulations is used to train the ML models.Project Areas: Electric Vehicles, Machine Learning.Keywords: Machine Learning, State of Charge, BMS, Load Forecasting, BLDC motor.IntroductionElectric Vehicles are becoming increasingly integrated in several cities. With increasing EV technology and smart power grids, they have emerged as an alternative to the conventional gas driven vehicles. Being more environmentally friendly, people are rapidly moving towards EVs.Battery charging in EVs is still a major challenge and continuous research efforts are being made to build an efficient system to analyze the various parameters of the batteries being used. Accurate analysis needs to be performed to prevent any battery failure and occurrence of any accidents.Problem StatementThe aim of this project is to analyze batteries in low power Electric Vehicles using Machine Learning techniques. CEERI Pilani has developed an electric assisted tricycle for the outdoor mobility of differently abled persons and aims to improve it by incorporating an efficient Battery Management System using IoT and ML techniques.Challenges AddressedA number of challenges are involved in analyzing the batteries of Electric Vehicles, which include: Prediction of driving range Performance of battery Charge load conditions State of charge (SOC) estimationThe absence of high-quality, full-scale datasets is one of the most critical challenges faced in the accurate prediction of batteries in EVs. But in recent times, one major starting point in the prediction of battery safety is the simulation of the loading conditions. My personal objective is to focus on the EV load conditions and SOC estimation techniques to help build an efficient battery management system (BMS).Analysis of EV charge load conditions implies analyzing the power of the battery during peak demands. It also looks at how charging the EV affects the battery. It is important because EV load balancing instructs the chargers to deliver the right amount of energy and manages the energy flow. So analyzing the energy trends of the battery would tell us its charge load characteristics.The most critical inputs for controlling EV charging and improving power system operation and performance are accurate early knowledge of the EV load demand. Moreover, factors such as driving and travel patterns of each EV lead to a stochastic nature of the EV charging demand.SOC is a number between 0 and 1 that indicates the amount of charge available in the cell relative to the maximum amount of charge. A major challenge with SOC is that it cannot be measured directly but it needs to be inferred based on measurements of voltage, current, temperature and the use of a model of the cell. However, some Simulink models of batteries let us obtain SOC data directly which is extremely beneficial.Machine LearningThere exist a number of traditional techniques for EV load analysis and SOC estimation. But most of them have been proved less efficient in recent times, and with the rise of machine learning, we have a lot more options to help in load forecasting and SOC estimation.A popular way of modeling batteries with the purpose of system design and charge load analysis is to create an electro-thermal analogy for the cell‚Äôs behavior, i.e, to create an equivalent circuit. The topology and parameters of the equivalent circuit should allow us to load it in the same way we would load a battery and observe the same response a battery would have. This includes voltage drop, relaxation times and open circuit voltage, as functions of temperature and state of charge (SOC).The easiest way to estimate SOC is integrating current. It is a computationally cheap method but the disadvantage is that any error in the measurement of current will accumulate over time, making the SOC diverge from its true value.A more advanced method of estimating SOC is the use of observers such as Kalman filters. The Kalman filter takes in the model of the cell and uses a recursive algorithm that continuously predicts a future state and corrects it using measurements performed on the system. But this is computationally very expensive.An alternative would be to use Machine Learning models that take the relevant parameters and use them to predict the desired features. Trained with a sufficiently large amount of data measured under sufficiently varied conditions, ML models would be able to learn the patterns of change in the measurable quantities of the battery and link them to predict the SOC and power/energy trends.Machine learning algorithms provide a variety of approaches for forecasting EV load conditions and SOC estimation. Random Forest (RF) and Gradient Boosting Regression Tree (GBRT) are two such techniques that have lately proven to be useful.Random Forest (RF):The Random Forest algorithm is based on decision trees. A decision tree is a model that follows a path from the branched observations to the outcomes. We use regression trees since the outcomes (power/energy consumption and SOC) have continuous values.Random Forest Regression is a supervised learning approach for regression that employs ensemble learning methods. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.The diagram above shows the structure of a RF. The trees run in parallel with no interaction amongst them. An RF operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.Random Forest Regression is a powerful and precise model. It usually works well on a wide range of issues, including those with non-linear relationships.Gradient Boosting Regression Tree (GBRT):GBRT is also based on decision trees and ensemble learning. It also works on the concept of Boosting, which works on the principle of improving mistakes of the previous learner through the next learner. In boosting, weak learners are used which perform only slightly better than a random chance.In GBRT, we combine many weak learners to come up with one strong learner. The weak learners here are the individual decision trees. All the trees are connected in series and each tree tries to minimize the error of the previous tree. Due to this sequential connection, boosting algorithms are usually slow to learn, but also highly accurate.Overfitting owing to the inclusion of too many trees is a problem that we may face in GBRT but not in Random Forests. Overfitting will not occur in RF if you add too many trees. Although the model‚Äôs accuracy does not improve after a certain point, there is no issue with overfitting. However, when using GBRT, we must be cautious about the amount of trees we choose, as having too many weak learners in the model can lead to data overfitting. As a result, hyperparameter optimization is quite important.SimulationsMATLAB is a great software platform that provides us with a number of tools that can be used to model an EV system. Two Simulink models were created to obtain data used for SOC estimation and load forecasting. Lithium-Ion Temperature Dependent Battery Model Speed Control of a BLDC Motor using PID ControllerThese two individual models were then interfaced together to create a single large model that predicts SOC and power trends in the presence of a BLDC motor as load.Lithium-Ion Temperature Dependent Battery ModelThis model shows the variation in performance of a 7.2 V, 5.4 Ah Lithium-Ion battery. Throughout a discharge and charge operation, the model is exposed to an environment with varying ambient temperature. Its performance is contrasted with the scenario in which temperature effects are disregarded. The temperature dependent battery model functions reasonably well, as seen from the scope. The output voltage and capacity change together with the cell‚Äôs internal temperature as a result of charge (or discharge) heat losses and variations in the outside temperature.The data from the scope can be extracted into an excel sheet, as shown below, containing all the necessary parameters. By using data of only the relevant parameters, ML models can be trained to estimate the SOC and power trends (for load forecasting).Speed Control of a BLDC Motor using PID ControllerThis model displays the functionality of a Brushless DC motor coupled to a PID controller for precision and stability. Ignoring the effects of temperature, a Lithium-Ion battery is connected to the motor. We use the scope to monitor changes in stator currents, motor speed, and electromagnetic torque. The PID controller ensures that the motor‚Äôs speed is consistently maintained at the required rpm specified in the input.Complete ModelThe two models generated above are combined to obtain one single large model. This demo shows the performance of the Lithium-Ion battery in the presence of a load, i.e., BLDC motor, its variation with change in temperature and allows us to measure motor speed as well.The information from the scope can be retrieved into an excel file that includes all the relevant parameters. This complete data can then be used to train the ML models that will accurately predict the SOC and the power/energy consumed for load forecasting.ResultsThe complete MATLAB simulation, however, is extremely heavy and time consuming, and requires higher specs and computational power to run. So the ML models were trained based on data generated from the battery without the presence of a load.DatasetsThe data obtained from the MATLAB simulation in an excel sheet is shuffled randomly and divided into training sets and testing sets.Random ForestScikit learn library is used to run the Random Forest model and is trained on the training dataset. The trained model is then tested on the test dataset and the accuracy is measured.Code:# Importing the librariesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Train settrain = pd.read_csv('train_parameters_updated.csv')energy = train.iloc[:,4].valuessoc = train.iloc[:,2:3].values time = train.iloc[:,0:1].values # X# Test settest = pd.read_csv('test_parameters_updated.csv')energy_test = test.iloc[:,4].valuessoc_test = test.iloc[:,2:3].values time_test = test.iloc[:,0:1].values # X# Predict SOCfrom sklearn.ensemble import RandomForestRegressorregressor_1 = RandomForestRegressor(n_estimators = 10, random_state = 0)regressor_1.fit(time,soc)# Visualizing the SOC Set X_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, soc_pred, color = 'red')plt.plot(X_grid, regressor_1.predict(X_grid), color = 'blue')plt.title('Random Forest Regression')plt.xlabel('Time')plt.ylabel('SOC')plt.show()# Predict Energyfrom sklearn.ensemble import RandomForestRegressorregressor_2 = RandomForestRegressor(n_estimators = 10, random_state = 0)regressor_2.fit(time,energy)energy_pred = regressor_2.predict(time_test)# Visualizing the Energy Set X_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, energy_pred, color = 'red')plt.plot(X_grid, regressor_2.predict(X_grid), color = 'blue')plt.title('Random Forest Regression')plt.xlabel('Time')plt.ylabel('Energy')plt.show()# Accuracy of SOC estimationfrom sklearn.metrics import r2_scorer2_score(soc_pred,soc_test)# Accuracy of Energy estimationfrom sklearn.metrics import r2_scorer2_score(energy_pred,energy_test)Predicting SOC:Accuracy: 99%Predicting Energy:The data for energy is obtained by multiplying the voltage and the current. The trends in energy are used for load forecasting.Accuracy: 99%Model Summary:import sklearn.metrics as metricsmae = metrics.mean_absolute_error(soc_test, soc_pred)mse = metrics.mean_squared_error(soc_test, soc_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(soc_test,soc_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)mae = metrics.mean_absolute_error(energy_test, energy_pred)mse = metrics.mean_squared_error(energy_test, energy_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(energy_test,energy_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)For SOC:MAE: 0.0007033089960029275MSE: 7.885088251654824e-07RMSE: 0.0008879801941290596R-Squared: 0.9999999861769471For Energy:MAE: 0.0042136681167083585MSE: 0.15557361977356302RMSE: 0.3944282187845629R-Squared: 0.9995374972380205Gradient Boosting Regression TreeThis model also uses the Scikit learn library to train the model on the training data. The trained model is then tested on the test dataset and the accuracy is measured.Code:# Load librariesfrom sklearn.ensemble import GradientBoostingRegressorimport numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_errorfrom sklearn.metrics import r2_scoreimport warningswarnings.filterwarnings('ignore')# Train setdata = pd.read_csv('train_parameters_updated.csv')energy = data.iloc[:,4].valuessoc = data.iloc[:,2:3].values time = data.iloc[:,0:1].values # Xgradientregressor = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)# Test settest = pd.read_csv('test_parameters_updated.csv')energy_test = test.iloc[:,4].valuessoc_test = test.iloc[:,2:3].values time_test = test.iloc[:,0:1].values # X# Predict SOCmodel = gradientregressor.fit(time,soc)soc_pred = model.predict(time_test)# Visualizing the SOC Set import matplotlib.pyplot as plt%matplotlib inlineX_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time, soc, color = 'red')plt.title('GBRT')plt.xlabel('Time')plt.ylabel('SOC')plt.show()# Predict Energymodel_2 = gradientregressor.fit(time,energy)energy_pred = model_2.predict(time_test)# Visualizing the Voltage Set import matplotlib.pyplot as plt%matplotlib inlineX_grid = np.arange(min(time), max(time), 0.01)X_grid = X_grid.reshape((len(X_grid), 1))plt.scatter(time_test, energy_pred, color = 'red')plt.plot(X_grid, model_2.predict(X_grid), color = 'blue')plt.title('GBRT')plt.xlabel('Time')plt.ylabel('Energy')plt.show()# Accuracy of SOC estimationr2_score(soc_pred,soc_test)# Accuracy of Energy estimationr2_score(energy_pred,energy_test)Predicting SOC:Accuracy: 93%Predicting Energy:Accuracy: 99%Model Summary:import sklearn.metrics as metricsmae = metrics.mean_absolute_error(soc_test, soc_pred)mse = metrics.mean_squared_error(soc_test, soc_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(soc_test,soc_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)mae = metrics.mean_absolute_error(energy_test, energy_pred)mse = metrics.mean_squared_error(energy_test, energy_pred)rmse = np.sqrt(mse) # or mse**(0.5) r2 = metrics.r2_score(energy_test,energy_pred)print(\"Results of sklearn.metrics:\")print(\"MAE:\",mae)print(\"MSE:\", mse)print(\"RMSE:\", rmse)print(\"R-Squared:\", r2)For SOC:MAE: 1.594852472588293MSE: 3.6870569829698217RMSE: 1.9201710816929365R-Squared: 0.9353635847153323For Energy:MAE: 0.13236938942063645MSE: 0.18590540743040299RMSE: 0.4311674934760307R-Squared: 0.9994473242666165Comparison:By comparing the accuracies of the two models, RF and GBRT, we see that the RF algorithm performs better than the other over the same training set. While GBRT gives a comparable accuracy as that of RF in predicting the energy, the main difference comes in predicting the SOC.So the RF model would be a better choice for data estimation, as seen from the experiments performed on the data generated from the MATLAB simulations. Regression Algorithm Accuracy (R^2 score) ¬† Mean Absolute Error (MAE) ¬† Root Mean Square Error (RMSE) ¬† ¬† SOC Energy SOC Energy SOC Energy Random Forest 99.9% 99.9% 0.0007 0.0042 0.0008 0.3944 Gradient Boosting Regression Tree 93.5% 99.9% 1.5948 0.1324 1.9202 0.4312 ConclusionThe development of an efficient battery management system using the previously-mentioned methods would help build safer EVs. By making data analysis more practical, the research being conducted would help reduce the gap between the tests in labs and the real-time conditions. This will help prevent any accidents that can be caused by faults in batteries.The data generated from the MATLAB simulations were extracted and the essential parameters served as the input to the two different machine learning algorithms used to predict the SOC and power/energy consumed. Promising results were obtained with more than 90% accuracy in the regression models, and I believe that this can be increased with the help of more data." }, { "title": "Basic Electronics", "url": "/posts/electronics/", "categories": "Blog, Electronics", "tags": "arduino", "date": "2022-07-28 21:30:00 +0530", "snippet": "IntroductionThe following notes were created for Techtainment, an event that I conducted to teach the basics of electronics and programming to people who hadn‚Äôt previously explored the field of robotics.Arduino Uno Tech Specs Operating voltage = 5V Recommended input voltage = 7V-12V Input voltage (limit) = 6-20V DC current per I/O pin = 40mA DC current per 3.3V pin = 50mA Flash memory = 32 kB of which 0.5 kB is used by the bootloader SRAM = 2 kB EEPROM = 1 kB Clock speed = 16 MHz The ATmega328P chip has 28 pins.You can check out the datasheet of the ATmega328P chip if interested.What is the deal with Arduino Uno‚Äôs digital pin 13 LED?Digital pin 13 is harder to use as a digital input than the other digital pins because it has an LED and a resistor attached to it that‚Äôs soldered to the board on most boards. If you enable its internal 20k resistor, it will hang at around 1.7V instead of the expected 5V because the onboard LED and series resistor pull the voltage level down, meaning it always returns a LOW. If you must use digital pin 13 as a digital input, set its pinMode() to INPUT and use an external pull-down resistor.TX/RXOn the Arduino board, pin 0 (RX) and 1 (TX) are used for communicating with the computer. Connecting anything to these pins can interfere with the communication, and can cause failed uploads to the boards.Purpose of TX and RX LEDs:The TX and RX LEDs blink whenever there is a communication between the onboard microcontroller and the computer through the USB to Serial Converter chip present near the USB port.The lighting up of LEDs indicates the direction of flow of data. When a bit of data goes from the Arduino board to the computer, the TX LED glows. When a bit of data goes from the computer to the Arduino, the RX LED glows.When the computer uploads a code to the Arduino, it sends data to the board to set its fuse bits, determines the inputs and outputs to be used and other necessary instructions to make the code work. The Arduino board replies each time data is sent during the upload to tell the comp that it is receiving the instructions. So when the computer uploads code to the board, the TX and RX LEDs flash rapidly, showing the data exchange process. It is so fast that these LEDs seem to be turned on steadily.AREF pinUsed as an analog reference pin for analog inputs.Watch the following video to get a good understanding of why we use the AREF pin: https://youtu.be/qUySekwhXwsDocumentation: analogReference()Crystal OscillatorWhat is an oscillator?In general form, an oscillator is something that creates oscillation, which means that something is moving or swinging back &amp; forth. Now in the context of electrical &amp; electronic engineering oscillations that we talk about is the swinging back &amp; forth of the voltages. An oscillator not only can generate sine waves but it can also generate squarewaves, triangle waves, etc.What are oscillators used for?Oscillators are always used in electrical designs but commonly used for generating radio waves,tone generators,generating counters to keep track of time, and generating clock signals to maintain the speed of the digital processors including computers that we are regularly using.What is the use of a crystal oscillator in Arduino?Arduino crystals are used because it helps when Arduino is dealing with time issues, for example if we build a project that turns a switch A OFF after 15 minutes ON and turn ON switch B, C &amp; D one after another with a 20 minutes step. We can use Arduino software to program Arduino to do that, but how does Arduino calculate the time?The answer is by using a crystal oscillator. The number on the top of the Arduino crystal is 16.000H9H this gives us information about the frequency which is 16,000,000 Hertz or 16 Mhz. This small component can make 16 millions cycles per second.Why does Arduino have a 16 Mhz crystal instead of 32 Mhz or more?The datasheet of the ATmega328P chip says that the maximum frequency should not exceed 20MHz. Hence a clock of 16 Mhz is used.What is the accuracy of the crystal?Arduino crystal has 100ppm accuracy, and these crystals have an error margin of 100 cycles -or+ in each 1 million cycles. This means that the maximum -or+ error in time calculated by Arduino is 30 sec.Blocking CodeWhen a program ‚Äúhalts‚Äù to execute some code, that code that holds up the program is called blocking code. The delay() function is an example of blocking code. This function decreases the tightness of a loop.Blocking code is a generic term given to code that is going to take some time to execute and is going to stop other parts of our program from running while it executes.If you have two repetitive events and those events overlap, then you may find that using the delay function is not the best solution for programming it. We can use an alternative function called millis() instead.The millis() function will be discussed in detail in the upcoming sessions.Does the delay() function pause all activities?Certain things do go on while the delay() function is controlling the ATmega chip, because the delay function does not disable interrupts. Serial communication that appears at the RX pin is recorded, PWM (analogWrite) values and pin states are maintained, and interrupts will work as they should.We will discuss interrupts in detail in the upcoming sessions.LDRLight Dependent Resistor (LDR) is a device whose resistivity varies with the incident EM radiation. Hence they are light-sensitive devices. Also called photoconductors or photoconductive cells. They do not have a polarity.When light is incident on the LDR, electrons from the valence band jump to the conduction band, resulting in a decrease in resistance.We cannot connect an LDR directly to an Arduino pin. This is because on doing so, the current through the LDR changes due to change in its resistance. But the Arduino cannot measure current and it measures only changes in voltage. Therefore in order to use an LDR, we need to connect it in series with another resistor and measure the voltage across that resistor.Ways to Power Up Your Arduino UnoRefer to the following link: Power Scheme MethodsPWM from non-PWM pinsGo through the following simulation: TinkerCADCode explanation:The oscilloscope has a time per division equal to 100ms. The user gives the required duty cycle as input.Let‚Äôs say the duty cycle required is 50%. This means that the signal should stay HIGH for 50% of the time and stay LOW for the remaining 50% of the time. So in one time division, the signal will stay HIGH for 50ms (50% of 100ms = 50ms) and LOW for the remaining 50ms. This can be achieved by keeping a delay equal to the duty cycle, i.e., 50ms in this case.Now let‚Äôs say the duty cycle required is 25%. This means that the signal should stay HIGH for 20% of the time and stay LOW for the remaining 75% of the time. So in one time division, the signal will stay HIGH for 25ms (25% of 100ms = 25ms) and LOW for the remaining 75ms (100ms - 25ms). This can be achieved by keeping a delay equal to the duty cycle, i.e., 25ms in this case, for the HIGH signal, and a delay equal to 75ms (100 - duty cycle) for the LOW signal.So in general, if a particular duty cycle is required, you can give a delay equal to the duty cycle for the HIGH signal, and a delay equal to 100 - duty cycle for the LOW signal.We are using 100 here because the time per division of the oscilloscope is 100ms. This number can be changed depending on the time per division of the oscilloscope.You can also notice how the brightness of the LED changes with change in duty cycle.Servo MotorClosed loop system based on position feedback to control the motion and position of the shaft. The feedback signal is generated by comparing the output signal and reference input signal.The actual position of the shaft is captured by the potentiometer and is fed back to the error detector where it is compared to the target position. The controller tries to minimize this error and corrects the actual position of the motor to match with the target position.Components: DC Motor - high speed, low torque Gearbox - increase torque, reduces rpm Potentiometer - attached to the servo shaft. As the motor rotates, the potentiometer rotates as well. This produces a variable voltage related to the absolute angle of the shaft. Control circuit - potentiometer voltage is compared to the target voltage signal. If needed, it activates an internal H-bridge which enables the motor to rotate in either direction, until the two signals reach a difference of 0.When the shaft of the motor is at the desired position, power supplied to the motor is stopped. If not, the motor is turned in the appropriate direction.The motor‚Äôs speed is proportional to the difference between its actual position and desired position. So if the motor is near the target position, it will turn slowly, otherwise it will turn fast. This is called proportional control.How are Servos controlled?Servos are controlled by sending PWM signals. The PWM sent to the motor determines the position of the shaft, and based on the duration of the pulse sent, the shaft will turn to the desired position.The servo expects to see a pulse every 20ms (frequency=50Hz) and the length of the pulse will determine how far the motor turns, as shown below.When servos are controlled to move, they will move to the position and hold that position. If an external force pushes against the servo while it is holding its position, it will resist from moving out of its position.Torque Ratinghttps://automaticaddison.com/how-to-determine-what-torque-you-need-for-your-servo-motors/HC-SR04Sensor features: Operating voltage = 5V Theoretical measuring distance: 2cm to 450cm Practical measuring distance: 2cm to 80cm Accuracy: 3mm Measuring angle covered: &lt;15 degrees Pulse frequency: 40 kHzWorking:To start measurement, trig pin of the sensor must be given 5V (HIGH) for 10 microseconds. This will initiate the sensor, transmit out 8 cycles of ultrasonic pulse at 40 kHz and wait for the reflected pulse.When the pulse is sent, the echo pin is set from LOW to HIGH automatically. On receiving the reflected signal, the echo pin will go back to LOW and produce a pulse. The length/width of the pulse is proportional to the time it took for the transmitted signal to be detected. The width of the pulse varies between 150us-25ms.If the pulses are not reflected back, the echo signal will timeout after 38ms and return LOW, indicating no presence of an obstacle within the range of the sensor.The trigger of 10us is to allow the firmware in the microcontroller to recognize the input.Why transmit 8 cycles?pulseIn()PIRWorking:The PIR sensor has 2 slots in it, each made up of a special material sensitive to IR. When the sensor is idle, both slots detect the same amount of IR. When a warm body passes by, it first intercepts one half of the sensor, which causes a positive differential change between the two halves. When the warm body leaves the sensing area, the reverse happens,i.e., it generates a negative differential change. These change pulses are what is detected.The BISS0001 decoder chip is used in the PIR sensor. It takes the signal from the sensor and does some minor processing on it to emit a digital output pulse from the analog sensor.UARTThe protocol used by Arduino to communicate with the computer is called UART. UART stands for Universal Asynchronous Receiver Transmitter. It defines a protocol for exchanging serial data between two devices, in this case, between the Arduino and computer.It uses only 2 wires between the transmitter and receiver, TX and RX. The communication can be done is different modes- Simplex - data is sent in one direction only Half duplex - data is sent in both directions but only one at a time Full duplex - data is sent in both directions simultaneouslyUART is Asynchronous, which means that the transmitter and receiver do not share a common clock. The transmitter and receiver must therefore Transmit at the same known speed inorder to have the same bit timing. Most common baud rates used are 4800, 9600, etc. Use the same frame structureFrame structure:In the idle state, when no data is being transmitted, the line is held HIGH. This allows for easy detection of a damaged line/transmitter.The start bit indicates data is coming. It is a transition from the ideal HIGH state to LOW state. User data bits come immediately after the start bit.After the data bits are finished, the stop bits indicate the end of user data.The stop bit is either a transition back to the HIGH or idle state or will remain at the HIGH state for an additional bit time. A second optional stop bit can be configured, usually to give the receiver time to get ready for the next frame, but this is uncommon in practice.Length of data bits: 5-9 bits (usually 7 or 8)Data is typically sent with the least significant bit (LSB) first.An optional parity bit can be used for error detection. It is inserted between the end of the data bits and the stop bit.Even parity: bit is set such that the total no. of 1‚Äôs is evenOdd parity: bit is set such that the total no. of 1‚Äôs is oddIt can be used to detect only a single flipped bit. If more than one bit is flipped, there is no reliable way to detect these using a single parity bit." } ]
